{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfcf5ad-531c-4505-9fb2-48db18b4568e",
   "metadata": {},
   "source": [
    "# Sample code of Image Classification VGG16 Model with PyTorch\n",
    "\n",
    "This notebook is the sample code of training the image classification model using COCO2014 dataset.  \n",
    "COCO2014 dataset has not classification labels, therefore it makes classification dataset cropping bounding boxes.\n",
    "\n",
    "|Item|Description|\n",
    "|---|---|\n",
    "|DeepLearning Framework|PyTorch|\n",
    "|Dataset|COCO2014 Classification|\n",
    "|Model Architecture|VGG16|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2f3e25-58a2-4dae-9166-a85f649bde47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d563f93f-798e-4334-8094-334e84c69097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "#import json\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#from PIL import Image\n",
    "#from data_loader.data_loader import DataLoader\n",
    "#from models.pytorch import vgg16\n",
    "import itertools\n",
    "#\n",
    "#import torch\n",
    "#from torch.utils.data import Dataset\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pprint\n",
    "\n",
    "from data_loader.data_loader import DataLoader\n",
    "from models.pytorch import vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0d189-4ff4-4619-b8e4-91151436612c",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f272e2-e60d-4bf3-aff9-a255ce7e68f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9c74591e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53721ad-cdf2-4570-a63d-34eca78d01bf",
   "metadata": {},
   "source": [
    "## Device Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d22f5cd-18ef-4a30-9d92-ce0c0afffd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca638a-c879-4044-bd94-4181db50a2d2",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfee9194-b6b7-438e-a967-eb25c0f544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.001\n",
    "input_tensor_shape = (3, 224, 224)   # CHW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06247d29-f8d4-4ab5-83bf-830195df2ef8",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac110b-b5d8-4dbd-b982-49b82ee743c6",
   "metadata": {},
   "source": [
    "### Download and Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86cf2bd-366a-4e94-8835-4a01ed16deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 300000/300000 [47:44<00:00, 104.74it/s] \n",
      "100% 84509/84509 [01:03<00:00, 1322.84it/s]\n",
      "100% 80/80 [00:00<00:00, 1071.91it/s]\n",
      "100% 80000/80000 [00:00<00:00, 629665.47it/s]\n",
      "100% 291875/291875 [31:12<00:00, 155.87it/s] \n",
      "100% 56834/56834 [00:44<00:00, 1275.77it/s]\n",
      "100% 80/80 [00:00<00:00, 1299.48it/s]\n",
      "100% 80000/80000 [00:00<00:00, 597749.55it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '/tmp/dataset'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "dataloader = DataLoader(dataset_name='coco2014_classification_pytorch', resize=input_tensor_shape[1:], dataset_dir=dataset_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d834f0-a039-4ae1-b7db-de5b04812f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations\t\t      train2014.zip\t val2014      val2014_clf.csv\n",
      "annotations_trainval2014.zip  train2014_clf\t val2014.zip\n",
      "train2014\t\t      train2014_clf.csv  val2014_clf\n"
     ]
    }
   ],
   "source": [
    "!ls {dataset_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b3fa3a-64fa-4d11-81a0-7a5822b2d430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorcycle',\n",
       " 'airplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'boat',\n",
       " 'traffic light',\n",
       " 'fire hydrant',\n",
       " 'stop sign',\n",
       " 'parking meter',\n",
       " 'bench',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'sheep',\n",
       " 'cow',\n",
       " 'elephant',\n",
       " 'bear',\n",
       " 'zebra',\n",
       " 'giraffe',\n",
       " 'backpack',\n",
       " 'umbrella',\n",
       " 'handbag',\n",
       " 'tie',\n",
       " 'suitcase',\n",
       " 'frisbee',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'sports ball',\n",
       " 'kite',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'skateboard',\n",
       " 'surfboard',\n",
       " 'tennis racket',\n",
       " 'bottle',\n",
       " 'wine glass',\n",
       " 'cup',\n",
       " 'fork',\n",
       " 'knife',\n",
       " 'spoon',\n",
       " 'bowl',\n",
       " 'banana',\n",
       " 'apple',\n",
       " 'sandwich',\n",
       " 'orange',\n",
       " 'broccoli',\n",
       " 'carrot',\n",
       " 'hot dog',\n",
       " 'pizza',\n",
       " 'donut',\n",
       " 'cake',\n",
       " 'chair',\n",
       " 'couch',\n",
       " 'potted plant',\n",
       " 'bed',\n",
       " 'dining table',\n",
       " 'toilet',\n",
       " 'tv',\n",
       " 'laptop',\n",
       " 'mouse',\n",
       " 'remote',\n",
       " 'keyboard',\n",
       " 'cell phone',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " 'toaster',\n",
       " 'sink',\n",
       " 'refrigerator',\n",
       " 'book',\n",
       " 'clock',\n",
       " 'vase',\n",
       " 'scissors',\n",
       " 'teddy bear',\n",
       " 'hair drier',\n",
       " 'toothbrush']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset.class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aa766c6-60fb-437b-9647-119d398e2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = dataloader.dataset.trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e44063-dcd7-473c-a029-2e514d149c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2500/2500 [09:19<00:00,  4.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[69,\n",
       " 72,\n",
       " 69,\n",
       " 32,\n",
       " 78,\n",
       " 3,\n",
       " 9,\n",
       " 72,\n",
       " 71,\n",
       " 61,\n",
       " 1,\n",
       " 62,\n",
       " 73,\n",
       " 30,\n",
       " 63,\n",
       " 72,\n",
       " 87,\n",
       " 26,\n",
       " 36,\n",
       " 6,\n",
       " 50,\n",
       " 10,\n",
       " 74,\n",
       " 27,\n",
       " 41,\n",
       " 35,\n",
       " 17,\n",
       " 22,\n",
       " 62,\n",
       " 88,\n",
       " 77,\n",
       " 84,\n",
       " 31,\n",
       " 71,\n",
       " 27,\n",
       " 22,\n",
       " 55,\n",
       " 17,\n",
       " 60,\n",
       " 69,\n",
       " 87,\n",
       " 61,\n",
       " 8,\n",
       " 84,\n",
       " 86,\n",
       " 32,\n",
       " 1,\n",
       " 77,\n",
       " 23,\n",
       " 39,\n",
       " 16,\n",
       " 48,\n",
       " 14,\n",
       " 0,\n",
       " 80,\n",
       " 0,\n",
       " 12,\n",
       " 72,\n",
       " 60,\n",
       " 61,\n",
       " 73,\n",
       " 27,\n",
       " 38,\n",
       " 50,\n",
       " 24,\n",
       " 30,\n",
       " 26,\n",
       " 80,\n",
       " 57,\n",
       " 35,\n",
       " 85,\n",
       " 85,\n",
       " 75,\n",
       " 59,\n",
       " 41,\n",
       " 61,\n",
       " 45,\n",
       " 78,\n",
       " 48,\n",
       " 52,\n",
       " 3,\n",
       " 6,\n",
       " 80,\n",
       " 60,\n",
       " 0,\n",
       " 69,\n",
       " 46,\n",
       " 20,\n",
       " 72,\n",
       " 40,\n",
       " 27,\n",
       " 78,\n",
       " 45,\n",
       " 23,\n",
       " 88,\n",
       " 76,\n",
       " 89,\n",
       " 59,\n",
       " 88,\n",
       " 45,\n",
       " 73,\n",
       " 10,\n",
       " 75,\n",
       " 54,\n",
       " 33,\n",
       " 55,\n",
       " 88,\n",
       " 87,\n",
       " 41,\n",
       " 64,\n",
       " 47,\n",
       " 1,\n",
       " 35,\n",
       " 51,\n",
       " 57,\n",
       " 89,\n",
       " 41,\n",
       " 18,\n",
       " 75,\n",
       " 75,\n",
       " 74,\n",
       " 52,\n",
       " 32,\n",
       " 18,\n",
       " 30,\n",
       " 6,\n",
       " 87,\n",
       " 49,\n",
       " 62,\n",
       " 49,\n",
       " 55,\n",
       " 22,\n",
       " 86,\n",
       " 61,\n",
       " 45,\n",
       " 43,\n",
       " 9,\n",
       " 21,\n",
       " 14,\n",
       " 62,\n",
       " 38,\n",
       " 22,\n",
       " 72,\n",
       " 89,\n",
       " 10,\n",
       " 30,\n",
       " 87,\n",
       " 75,\n",
       " 79,\n",
       " 37,\n",
       " 50,\n",
       " 46,\n",
       " 7,\n",
       " 16,\n",
       " 42,\n",
       " 73,\n",
       " 81,\n",
       " 81,\n",
       " 76,\n",
       " 45,\n",
       " 18,\n",
       " 13,\n",
       " 3,\n",
       " 41,\n",
       " 84,\n",
       " 73,\n",
       " 60,\n",
       " 87,\n",
       " 0,\n",
       " 41,\n",
       " 69,\n",
       " 73,\n",
       " 39,\n",
       " 80,\n",
       " 60,\n",
       " 72,\n",
       " 47,\n",
       " 84,\n",
       " 36,\n",
       " 18,\n",
       " 63,\n",
       " 4,\n",
       " 76,\n",
       " 30,\n",
       " 59,\n",
       " 51,\n",
       " 41,\n",
       " 23,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 36,\n",
       " 56,\n",
       " 56,\n",
       " 10,\n",
       " 4,\n",
       " 46,\n",
       " 20,\n",
       " 23,\n",
       " 60,\n",
       " 79,\n",
       " 1,\n",
       " 40,\n",
       " 55,\n",
       " 79,\n",
       " 27,\n",
       " 27,\n",
       " 85,\n",
       " 43,\n",
       " 48,\n",
       " 50,\n",
       " 16,\n",
       " 16,\n",
       " 85,\n",
       " 85,\n",
       " 14,\n",
       " 86,\n",
       " 16,\n",
       " 26,\n",
       " 55,\n",
       " 32,\n",
       " 16,\n",
       " 7,\n",
       " 49,\n",
       " 24,\n",
       " 10,\n",
       " 87,\n",
       " 35,\n",
       " 63,\n",
       " 24,\n",
       " 3,\n",
       " 26,\n",
       " 35,\n",
       " 48,\n",
       " 32,\n",
       " 16,\n",
       " 5,\n",
       " 39,\n",
       " 50,\n",
       " 55,\n",
       " 34,\n",
       " 74,\n",
       " 10,\n",
       " 34,\n",
       " 41,\n",
       " 0,\n",
       " 20,\n",
       " 78,\n",
       " 13,\n",
       " 74,\n",
       " 62,\n",
       " 84,\n",
       " 33,\n",
       " 38,\n",
       " 48,\n",
       " 60,\n",
       " 22,\n",
       " 85,\n",
       " 87,\n",
       " 63,\n",
       " 53,\n",
       " 71,\n",
       " 75,\n",
       " 81,\n",
       " 37,\n",
       " 72,\n",
       " 63,\n",
       " 73,\n",
       " 58,\n",
       " 40,\n",
       " 81,\n",
       " 32,\n",
       " 73,\n",
       " 62,\n",
       " 66,\n",
       " 40,\n",
       " 13,\n",
       " 8,\n",
       " 81,\n",
       " 80,\n",
       " 50,\n",
       " 69,\n",
       " 47,\n",
       " 21,\n",
       " 40,\n",
       " 5,\n",
       " 59,\n",
       " 57,\n",
       " 2,\n",
       " 37,\n",
       " 43,\n",
       " 51,\n",
       " 34,\n",
       " 42,\n",
       " 15,\n",
       " 56,\n",
       " 40,\n",
       " 50,\n",
       " 71,\n",
       " 89,\n",
       " 50,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 63,\n",
       " 51,\n",
       " 50,\n",
       " 34,\n",
       " 13,\n",
       " 51,\n",
       " 30,\n",
       " 42,\n",
       " 12,\n",
       " 36,\n",
       " 42,\n",
       " 19,\n",
       " 26,\n",
       " 88,\n",
       " 31,\n",
       " 80,\n",
       " 21,\n",
       " 69,\n",
       " 79,\n",
       " 30,\n",
       " 75,\n",
       " 27,\n",
       " 39,\n",
       " 73,\n",
       " 6,\n",
       " 87,\n",
       " 78,\n",
       " 8,\n",
       " 64,\n",
       " 10,\n",
       " 46,\n",
       " 57,\n",
       " 46,\n",
       " 53,\n",
       " 15,\n",
       " 21,\n",
       " 33,\n",
       " 86,\n",
       " 23,\n",
       " 89,\n",
       " 48,\n",
       " 19,\n",
       " 53,\n",
       " 73,\n",
       " 57,\n",
       " 8,\n",
       " 18,\n",
       " 40,\n",
       " 53,\n",
       " 45,\n",
       " 32,\n",
       " 71,\n",
       " 63,\n",
       " 6,\n",
       " 74,\n",
       " 42,\n",
       " 9,\n",
       " 83,\n",
       " 24,\n",
       " 22,\n",
       " 17,\n",
       " 1,\n",
       " 31,\n",
       " 10,\n",
       " 7,\n",
       " 5,\n",
       " 32,\n",
       " 2,\n",
       " 57,\n",
       " 21,\n",
       " 71,\n",
       " 89,\n",
       " 7,\n",
       " 51,\n",
       " 6,\n",
       " 62,\n",
       " 64,\n",
       " 37,\n",
       " 79,\n",
       " 14,\n",
       " 23,\n",
       " 61,\n",
       " 17,\n",
       " 7,\n",
       " 86,\n",
       " 79,\n",
       " 60,\n",
       " 61,\n",
       " 85,\n",
       " 64,\n",
       " 74,\n",
       " 49,\n",
       " 48,\n",
       " 10,\n",
       " 66,\n",
       " 80,\n",
       " 33,\n",
       " 15,\n",
       " 83,\n",
       " 14,\n",
       " 33,\n",
       " 3,\n",
       " 76,\n",
       " 16,\n",
       " 75,\n",
       " 64,\n",
       " 14,\n",
       " 47,\n",
       " 33,\n",
       " 8,\n",
       " 57,\n",
       " 43,\n",
       " 75,\n",
       " 81,\n",
       " 7,\n",
       " 24,\n",
       " 62,\n",
       " 79,\n",
       " 58,\n",
       " 27,\n",
       " 16,\n",
       " 50,\n",
       " 48,\n",
       " 38,\n",
       " 32,\n",
       " 40,\n",
       " 5,\n",
       " 32,\n",
       " 26,\n",
       " 41,\n",
       " 27,\n",
       " 63,\n",
       " 76,\n",
       " 26,\n",
       " 66,\n",
       " 74,\n",
       " 63,\n",
       " 3,\n",
       " 6,\n",
       " 61,\n",
       " 61,\n",
       " 45,\n",
       " 36,\n",
       " 40,\n",
       " 62,\n",
       " 6,\n",
       " 35,\n",
       " 27,\n",
       " 1,\n",
       " 30,\n",
       " 35,\n",
       " 83,\n",
       " 64,\n",
       " 5,\n",
       " 20,\n",
       " 49,\n",
       " 52,\n",
       " 60,\n",
       " 20,\n",
       " 32,\n",
       " 47,\n",
       " 55,\n",
       " 0,\n",
       " 32,\n",
       " 40,\n",
       " 27,\n",
       " 81,\n",
       " 74,\n",
       " 6,\n",
       " 8,\n",
       " 85,\n",
       " 14,\n",
       " 12,\n",
       " 60,\n",
       " 27,\n",
       " 21,\n",
       " 86,\n",
       " 23,\n",
       " 13,\n",
       " 53,\n",
       " 2,\n",
       " 26,\n",
       " 19,\n",
       " 14,\n",
       " 55,\n",
       " 45,\n",
       " 37,\n",
       " 21,\n",
       " 48,\n",
       " 59,\n",
       " 19,\n",
       " 10,\n",
       " 63,\n",
       " 77,\n",
       " 19,\n",
       " 74,\n",
       " 51,\n",
       " 56,\n",
       " 40,\n",
       " 32,\n",
       " 88,\n",
       " 35,\n",
       " 21,\n",
       " 86,\n",
       " 34,\n",
       " 30,\n",
       " 2,\n",
       " 7,\n",
       " 16,\n",
       " 6,\n",
       " 27,\n",
       " 1,\n",
       " 26,\n",
       " 31,\n",
       " 77,\n",
       " 55,\n",
       " 34,\n",
       " 54,\n",
       " 14,\n",
       " 32,\n",
       " 77,\n",
       " 51,\n",
       " 1,\n",
       " 79,\n",
       " 37,\n",
       " 74,\n",
       " 78,\n",
       " 8,\n",
       " 32,\n",
       " 66,\n",
       " 51,\n",
       " 49,\n",
       " 13,\n",
       " 27,\n",
       " 27,\n",
       " 47,\n",
       " 84,\n",
       " 36,\n",
       " 49,\n",
       " 5,\n",
       " 57,\n",
       " 2,\n",
       " 43,\n",
       " 15,\n",
       " 33,\n",
       " 66,\n",
       " 2,\n",
       " 64,\n",
       " 85,\n",
       " 51,\n",
       " 85,\n",
       " 73,\n",
       " 74,\n",
       " 6,\n",
       " 26,\n",
       " 63,\n",
       " 86,\n",
       " 13,\n",
       " 86,\n",
       " 85,\n",
       " 21,\n",
       " 61,\n",
       " 24,\n",
       " 53,\n",
       " 39,\n",
       " 13,\n",
       " 37,\n",
       " 33,\n",
       " 79,\n",
       " 87,\n",
       " 4,\n",
       " 77,\n",
       " 41,\n",
       " 46,\n",
       " 16,\n",
       " 61,\n",
       " 20,\n",
       " 41,\n",
       " 88,\n",
       " 24,\n",
       " 51,\n",
       " 1,\n",
       " 13,\n",
       " 46,\n",
       " 52,\n",
       " 5,\n",
       " 5,\n",
       " 17,\n",
       " 38,\n",
       " 59,\n",
       " 49,\n",
       " 41,\n",
       " 86,\n",
       " 51,\n",
       " 52,\n",
       " 33,\n",
       " 51,\n",
       " 51,\n",
       " 46,\n",
       " 1,\n",
       " 24,\n",
       " 47,\n",
       " 35,\n",
       " 83,\n",
       " 66,\n",
       " 7,\n",
       " 42,\n",
       " 62,\n",
       " 19,\n",
       " 64,\n",
       " 79,\n",
       " 52,\n",
       " 56,\n",
       " 45,\n",
       " 20,\n",
       " 7,\n",
       " 63,\n",
       " 77,\n",
       " 7,\n",
       " 33,\n",
       " 17,\n",
       " 32,\n",
       " 79,\n",
       " 5,\n",
       " 26,\n",
       " 48,\n",
       " 10,\n",
       " 50,\n",
       " 36,\n",
       " 56,\n",
       " 81,\n",
       " 4,\n",
       " 19,\n",
       " 45,\n",
       " 83,\n",
       " 9,\n",
       " 71,\n",
       " 61,\n",
       " 71,\n",
       " 77,\n",
       " 45,\n",
       " 23,\n",
       " 10,\n",
       " 56,\n",
       " 36,\n",
       " 13,\n",
       " 89,\n",
       " 57,\n",
       " 4,\n",
       " 47,\n",
       " 50,\n",
       " 52,\n",
       " 53,\n",
       " 85,\n",
       " 63,\n",
       " 14,\n",
       " 72,\n",
       " 72,\n",
       " 75,\n",
       " 27,\n",
       " 9,\n",
       " 76,\n",
       " 88,\n",
       " 10,\n",
       " 1,\n",
       " 21,\n",
       " 58,\n",
       " 52,\n",
       " 76,\n",
       " 74,\n",
       " 8,\n",
       " 8,\n",
       " 81,\n",
       " 59,\n",
       " 2,\n",
       " 26,\n",
       " 81,\n",
       " 60,\n",
       " 9,\n",
       " 62,\n",
       " 55,\n",
       " 5,\n",
       " 52,\n",
       " 50,\n",
       " 56,\n",
       " 75,\n",
       " 24,\n",
       " 43,\n",
       " 23,\n",
       " 73,\n",
       " 33,\n",
       " 80,\n",
       " 73,\n",
       " 86,\n",
       " 88,\n",
       " 71,\n",
       " 74,\n",
       " 50,\n",
       " 26,\n",
       " 42,\n",
       " 21,\n",
       " 21,\n",
       " 13,\n",
       " 19,\n",
       " 30,\n",
       " 20,\n",
       " 20,\n",
       " 50,\n",
       " 1,\n",
       " 55,\n",
       " 80,\n",
       " 85,\n",
       " 7,\n",
       " 47,\n",
       " 81,\n",
       " 56,\n",
       " 76,\n",
       " 64,\n",
       " 79,\n",
       " 48,\n",
       " 43,\n",
       " 3,\n",
       " 77,\n",
       " 71,\n",
       " 16,\n",
       " 42,\n",
       " 46,\n",
       " 71,\n",
       " 5,\n",
       " 13,\n",
       " 16,\n",
       " 45,\n",
       " 76,\n",
       " 13,\n",
       " 58,\n",
       " 77,\n",
       " 6,\n",
       " 23,\n",
       " 69,\n",
       " 73,\n",
       " 33,\n",
       " 80,\n",
       " 35,\n",
       " 7,\n",
       " 3,\n",
       " 12,\n",
       " 12,\n",
       " 52,\n",
       " 79,\n",
       " 62,\n",
       " 10,\n",
       " 55,\n",
       " 56,\n",
       " 80,\n",
       " 49,\n",
       " 86,\n",
       " 84,\n",
       " 79,\n",
       " 33,\n",
       " 50,\n",
       " 7,\n",
       " 39,\n",
       " 58,\n",
       " 7,\n",
       " 86,\n",
       " 53,\n",
       " 0,\n",
       " 66,\n",
       " 2,\n",
       " 76,\n",
       " 3,\n",
       " 89,\n",
       " 74,\n",
       " 81,\n",
       " 41,\n",
       " 13,\n",
       " 71,\n",
       " 0,\n",
       " 10,\n",
       " 62,\n",
       " 55,\n",
       " 62,\n",
       " 89,\n",
       " 30,\n",
       " 20,\n",
       " 80,\n",
       " 55,\n",
       " 87,\n",
       " 47,\n",
       " 8,\n",
       " 71,\n",
       " 1,\n",
       " 38,\n",
       " 88,\n",
       " 85,\n",
       " 64,\n",
       " 50,\n",
       " 20,\n",
       " 46,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 32,\n",
       " 86,\n",
       " 52,\n",
       " 9,\n",
       " 59,\n",
       " 30,\n",
       " 78,\n",
       " 75,\n",
       " 76,\n",
       " 15,\n",
       " 17,\n",
       " 76,\n",
       " 5,\n",
       " 1,\n",
       " 26,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 54,\n",
       " 69,\n",
       " 69,\n",
       " 52,\n",
       " 56,\n",
       " 71,\n",
       " 84,\n",
       " 76,\n",
       " 8,\n",
       " 12,\n",
       " 81,\n",
       " 2,\n",
       " 47,\n",
       " 8,\n",
       " 87,\n",
       " 23,\n",
       " 27,\n",
       " 27,\n",
       " 66,\n",
       " 85,\n",
       " 5,\n",
       " 69,\n",
       " 18,\n",
       " 54,\n",
       " 20,\n",
       " 40,\n",
       " 3,\n",
       " 20,\n",
       " 50,\n",
       " 38,\n",
       " 36,\n",
       " 18,\n",
       " 69,\n",
       " 4,\n",
       " 22,\n",
       " 9,\n",
       " 16,\n",
       " 48,\n",
       " 38,\n",
       " 13,\n",
       " 38,\n",
       " 47,\n",
       " 57,\n",
       " 76,\n",
       " 46,\n",
       " 17,\n",
       " 61,\n",
       " 43,\n",
       " 12,\n",
       " 64,\n",
       " 3,\n",
       " 73,\n",
       " 6,\n",
       " 17,\n",
       " 76,\n",
       " 8,\n",
       " 5,\n",
       " 69,\n",
       " 8,\n",
       " 88,\n",
       " 7,\n",
       " 58,\n",
       " 79,\n",
       " 85,\n",
       " 21,\n",
       " 50,\n",
       " 33,\n",
       " 27,\n",
       " 72,\n",
       " 35,\n",
       " 41,\n",
       " 86,\n",
       " 72,\n",
       " 12,\n",
       " 42,\n",
       " 63,\n",
       " 81,\n",
       " 48,\n",
       " 13,\n",
       " 38,\n",
       " 77,\n",
       " 34,\n",
       " 12,\n",
       " 83,\n",
       " 31,\n",
       " 39,\n",
       " 32,\n",
       " 78,\n",
       " 59,\n",
       " 78,\n",
       " 75,\n",
       " 9,\n",
       " 76,\n",
       " 34,\n",
       " 89,\n",
       " 78,\n",
       " 13,\n",
       " 83,\n",
       " 45,\n",
       " 16,\n",
       " 62,\n",
       " 18,\n",
       " 20,\n",
       " 78,\n",
       " 8,\n",
       " 15,\n",
       " 8,\n",
       " 38,\n",
       " 53,\n",
       " 3,\n",
       " 72,\n",
       " 46,\n",
       " 74,\n",
       " 59,\n",
       " 59,\n",
       " 56,\n",
       " 62,\n",
       " 59,\n",
       " 1,\n",
       " 80,\n",
       " 30,\n",
       " 46,\n",
       " 41,\n",
       " 35,\n",
       " 50,\n",
       " 84,\n",
       " 81,\n",
       " 0,\n",
       " 88,\n",
       " 52,\n",
       " 47,\n",
       " 1,\n",
       " 9,\n",
       " 41,\n",
       " 1,\n",
       " 84,\n",
       " 62,\n",
       " 47,\n",
       " 37,\n",
       " 33,\n",
       " 4,\n",
       " 80,\n",
       " 71,\n",
       " 80,\n",
       " 17,\n",
       " 48,\n",
       " 36,\n",
       " 36,\n",
       " 45,\n",
       " 61,\n",
       " 26,\n",
       " 16,\n",
       " 7,\n",
       " 32,\n",
       " 33,\n",
       " 19,\n",
       " 16,\n",
       " 72,\n",
       " 38,\n",
       " 52,\n",
       " 33,\n",
       " 9,\n",
       " 77,\n",
       " 63,\n",
       " 30,\n",
       " 43,\n",
       " 85,\n",
       " 85,\n",
       " 62,\n",
       " 20,\n",
       " 74,\n",
       " 3,\n",
       " 24,\n",
       " 18,\n",
       " 39,\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = []\n",
    "i = 0\n",
    "for inputs, labels in tqdm(trainloader):\n",
    "    train_labels += labels.tolist()\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7897b529-e6f2-48fe-9540-9ff7c7bb9424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1f58e3e-864f-4cd1-ae3e-df466a4cbde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1000\n",
       "1     1000\n",
       "63    1000\n",
       "62    1000\n",
       "61    1000\n",
       "      ... \n",
       "27    1000\n",
       "26    1000\n",
       "24    1000\n",
       "23    1000\n",
       "89    1000\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bccf373-81f5-410f-a26f-30ec64268064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGdCAYAAADzOWwgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm2ElEQVR4nO3deXTV9Z3/8VcWbhIwC4tJyBAgoygguEAUU9COQ4YosYetPaUGoZhK1VAJqbLMaKwLBoKkbJVIW7ZTkOUMtggFTIPCWCPBIKsYaEWDJjehA8kFNAu5398fHL4/rnGsXJJ8A5/n45x7jvl+P/nmffkG8vR7lwRYlmUJAADAYIFODwAAAOA0gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8YKdHuBq4PV6VV5ervDwcAUEBDg9DgAA+A4sy9KZM2cUFxenwMBvvwZEEH0H5eXlio+Pd3oMAADghxMnTqhbt27fuoYg+g7Cw8MlXfgDjYiIcHgaAADwXXg8HsXHx9s/x78NQfQdXHyYLCIigiACAOAq812e7sKTqgEAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYLxgpweA1HPGFqdHuGyfzk51egSg2VyNfwevVlfjvx1X4/fH1fjn7DSuEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4jgZRY2Ojnn32WSUkJCgsLEw33HCDXnzxRVmWZa+xLEvZ2dnq2rWrwsLClJycrGPHjvkc59SpU0pLS1NERISioqKUnp6us2fP+qw5cOCA7rnnHoWGhio+Pl65ubmtch8BAEDb52gQzZkzR0uWLNHixYt15MgRzZkzR7m5uVq0aJG9Jjc3VwsXLlR+fr52796tDh06KCUlRbW1tfaatLQ0HT58WAUFBdq8ebN27dqlSZMm2fs9Ho+GDRumHj16qKSkRHPnztWvfvUrLV26tFXvLwAAaJuCnfzi7733nkaMGKHU1FRJUs+ePfX666+ruLhY0oWrQ/Pnz9czzzyjESNGSJJWrVqlmJgY/fGPf9TYsWN15MgRbdu2TXv27FFiYqIkadGiRRo+fLheeeUVxcXFafXq1aqvr9eyZcvkcrl0yy23aN++fcrLy/MJJwAAYCZHrxB973vfU2FhoY4ePSpJ2r9/v95991098MADkqTjx4/L7XYrOTnZ/pzIyEgNGjRIRUVFkqSioiJFRUXZMSRJycnJCgwM1O7du+019957r1wul70mJSVFpaWlOn36dJO56urq5PF4fG4AAODa5egVohkzZsjj8ah3794KCgpSY2OjZs2apbS0NEmS2+2WJMXExPh8XkxMjL3P7XYrOjraZ39wcLA6derksyYhIaHJMS7u69ixo8++nJwcPf/88810L69NPWdscXoEI3w6O9XpES4b3xuA867Gv4dO/3vn6BWi9evXa/Xq1VqzZo327t2rlStX6pVXXtHKlSudHEszZ85UTU2NfTtx4oSj8wAAgJbl6BWip59+WjNmzNDYsWMlSf3799dnn32mnJwcTZgwQbGxsZKkyspKde3a1f68yspK3X777ZKk2NhYVVVV+Rz3/PnzOnXqlP35sbGxqqys9Flz8eOLay4VEhKikJCQ5rmTAACgzXP0CtGXX36pwEDfEYKCguT1eiVJCQkJio2NVWFhob3f4/Fo9+7dSkpKkiQlJSWpurpaJSUl9podO3bI6/Vq0KBB9ppdu3apoaHBXlNQUKCbb765ycNlAADAPI4G0Q9+8APNmjVLW7Zs0aeffqo33nhDeXl5GjVqlCQpICBAmZmZeumll7Rp0yYdPHhQ48ePV1xcnEaOHClJ6tOnj+6//349+uijKi4u1l//+ldNnjxZY8eOVVxcnCTpoYceksvlUnp6ug4fPqx169ZpwYIFysrKcuquAwCANsTRh8wWLVqkZ599Vk888YSqqqoUFxenn//858rOzrbXTJs2TefOndOkSZNUXV2tIUOGaNu2bQoNDbXXrF69WpMnT9bQoUMVGBioMWPGaOHChfb+yMhIvfXWW8rIyNDAgQPVpUsXZWdn85J7AAAgSQqwLn1baHwjj8ejyMhI1dTUKCIiotmPfzW+GgCtw+lXXfiD72d8G76n8X9pie+Ny/n5ze8yAwAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYLdnoAAP+3njO2OD0CABiBK0QAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeI4H0RdffKFx48apc+fOCgsLU//+/fXBBx/Y+y3LUnZ2trp27aqwsDAlJyfr2LFjPsc4deqU0tLSFBERoaioKKWnp+vs2bM+aw4cOKB77rlHoaGhio+PV25ubqvcPwAA0PY5GkSnT5/W4MGD1a5dO23dulUfffSR5s2bp44dO9prcnNztXDhQuXn52v37t3q0KGDUlJSVFtba69JS0vT4cOHVVBQoM2bN2vXrl2aNGmSvd/j8WjYsGHq0aOHSkpKNHfuXP3qV7/S0qVLW/X+AgCAtinYyS8+Z84cxcfHa/ny5fa2hIQE+78ty9L8+fP1zDPPaMSIEZKkVatWKSYmRn/84x81duxYHTlyRNu2bdOePXuUmJgoSVq0aJGGDx+uV155RXFxcVq9erXq6+u1bNkyuVwu3XLLLdq3b5/y8vJ8wgkAAJjJ0StEmzZtUmJion70ox8pOjpad9xxh37729/a+48fPy63263k5GR7W2RkpAYNGqSioiJJUlFRkaKiouwYkqTk5GQFBgZq9+7d9pp7771XLpfLXpOSkqLS0lKdPn26pe8mAABo4xwNok8++URLlixRr169tH37dj3++ON68skntXLlSkmS2+2WJMXExPh8XkxMjL3P7XYrOjraZ39wcLA6derks+abjnHp17hUXV2dPB6Pzw0AAFy7HH3IzOv1KjExUS+//LIk6Y477tChQ4eUn5+vCRMmODZXTk6Onn/+ece+PgAAaF2OXiHq2rWr+vbt67OtT58+KisrkyTFxsZKkiorK33WVFZW2vtiY2NVVVXls//8+fM6deqUz5pvOsalX+NSM2fOVE1NjX07ceKEv3cRAABcBRwNosGDB6u0tNRn29GjR9WjRw9JF55gHRsbq8LCQnu/x+PR7t27lZSUJElKSkpSdXW1SkpK7DU7duyQ1+vVoEGD7DW7du1SQ0ODvaagoEA333yzzyvaLgoJCVFERITPDQAAXLscDaKpU6fq/fff18svv6y//e1vWrNmjZYuXaqMjAxJUkBAgDIzM/XSSy9p06ZNOnjwoMaPH6+4uDiNHDlS0oUrSvfff78effRRFRcX669//asmT56ssWPHKi4uTpL00EMPyeVyKT09XYcPH9a6deu0YMECZWVlOXXXAQBAG+Loc4juvPNOvfHGG5o5c6ZeeOEFJSQkaP78+UpLS7PXTJs2TefOndOkSZNUXV2tIUOGaNu2bQoNDbXXrF69WpMnT9bQoUMVGBioMWPGaOHChfb+yMhIvfXWW8rIyNDAgQPVpUsXZWdn85J7AAAgSQqwLMtyeoi2zuPxKDIyUjU1NS3y8FnPGVua/ZgA0BZ9OjvV6REuG/9Gt46W+N64nJ/fjv/qDgAAAKcRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADCeX0H0ySefNPccAAAAjvEriG688Ubdd999+sMf/qDa2trmngkAAKBV+RVEe/fu1a233qqsrCzFxsbq5z//uYqLi5t7NgAAgFbhVxDdfvvtWrBggcrLy7Vs2TJVVFRoyJAh6tevn/Ly8nTy5MnmnhMAAKDFXNGTqoODgzV69Ght2LBBc+bM0d/+9jc99dRTio+P1/jx41VRUdFccwIAALSYKwqiDz74QE888YS6du2qvLw8PfXUU/r73/+ugoIClZeXa8SIEc01JwAAQIsJ9ueT8vLytHz5cpWWlmr48OFatWqVhg8frsDAC32VkJCgFStWqGfPns05KwAAQIvwK4iWLFmiRx55RD/96U/VtWvXb1wTHR2t3//+91c0HAAAQGvwK4iOHTv2T9e4XC5NmDDBn8MDAAC0Kr+eQ7R8+XJt2LChyfYNGzZo5cqVVzwUAABAa/IriHJyctSlS5cm26Ojo/Xyyy9f8VAAAACtya8gKisrU0JCQpPtPXr0UFlZ2RUPBQAA0Jr8CqLo6GgdOHCgyfb9+/erc+fOVzwUAABAa/IriH7yk5/oySef1Ntvv63GxkY1NjZqx44dmjJlisaOHdvcMwIAALQov15l9uKLL+rTTz/V0KFDFRx84RBer1fjx4/nOUQAAOCq41cQuVwurVu3Ti+++KL279+vsLAw9e/fXz169Gju+QAAAFqcX0F00U033aSbbrqpuWYBAABwhF9B1NjYqBUrVqiwsFBVVVXyer0++3fs2NEswwEAALQGv4JoypQpWrFihVJTU9WvXz8FBAQ091wAAACtxq8gWrt2rdavX6/hw4c39zwAAACtzq+X3btcLt14443NPQsAAIAj/AqiX/7yl1qwYIEsy2rueQAAAFqdXw+Zvfvuu3r77be1detW3XLLLWrXrp3P/o0bNzbLcACAa0vPGVucHgH4Rn4FUVRUlEaNGtXcswAAADjCryBavnx5c88BAADgGL+eQyRJ58+f11/+8he99tprOnPmjCSpvLxcZ8+ebbbhAAAAWoNfV4g+++wz3X///SorK1NdXZ3+4z/+Q+Hh4ZozZ47q6uqUn5/f3HMCAAC0GL+uEE2ZMkWJiYk6ffq0wsLC7O2jRo1SYWFhsw0HAADQGvy6QvQ///M/eu+99+RyuXy29+zZU1988UWzDAYAANBa/LpC5PV61djY2GT7559/rvDw8CseCgAAoDX5FUTDhg3T/Pnz7Y8DAgJ09uxZPffcc/w6DwAAcNXx6yGzefPmKSUlRX379lVtba0eeughHTt2TF26dNHrr7/e3DMCAAC0KL+CqFu3btq/f7/Wrl2rAwcO6OzZs0pPT1daWprPk6wBAACuBn4FkSQFBwdr3LhxzTkLAACAI/wKolWrVn3r/vHjx/s1DAAAgBP8CqIpU6b4fNzQ0KAvv/xSLpdL7du3J4gAAMBVxa9XmZ0+fdrndvbsWZWWlmrIkCE8qRoAAFx1/P5dZl/Xq1cvzZ49u8nVIwAAgLau2YJIuvBE6/Ly8uY8JAAAQIvz6zlEmzZt8vnYsixVVFRo8eLFGjx4cLMMBgAA0Fr8CqKRI0f6fBwQEKDrr79e//7v/6558+Y1x1wAAACtxq8g8nq9zT0HAACAY5r1OUQAAABXI7+uEGVlZX3ntXl5ef58CQAAgFbjVxB9+OGH+vDDD9XQ0KCbb75ZknT06FEFBQVpwIAB9rqAgIDmmRIAAKAF+RVEP/jBDxQeHq6VK1eqY8eOki68WePEiRN1zz336Je//GWzDgkAANCS/HoO0bx585STk2PHkCR17NhRL730Eq8yAwAAVx2/gsjj8ejkyZNNtp88eVJnzpy54qEAAABak19BNGrUKE2cOFEbN27U559/rs8//1z//d//rfT0dI0ePbq5ZwQAAGhRfj2HKD8/X0899ZQeeughNTQ0XDhQcLDS09M1d+7cZh0QAACgpfkVRO3bt9err76quXPn6u9//7sk6YYbblCHDh2adTgAAIDWcEVvzFhRUaGKigr16tVLHTp0kGVZzTUXAABAq/EriP73f/9XQ4cO1U033aThw4eroqJCkpSens5L7gEAwFXHryCaOnWq2rVrp7KyMrVv397e/uMf/1jbtm1rtuEAAABag1/PIXrrrbe0fft2devWzWd7r1699NlnnzXLYAAAAK3FrytE586d87kydNGpU6cUEhJyxUMBAAC0Jr+C6J577tGqVavsjwMCAuT1epWbm6v77ruv2YYDAABoDX4FUW5urpYuXaoHHnhA9fX1mjZtmvr166ddu3Zpzpw5fg0ye/ZsBQQEKDMz095WW1urjIwMde7cWdddd53GjBmjyspKn88rKytTamqq2rdvr+joaD399NM6f/68z5p33nlHAwYMUEhIiG688UatWLHCrxkBAMC1ya8g6tevn44ePaohQ4ZoxIgROnfunEaPHq0PP/xQN9xww2Ufb8+ePXrttdd06623+myfOnWq3nzzTW3YsEE7d+5UeXm5zzthNzY2KjU1VfX19Xrvvfe0cuVKrVixQtnZ2faa48ePKzU1Vffdd5/27dunzMxM/exnP9P27dv9uesAAOAaFGBd5psHNTQ06P7771d+fr569ep1xQOcPXtWAwYM0KuvvqqXXnpJt99+u+bPn6+amhpdf/31WrNmjX74wx9Kkj7++GP16dNHRUVFuvvuu7V161Y9+OCDKi8vV0xMjKQL76I9ffp0nTx5Ui6XS9OnT9eWLVt06NAh+2uOHTtW1dXV3/kVcR6PR5GRkaqpqVFERMQV3+ev6zljS7MfEwCAq8mns1Ob/ZiX8/P7sq8QtWvXTgcOHPB7uK/LyMhQamqqkpOTfbaXlJSooaHBZ3vv3r3VvXt3FRUVSZKKiorUv39/O4YkKSUlRR6PR4cPH7bXfP3YKSkp9jG+SV1dnTwej88NAABcu/x6yGzcuHH6/e9/f8VffO3atdq7d69ycnKa7HO73XK5XIqKivLZHhMTI7fbba+5NIYu7r+479vWeDweffXVV984V05OjiIjI+1bfHy8X/cPAABcHfx6H6Lz589r2bJl+stf/qKBAwc2+R1meXl5//QYJ06c0JQpU1RQUKDQ0FB/xmgxM2fOVFZWlv2xx+MhigAAuIZdVhB98skn6tmzpw4dOqQBAwZIko4ePeqzJiAg4Dsdq6SkRFVVVfZxpAtPkt61a5cWL16s7du3q76+XtXV1T5XiSorKxUbGytJio2NVXFxsc9xL74K7dI1X39lWmVlpSIiIhQWFvaNs4WEhPB+SgAAGOSygqhXr16qqKjQ22+/LenCr+pYuHBhk4ekvouhQ4fq4MGDPtsmTpyo3r17a/r06YqPj1e7du1UWFioMWPGSJJKS0tVVlampKQkSVJSUpJmzZqlqqoqRUdHS5IKCgoUERGhvn372mv+/Oc/+3ydgoIC+xgAAACXFURff0Ha1q1bde7cOb++cHh4uPr16+ezrUOHDurcubO9PT09XVlZWerUqZMiIiL0i1/8QklJSbr77rslScOGDVPfvn318MMPKzc3V263W88884wyMjLsKzyPPfaYFi9erGnTpumRRx7Rjh07tH79em3Zwiu7AADABX49h+iiy3zF/mX79a9/rcDAQI0ZM0Z1dXVKSUnRq6++au8PCgrS5s2b9fjjjyspKUkdOnTQhAkT9MILL9hrEhIStGXLFk2dOlULFixQt27d9Lvf/U4pKSktOjsAALh6XNb7EAUFBcntduv666+XdOEqz4EDB5SQkNBiA7YFvA8RAAAty+n3Ibrsh8x++tOf2g9H1dbW6rHHHmvyKrONGzde5sgAAADOuawgmjBhgs/H48aNa9ZhAAAAnHBZQbR8+fKWmgMAAMAxfr1TNQAAwLWEIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAYjyACAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDxHgygnJ0d33nmnwsPDFR0drZEjR6q0tNRnTW1trTIyMtS5c2ddd911GjNmjCorK33WlJWVKTU1Ve3bt1d0dLSefvppnT9/3mfNO++8owEDBigkJEQ33nijVqxY0dJ3DwAAXCUcDaKdO3cqIyND77//vgoKCtTQ0KBhw4bp3Llz9pqpU6fqzTff1IYNG7Rz506Vl5dr9OjR9v7Gxkalpqaqvr5e7733nlauXKkVK1YoOzvbXnP8+HGlpqbqvvvu0759+5SZmamf/exn2r59e6veXwAA0DYFWJZlOT3ERSdPnlR0dLR27type++9VzU1Nbr++uu1Zs0a/fCHP5Qkffzxx+rTp4+Kiop09913a+vWrXrwwQdVXl6umJgYSVJ+fr6mT5+ukydPyuVyafr06dqyZYsOHTpkf62xY8equrpa27Zt+6dzeTweRUZGqqamRhEREc1+v3vO2NLsxwQA4Gry6ezUZj/m5fz8blPPIaqpqZEkderUSZJUUlKihoYGJScn22t69+6t7t27q6ioSJJUVFSk/v372zEkSSkpKfJ4PDp8+LC95tJjXFxz8RhfV1dXJ4/H43MDAADXrjYTRF6vV5mZmRo8eLD69esnSXK73XK5XIqKivJZGxMTI7fbba+5NIYu7r+479vWeDweffXVV01mycnJUWRkpH2Lj49vlvsIAADapjYTRBkZGTp06JDWrl3r9CiaOXOmampq7NuJEyecHgkAALSgYKcHkKTJkydr8+bN2rVrl7p162Zvj42NVX19vaqrq32uElVWVio2NtZeU1xc7HO8i69Cu3TN11+ZVllZqYiICIWFhTWZJyQkRCEhIc1y3wAAQNvn6BUiy7I0efJkvfHGG9qxY4cSEhJ89g8cOFDt2rVTYWGhva20tFRlZWVKSkqSJCUlJengwYOqqqqy1xQUFCgiIkJ9+/a111x6jItrLh4DAACYzdErRBkZGVqzZo3+9Kc/KTw83H7OT2RkpMLCwhQZGan09HRlZWWpU6dOioiI0C9+8QslJSXp7rvvliQNGzZMffv21cMPP6zc3Fy53W4988wzysjIsK/yPPbYY1q8eLGmTZumRx55RDt27ND69eu1ZQuv7gIAAA5fIVqyZIlqamr0b//2b+ratat9W7dunb3m17/+tR588EGNGTNG9957r2JjY7Vx40Z7f1BQkDZv3qygoCAlJSVp3LhxGj9+vF544QV7TUJCgrZs2aKCggLddtttmjdvnn73u98pJSWlVe8vAABom9rU+xC1VbwPEQAALYv3IQIAAHAYQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xFEAADAeAQRAAAwHkEEAACMRxABAADjEUQAAMB4BBEAADAeQQQAAIxHEAEAAOMRRAAAwHgEEQAAMB5BBAAAjEcQAQAA4xkVRL/5zW/Us2dPhYaGatCgQSouLnZ6JAAA0AYYE0Tr1q1TVlaWnnvuOe3du1e33XabUlJSVFVV5fRoAADAYcYEUV5enh599FFNnDhRffv2VX5+vtq3b69ly5Y5PRoAAHBYsNMDtIb6+nqVlJRo5syZ9rbAwEAlJyerqKioyfq6ujrV1dXZH9fU1EiSPB5Pi8znrfuyRY4LAMDVoiV+xl48pmVZ/3StEUH0j3/8Q42NjYqJifHZHhMTo48//rjJ+pycHD3//PNNtsfHx7fYjAAAmCxyfssd+8yZM4qMjPzWNUYE0eWaOXOmsrKy7I+9Xq9OnTqlzp07KyAgoFm/lsfjUXx8vE6cOKGIiIhmPTb8wzlpmzgvbQ/npO3hnPiyLEtnzpxRXFzcP11rRBB16dJFQUFBqqys9NleWVmp2NjYJutDQkIUEhLisy0qKqolR1RERATfvG0M56Rt4ry0PZyTtodz8v/9sytDFxnxpGqXy6WBAweqsLDQ3ub1elVYWKikpCQHJwMAAG2BEVeIJCkrK0sTJkxQYmKi7rrrLs2fP1/nzp3TxIkTnR4NAAA4zJgg+vGPf6yTJ08qOztbbrdbt99+u7Zt29bkidatLSQkRM8991yTh+jgHM5J28R5aXs4J20P58R/AdZ3eS0aAADANcyI5xABAAB8G4IIAAAYjyACAADGI4gAAIDxCCIH/eY3v1HPnj0VGhqqQYMGqbi42OmRjJKTk6M777xT4eHhio6O1siRI1VaWuqzpra2VhkZGercubOuu+46jRkzpskbfKLlzJ49WwEBAcrMzLS3cU5a3xdffKFx48apc+fOCgsLU//+/fXBBx/Y+y3LUnZ2trp27aqwsDAlJyfr2LFjDk58bWtsbNSzzz6rhIQEhYWF6YYbbtCLL77o8/u6OCeXjyByyLp165SVlaXnnntOe/fu1W233aaUlBRVVVU5PZoxdu7cqYyMDL3//vsqKChQQ0ODhg0bpnPnztlrpk6dqjfffFMbNmzQzp07VV5ertGjRzs4tTn27Nmj1157TbfeeqvPds5J6zp9+rQGDx6sdu3aaevWrfroo480b948dezY0V6Tm5urhQsXKj8/X7t371aHDh2UkpKi2tpaBye/ds2ZM0dLlizR4sWLdeTIEc2ZM0e5ublatGiRvYZz4gcLjrjrrrusjIwM++PGxkYrLi7OysnJcXAqs1VVVVmSrJ07d1qWZVnV1dVWu3btrA0bNthrjhw5YkmyioqKnBrTCGfOnLF69eplFRQUWN///vetKVOmWJbFOXHC9OnTrSFDhvyf+71erxUbG2vNnTvX3lZdXW2FhIRYr7/+emuMaJzU1FTrkUce8dk2evRoKy0tzbIszom/uELkgPr6epWUlCg5OdneFhgYqOTkZBUVFTk4mdlqamokSZ06dZIklZSUqKGhwec89e7dW927d+c8tbCMjAylpqb6/NlLnBMnbNq0SYmJifrRj36k6Oho3XHHHfrtb39r7z9+/LjcbrfPOYmMjNSgQYM4Jy3ke9/7ngoLC3X06FFJ0v79+/Xuu+/qgQcekMQ58Zcx71TdlvzjH/9QY2Njk3fJjomJ0ccff+zQVGbzer3KzMzU4MGD1a9fP0mS2+2Wy+Vq8ot9Y2Ji5Ha7HZjSDGvXrtXevXu1Z8+eJvs4J63vk08+0ZIlS5SVlaX//M//1J49e/Tkk0/K5XJpwoQJ9p/7N/17xjlpGTNmzJDH41Hv3r0VFBSkxsZGzZo1S2lpaZLEOfETQQTowhWJQ4cO6d1333V6FKOdOHFCU6ZMUUFBgUJDQ50eB7rwPwuJiYl6+eWXJUl33HGHDh06pPz8fE2YMMHh6cy0fv16rV69WmvWrNEtt9yiffv2KTMzU3FxcZyTK8BDZg7o0qWLgoKCmrwyprKyUrGxsQ5NZa7Jkydr8+bNevvtt9WtWzd7e2xsrOrr61VdXe2znvPUckpKSlRVVaUBAwYoODhYwcHB2rlzpxYuXKjg4GDFxMRwTlpZ165d1bdvX59tffr0UVlZmSTZf+78e9Z6nn76ac2YMUNjx45V//799fDDD2vq1KnKycmRxDnxF0HkAJfLpYEDB6qwsNDe5vV6VVhYqKSkJAcnM4tlWZo8ebLeeOMN7dixQwkJCT77Bw4cqHbt2vmcp9LSUpWVlXGeWsjQoUN18OBB7du3z74lJiYqLS3N/m/OSesaPHhwk7ejOHr0qHr06CFJSkhIUGxsrM858Xg82r17N+ekhXz55ZcKDPT98R0UFCSv1yuJc+I3p5/Vbaq1a9daISEh1ooVK6yPPvrImjRpkhUVFWW53W6nRzPG448/bkVGRlrvvPOOVVFRYd++/PJLe81jjz1mde/e3dqxY4f1wQcfWElJSVZSUpKDU5vn0leZWRbnpLUVFxdbwcHB1qxZs6xjx45Zq1evttq3b2/94Q9/sNfMnj3bioqKsv70pz9ZBw4csEaMGGElJCRYX331lYOTX7smTJhg/cu//Iu1efNm6/jx49bGjRutLl26WNOmTbPXcE4uH0HkoEWLFlndu3e3XC6Xddddd1nvv/++0yMZRdI33pYvX26v+eqrr6wnnnjC6tixo9W+fXtr1KhRVkVFhXNDG+jrQcQ5aX1vvvmm1a9fPyskJMTq3bu3tXTpUp/9Xq/XevbZZ62YmBgrJCTEGjp0qFVaWurQtNc+j8djTZkyxerevbsVGhpq/eu//qv1X//1X1ZdXZ29hnNy+QIs65K3tgQAADAQzyECAADGI4gAAIDxCCIAAGA8gggAABiPIAIAAMYjiAAAgPEIIgAAYDyCCAAAGI8gAgAAxiOIAACA8QgiAABgPIIIAAAY7/8B4hnYIOM4QM8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array(train_labels, dtype=int)).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e7b42b8-bf32-4027-ae82-19319487d35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 72, 32, 78,  3,  9, 71, 61,  1, 62, 73, 30, 63, 87, 26, 36,  6,\n",
       "       50, 10, 74, 27, 41, 35, 17, 22, 88, 77, 84, 31, 55, 60,  8, 86, 23,\n",
       "       39, 16, 48, 14,  0, 80, 12, 38, 24, 57, 85, 75, 59, 45, 52, 46, 20,\n",
       "       40, 76, 89, 54, 33, 64, 47, 51, 18, 49, 43, 21, 79, 37,  7, 42, 81,\n",
       "       13,  4, 56,  5, 34, 53, 58, 66,  2, 15, 19, 83])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.array(train_labels, dtype=int)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d051482-6e8a-4047-bacc-2c9fe9565b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d567088-238b-45ef-93a2-d490ceafcef1",
   "metadata": {},
   "source": [
    "## Training VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "803a91f2-3e38-4378-be27-0267d8892b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [32, 90]                  --\n",
      "├─Sequential: 1-1                        [32, 64, 224, 224]        --\n",
      "│    └─Conv2d: 2-1                       [32, 64, 224, 224]        1,792\n",
      "│    └─BatchNorm2d: 2-2                  [32, 64, 224, 224]        128\n",
      "│    └─ReLU: 2-3                         [32, 64, 224, 224]        --\n",
      "├─Sequential: 1-2                        [32, 64, 112, 112]        --\n",
      "│    └─Conv2d: 2-4                       [32, 64, 224, 224]        36,928\n",
      "│    └─BatchNorm2d: 2-5                  [32, 64, 224, 224]        128\n",
      "│    └─ReLU: 2-6                         [32, 64, 224, 224]        --\n",
      "│    └─MaxPool2d: 2-7                    [32, 64, 112, 112]        --\n",
      "├─Sequential: 1-3                        [32, 128, 112, 112]       --\n",
      "│    └─Conv2d: 2-8                       [32, 128, 112, 112]       73,856\n",
      "│    └─BatchNorm2d: 2-9                  [32, 128, 112, 112]       256\n",
      "│    └─ReLU: 2-10                        [32, 128, 112, 112]       --\n",
      "├─Sequential: 1-4                        [32, 128, 56, 56]         --\n",
      "│    └─Conv2d: 2-11                      [32, 128, 112, 112]       147,584\n",
      "│    └─BatchNorm2d: 2-12                 [32, 128, 112, 112]       256\n",
      "│    └─ReLU: 2-13                        [32, 128, 112, 112]       --\n",
      "│    └─MaxPool2d: 2-14                   [32, 128, 56, 56]         --\n",
      "├─Sequential: 1-5                        [32, 256, 56, 56]         --\n",
      "│    └─Conv2d: 2-15                      [32, 256, 56, 56]         295,168\n",
      "│    └─BatchNorm2d: 2-16                 [32, 256, 56, 56]         512\n",
      "│    └─ReLU: 2-17                        [32, 256, 56, 56]         --\n",
      "├─Sequential: 1-6                        [32, 256, 56, 56]         --\n",
      "│    └─Conv2d: 2-18                      [32, 256, 56, 56]         590,080\n",
      "│    └─BatchNorm2d: 2-19                 [32, 256, 56, 56]         512\n",
      "│    └─ReLU: 2-20                        [32, 256, 56, 56]         --\n",
      "├─Sequential: 1-7                        [32, 256, 28, 28]         --\n",
      "│    └─Conv2d: 2-21                      [32, 256, 56, 56]         590,080\n",
      "│    └─BatchNorm2d: 2-22                 [32, 256, 56, 56]         512\n",
      "│    └─ReLU: 2-23                        [32, 256, 56, 56]         --\n",
      "│    └─MaxPool2d: 2-24                   [32, 256, 28, 28]         --\n",
      "├─Sequential: 1-8                        [32, 512, 28, 28]         --\n",
      "│    └─Conv2d: 2-25                      [32, 512, 28, 28]         1,180,160\n",
      "│    └─BatchNorm2d: 2-26                 [32, 512, 28, 28]         1,024\n",
      "│    └─ReLU: 2-27                        [32, 512, 28, 28]         --\n",
      "├─Sequential: 1-9                        [32, 512, 28, 28]         --\n",
      "│    └─Conv2d: 2-28                      [32, 512, 28, 28]         2,359,808\n",
      "│    └─BatchNorm2d: 2-29                 [32, 512, 28, 28]         1,024\n",
      "│    └─ReLU: 2-30                        [32, 512, 28, 28]         --\n",
      "├─Sequential: 1-10                       [32, 512, 14, 14]         --\n",
      "│    └─Conv2d: 2-31                      [32, 512, 28, 28]         2,359,808\n",
      "│    └─BatchNorm2d: 2-32                 [32, 512, 28, 28]         1,024\n",
      "│    └─ReLU: 2-33                        [32, 512, 28, 28]         --\n",
      "│    └─MaxPool2d: 2-34                   [32, 512, 14, 14]         --\n",
      "├─Sequential: 1-11                       [32, 512, 14, 14]         --\n",
      "│    └─Conv2d: 2-35                      [32, 512, 14, 14]         2,359,808\n",
      "│    └─BatchNorm2d: 2-36                 [32, 512, 14, 14]         1,024\n",
      "│    └─ReLU: 2-37                        [32, 512, 14, 14]         --\n",
      "├─Sequential: 1-12                       [32, 512, 14, 14]         --\n",
      "│    └─Conv2d: 2-38                      [32, 512, 14, 14]         2,359,808\n",
      "│    └─BatchNorm2d: 2-39                 [32, 512, 14, 14]         1,024\n",
      "│    └─ReLU: 2-40                        [32, 512, 14, 14]         --\n",
      "├─Sequential: 1-13                       [32, 512, 7, 7]           --\n",
      "│    └─Conv2d: 2-41                      [32, 512, 14, 14]         2,359,808\n",
      "│    └─BatchNorm2d: 2-42                 [32, 512, 14, 14]         1,024\n",
      "│    └─ReLU: 2-43                        [32, 512, 14, 14]         --\n",
      "│    └─MaxPool2d: 2-44                   [32, 512, 7, 7]           --\n",
      "├─AdaptiveAvgPool2d: 1-14                [32, 512, 7, 7]           --\n",
      "├─Sequential: 1-15                       [32, 4096]                --\n",
      "│    └─Linear: 2-45                      [32, 4096]                102,764,544\n",
      "│    └─ReLU: 2-46                        [32, 4096]                --\n",
      "│    └─Dropout: 2-47                     [32, 4096]                --\n",
      "├─Sequential: 1-16                       [32, 4096]                --\n",
      "│    └─Linear: 2-48                      [32, 4096]                16,781,312\n",
      "│    └─ReLU: 2-49                        [32, 4096]                --\n",
      "│    └─Dropout: 2-50                     [32, 4096]                --\n",
      "├─Sequential: 1-17                       [32, 90]                  --\n",
      "│    └─Linear: 2-51                      [32, 90]                  368,730\n",
      "==========================================================================================\n",
      "Total params: 134,637,722\n",
      "Trainable params: 134,637,722\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 495.36\n",
      "==========================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 6938.45\n",
      "Params size (MB): 538.55\n",
      "Estimated Total Size (MB): 7496.27\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "input_size = (batch_size, 3, 224, 224)\n",
    "#num_classes = len(dataloader.dataset.class_name)\n",
    "num_classes = max(train_labels)+1\n",
    "model = vgg16.VGG16(device, input_size=input_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528c4a81-3ed8-4c1a-8f9c-acd8e1b23dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH #0, step #0] loss: 4.7048163414001465\n",
      "[EPOCH #0, step #2] loss: 4.83914057413737\n",
      "[EPOCH #0, step #4] loss: 4.764868640899659\n",
      "[EPOCH #0, step #6] loss: 4.779199668339321\n",
      "[EPOCH #0, step #8] loss: 4.799923631880018\n",
      "[EPOCH #0, step #10] loss: 4.782046534798362\n",
      "[EPOCH #0, step #12] loss: 4.766810233776386\n",
      "[EPOCH #0, step #14] loss: 4.765534687042236\n",
      "[EPOCH #0, step #16] loss: 4.766178467694451\n",
      "[EPOCH #0, step #18] loss: 4.777909379256399\n",
      "[EPOCH #0, step #20] loss: 4.777234940301804\n",
      "[EPOCH #0, step #22] loss: 4.779937557552172\n",
      "[EPOCH #0, step #24] loss: 4.786933193206787\n",
      "[EPOCH #0, step #26] loss: 4.798071790624548\n",
      "[EPOCH #0, step #28] loss: 4.79944607307171\n",
      "[EPOCH #0, step #30] loss: 4.804860514979208\n",
      "[EPOCH #0, step #32] loss: 4.811790090618712\n",
      "[EPOCH #0, step #34] loss: 4.809725461687361\n",
      "[EPOCH #0, step #36] loss: 4.810154102944039\n",
      "[EPOCH #0, step #38] loss: 4.803999766325339\n",
      "[EPOCH #0, step #40] loss: 4.808518037563417\n",
      "[EPOCH #0, step #42] loss: 4.8030270199443015\n",
      "[EPOCH #0, step #44] loss: 4.803924083709717\n",
      "[EPOCH #0, step #46] loss: 4.802374778909886\n",
      "[EPOCH #0, step #48] loss: 4.805371372067198\n",
      "[EPOCH #0, step #50] loss: 4.801478367225797\n",
      "[EPOCH #0, step #52] loss: 4.804204598912653\n",
      "[EPOCH #0, step #54] loss: 4.805458250912753\n",
      "[EPOCH #0, step #56] loss: 4.801784364800704\n",
      "[EPOCH #0, step #58] loss: 4.806746191897635\n",
      "[EPOCH #0, step #60] loss: 4.810401103535637\n",
      "[EPOCH #0, step #62] loss: 4.809887742239331\n",
      "[EPOCH #0, step #64] loss: 4.803502302903396\n",
      "[EPOCH #0, step #66] loss: 4.804986249155073\n",
      "[EPOCH #0, step #68] loss: 4.812032409336256\n",
      "[EPOCH #0, step #70] loss: 4.809765345613721\n",
      "[EPOCH #0, step #72] loss: 4.809149748658481\n",
      "[EPOCH #0, step #74] loss: 4.808012046813965\n",
      "[EPOCH #0, step #76] loss: 4.804543606646649\n",
      "[EPOCH #0, step #78] loss: 4.803741050671928\n",
      "[EPOCH #0, step #80] loss: 4.802245163623198\n",
      "[EPOCH #0, step #82] loss: 4.801272863365082\n",
      "[EPOCH #0, step #84] loss: 4.799393171422622\n",
      "[EPOCH #0, step #86] loss: 4.795535202684073\n",
      "[EPOCH #0, step #88] loss: 4.796205515272162\n",
      "[EPOCH #0, step #90] loss: 4.795355587215214\n",
      "[EPOCH #0, step #92] loss: 4.792795555565947\n",
      "[EPOCH #0, step #94] loss: 4.791891072925768\n",
      "[EPOCH #0, step #96] loss: 4.792887677851412\n",
      "[EPOCH #0, step #98] loss: 4.793987139306887\n",
      "[EPOCH #0, step #100] loss: 4.7969838982761495\n",
      "[EPOCH #0, step #102] loss: 4.798962106982481\n",
      "[EPOCH #0, step #104] loss: 4.79837558837164\n",
      "[EPOCH #0, step #106] loss: 4.798306536451679\n",
      "[EPOCH #0, step #108] loss: 4.7978265044886035\n",
      "[EPOCH #0, step #110] loss: 4.80039999076912\n",
      "[EPOCH #0, step #112] loss: 4.8028049890973925\n",
      "[EPOCH #0, step #114] loss: 4.801061169997506\n",
      "[EPOCH #0, step #116] loss: 4.79787987521571\n",
      "[EPOCH #0, step #118] loss: 4.798303996815401\n",
      "[EPOCH #0, step #120] loss: 4.799263272403685\n",
      "[EPOCH #0, step #122] loss: 4.796056821094296\n",
      "[EPOCH #0, step #124] loss: 4.794933467864991\n",
      "[EPOCH #0, step #126] loss: 4.7931219949497015\n",
      "[EPOCH #0, step #128] loss: 4.792266756989235\n",
      "[EPOCH #0, step #130] loss: 4.793115437485789\n",
      "[EPOCH #0, step #132] loss: 4.792574749853378\n",
      "[EPOCH #0, step #134] loss: 4.794009014412209\n",
      "[EPOCH #0, step #136] loss: 4.7932120030813845\n",
      "[EPOCH #0, step #138] loss: 4.792165763086552\n",
      "[EPOCH #0, step #140] loss: 4.792788573190675\n",
      "[EPOCH #0, step #142] loss: 4.79227094050054\n",
      "[EPOCH #0, step #144] loss: 4.790322951612802\n",
      "[EPOCH #0, step #146] loss: 4.791786742048199\n",
      "[EPOCH #0, step #148] loss: 4.79514362988056\n",
      "[EPOCH #0, step #150] loss: 4.796400878603095\n",
      "[EPOCH #0, step #152] loss: 4.795970280965169\n",
      "[EPOCH #0, step #154] loss: 4.797283781728437\n",
      "[EPOCH #0, step #156] loss: 4.798401109731881\n",
      "[EPOCH #0, step #158] loss: 4.7970861788815675\n",
      "[EPOCH #0, step #160] loss: 4.797373999720034\n",
      "[EPOCH #0, step #162] loss: 4.7973123913162325\n",
      "[EPOCH #0, step #164] loss: 4.798190368305553\n",
      "[EPOCH #0, step #166] loss: 4.797133768390038\n",
      "[EPOCH #0, step #168] loss: 4.796119125637077\n",
      "[EPOCH #0, step #170] loss: 4.795329250090304\n",
      "[EPOCH #0, step #172] loss: 4.795174879834831\n",
      "[EPOCH #0, step #174] loss: 4.795597686767578\n",
      "[EPOCH #0, step #176] loss: 4.7952955779382735\n",
      "[EPOCH #0, step #178] loss: 4.795711037832931\n",
      "[EPOCH #0, step #180] loss: 4.795117409848377\n",
      "[EPOCH #0, step #182] loss: 4.796734614450424\n",
      "[EPOCH #0, step #184] loss: 4.798010591558508\n",
      "[EPOCH #0, step #186] loss: 4.797480672438515\n",
      "[EPOCH #0, step #188] loss: 4.798962330692029\n",
      "[EPOCH #0, step #190] loss: 4.799692688188004\n",
      "[EPOCH #0, step #192] loss: 4.799838135279522\n",
      "[EPOCH #0, step #194] loss: 4.799843259958121\n",
      "[EPOCH #0, step #196] loss: 4.798272851760013\n",
      "[EPOCH #0, step #198] loss: 4.7993827297459895\n",
      "[EPOCH #0, step #200] loss: 4.7995214818128895\n",
      "[EPOCH #0, step #202] loss: 4.7997387735714465\n",
      "[EPOCH #0, step #204] loss: 4.801476657681349\n",
      "[EPOCH #0, step #206] loss: 4.799446193492355\n",
      "[EPOCH #0, step #208] loss: 4.798648993934741\n",
      "[EPOCH #0, step #210] loss: 4.798287807482679\n",
      "[EPOCH #0, step #212] loss: 4.7977218157808545\n",
      "[EPOCH #0, step #214] loss: 4.798852496923402\n",
      "[EPOCH #0, step #216] loss: 4.799270087123467\n",
      "[EPOCH #0, step #218] loss: 4.799322687871924\n",
      "[EPOCH #0, step #220] loss: 4.799296465394724\n",
      "[EPOCH #0, step #222] loss: 4.8002856878956335\n",
      "[EPOCH #0, step #224] loss: 4.799856344858806\n",
      "[EPOCH #0, step #226] loss: 4.799760961322533\n",
      "[EPOCH #0, step #228] loss: 4.7992163474903355\n",
      "[EPOCH #0, step #230] loss: 4.7993973550342375\n",
      "[EPOCH #0, step #232] loss: 4.798389758163256\n",
      "[EPOCH #0, step #234] loss: 4.797536963604866\n",
      "[EPOCH #0, step #236] loss: 4.797059061155037\n",
      "[EPOCH #0, step #238] loss: 4.795678254450714\n",
      "[EPOCH #0, step #240] loss: 4.794457556301133\n",
      "[EPOCH #0, step #242] loss: 4.793303438665445\n",
      "[EPOCH #0, step #244] loss: 4.793781784602574\n",
      "[EPOCH #0, step #246] loss: 4.793609329563403\n",
      "[EPOCH #0, step #248] loss: 4.793414108245727\n",
      "[EPOCH #0, step #250] loss: 4.792674239413197\n",
      "[EPOCH #0, step #252] loss: 4.793953071941029\n",
      "[EPOCH #0, step #254] loss: 4.793171983606675\n",
      "[EPOCH #0, step #256] loss: 4.793405343586369\n",
      "[EPOCH #0, step #258] loss: 4.7940132332584575\n",
      "[EPOCH #0, step #260] loss: 4.7946541519457355\n",
      "[EPOCH #0, step #262] loss: 4.794169464038806\n",
      "[EPOCH #0, step #264] loss: 4.79478745370541\n",
      "[EPOCH #0, step #266] loss: 4.794185736652617\n",
      "[EPOCH #0, step #268] loss: 4.793712678008807\n",
      "[EPOCH #0, step #270] loss: 4.792984255125602\n",
      "[EPOCH #0, step #272] loss: 4.792571825858874\n",
      "[EPOCH #0, step #274] loss: 4.79319087635387\n",
      "[EPOCH #0, step #276] loss: 4.792513411828327\n",
      "[EPOCH #0, step #278] loss: 4.79294961669539\n",
      "[EPOCH #0, step #280] loss: 4.792225053726142\n",
      "[EPOCH #0, step #282] loss: 4.792227505795105\n",
      "[EPOCH #0, step #284] loss: 4.791182541428951\n",
      "[EPOCH #0, step #286] loss: 4.79074761427238\n",
      "[EPOCH #0, step #288] loss: 4.791050001824191\n",
      "[EPOCH #0, step #290] loss: 4.791561418382573\n",
      "[EPOCH #0, step #292] loss: 4.7915849848412\n",
      "[EPOCH #0, step #294] loss: 4.792312775628042\n",
      "[EPOCH #0, step #296] loss: 4.792792498463332\n",
      "[EPOCH #0, step #298] loss: 4.792872282174917\n",
      "[EPOCH #0, step #300] loss: 4.791915907812277\n",
      "[EPOCH #0, step #302] loss: 4.791170976342935\n",
      "[EPOCH #0, step #304] loss: 4.791403468710477\n",
      "[EPOCH #0, step #306] loss: 4.791584171767344\n",
      "[EPOCH #0, step #308] loss: 4.791868553964066\n",
      "[EPOCH #0, step #310] loss: 4.7926403962530895\n",
      "[EPOCH #0, step #312] loss: 4.792106331346896\n",
      "[EPOCH #0, step #314] loss: 4.790896367269849\n",
      "[EPOCH #0, step #316] loss: 4.790072432075765\n",
      "[EPOCH #0, step #318] loss: 4.790467839255975\n",
      "[EPOCH #0, step #320] loss: 4.790583080220445\n",
      "[EPOCH #0, step #322] loss: 4.7907906169123695\n",
      "[EPOCH #0, step #324] loss: 4.790747542748084\n",
      "[EPOCH #0, step #326] loss: 4.7905814436233\n",
      "[EPOCH #0, step #328] loss: 4.79093475399771\n",
      "[EPOCH #0, step #330] loss: 4.791786649075879\n",
      "[EPOCH #0, step #332] loss: 4.7909069591098365\n",
      "[EPOCH #0, step #334] loss: 4.790987783403539\n",
      "[EPOCH #0, step #336] loss: 4.792098467003344\n",
      "[EPOCH #0, step #338] loss: 4.791585154238001\n",
      "[EPOCH #0, step #340] loss: 4.792709118459931\n",
      "[EPOCH #0, step #342] loss: 4.792328473082784\n",
      "[EPOCH #0, step #344] loss: 4.790840884222501\n",
      "[EPOCH #0, step #346] loss: 4.791518769277963\n",
      "[EPOCH #0, step #348] loss: 4.791539354788881\n",
      "[EPOCH #0, step #350] loss: 4.790992162166497\n",
      "[EPOCH #0, step #352] loss: 4.791382672091044\n",
      "[EPOCH #0, step #354] loss: 4.790491046368237\n",
      "[EPOCH #0, step #356] loss: 4.790538475293071\n",
      "[EPOCH #0, step #358] loss: 4.7908289093825145\n",
      "[EPOCH #0, step #360] loss: 4.790127706659798\n",
      "[EPOCH #0, step #362] loss: 4.790393975155412\n",
      "[EPOCH #0, step #364] loss: 4.790833268100268\n",
      "[EPOCH #0, step #366] loss: 4.7897630192603335\n",
      "[EPOCH #0, step #368] loss: 4.79012133306281\n",
      "[EPOCH #0, step #370] loss: 4.79051076883897\n",
      "[EPOCH #0, step #372] loss: 4.789687372724108\n",
      "[EPOCH #0, step #374] loss: 4.788982906341553\n",
      "[EPOCH #0, step #376] loss: 4.789171285907533\n",
      "[EPOCH #0, step #378] loss: 4.788774685369003\n",
      "[EPOCH #0, step #380] loss: 4.788314120976005\n",
      "[EPOCH #0, step #382] loss: 4.788161941358999\n",
      "[EPOCH #0, step #384] loss: 4.787514710116696\n",
      "[EPOCH #0, step #386] loss: 4.7874915458127205\n",
      "[EPOCH #0, step #388] loss: 4.787718555921147\n",
      "[EPOCH #0, step #390] loss: 4.787662426833911\n",
      "[EPOCH #0, step #392] loss: 4.787326142988132\n",
      "[EPOCH #0, step #394] loss: 4.788088281848762\n",
      "[EPOCH #0, step #396] loss: 4.789013312505535\n",
      "[EPOCH #0, step #398] loss: 4.788956911044013\n",
      "[EPOCH #0, step #400] loss: 4.789963394031858\n",
      "[EPOCH #0, step #402] loss: 4.790459197449033\n",
      "[EPOCH #0, step #404] loss: 4.790858527760447\n",
      "[EPOCH #0, step #406] loss: 4.79115967785697\n",
      "[EPOCH #0, step #408] loss: 4.79112929178625\n",
      "[EPOCH #0, step #410] loss: 4.791249984082224\n",
      "[EPOCH #0, step #412] loss: 4.7909543346839145\n",
      "[EPOCH #0, step #414] loss: 4.790311349156391\n",
      "[EPOCH #0, step #416] loss: 4.789518328879377\n",
      "[EPOCH #0, step #418] loss: 4.790051404502341\n",
      "[EPOCH #0, step #420] loss: 4.7900595132642\n",
      "[EPOCH #0, step #422] loss: 4.790113583242357\n",
      "[EPOCH #0, step #424] loss: 4.789311333824607\n",
      "[EPOCH #0, step #426] loss: 4.78878027382165\n",
      "[EPOCH #0, step #428] loss: 4.788662889342763\n",
      "[EPOCH #0, step #430] loss: 4.788615993446651\n",
      "[EPOCH #0, step #432] loss: 4.788215868467547\n",
      "[EPOCH #0, step #434] loss: 4.7890699967570685\n",
      "[EPOCH #0, step #436] loss: 4.788302684539516\n",
      "[EPOCH #0, step #438] loss: 4.787984047500854\n",
      "[EPOCH #0, step #440] loss: 4.78831966123343\n",
      "[EPOCH #0, step #442] loss: 4.788735141065266\n",
      "[EPOCH #0, step #444] loss: 4.788592007454861\n",
      "[EPOCH #0, step #446] loss: 4.789349198608057\n",
      "[EPOCH #0, step #448] loss: 4.79004043353958\n",
      "[EPOCH #0, step #450] loss: 4.790245485411515\n",
      "[EPOCH #0, step #452] loss: 4.789855233880858\n",
      "[EPOCH #0, step #454] loss: 4.788906988206801\n",
      "[EPOCH #0, step #456] loss: 4.789075724144733\n",
      "[EPOCH #0, step #458] loss: 4.788748070045754\n",
      "[EPOCH #0, step #460] loss: 4.788810694812436\n",
      "[EPOCH #0, step #462] loss: 4.7891353860529655\n",
      "[EPOCH #0, step #464] loss: 4.7895784224233315\n",
      "[EPOCH #0, step #466] loss: 4.789154932892042\n",
      "[EPOCH #0, step #468] loss: 4.789492671169452\n",
      "[EPOCH #0, step #470] loss: 4.789761667798279\n",
      "[EPOCH #0, step #472] loss: 4.7901690374217125\n",
      "[EPOCH #0, step #474] loss: 4.790082561091372\n",
      "[EPOCH #0, step #476] loss: 4.789544180504181\n",
      "[EPOCH #0, step #478] loss: 4.789336898381626\n",
      "[EPOCH #0, step #480] loss: 4.7888308592496935\n",
      "[EPOCH #0, step #482] loss: 4.788617462845322\n",
      "[EPOCH #0, step #484] loss: 4.78857166939175\n",
      "[EPOCH #0, step #486] loss: 4.788486983007474\n",
      "[EPOCH #0, step #488] loss: 4.788901232502943\n",
      "[EPOCH #0, step #490] loss: 4.7885136623732185\n",
      "[EPOCH #0, step #492] loss: 4.78807754980865\n",
      "[EPOCH #0, step #494] loss: 4.788749253629434\n",
      "[EPOCH #0, step #496] loss: 4.7890018083200125\n",
      "[EPOCH #0, step #498] loss: 4.789629851171154\n",
      "[EPOCH #0, step #500] loss: 4.789816109244219\n",
      "[EPOCH #0, step #502] loss: 4.790225956122397\n",
      "[EPOCH #0, step #504] loss: 4.790392411109244\n",
      "[EPOCH #0, step #506] loss: 4.790437651338897\n",
      "[EPOCH #0, step #508] loss: 4.7906239618252675\n",
      "[EPOCH #0, step #510] loss: 4.790153518348291\n",
      "[EPOCH #0, step #512] loss: 4.790321797190586\n",
      "[EPOCH #0, step #514] loss: 4.7907597282557814\n",
      "[EPOCH #0, step #516] loss: 4.790259018165692\n",
      "[EPOCH #0, step #518] loss: 4.789855660271323\n",
      "[EPOCH #0, step #520] loss: 4.789987561341211\n",
      "[EPOCH #0, step #522] loss: 4.789963370299476\n",
      "[EPOCH #0, step #524] loss: 4.790477278573173\n",
      "[EPOCH #0, step #526] loss: 4.790403327181851\n",
      "[EPOCH #0, step #528] loss: 4.78992248257527\n",
      "[EPOCH #0, step #530] loss: 4.790324116830772\n",
      "[EPOCH #0, step #532] loss: 4.789736439094758\n",
      "[EPOCH #0, step #534] loss: 4.790208542904007\n",
      "[EPOCH #0, step #536] loss: 4.7901304775998135\n",
      "[EPOCH #0, step #538] loss: 4.78948653297212\n",
      "[EPOCH #0, step #540] loss: 4.7892369774485255\n",
      "[EPOCH #0, step #542] loss: 4.789310215586457\n",
      "[EPOCH #0, step #544] loss: 4.788873632238546\n",
      "[EPOCH #0, step #546] loss: 4.788269767377669\n",
      "[EPOCH #0, step #548] loss: 4.7887607836766755\n",
      "[EPOCH #0, step #550] loss: 4.789430847617979\n",
      "[EPOCH #0, step #552] loss: 4.789542968191247\n",
      "[EPOCH #0, step #554] loss: 4.7893189172487\n",
      "[EPOCH #0, step #556] loss: 4.78921865518046\n",
      "[EPOCH #0, step #558] loss: 4.789601378022878\n",
      "[EPOCH #0, step #560] loss: 4.789966743897627\n",
      "[EPOCH #0, step #562] loss: 4.7889458307364485\n",
      "[EPOCH #0, step #564] loss: 4.788976014398895\n",
      "[EPOCH #0, step #566] loss: 4.788894115091422\n",
      "[EPOCH #0, step #568] loss: 4.78948858877896\n",
      "[EPOCH #0, step #570] loss: 4.7895213801890035\n",
      "[EPOCH #0, step #572] loss: 4.789643544801243\n",
      "[EPOCH #0, step #574] loss: 4.7893858320816705\n",
      "[EPOCH #0, step #576] loss: 4.789296419616382\n",
      "[EPOCH #0, step #578] loss: 4.789143279838233\n",
      "[EPOCH #0, step #580] loss: 4.7886542108506225\n",
      "[EPOCH #0, step #582] loss: 4.788877019342387\n",
      "[EPOCH #0, step #584] loss: 4.789373745062412\n",
      "[EPOCH #0, step #586] loss: 4.789546539267649\n",
      "[EPOCH #0, step #588] loss: 4.789151028178139\n",
      "[EPOCH #0, step #590] loss: 4.788743533055391\n",
      "[EPOCH #0, step #592] loss: 4.788858112959194\n",
      "[EPOCH #0, step #594] loss: 4.7891316646287425\n",
      "[EPOCH #0, step #596] loss: 4.7890642252399696\n",
      "[EPOCH #0, step #598] loss: 4.789155905951244\n",
      "[EPOCH #0, step #600] loss: 4.788882077037792\n",
      "[EPOCH #0, step #602] loss: 4.78892324931586\n",
      "[EPOCH #0, step #604] loss: 4.789146089159753\n",
      "[EPOCH #0, step #606] loss: 4.788878214614591\n",
      "[EPOCH #0, step #608] loss: 4.788530888424327\n",
      "[EPOCH #0, step #610] loss: 4.788822967760302\n",
      "[EPOCH #0, step #612] loss: 4.789133348044913\n",
      "[EPOCH #0, step #614] loss: 4.789018855831487\n",
      "[EPOCH #0, step #616] loss: 4.789117692548607\n",
      "[EPOCH #0, step #618] loss: 4.788819298412958\n",
      "[EPOCH #0, step #620] loss: 4.788754207500513\n",
      "[EPOCH #0, step #622] loss: 4.78870807757921\n",
      "[EPOCH #0, step #624] loss: 4.788429039764404\n",
      "[EPOCH #0, step #626] loss: 4.788384137161231\n",
      "[EPOCH #0, step #628] loss: 4.788761699900908\n",
      "[EPOCH #0, step #630] loss: 4.788346503691515\n",
      "[EPOCH #0, step #632] loss: 4.788469084819535\n",
      "[EPOCH #0, step #634] loss: 4.788647201868493\n",
      "[EPOCH #0, step #636] loss: 4.788306415174595\n",
      "[EPOCH #0, step #638] loss: 4.788748335950252\n",
      "[EPOCH #0, step #640] loss: 4.788959492015392\n",
      "[EPOCH #0, step #642] loss: 4.78908564106302\n",
      "[EPOCH #0, step #644] loss: 4.789000823885895\n",
      "[EPOCH #0, step #646] loss: 4.789159342899942\n",
      "[EPOCH #0, step #648] loss: 4.78941655710041\n",
      "[EPOCH #0, step #650] loss: 4.789418557455646\n",
      "[EPOCH #0, step #652] loss: 4.789569342118128\n",
      "[EPOCH #0, step #654] loss: 4.78903144297709\n",
      "[EPOCH #0, step #656] loss: 4.7890548393969485\n",
      "[EPOCH #0, step #658] loss: 4.789419345682054\n",
      "[EPOCH #0, step #660] loss: 4.789162729583602\n",
      "[EPOCH #0, step #662] loss: 4.789271812093744\n",
      "[EPOCH #0, step #664] loss: 4.789244982353726\n",
      "[EPOCH #0, step #666] loss: 4.789176893734682\n",
      "[EPOCH #0, step #668] loss: 4.788927009109246\n",
      "[EPOCH #0, step #670] loss: 4.788542084473612\n",
      "[EPOCH #0, step #672] loss: 4.788352843025287\n",
      "[EPOCH #0, step #674] loss: 4.788736726266366\n",
      "[EPOCH #0, step #676] loss: 4.788306582097289\n",
      "[EPOCH #0, step #678] loss: 4.788394313497641\n",
      "[EPOCH #0, step #680] loss: 4.788223394038218\n",
      "[EPOCH #0, step #682] loss: 4.7882912693051605\n",
      "[EPOCH #0, step #684] loss: 4.7885613900901625\n",
      "[EPOCH #0, step #686] loss: 4.7888212807834405\n",
      "[EPOCH #0, step #688] loss: 4.789032513240598\n",
      "[EPOCH #0, step #690] loss: 4.7890819102396325\n",
      "[EPOCH #0, step #692] loss: 4.789071945917039\n",
      "[EPOCH #0, step #694] loss: 4.7888163106904615\n",
      "[EPOCH #0, step #696] loss: 4.7886293163600575\n",
      "[EPOCH #0, step #698] loss: 4.788617619117442\n",
      "[EPOCH #0, step #700] loss: 4.788500617812262\n",
      "[EPOCH #0, step #702] loss: 4.788165846047327\n",
      "[EPOCH #0, step #704] loss: 4.788244081727156\n",
      "[EPOCH #0, step #706] loss: 4.788362962353179\n",
      "[EPOCH #0, step #708] loss: 4.787867903877549\n",
      "[EPOCH #0, step #710] loss: 4.788231575371679\n",
      "[EPOCH #0, step #712] loss: 4.788298945941898\n",
      "[EPOCH #0, step #714] loss: 4.7881353198231515\n",
      "[EPOCH #0, step #716] loss: 4.787975859276254\n",
      "[EPOCH #0, step #718] loss: 4.787890657098635\n",
      "[EPOCH #0, step #720] loss: 4.787732974170151\n",
      "[EPOCH #0, step #722] loss: 4.7880041866381635\n",
      "[EPOCH #0, step #724] loss: 4.788126325936153\n",
      "[EPOCH #0, step #726] loss: 4.788279328746349\n",
      "[EPOCH #0, step #728] loss: 4.7881862708407015\n",
      "[EPOCH #0, step #730] loss: 4.788222128324078\n",
      "[EPOCH #0, step #732] loss: 4.788399495987417\n",
      "[EPOCH #0, step #734] loss: 4.788602651219789\n",
      "[EPOCH #0, step #736] loss: 4.78906287267056\n",
      "[EPOCH #0, step #738] loss: 4.788515762643982\n",
      "[EPOCH #0, step #740] loss: 4.7886815926967525\n",
      "[EPOCH #0, step #742] loss: 4.789140925272638\n",
      "[EPOCH #0, step #744] loss: 4.789246936772494\n",
      "[EPOCH #0, step #746] loss: 4.789038070872765\n",
      "[EPOCH #0, step #748] loss: 4.7892479718288525\n",
      "[EPOCH #0, step #750] loss: 4.789456639245411\n",
      "[EPOCH #0, step #752] loss: 4.789744850173889\n",
      "[EPOCH #0, step #754] loss: 4.7894453023443155\n",
      "[EPOCH #0, step #756] loss: 4.789367512006268\n",
      "[EPOCH #0, step #758] loss: 4.789789295950426\n",
      "[EPOCH #0, step #760] loss: 4.789707500267279\n",
      "[EPOCH #0, step #762] loss: 4.789817611949315\n",
      "[EPOCH #0, step #764] loss: 4.789841190038943\n",
      "[EPOCH #0, step #766] loss: 4.789795994914029\n",
      "[EPOCH #0, step #768] loss: 4.789632769337866\n",
      "[EPOCH #0, step #770] loss: 4.789505672825913\n",
      "[EPOCH #0, step #772] loss: 4.789506662400852\n",
      "[EPOCH #0, step #774] loss: 4.7893028000862365\n",
      "[EPOCH #0, step #776] loss: 4.789056463744803\n",
      "[EPOCH #0, step #778] loss: 4.789150173765092\n",
      "[EPOCH #0, step #780] loss: 4.789106871498684\n",
      "[EPOCH #0, step #782] loss: 4.788784047805182\n",
      "[EPOCH #0, step #784] loss: 4.788984981463973\n",
      "[EPOCH #0, step #786] loss: 4.788646595602266\n",
      "[EPOCH #0, step #788] loss: 4.788237719783614\n",
      "[EPOCH #0, step #790] loss: 4.788242586644651\n",
      "[EPOCH #0, step #792] loss: 4.788106278543364\n",
      "[EPOCH #0, step #794] loss: 4.788148996365146\n",
      "[EPOCH #0, step #796] loss: 4.7877251422839\n",
      "[EPOCH #0, step #798] loss: 4.787847987999755\n",
      "[EPOCH #0, step #800] loss: 4.78756926985418\n",
      "[EPOCH #0, step #802] loss: 4.78741409294632\n",
      "[EPOCH #0, step #804] loss: 4.787208773926919\n",
      "[EPOCH #0, step #806] loss: 4.7875697680713225\n",
      "[EPOCH #0, step #808] loss: 4.787736434724334\n",
      "[EPOCH #0, step #810] loss: 4.7877023369992555\n",
      "[EPOCH #0, step #812] loss: 4.787692391417856\n",
      "[EPOCH #0, step #814] loss: 4.78773852389283\n",
      "[EPOCH #0, step #816] loss: 4.787663791871275\n",
      "[EPOCH #0, step #818] loss: 4.7878621774424275\n",
      "[EPOCH #0, step #820] loss: 4.78780730546029\n",
      "[EPOCH #0, step #822] loss: 4.787864405793089\n",
      "[EPOCH #0, step #824] loss: 4.787798905228123\n",
      "[EPOCH #0, step #826] loss: 4.7880191243689465\n",
      "[EPOCH #0, step #828] loss: 4.788378644766077\n",
      "[EPOCH #0, step #830] loss: 4.788532514147523\n",
      "[EPOCH #0, step #832] loss: 4.788561259998994\n",
      "[EPOCH #0, step #834] loss: 4.788571598144348\n",
      "[EPOCH #0, step #836] loss: 4.788495979855992\n",
      "[EPOCH #0, step #838] loss: 4.788721260780089\n",
      "[EPOCH #0, step #840] loss: 4.788706776078051\n",
      "[EPOCH #0, step #842] loss: 4.788867975893394\n",
      "[EPOCH #0, step #844] loss: 4.789289140419142\n",
      "[EPOCH #0, step #846] loss: 4.789288930656496\n",
      "[EPOCH #0, step #848] loss: 4.789159641108609\n",
      "[EPOCH #0, step #850] loss: 4.789326779850781\n",
      "[EPOCH #0, step #852] loss: 4.789117624723338\n",
      "[EPOCH #0, step #854] loss: 4.789131711100974\n",
      "[EPOCH #0, step #856] loss: 4.789052242715273\n",
      "[EPOCH #0, step #858] loss: 4.789148808080742\n",
      "[EPOCH #0, step #860] loss: 4.789110821160151\n",
      "[EPOCH #0, step #862] loss: 4.789041539056276\n",
      "[EPOCH #0, step #864] loss: 4.7892529096217515\n",
      "[EPOCH #0, step #866] loss: 4.78935933525587\n",
      "[EPOCH #0, step #868] loss: 4.789273514983569\n",
      "[EPOCH #0, step #870] loss: 4.789393937957273\n",
      "[EPOCH #0, step #872] loss: 4.7893765991359505\n",
      "[EPOCH #0, step #874] loss: 4.789276288168771\n",
      "[EPOCH #0, step #876] loss: 4.788711618534379\n",
      "[EPOCH #0, step #878] loss: 4.788642432502512\n",
      "[EPOCH #0, step #880] loss: 4.788898904261335\n",
      "[EPOCH #0, step #882] loss: 4.78905437594767\n",
      "[EPOCH #0, step #884] loss: 4.788861855544613\n",
      "[EPOCH #0, step #886] loss: 4.7887231510682735\n",
      "[EPOCH #0, step #888] loss: 4.788943378675909\n",
      "[EPOCH #0, step #890] loss: 4.788968078215114\n",
      "[EPOCH #0, step #892] loss: 4.788920744803024\n",
      "[EPOCH #0, step #894] loss: 4.788534992367196\n",
      "[EPOCH #0, step #896] loss: 4.788653675661969\n",
      "[EPOCH #0, step #898] loss: 4.788429344058435\n",
      "[EPOCH #0, step #900] loss: 4.788638506030931\n",
      "[EPOCH #0, step #902] loss: 4.7885420414828515\n",
      "[EPOCH #0, step #904] loss: 4.788491153717041\n",
      "[EPOCH #0, step #906] loss: 4.788310862691557\n",
      "[EPOCH #0, step #908] loss: 4.788144037668461\n",
      "[EPOCH #0, step #910] loss: 4.788127993385825\n",
      "[EPOCH #0, step #912] loss: 4.788093853623557\n",
      "[EPOCH #0, step #914] loss: 4.787997791936489\n",
      "[EPOCH #0, step #916] loss: 4.7878643184607785\n",
      "[EPOCH #0, step #918] loss: 4.787867018914456\n",
      "[EPOCH #0, step #920] loss: 4.788036060643895\n",
      "[EPOCH #0, step #922] loss: 4.7882931317694135\n",
      "[EPOCH #0, step #924] loss: 4.787975674191037\n",
      "[EPOCH #0, step #926] loss: 4.788138154512342\n",
      "[EPOCH #0, step #928] loss: 4.7882439019990555\n",
      "[EPOCH #0, step #930] loss: 4.788482963590694\n",
      "[EPOCH #0, step #932] loss: 4.788803349992725\n",
      "[EPOCH #0, step #934] loss: 4.788764955653226\n",
      "[EPOCH #0, step #936] loss: 4.788957125603708\n",
      "[EPOCH #0, step #938] loss: 4.789157591688747\n",
      "[EPOCH #0, step #940] loss: 4.789434855093232\n",
      "[EPOCH #0, step #942] loss: 4.789513366472683\n",
      "[EPOCH #0, step #944] loss: 4.7895002642636575\n",
      "[EPOCH #0, step #946] loss: 4.7897342447242615\n",
      "[EPOCH #0, step #948] loss: 4.789962036968911\n",
      "[EPOCH #0, step #950] loss: 4.790122764218368\n",
      "[EPOCH #0, step #952] loss: 4.789762470429491\n",
      "[EPOCH #0, step #954] loss: 4.79032602759556\n",
      "[EPOCH #0, step #956] loss: 4.790244074077068\n",
      "[EPOCH #0, step #958] loss: 4.790430360342588\n",
      "[EPOCH #0, step #960] loss: 4.790568329417122\n",
      "[EPOCH #0, step #962] loss: 4.790292398209141\n",
      "[EPOCH #0, step #964] loss: 4.790261339276566\n",
      "[EPOCH #0, step #966] loss: 4.79025769603536\n",
      "[EPOCH #0, step #968] loss: 4.790300263462913\n",
      "[EPOCH #0, step #970] loss: 4.790225482748171\n",
      "[EPOCH #0, step #972] loss: 4.790200322161966\n",
      "[EPOCH #0, step #974] loss: 4.790064111856314\n",
      "[EPOCH #0, step #976] loss: 4.79018042836731\n",
      "[EPOCH #0, step #978] loss: 4.789943068702082\n",
      "[EPOCH #0, step #980] loss: 4.789849679890515\n",
      "[EPOCH #0, step #982] loss: 4.78974163932451\n",
      "[EPOCH #0, step #984] loss: 4.790026536447748\n",
      "[EPOCH #0, step #986] loss: 4.790006356398748\n",
      "[EPOCH #0, step #988] loss: 4.789749388506729\n",
      "[EPOCH #0, step #990] loss: 4.789598986070404\n",
      "[EPOCH #0, step #992] loss: 4.7896122154512435\n",
      "[EPOCH #0, step #994] loss: 4.789465831871608\n",
      "[EPOCH #0, step #996] loss: 4.789404445808893\n",
      "[EPOCH #0, step #998] loss: 4.789495699159853\n",
      "[EPOCH #0, step #1000] loss: 4.789663097598813\n",
      "[EPOCH #0, step #1002] loss: 4.789647028667264\n",
      "[EPOCH #0, step #1004] loss: 4.789760862891354\n",
      "[EPOCH #0, step #1006] loss: 4.789426810218182\n",
      "[EPOCH #0, step #1008] loss: 4.789534421813027\n",
      "[EPOCH #0, step #1010] loss: 4.789490179539198\n",
      "[EPOCH #0, step #1012] loss: 4.78924094912801\n",
      "[EPOCH #0, step #1014] loss: 4.789283311778101\n",
      "[EPOCH #0, step #1016] loss: 4.789255748925073\n",
      "[EPOCH #0, step #1018] loss: 4.789412988413773\n",
      "[EPOCH #0, step #1020] loss: 4.789084056682381\n",
      "[EPOCH #0, step #1022] loss: 4.788920320024238\n",
      "[EPOCH #0, step #1024] loss: 4.788558429159769\n",
      "[EPOCH #0, step #1026] loss: 4.788767400944012\n",
      "[EPOCH #0, step #1028] loss: 4.789262391156195\n",
      "[EPOCH #0, step #1030] loss: 4.78934379183587\n",
      "[EPOCH #0, step #1032] loss: 4.789494381523317\n",
      "[EPOCH #0, step #1034] loss: 4.789715467904501\n",
      "[EPOCH #0, step #1036] loss: 4.789798902568615\n",
      "[EPOCH #0, step #1038] loss: 4.789979743773944\n",
      "[EPOCH #0, step #1040] loss: 4.789827131056992\n",
      "[EPOCH #0, step #1042] loss: 4.790229216289429\n",
      "[EPOCH #0, step #1044] loss: 4.7901954290399145\n",
      "[EPOCH #0, step #1046] loss: 4.790219939996088\n",
      "[EPOCH #0, step #1048] loss: 4.790334396525948\n",
      "[EPOCH #0, step #1050] loss: 4.790429024102005\n",
      "[EPOCH #0, step #1052] loss: 4.790654457306024\n",
      "[EPOCH #0, step #1054] loss: 4.790884549470874\n",
      "[EPOCH #0, step #1056] loss: 4.790861432464369\n",
      "[EPOCH #0, step #1058] loss: 4.790900699590264\n",
      "[EPOCH #0, step #1060] loss: 4.791146388040412\n",
      "[EPOCH #0, step #1062] loss: 4.791133058822503\n",
      "[EPOCH #0, step #1064] loss: 4.791084696989104\n",
      "[EPOCH #0, step #1066] loss: 4.790918840910636\n",
      "[EPOCH #0, step #1068] loss: 4.790934740660679\n",
      "[EPOCH #0, step #1070] loss: 4.790912658894429\n",
      "[EPOCH #0, step #1072] loss: 4.791177669642467\n",
      "[EPOCH #0, step #1074] loss: 4.791301340945932\n",
      "[EPOCH #0, step #1076] loss: 4.791176627273347\n",
      "[EPOCH #0, step #1078] loss: 4.791276796533622\n",
      "[EPOCH #0, step #1080] loss: 4.791084541423138\n",
      "[EPOCH #0, step #1082] loss: 4.791232439729756\n",
      "[EPOCH #0, step #1084] loss: 4.791163779843238\n",
      "[EPOCH #0, step #1086] loss: 4.791224061620269\n",
      "[EPOCH #0, step #1088] loss: 4.790983160227346\n",
      "[EPOCH #0, step #1090] loss: 4.790992055870217\n",
      "[EPOCH #0, step #1092] loss: 4.791014361141586\n",
      "[EPOCH #0, step #1094] loss: 4.791232566746403\n",
      "[EPOCH #0, step #1096] loss: 4.790844325271649\n",
      "[EPOCH #0, step #1098] loss: 4.7908622326907295\n",
      "[EPOCH #0, step #1100] loss: 4.790620445663771\n",
      "[EPOCH #0, step #1102] loss: 4.790929310125107\n",
      "[EPOCH #0, step #1104] loss: 4.79093462352839\n",
      "[EPOCH #0, step #1106] loss: 4.790547262793096\n",
      "[EPOCH #0, step #1108] loss: 4.790420848684767\n",
      "[EPOCH #0, step #1110] loss: 4.790101984403457\n",
      "[EPOCH #0, step #1112] loss: 4.790269706448455\n",
      "[EPOCH #0, step #1114] loss: 4.790412839111191\n",
      "[EPOCH #0, step #1116] loss: 4.790473350159484\n",
      "[EPOCH #0, step #1118] loss: 4.790432565653292\n",
      "[EPOCH #0, step #1120] loss: 4.790897540807936\n",
      "[EPOCH #0, step #1122] loss: 4.790996772735541\n",
      "[EPOCH #0, step #1124] loss: 4.791025123596191\n",
      "[EPOCH #0, step #1126] loss: 4.791110282464691\n",
      "[EPOCH #0, step #1128] loss: 4.790984458686822\n",
      "[EPOCH #0, step #1130] loss: 4.791133256215737\n",
      "[EPOCH #0, step #1132] loss: 4.790852411595047\n",
      "[EPOCH #0, step #1134] loss: 4.790798399416886\n",
      "[EPOCH #0, step #1136] loss: 4.790608093627419\n",
      "[EPOCH #0, step #1138] loss: 4.790668372839977\n",
      "[EPOCH #0, step #1140] loss: 4.79067936787785\n",
      "[EPOCH #0, step #1142] loss: 4.790953665372059\n",
      "[EPOCH #0, step #1144] loss: 4.790854828014124\n",
      "[EPOCH #0, step #1146] loss: 4.790789888958985\n",
      "[EPOCH #0, step #1148] loss: 4.790661202605856\n",
      "[EPOCH #0, step #1150] loss: 4.7905690245375645\n",
      "[EPOCH #0, step #1152] loss: 4.790741799711044\n",
      "[EPOCH #0, step #1154] loss: 4.790435387252213\n",
      "[EPOCH #0, step #1156] loss: 4.790683601267294\n",
      "[EPOCH #0, step #1158] loss: 4.790795988621024\n",
      "[EPOCH #0, step #1160] loss: 4.790664950082468\n",
      "[EPOCH #0, step #1162] loss: 4.790688859524649\n",
      "[EPOCH #0, step #1164] loss: 4.7906979217038135\n",
      "[EPOCH #0, step #1166] loss: 4.7905265032581115\n",
      "[EPOCH #0, step #1168] loss: 4.790206908771785\n",
      "[EPOCH #0, step #1170] loss: 4.790401799163688\n",
      "[EPOCH #0, step #1172] loss: 4.790322222469815\n",
      "[EPOCH #0, step #1174] loss: 4.79033435902697\n",
      "[EPOCH #0, step #1176] loss: 4.7904544767932355\n",
      "[EPOCH #0, step #1178] loss: 4.790108031191191\n",
      "[EPOCH #0, step #1180] loss: 4.7900566566202425\n",
      "[EPOCH #0, step #1182] loss: 4.790098216104386\n",
      "[EPOCH #0, step #1184] loss: 4.790002643005757\n",
      "[EPOCH #0, step #1186] loss: 4.790035165049193\n",
      "[EPOCH #0, step #1188] loss: 4.790144096210887\n",
      "[EPOCH #0, step #1190] loss: 4.7901952060504085\n",
      "[EPOCH #0, step #1192] loss: 4.790116071501202\n",
      "[EPOCH #0, step #1194] loss: 4.790352708026455\n",
      "[EPOCH #0, step #1196] loss: 4.790457493679266\n",
      "[EPOCH #0, step #1198] loss: 4.790615820705741\n",
      "[EPOCH #0, step #1200] loss: 4.790591438048884\n",
      "[EPOCH #0, step #1202] loss: 4.790689624951268\n",
      "[EPOCH #0, step #1204] loss: 4.790774396840962\n",
      "[EPOCH #0, step #1206] loss: 4.791089963004326\n",
      "[EPOCH #0, step #1208] loss: 4.791198907951563\n",
      "[EPOCH #0, step #1210] loss: 4.791303425165959\n",
      "[EPOCH #0, step #1212] loss: 4.791216809790879\n",
      "[EPOCH #0, step #1214] loss: 4.791354686046334\n",
      "[EPOCH #0, step #1216] loss: 4.791387486673322\n",
      "[EPOCH #0, step #1218] loss: 4.791609709335215\n",
      "[EPOCH #0, step #1220] loss: 4.791739536444736\n",
      "[EPOCH #0, step #1222] loss: 4.791818728723784\n",
      "[EPOCH #0, step #1224] loss: 4.791847752162388\n",
      "[EPOCH #0, step #1226] loss: 4.791639939691346\n",
      "[EPOCH #0, step #1228] loss: 4.7920460022009905\n",
      "[EPOCH #0, step #1230] loss: 4.792085349705043\n",
      "[EPOCH #0, step #1232] loss: 4.792022652482948\n",
      "[EPOCH #0, step #1234] loss: 4.7921091002491325\n",
      "[EPOCH #0, step #1236] loss: 4.792092177592272\n",
      "[EPOCH #0, step #1238] loss: 4.791951125239633\n",
      "[EPOCH #0, step #1240] loss: 4.792084447229033\n",
      "[EPOCH #0, step #1242] loss: 4.791679083485807\n",
      "[EPOCH #0, step #1244] loss: 4.791471689293184\n",
      "[EPOCH #0, step #1246] loss: 4.791298965692711\n",
      "[EPOCH #0, step #1248] loss: 4.79119870450804\n",
      "[EPOCH #0, step #1250] loss: 4.791282888606108\n",
      "[EPOCH #0, step #1252] loss: 4.7914776889590955\n",
      "[EPOCH #0, step #1254] loss: 4.791579444474908\n",
      "[EPOCH #0, step #1256] loss: 4.79147972108451\n",
      "[EPOCH #0, step #1258] loss: 4.791569921873788\n",
      "[EPOCH #0, step #1260] loss: 4.79188127956345\n",
      "[EPOCH #0, step #1262] loss: 4.791949608546821\n",
      "[EPOCH #0, step #1264] loss: 4.792233880612219\n",
      "[EPOCH #0, step #1266] loss: 4.792192989761847\n",
      "[EPOCH #0, step #1268] loss: 4.791999907828016\n",
      "[EPOCH #0, step #1270] loss: 4.792000859483408\n",
      "[EPOCH #0, step #1272] loss: 4.792002084961297\n",
      "[EPOCH #0, step #1274] loss: 4.791882771884694\n",
      "[EPOCH #0, step #1276] loss: 4.792070376845011\n",
      "[EPOCH #0, step #1278] loss: 4.792137095292533\n",
      "[EPOCH #0, step #1280] loss: 4.792338313579932\n",
      "[EPOCH #0, step #1282] loss: 4.792345474032063\n",
      "[EPOCH #0, step #1284] loss: 4.7923054016053905\n",
      "[EPOCH #0, step #1286] loss: 4.792585753060721\n",
      "[EPOCH #0, step #1288] loss: 4.7923645754948065\n",
      "[EPOCH #0, step #1290] loss: 4.792251229747888\n",
      "[EPOCH #0, step #1292] loss: 4.7922554772119605\n",
      "[EPOCH #0, step #1294] loss: 4.791987740947473\n",
      "[EPOCH #0, step #1296] loss: 4.791983426858024\n",
      "[EPOCH #0, step #1298] loss: 4.79173527578834\n",
      "[EPOCH #0, step #1300] loss: 4.791807241021991\n",
      "[EPOCH #0, step #1302] loss: 4.791845072441072\n",
      "[EPOCH #0, step #1304] loss: 4.791915523412127\n",
      "[EPOCH #0, step #1306] loss: 4.791836979741255\n",
      "[EPOCH #0, step #1308] loss: 4.791710439394957\n",
      "[EPOCH #0, step #1310] loss: 4.791890217092181\n",
      "[EPOCH #0, step #1312] loss: 4.791922927356729\n",
      "[EPOCH #0, step #1314] loss: 4.791989243801102\n",
      "[EPOCH #0, step #1316] loss: 4.791922504103537\n",
      "[EPOCH #0, step #1318] loss: 4.791775625704634\n",
      "[EPOCH #0, step #1320] loss: 4.791741878674122\n",
      "[EPOCH #0, step #1322] loss: 4.791728470240771\n",
      "[EPOCH #0, step #1324] loss: 4.7915915924648065\n",
      "[EPOCH #0, step #1326] loss: 4.79146980143348\n",
      "[EPOCH #0, step #1328] loss: 4.791616283743968\n",
      "[EPOCH #0, step #1330] loss: 4.791582032668689\n",
      "[EPOCH #0, step #1332] loss: 4.791502940413295\n",
      "[EPOCH #0, step #1334] loss: 4.791728089364727\n",
      "[EPOCH #0, step #1336] loss: 4.791810935526172\n",
      "[EPOCH #0, step #1338] loss: 4.79202142164189\n",
      "[EPOCH #0, step #1340] loss: 4.792194501576719\n",
      "[EPOCH #0, step #1342] loss: 4.792242881351679\n",
      "[EPOCH #0, step #1344] loss: 4.7922176385900785\n",
      "[EPOCH #0, step #1346] loss: 4.792191351088402\n",
      "[EPOCH #0, step #1348] loss: 4.791771490013626\n",
      "[EPOCH #0, step #1350] loss: 4.791956515068659\n",
      "[EPOCH #0, step #1352] loss: 4.791774156616955\n",
      "[EPOCH #0, step #1354] loss: 4.791756581732268\n",
      "[EPOCH #0, step #1356] loss: 4.791723115705969\n",
      "[EPOCH #0, step #1358] loss: 4.791630153361272\n",
      "[EPOCH #0, step #1360] loss: 4.791505906793845\n",
      "[EPOCH #0, step #1362] loss: 4.791893296791015\n",
      "[EPOCH #0, step #1364] loss: 4.791899953220353\n",
      "[EPOCH #0, step #1366] loss: 4.792036347434568\n",
      "[EPOCH #0, step #1368] loss: 4.79179307332955\n",
      "[EPOCH #0, step #1370] loss: 4.791943243348103\n",
      "[EPOCH #0, step #1372] loss: 4.791963015345874\n",
      "[EPOCH #0, step #1374] loss: 4.791647390538996\n",
      "[EPOCH #0, step #1376] loss: 4.791777481613769\n",
      "[EPOCH #0, step #1378] loss: 4.791904958112245\n",
      "[EPOCH #0, step #1380] loss: 4.791765544140019\n",
      "[EPOCH #0, step #1382] loss: 4.791523698612304\n",
      "[EPOCH #0, step #1384] loss: 4.7916881296178495\n",
      "[EPOCH #0, step #1386] loss: 4.791725058648426\n",
      "[EPOCH #0, step #1388] loss: 4.791739980122269\n",
      "[EPOCH #0, step #1390] loss: 4.791598870889649\n",
      "[EPOCH #0, step #1392] loss: 4.791713940181581\n",
      "[EPOCH #0, step #1394] loss: 4.791667773133965\n",
      "[EPOCH #0, step #1396] loss: 4.791502452186798\n",
      "[EPOCH #0, step #1398] loss: 4.791724983158071\n",
      "[EPOCH #0, step #1400] loss: 4.79171498650572\n",
      "[EPOCH #0, step #1402] loss: 4.791531539014298\n",
      "[EPOCH #0, step #1404] loss: 4.791717949456592\n",
      "[EPOCH #0, step #1406] loss: 4.791707693640865\n",
      "[EPOCH #0, step #1408] loss: 4.791637827438019\n",
      "[EPOCH #0, step #1410] loss: 4.791611282475699\n",
      "[EPOCH #0, step #1412] loss: 4.79156130624003\n",
      "[EPOCH #0, step #1414] loss: 4.7916782301643295\n",
      "[EPOCH #0, step #1416] loss: 4.791925322277578\n",
      "[EPOCH #0, step #1418] loss: 4.792063241613172\n",
      "[EPOCH #0, step #1420] loss: 4.79192847688797\n",
      "[EPOCH #0, step #1422] loss: 4.791967261281553\n",
      "[EPOCH #0, step #1424] loss: 4.792098763114527\n",
      "[EPOCH #0, step #1426] loss: 4.792076323027366\n",
      "[EPOCH #0, step #1428] loss: 4.792331342183267\n",
      "[EPOCH #0, step #1430] loss: 4.792596362171933\n",
      "[EPOCH #0, step #1432] loss: 4.7924339912691405\n",
      "[EPOCH #0, step #1434] loss: 4.792347598989666\n",
      "[EPOCH #0, step #1436] loss: 4.792338950317107\n",
      "[EPOCH #0, step #1438] loss: 4.792387836421836\n",
      "[EPOCH #0, step #1440] loss: 4.792426964708867\n",
      "[EPOCH #0, step #1442] loss: 4.7925716897141\n",
      "[EPOCH #0, step #1444] loss: 4.792693847603451\n",
      "[EPOCH #0, step #1446] loss: 4.7925886494914005\n",
      "[EPOCH #0, step #1448] loss: 4.792598872122229\n",
      "[EPOCH #0, step #1450] loss: 4.792500622760502\n",
      "[EPOCH #0, step #1452] loss: 4.792443345352772\n",
      "[EPOCH #0, step #1454] loss: 4.79245250839548\n",
      "[EPOCH #0, step #1456] loss: 4.792493135534744\n",
      "[EPOCH #0, step #1458] loss: 4.7924456472181145\n",
      "[EPOCH #0, step #1460] loss: 4.792439672246851\n",
      "[EPOCH #0, step #1462] loss: 4.792484371122724\n",
      "[EPOCH #0, step #1464] loss: 4.792323663861271\n",
      "[EPOCH #0, step #1466] loss: 4.792219889432131\n",
      "[EPOCH #0, step #1468] loss: 4.792449425320141\n",
      "[EPOCH #0, step #1470] loss: 4.792429442635854\n",
      "[EPOCH #0, step #1472] loss: 4.792252075081364\n",
      "[EPOCH #0, step #1474] loss: 4.7922983214814785\n",
      "[EPOCH #0, step #1476] loss: 4.792201697221057\n",
      "[EPOCH #0, step #1478] loss: 4.792358871083392\n",
      "[EPOCH #0, step #1480] loss: 4.792559992695563\n",
      "[EPOCH #0, step #1482] loss: 4.7925866847896765\n",
      "[EPOCH #0, step #1484] loss: 4.792531707712295\n",
      "[EPOCH #0, step #1486] loss: 4.792593318179861\n",
      "[EPOCH #0, step #1488] loss: 4.792690849368247\n",
      "[EPOCH #0, step #1490] loss: 4.792718445990887\n",
      "[EPOCH #0, step #1492] loss: 4.792641103467283\n",
      "[EPOCH #0, step #1494] loss: 4.792891683546595\n",
      "[EPOCH #0, step #1496] loss: 4.792828006272963\n",
      "[EPOCH #0, step #1498] loss: 4.792816577234452\n",
      "[EPOCH #0, step #1500] loss: 4.792744181300702\n",
      "[EPOCH #0, step #1502] loss: 4.792724763561866\n",
      "[EPOCH #0, step #1504] loss: 4.79291925081779\n",
      "[EPOCH #0, step #1506] loss: 4.793084797672507\n",
      "[EPOCH #0, step #1508] loss: 4.792851315656982\n",
      "[EPOCH #0, step #1510] loss: 4.7928820517897845\n",
      "[EPOCH #0, step #1512] loss: 4.792967452170907\n",
      "[EPOCH #0, step #1514] loss: 4.793030188658056\n",
      "[EPOCH #0, step #1516] loss: 4.79287190742216\n",
      "[EPOCH #0, step #1518] loss: 4.792975082924839\n",
      "[EPOCH #0, step #1520] loss: 4.792968044306086\n",
      "[EPOCH #0, step #1522] loss: 4.79295182275052\n",
      "[EPOCH #0, step #1524] loss: 4.792843779892218\n",
      "[EPOCH #0, step #1526] loss: 4.792773145901743\n",
      "[EPOCH #0, step #1528] loss: 4.79289298091861\n",
      "[EPOCH #0, step #1530] loss: 4.792842511800595\n",
      "[EPOCH #0, step #1532] loss: 4.792783440442468\n",
      "[EPOCH #0, step #1534] loss: 4.79251653565646\n",
      "[EPOCH #0, step #1536] loss: 4.7927580466112225\n",
      "[EPOCH #0, step #1538] loss: 4.792741237018231\n",
      "[EPOCH #0, step #1540] loss: 4.792744993931438\n",
      "[EPOCH #0, step #1542] loss: 4.79268887069267\n",
      "[EPOCH #0, step #1544] loss: 4.792756303151449\n",
      "[EPOCH #0, step #1546] loss: 4.792734058579862\n",
      "[EPOCH #0, step #1548] loss: 4.792692712078255\n",
      "[EPOCH #0, step #1550] loss: 4.792416017644256\n",
      "[EPOCH #0, step #1552] loss: 4.792523434449225\n",
      "[EPOCH #0, step #1554] loss: 4.79264509440235\n",
      "[EPOCH #0, step #1556] loss: 4.792679913777453\n",
      "[EPOCH #0, step #1558] loss: 4.792629571355559\n",
      "[EPOCH #0, step #1560] loss: 4.792679742694588\n",
      "[EPOCH #0, step #1562] loss: 4.792533902578909\n",
      "[EPOCH #0, step #1564] loss: 4.792546305488854\n",
      "[EPOCH #0, step #1566] loss: 4.792611964736383\n",
      "[EPOCH #0, step #1568] loss: 4.792621491895959\n",
      "[EPOCH #0, step #1570] loss: 4.792657506154799\n",
      "[EPOCH #0, step #1572] loss: 4.792699182101745\n",
      "[EPOCH #0, step #1574] loss: 4.792803898463173\n",
      "[EPOCH #0, step #1576] loss: 4.792699116578039\n",
      "[EPOCH #0, step #1578] loss: 4.792747666671202\n",
      "[EPOCH #0, step #1580] loss: 4.792958857966404\n",
      "[EPOCH #0, step #1582] loss: 4.793070874783485\n",
      "[EPOCH #0, step #1584] loss: 4.793313039364499\n",
      "[EPOCH #0, step #1586] loss: 4.793260401874199\n",
      "[EPOCH #0, step #1588] loss: 4.793314927174957\n",
      "[EPOCH #0, step #1590] loss: 4.7933696681489595\n",
      "[EPOCH #0, step #1592] loss: 4.793372710096858\n",
      "[EPOCH #0, step #1594] loss: 4.793220450960357\n",
      "[EPOCH #0, step #1596] loss: 4.793282235945174\n",
      "[EPOCH #0, step #1598] loss: 4.793140238117172\n",
      "[EPOCH #0, step #1600] loss: 4.79335331216892\n",
      "[EPOCH #0, step #1602] loss: 4.793268269177756\n",
      "[EPOCH #0, step #1604] loss: 4.793300427826023\n",
      "[EPOCH #0, step #1606] loss: 4.793281887507839\n",
      "[EPOCH #0, step #1608] loss: 4.79324163984556\n",
      "[EPOCH #0, step #1610] loss: 4.793257878183357\n",
      "[EPOCH #0, step #1612] loss: 4.793295181839642\n",
      "[EPOCH #0, step #1614] loss: 4.793317354279037\n",
      "[EPOCH #0, step #1616] loss: 4.793244958145056\n",
      "[EPOCH #0, step #1618] loss: 4.793388405576615\n",
      "[EPOCH #0, step #1620] loss: 4.793465701096739\n",
      "[EPOCH #0, step #1622] loss: 4.793422064660731\n",
      "[EPOCH #0, step #1624] loss: 4.793565927358774\n",
      "[EPOCH #0, step #1626] loss: 4.793727864791223\n",
      "[EPOCH #0, step #1628] loss: 4.793796986579309\n",
      "[EPOCH #0, step #1630] loss: 4.793733714329984\n",
      "[EPOCH #0, step #1632] loss: 4.793761255400939\n",
      "[EPOCH #0, step #1634] loss: 4.793791978001959\n",
      "[EPOCH #0, step #1636] loss: 4.793771652601518\n",
      "[EPOCH #0, step #1638] loss: 4.793812963859274\n",
      "[EPOCH #0, step #1640] loss: 4.79383261533571\n",
      "[EPOCH #0, step #1642] loss: 4.794065978421138\n",
      "[EPOCH #0, step #1644] loss: 4.794106819419513\n",
      "[EPOCH #0, step #1646] loss: 4.793916767122678\n",
      "[EPOCH #0, step #1648] loss: 4.793852663531746\n",
      "[EPOCH #0, step #1650] loss: 4.7940200427891195\n",
      "[EPOCH #0, step #1652] loss: 4.793807971859728\n",
      "[EPOCH #0, step #1654] loss: 4.793900916799678\n",
      "[EPOCH #0, step #1656] loss: 4.793986304092638\n",
      "[EPOCH #0, step #1658] loss: 4.7939574547457795\n",
      "[EPOCH #0, step #1660] loss: 4.793991586948144\n",
      "[EPOCH #0, step #1662] loss: 4.7939014122275365\n",
      "[EPOCH #0, step #1664] loss: 4.793829817671676\n",
      "[EPOCH #0, step #1666] loss: 4.793603538680234\n",
      "[EPOCH #0, step #1668] loss: 4.793633882123743\n",
      "[EPOCH #0, step #1670] loss: 4.793465439963669\n",
      "[EPOCH #0, step #1672] loss: 4.79346419948343\n",
      "[EPOCH #0, step #1674] loss: 4.793566134723265\n",
      "[EPOCH #0, step #1676] loss: 4.793399478113929\n",
      "[EPOCH #0, step #1678] loss: 4.793361128193059\n",
      "[EPOCH #0, step #1680] loss: 4.793239734769931\n",
      "[EPOCH #0, step #1682] loss: 4.793112132638534\n",
      "[EPOCH #0, step #1684] loss: 4.793146214923802\n",
      "[EPOCH #0, step #1686] loss: 4.793089989875291\n",
      "[EPOCH #0, step #1688] loss: 4.793158921918762\n",
      "[EPOCH #0, step #1690] loss: 4.793236072919272\n",
      "[EPOCH #0, step #1692] loss: 4.793298762434697\n",
      "[EPOCH #0, step #1694] loss: 4.793144478699451\n",
      "[EPOCH #0, step #1696] loss: 4.79316020897015\n",
      "[EPOCH #0, step #1698] loss: 4.793177456207455\n",
      "[EPOCH #0, step #1700] loss: 4.793144495468992\n",
      "[EPOCH #0, step #1702] loss: 4.7930367747545946\n",
      "[EPOCH #0, step #1704] loss: 4.792992845518498\n",
      "[EPOCH #0, step #1706] loss: 4.793051127180693\n",
      "[EPOCH #0, step #1708] loss: 4.793018893793775\n",
      "[EPOCH #0, step #1710] loss: 4.793270613839235\n",
      "[EPOCH #0, step #1712] loss: 4.79326430246695\n",
      "[EPOCH #0, step #1714] loss: 4.793369291544655\n",
      "[EPOCH #0, step #1716] loss: 4.7934424070403825\n",
      "[EPOCH #0, step #1718] loss: 4.793555961229414\n",
      "[EPOCH #0, step #1720] loss: 4.793631723770771\n",
      "[EPOCH #0, step #1722] loss: 4.793867345058288\n",
      "[EPOCH #0, step #1724] loss: 4.793862946551779\n",
      "[EPOCH #0, step #1726] loss: 4.793854875238769\n",
      "[EPOCH #0, step #1728] loss: 4.793815684222016\n",
      "[EPOCH #0, step #1730] loss: 4.793553353182364\n",
      "[EPOCH #0, step #1732] loss: 4.79356825289806\n",
      "[EPOCH #0, step #1734] loss: 4.793480710955793\n",
      "[EPOCH #0, step #1736] loss: 4.793438541937231\n",
      "[EPOCH #0, step #1738] loss: 4.793458208124688\n",
      "[EPOCH #0, step #1740] loss: 4.79343173067026\n",
      "[EPOCH #0, step #1742] loss: 4.7934816951937735\n",
      "[EPOCH #0, step #1744] loss: 4.793430877346705\n",
      "[EPOCH #0, step #1746] loss: 4.793471494127698\n",
      "[EPOCH #0, step #1748] loss: 4.7936143828774265\n",
      "[EPOCH #0, step #1750] loss: 4.793674513110428\n",
      "[EPOCH #0, step #1752] loss: 4.793997370169901\n",
      "[EPOCH #0, step #1754] loss: 4.793902986477583\n",
      "[EPOCH #0, step #1756] loss: 4.7939425556918875\n",
      "[EPOCH #0, step #1758] loss: 4.7940143253940155\n",
      "[EPOCH #0, step #1760] loss: 4.794175051884649\n",
      "[EPOCH #0, step #1762] loss: 4.794092676548949\n",
      "[EPOCH #0, step #1764] loss: 4.7940596769619255\n",
      "[EPOCH #0, step #1766] loss: 4.794213713261119\n",
      "[EPOCH #0, step #1768] loss: 4.794233594390886\n",
      "[EPOCH #0, step #1770] loss: 4.794316034139313\n",
      "[EPOCH #0, step #1772] loss: 4.794397155645535\n",
      "[EPOCH #0, step #1774] loss: 4.794231793309601\n",
      "[EPOCH #0, step #1776] loss: 4.794033753046778\n",
      "[EPOCH #0, step #1778] loss: 4.793928068631683\n",
      "[EPOCH #0, step #1780] loss: 4.793762060044389\n",
      "[EPOCH #0, step #1782] loss: 4.793693014295731\n",
      "[EPOCH #0, step #1784] loss: 4.793737576321727\n",
      "[EPOCH #0, step #1786] loss: 4.793676631438646\n",
      "[EPOCH #0, step #1788] loss: 4.793614486131407\n",
      "[EPOCH #0, step #1790] loss: 4.793485054951151\n",
      "[EPOCH #0, step #1792] loss: 4.793488459419553\n",
      "[EPOCH #0, step #1794] loss: 4.793579513581682\n",
      "[EPOCH #0, step #1796] loss: 4.793495145584386\n",
      "[EPOCH #0, step #1798] loss: 4.793561168085409\n",
      "[EPOCH #0, step #1800] loss: 4.793670147806323\n",
      "[EPOCH #0, step #1802] loss: 4.793525962649751\n",
      "[EPOCH #0, step #1804] loss: 4.793490024154536\n",
      "[EPOCH #0, step #1806] loss: 4.793548139077099\n",
      "[EPOCH #0, step #1808] loss: 4.793583745518859\n",
      "[EPOCH #0, step #1810] loss: 4.793589444531715\n",
      "[EPOCH #0, step #1812] loss: 4.793571609459197\n",
      "[EPOCH #0, step #1814] loss: 4.7935877679136505\n",
      "[EPOCH #0, step #1816] loss: 4.7934250246382835\n",
      "[EPOCH #0, step #1818] loss: 4.7934556639148616\n",
      "[EPOCH #0, step #1820] loss: 4.793379071385961\n",
      "[EPOCH #0, step #1822] loss: 4.793397893764394\n",
      "[EPOCH #0, step #1824] loss: 4.793429539301624\n",
      "[EPOCH #0, step #1826] loss: 4.793508223777399\n",
      "[EPOCH #0, step #1828] loss: 4.793594057146743\n",
      "[EPOCH #0, step #1830] loss: 4.793795516359422\n",
      "[EPOCH #0, step #1832] loss: 4.793635684937906\n",
      "[EPOCH #0, step #1834] loss: 4.793657618582411\n",
      "[EPOCH #0, step #1836] loss: 4.793540491250526\n",
      "[EPOCH #0, step #1838] loss: 4.793780648364782\n",
      "[EPOCH #0, step #1840] loss: 4.793773943027163\n",
      "[EPOCH #0, step #1842] loss: 4.793713668475511\n",
      "[EPOCH #0, step #1844] loss: 4.793696971572834\n",
      "[EPOCH #0, step #1846] loss: 4.7937243332008315\n",
      "[EPOCH #0, step #1848] loss: 4.793687968720869\n",
      "[EPOCH #0, step #1850] loss: 4.793565537850964\n",
      "[EPOCH #0, step #1852] loss: 4.793529065570121\n",
      "[EPOCH #0, step #1854] loss: 4.793502659810521\n",
      "[EPOCH #0, step #1856] loss: 4.793595962134111\n",
      "[EPOCH #0, step #1858] loss: 4.793526597148696\n",
      "[EPOCH #0, step #1860] loss: 4.793455708353575\n",
      "[EPOCH #0, step #1862] loss: 4.793435173782045\n",
      "[EPOCH #0, step #1864] loss: 4.793503464962138\n",
      "[EPOCH #0, step #1866] loss: 4.793620548851042\n",
      "[EPOCH #0, step #1868] loss: 4.793577441179287\n",
      "[EPOCH #0, step #1870] loss: 4.793417403049357\n",
      "[EPOCH #0, step #1872] loss: 4.79351657123026\n",
      "[EPOCH #0, step #1874] loss: 4.793368237050374\n",
      "[EPOCH #0, step #1876] loss: 4.7932524561691485\n",
      "[EPOCH #0, step #1878] loss: 4.793322958294034\n",
      "[EPOCH #0, step #1880] loss: 4.793396104846084\n",
      "[EPOCH #0, step #1882] loss: 4.793397116901643\n",
      "[EPOCH #0, step #1884] loss: 4.793402107198295\n",
      "[EPOCH #0, step #1886] loss: 4.793369334907713\n",
      "[EPOCH #0, step #1888] loss: 4.7931681835443145\n",
      "[EPOCH #0, step #1890] loss: 4.793035330941348\n",
      "[EPOCH #0, step #1892] loss: 4.793241513701135\n",
      "[EPOCH #0, step #1894] loss: 4.7934336463505485\n",
      "[EPOCH #0, step #1896] loss: 4.793551657661112\n",
      "[EPOCH #0, step #1898] loss: 4.793630699159723\n",
      "[EPOCH #0, step #1900] loss: 4.793565290843356\n",
      "[EPOCH #0, step #1902] loss: 4.793422721526025\n",
      "[EPOCH #0, step #1904] loss: 4.793451987539377\n",
      "[EPOCH #0, step #1906] loss: 4.793311858964829\n",
      "[EPOCH #0, step #1908] loss: 4.7932352849861415\n",
      "[EPOCH #0, step #1910] loss: 4.79321921127495\n",
      "[EPOCH #0, step #1912] loss: 4.793267188742718\n",
      "[EPOCH #0, step #1914] loss: 4.793287572786017\n",
      "[EPOCH #0, step #1916] loss: 4.793377045236905\n",
      "[EPOCH #0, step #1918] loss: 4.79330040204643\n",
      "[EPOCH #0, step #1920] loss: 4.793360233803331\n",
      "[EPOCH #0, step #1922] loss: 4.793544728868775\n",
      "[EPOCH #0, step #1924] loss: 4.793500719937411\n",
      "[EPOCH #0, step #1926] loss: 4.793511341690832\n",
      "[EPOCH #0, step #1928] loss: 4.793603499854025\n",
      "[EPOCH #0, step #1930] loss: 4.793517505363255\n",
      "[EPOCH #0, step #1932] loss: 4.79349952032698\n",
      "[EPOCH #0, step #1934] loss: 4.793522759617453\n",
      "[EPOCH #0, step #1936] loss: 4.79339909935096\n",
      "[EPOCH #0, step #1938] loss: 4.7931597809498925\n",
      "[EPOCH #0, step #1940] loss: 4.793025330745455\n",
      "[EPOCH #0, step #1942] loss: 4.792890131995043\n",
      "[EPOCH #0, step #1944] loss: 4.79276357444822\n",
      "[EPOCH #0, step #1946] loss: 4.792613284184985\n",
      "[EPOCH #0, step #1948] loss: 4.792548437005988\n",
      "[EPOCH #0, step #1950] loss: 4.792561850017428\n",
      "[EPOCH #0, step #1952] loss: 4.792621007041325\n",
      "[EPOCH #0, step #1954] loss: 4.792602565892212\n",
      "[EPOCH #0, step #1956] loss: 4.792737014207172\n",
      "[EPOCH #0, step #1958] loss: 4.792680863521122\n",
      "[EPOCH #0, step #1960] loss: 4.7926318422985705\n",
      "[EPOCH #0, step #1962] loss: 4.792591108330888\n",
      "[EPOCH #0, step #1964] loss: 4.792496026017283\n",
      "[EPOCH #0, step #1966] loss: 4.7925646708711005\n",
      "[EPOCH #0, step #1968] loss: 4.79260309363695\n",
      "[EPOCH #0, step #1970] loss: 4.792677502411988\n",
      "[EPOCH #0, step #1972] loss: 4.792691149569017\n",
      "[EPOCH #0, step #1974] loss: 4.792735311532322\n",
      "[EPOCH #0, step #1976] loss: 4.792551330562548\n",
      "[EPOCH #0, step #1978] loss: 4.792611556802512\n",
      "[EPOCH #0, step #1980] loss: 4.7926461067180455\n",
      "[EPOCH #0, step #1982] loss: 4.792433724521208\n",
      "[EPOCH #0, step #1984] loss: 4.792393337989634\n",
      "[EPOCH #0, step #1986] loss: 4.7923213808033776\n",
      "[EPOCH #0, step #1988] loss: 4.792285495573939\n",
      "[EPOCH #0, step #1990] loss: 4.792302236791714\n",
      "[EPOCH #0, step #1992] loss: 4.792280257196804\n",
      "[EPOCH #0, step #1994] loss: 4.79234426541436\n",
      "[EPOCH #0, step #1996] loss: 4.792356176619895\n",
      "[EPOCH #0, step #1998] loss: 4.7923299055686295\n",
      "[EPOCH #0, step #2000] loss: 4.792401549698173\n",
      "[EPOCH #0, step #2002] loss: 4.792399708295071\n",
      "[EPOCH #0, step #2004] loss: 4.792469724336467\n",
      "[EPOCH #0, step #2006] loss: 4.792509755389751\n",
      "[EPOCH #0, step #2008] loss: 4.792585519370064\n",
      "[EPOCH #0, step #2010] loss: 4.792557342677256\n",
      "[EPOCH #0, step #2012] loss: 4.792564444676952\n",
      "[EPOCH #0, step #2014] loss: 4.79254703616386\n",
      "[EPOCH #0, step #2016] loss: 4.792845578493904\n",
      "[EPOCH #0, step #2018] loss: 4.792782093169017\n",
      "[EPOCH #0, step #2020] loss: 4.79279159756358\n",
      "[EPOCH #0, step #2022] loss: 4.792754928722749\n",
      "[EPOCH #0, step #2024] loss: 4.7926588338687095\n",
      "[EPOCH #0, step #2026] loss: 4.792671926984895\n",
      "[EPOCH #0, step #2028] loss: 4.792709344945675\n",
      "[EPOCH #0, step #2030] loss: 4.7926156148718\n",
      "[EPOCH #0, step #2032] loss: 4.7926373732717416\n",
      "[EPOCH #0, step #2034] loss: 4.792775130681968\n",
      "[EPOCH #0, step #2036] loss: 4.792752526470067\n",
      "[EPOCH #0, step #2038] loss: 4.792710475912276\n",
      "[EPOCH #0, step #2040] loss: 4.792853883409664\n",
      "[EPOCH #0, step #2042] loss: 4.792808372368722\n",
      "[EPOCH #0, step #2044] loss: 4.792945291244022\n",
      "[EPOCH #0, step #2046] loss: 4.792988934165161\n",
      "[EPOCH #0, step #2048] loss: 4.7929389584989535\n",
      "[EPOCH #0, step #2050] loss: 4.792930520493365\n",
      "[EPOCH #0, step #2052] loss: 4.79293051360586\n",
      "[EPOCH #0, step #2054] loss: 4.792872731935079\n",
      "[EPOCH #0, step #2056] loss: 4.792709865903878\n",
      "[EPOCH #0, step #2058] loss: 4.792683268589207\n",
      "[EPOCH #0, step #2060] loss: 4.792634370545633\n",
      "[EPOCH #0, step #2062] loss: 4.79265114339994\n",
      "[EPOCH #0, step #2064] loss: 4.792687184008213\n",
      "[EPOCH #0, step #2066] loss: 4.792623747588934\n",
      "[EPOCH #0, step #2068] loss: 4.792534801221343\n",
      "[EPOCH #0, step #2070] loss: 4.792615351269408\n",
      "[EPOCH #0, step #2072] loss: 4.792542499214684\n",
      "[EPOCH #0, step #2074] loss: 4.792624394520219\n",
      "[EPOCH #0, step #2076] loss: 4.792611274113148\n",
      "[EPOCH #0, step #2078] loss: 4.792646829642718\n",
      "[EPOCH #0, step #2080] loss: 4.792680960100239\n",
      "[EPOCH #0, step #2082] loss: 4.792579833326921\n",
      "[EPOCH #0, step #2084] loss: 4.792526716465573\n",
      "[EPOCH #0, step #2086] loss: 4.792595527705589\n",
      "[EPOCH #0, step #2088] loss: 4.792778326028155\n",
      "[EPOCH #0, step #2090] loss: 4.792810177586399\n",
      "[EPOCH #0, step #2092] loss: 4.792866290156715\n",
      "[EPOCH #0, step #2094] loss: 4.7929359615844875\n",
      "[EPOCH #0, step #2096] loss: 4.792818373478647\n",
      "[EPOCH #0, step #2098] loss: 4.792784406435723\n",
      "[EPOCH #0, step #2100] loss: 4.792833106963763\n",
      "[EPOCH #0, step #2102] loss: 4.7926924080152595\n",
      "[EPOCH #0, step #2104] loss: 4.792656459038161\n",
      "[EPOCH #0, step #2106] loss: 4.79279748014098\n",
      "[EPOCH #0, step #2108] loss: 4.792800146123706\n",
      "[EPOCH #0, step #2110] loss: 4.792812951646784\n",
      "[EPOCH #0, step #2112] loss: 4.792625418896366\n",
      "[EPOCH #0, step #2114] loss: 4.792598372430103\n",
      "[EPOCH #0, step #2116] loss: 4.792579426312706\n",
      "[EPOCH #0, step #2118] loss: 4.792623900422064\n",
      "[EPOCH #0, step #2120] loss: 4.79262943775909\n",
      "[EPOCH #0, step #2122] loss: 4.792599023539561\n",
      "[EPOCH #0, step #2124] loss: 4.792709273843204\n",
      "[EPOCH #0, step #2126] loss: 4.792741063955977\n",
      "[EPOCH #0, step #2128] loss: 4.792669594819807\n",
      "[EPOCH #0, step #2130] loss: 4.792650095681415\n",
      "[EPOCH #0, step #2132] loss: 4.7926949294475225\n",
      "[EPOCH #0, step #2134] loss: 4.7926510227927\n",
      "[EPOCH #0, step #2136] loss: 4.792710438586866\n",
      "[EPOCH #0, step #2138] loss: 4.7927268805376775\n",
      "[EPOCH #0, step #2140] loss: 4.792712812722147\n",
      "[EPOCH #0, step #2142] loss: 4.79282447683216\n",
      "[EPOCH #0, step #2144] loss: 4.79269608673238\n",
      "[EPOCH #0, step #2146] loss: 4.792701802033849\n",
      "[EPOCH #0, step #2148] loss: 4.792931587100306\n",
      "[EPOCH #0, step #2150] loss: 4.792993258676214\n",
      "[EPOCH #0, step #2152] loss: 4.793223283593178\n",
      "[EPOCH #0, step #2154] loss: 4.793182800983332\n",
      "[EPOCH #0, step #2156] loss: 4.7933272919723375\n",
      "[EPOCH #0, step #2158] loss: 4.793207107621688\n",
      "[EPOCH #0, step #2160] loss: 4.793143794030185\n",
      "[EPOCH #0, step #2162] loss: 4.793260629012857\n",
      "[EPOCH #0, step #2164] loss: 4.793336326255534\n",
      "[EPOCH #0, step #2166] loss: 4.793495146316155\n",
      "[EPOCH #0, step #2168] loss: 4.793556934240613\n",
      "[EPOCH #0, step #2170] loss: 4.793417760382485\n",
      "[EPOCH #0, step #2172] loss: 4.793394175861708\n",
      "[EPOCH #0, step #2174] loss: 4.793528238274585\n",
      "[EPOCH #0, step #2176] loss: 4.793505633594687\n",
      "[EPOCH #0, step #2178] loss: 4.793493552074459\n",
      "[EPOCH #0, step #2180] loss: 4.793562164332877\n",
      "[EPOCH #0, step #2182] loss: 4.793469899518742\n",
      "[EPOCH #0, step #2184] loss: 4.793538937688692\n",
      "[EPOCH #0, step #2186] loss: 4.793461719821673\n",
      "[EPOCH #0, step #2188] loss: 4.793148301300146\n",
      "[EPOCH #0, step #2190] loss: 4.792989082066559\n",
      "[EPOCH #0, step #2192] loss: 4.793107390675543\n",
      "[EPOCH #0, step #2194] loss: 4.793095197004175\n",
      "[EPOCH #0, step #2196] loss: 4.793061777739076\n",
      "[EPOCH #0, step #2198] loss: 4.793037113139389\n",
      "[EPOCH #0, step #2200] loss: 4.793119032130573\n",
      "[EPOCH #0, step #2202] loss: 4.793207753640762\n",
      "[EPOCH #0, step #2204] loss: 4.7931727422337955\n",
      "[EPOCH #0, step #2206] loss: 4.793110849201598\n",
      "[EPOCH #0, step #2208] loss: 4.793094395835673\n",
      "[EPOCH #0, step #2210] loss: 4.792955937724154\n",
      "[EPOCH #0, step #2212] loss: 4.792899620893904\n",
      "[EPOCH #0, step #2214] loss: 4.792825136335386\n",
      "[EPOCH #0, step #2216] loss: 4.79286387206522\n",
      "[EPOCH #0, step #2218] loss: 4.7927614420907565\n",
      "[EPOCH #0, step #2220] loss: 4.79264864047762\n",
      "[EPOCH #0, step #2222] loss: 4.792668840019648\n",
      "[EPOCH #0, step #2224] loss: 4.792692924670959\n",
      "[EPOCH #0, step #2226] loss: 4.792754255285379\n",
      "[EPOCH #0, step #2228] loss: 4.7925924377732345\n",
      "[EPOCH #0, step #2230] loss: 4.792674829584769\n",
      "[EPOCH #0, step #2232] loss: 4.792691958094732\n",
      "[EPOCH #0, step #2234] loss: 4.792832999901483\n",
      "[EPOCH #0, step #2236] loss: 4.792863884620172\n",
      "[EPOCH #0, step #2238] loss: 4.792860572494637\n",
      "[EPOCH #0, step #2240] loss: 4.792958765995922\n",
      "[EPOCH #0, step #2242] loss: 4.7928994104791\n",
      "[EPOCH #0, step #2244] loss: 4.792930312889456\n",
      "[EPOCH #0, step #2246] loss: 4.792810597099301\n",
      "[EPOCH #0, step #2248] loss: 4.792791994797913\n",
      "[EPOCH #0, step #2250] loss: 4.792865304722356\n",
      "[EPOCH #0, step #2252] loss: 4.792920821833706\n",
      "[EPOCH #0, step #2254] loss: 4.792899750923106\n",
      "[EPOCH #0, step #2256] loss: 4.792825038421455\n",
      "[EPOCH #0, step #2258] loss: 4.792745728258434\n",
      "[EPOCH #0, step #2260] loss: 4.792812762878364\n",
      "[EPOCH #0, step #2262] loss: 4.79270174774089\n",
      "[EPOCH #0, step #2264] loss: 4.792734246327651\n",
      "[EPOCH #0, step #2266] loss: 4.792793377201937\n",
      "[EPOCH #0, step #2268] loss: 4.79274291117951\n",
      "[EPOCH #0, step #2270] loss: 4.79277372612191\n",
      "[EPOCH #0, step #2272] loss: 4.792903130661796\n",
      "[EPOCH #0, step #2274] loss: 4.7930068389138025\n",
      "[EPOCH #0, step #2276] loss: 4.793079085343754\n",
      "[EPOCH #0, step #2278] loss: 4.793057193772842\n",
      "[EPOCH #0, step #2280] loss: 4.793075225375191\n",
      "[EPOCH #0, step #2282] loss: 4.793161847536051\n",
      "[EPOCH #0, step #2284] loss: 4.793211065079503\n",
      "[EPOCH #0, step #2286] loss: 4.793324679227495\n",
      "[EPOCH #0, step #2288] loss: 4.7932487071576\n",
      "[EPOCH #0, step #2290] loss: 4.793346453941009\n",
      "[EPOCH #0, step #2292] loss: 4.793379881160518\n",
      "[EPOCH #0, step #2294] loss: 4.793307736756235\n",
      "[EPOCH #0, step #2296] loss: 4.793306415061303\n",
      "[EPOCH #0, step #2298] loss: 4.7933973969454975\n",
      "[EPOCH #0, step #2300] loss: 4.793362607752846\n",
      "[EPOCH #0, step #2302] loss: 4.793254886458244\n",
      "[EPOCH #0, step #2304] loss: 4.7933177325316985\n",
      "[EPOCH #0, step #2306] loss: 4.793407511700786\n",
      "[EPOCH #0, step #2308] loss: 4.7934008621457025\n",
      "[EPOCH #0, step #2310] loss: 4.7934031715541625\n",
      "[EPOCH #0, step #2312] loss: 4.793441206770854\n",
      "[EPOCH #0, step #2314] loss: 4.7932441637274\n",
      "[EPOCH #0, step #2316] loss: 4.79315269533601\n",
      "[EPOCH #0, step #2318] loss: 4.793189954531506\n",
      "[EPOCH #0, step #2320] loss: 4.793162872235563\n",
      "[EPOCH #0, step #2322] loss: 4.793102743294133\n",
      "[EPOCH #0, step #2324] loss: 4.793175184598533\n",
      "[EPOCH #0, step #2326] loss: 4.79328528893163\n",
      "[EPOCH #0, step #2328] loss: 4.793177705406887\n",
      "[EPOCH #0, step #2330] loss: 4.7931895937238425\n",
      "[EPOCH #0, step #2332] loss: 4.793268522784989\n",
      "[EPOCH #0, step #2334] loss: 4.793283594037532\n",
      "[EPOCH #0, step #2336] loss: 4.793078209734595\n",
      "[EPOCH #0, step #2338] loss: 4.792967530283575\n",
      "[EPOCH #0, step #2340] loss: 4.7930290600215715\n",
      "[EPOCH #0, step #2342] loss: 4.792976235101938\n",
      "[EPOCH #0, step #2344] loss: 4.792965404412894\n",
      "[EPOCH #0, step #2346] loss: 4.79306246566935\n",
      "[EPOCH #0, step #2348] loss: 4.793155937105506\n",
      "[EPOCH #0, step #2350] loss: 4.793256935288378\n",
      "[EPOCH #0, step #2352] loss: 4.793222089752055\n",
      "[EPOCH #0, step #2354] loss: 4.793278262670886\n",
      "[EPOCH #0, step #2356] loss: 4.793240412646145\n",
      "[EPOCH #0, step #2358] loss: 4.793249314192141\n",
      "[EPOCH #0, step #2360] loss: 4.793247358216314\n",
      "[EPOCH #0, step #2362] loss: 4.793435328197439\n",
      "[EPOCH #0, step #2364] loss: 4.793411011726105\n",
      "[EPOCH #0, step #2366] loss: 4.79341264570822\n",
      "[EPOCH #0, step #2368] loss: 4.793496572704646\n",
      "[EPOCH #0, step #2370] loss: 4.793355674896498\n",
      "[EPOCH #0, step #2372] loss: 4.7933856585937065\n",
      "[EPOCH #0, step #2374] loss: 4.793409056412546\n",
      "[EPOCH #0, step #2376] loss: 4.793408704195195\n",
      "[EPOCH #0, step #2378] loss: 4.793241563590949\n",
      "[EPOCH #0, step #2380] loss: 4.793282235885357\n",
      "[EPOCH #0, step #2382] loss: 4.7930036637645985\n",
      "[EPOCH #0, step #2384] loss: 4.79292036992199\n",
      "[EPOCH #0, step #2386] loss: 4.793045909923142\n",
      "[EPOCH #0, step #2388] loss: 4.792894357215815\n",
      "[EPOCH #0, step #2390] loss: 4.7927818106887035\n",
      "[EPOCH #0, step #2392] loss: 4.792778893688758\n",
      "[EPOCH #0, step #2394] loss: 4.792785227224075\n",
      "[EPOCH #0, step #2396] loss: 4.792733351190636\n",
      "[EPOCH #0, step #2398] loss: 4.792870444623367\n",
      "[EPOCH #0, step #2400] loss: 4.7928898581759425\n",
      "[EPOCH #0, step #2402] loss: 4.792859046101024\n",
      "[EPOCH #0, step #2404] loss: 4.792882311220229\n",
      "[EPOCH #0, step #2406] loss: 4.792995353828687\n",
      "[EPOCH #0, step #2408] loss: 4.793092425406944\n",
      "[EPOCH #0, step #2410] loss: 4.793047335176535\n",
      "[EPOCH #0, step #2412] loss: 4.793019707352109\n",
      "[EPOCH #0, step #2414] loss: 4.793000898104523\n",
      "[EPOCH #0, step #2416] loss: 4.7929875100009145\n",
      "[EPOCH #0, step #2418] loss: 4.792907642105878\n",
      "[EPOCH #0, step #2420] loss: 4.792946851928093\n",
      "[EPOCH #0, step #2422] loss: 4.792986661601647\n",
      "[EPOCH #0, step #2424] loss: 4.792977686419929\n",
      "[EPOCH #0, step #2426] loss: 4.7929740303771995\n",
      "[EPOCH #0, step #2428] loss: 4.7930859581047045\n",
      "[EPOCH #0, step #2430] loss: 4.793050897999194\n",
      "[EPOCH #0, step #2432] loss: 4.793110677552135\n",
      "[EPOCH #0, step #2434] loss: 4.7931613516758595\n",
      "[EPOCH #0, step #2436] loss: 4.793244878043392\n",
      "[EPOCH #0, step #2438] loss: 4.7931958670105885\n",
      "[EPOCH #0, step #2440] loss: 4.793236990338317\n",
      "[EPOCH #0, step #2442] loss: 4.793369989828452\n",
      "[EPOCH #0, step #2444] loss: 4.793459010953065\n",
      "[EPOCH #0, step #2446] loss: 4.793516549569127\n",
      "[EPOCH #0, step #2448] loss: 4.793593715871777\n",
      "[EPOCH #0, step #2450] loss: 4.793694580004975\n",
      "[EPOCH #0, step #2452] loss: 4.793799279808172\n",
      "[EPOCH #0, step #2454] loss: 4.79366179425459\n",
      "[EPOCH #0, step #2456] loss: 4.793639032249777\n",
      "[EPOCH #0, step #2458] loss: 4.793691407475543\n",
      "[EPOCH #0, step #2460] loss: 4.793714565942075\n",
      "[EPOCH #0, step #2462] loss: 4.793612619555679\n",
      "[EPOCH #0, step #2464] loss: 4.793666404546154\n",
      "[EPOCH #0, step #2466] loss: 4.793562460583523\n",
      "[EPOCH #0, step #2468] loss: 4.793592897201173\n",
      "[EPOCH #0, step #2470] loss: 4.793563446020221\n",
      "[EPOCH #0, step #2472] loss: 4.793505395032402\n",
      "[EPOCH #0, step #2474] loss: 4.793467833971737\n",
      "[EPOCH #0, step #2476] loss: 4.793489879882283\n",
      "[EPOCH #0, step #2478] loss: 4.79342361283235\n",
      "[EPOCH #0, step #2480] loss: 4.793347286438086\n",
      "[EPOCH #0, step #2482] loss: 4.7933302912748585\n",
      "[EPOCH #0, step #2484] loss: 4.793249564103677\n",
      "[EPOCH #0, step #2486] loss: 4.793134123338578\n",
      "[EPOCH #0, step #2488] loss: 4.793107608850341\n",
      "[EPOCH #0, step #2490] loss: 4.792978749797795\n",
      "[EPOCH #0, step #2492] loss: 4.793064099643492\n",
      "[EPOCH #0, step #2494] loss: 4.7929922589319265\n",
      "[EPOCH #0, step #2496] loss: 4.792949841888514\n",
      "[EPOCH #0, step #2498] loss: 4.792962134194498\n",
      "[EPOCH #0] loss: 4.793031246376038\n",
      "[EPOCH #1, step #0] loss: 5.029282093048096\n",
      "[EPOCH #1, step #2] loss: 4.806010723114014\n",
      "[EPOCH #1, step #4] loss: 4.743550872802734\n",
      "[EPOCH #1, step #6] loss: 4.754983016422817\n",
      "[EPOCH #1, step #8] loss: 4.762307908799913\n",
      "[EPOCH #1, step #10] loss: 4.751986590298739\n",
      "[EPOCH #1, step #12] loss: 4.75025173333975\n",
      "[EPOCH #1, step #14] loss: 4.747889105478922\n",
      "[EPOCH #1, step #16] loss: 4.7435244672438674\n",
      "[EPOCH #1, step #18] loss: 4.747907814226653\n",
      "[EPOCH #1, step #20] loss: 4.747192246573312\n",
      "[EPOCH #1, step #22] loss: 4.765109580496083\n",
      "[EPOCH #1, step #24] loss: 4.76163990020752\n",
      "[EPOCH #1, step #26] loss: 4.7666473388671875\n",
      "[EPOCH #1, step #28] loss: 4.773605543991615\n",
      "[EPOCH #1, step #30] loss: 4.762425038122362\n",
      "[EPOCH #1, step #32] loss: 4.767257748228131\n",
      "[EPOCH #1, step #34] loss: 4.777721786499024\n",
      "[EPOCH #1, step #36] loss: 4.774040119068043\n",
      "[EPOCH #1, step #38] loss: 4.774339688129914\n",
      "[EPOCH #1, step #40] loss: 4.779364050888434\n",
      "[EPOCH #1, step #42] loss: 4.770227787106536\n",
      "[EPOCH #1, step #44] loss: 4.772744327121311\n",
      "[EPOCH #1, step #46] loss: 4.765626714584675\n",
      "[EPOCH #1, step #48] loss: 4.758994783673968\n",
      "[EPOCH #1, step #50] loss: 4.759388512256098\n",
      "[EPOCH #1, step #52] loss: 4.761832732074666\n",
      "[EPOCH #1, step #54] loss: 4.763698794625022\n",
      "[EPOCH #1, step #56] loss: 4.766066400628341\n",
      "[EPOCH #1, step #58] loss: 4.761646618277339\n",
      "[EPOCH #1, step #60] loss: 4.759388102859747\n",
      "[EPOCH #1, step #62] loss: 4.759986665513781\n",
      "[EPOCH #1, step #64] loss: 4.7564882351801945\n",
      "[EPOCH #1, step #66] loss: 4.7571964690934365\n",
      "[EPOCH #1, step #68] loss: 4.7597729917885605\n",
      "[EPOCH #1, step #70] loss: 4.761074690751626\n",
      "[EPOCH #1, step #72] loss: 4.760669322863017\n",
      "[EPOCH #1, step #74] loss: 4.760624179840088\n",
      "[EPOCH #1, step #76] loss: 4.763993671962193\n",
      "[EPOCH #1, step #78] loss: 4.760732258422466\n",
      "[EPOCH #1, step #80] loss: 4.759126922230662\n",
      "[EPOCH #1, step #82] loss: 4.761866345463029\n",
      "[EPOCH #1, step #84] loss: 4.758661752588608\n",
      "[EPOCH #1, step #86] loss: 4.757244740409413\n",
      "[EPOCH #1, step #88] loss: 4.757910251617432\n",
      "[EPOCH #1, step #90] loss: 4.754311865502661\n",
      "[EPOCH #1, step #92] loss: 4.755401134490967\n",
      "[EPOCH #1, step #94] loss: 4.750761559135036\n",
      "[EPOCH #1, step #96] loss: 4.74632418032774\n",
      "[EPOCH #1, step #98] loss: 4.744531395459416\n",
      "[EPOCH #1, step #100] loss: 4.744395610129479\n",
      "[EPOCH #1, step #102] loss: 4.747578162591434\n",
      "[EPOCH #1, step #104] loss: 4.749955749511718\n",
      "[EPOCH #1, step #106] loss: 4.750656092278311\n",
      "[EPOCH #1, step #108] loss: 4.753192311033197\n",
      "[EPOCH #1, step #110] loss: 4.750603362246677\n",
      "[EPOCH #1, step #112] loss: 4.74791660140046\n",
      "[EPOCH #1, step #114] loss: 4.743773236482039\n",
      "[EPOCH #1, step #116] loss: 4.74195449372642\n",
      "[EPOCH #1, step #118] loss: 4.7417936164791845\n",
      "[EPOCH #1, step #120] loss: 4.741086884963611\n",
      "[EPOCH #1, step #122] loss: 4.737535147162957\n",
      "[EPOCH #1, step #124] loss: 4.736834568023681\n",
      "[EPOCH #1, step #126] loss: 4.735171021438959\n",
      "[EPOCH #1, step #128] loss: 4.732664222865141\n",
      "[EPOCH #1, step #130] loss: 4.73277299458744\n",
      "[EPOCH #1, step #132] loss: 4.732153896102331\n",
      "[EPOCH #1, step #134] loss: 4.731765676427771\n",
      "[EPOCH #1, step #136] loss: 4.731584145204864\n",
      "[EPOCH #1, step #138] loss: 4.7315374141116795\n",
      "[EPOCH #1, step #140] loss: 4.730378779959171\n",
      "[EPOCH #1, step #142] loss: 4.731711204235371\n",
      "[EPOCH #1, step #144] loss: 4.731569056675352\n",
      "[EPOCH #1, step #146] loss: 4.733186095750251\n",
      "[EPOCH #1, step #148] loss: 4.7325370967788185\n",
      "[EPOCH #1, step #150] loss: 4.731639394696975\n",
      "[EPOCH #1, step #152] loss: 4.729932002771914\n",
      "[EPOCH #1, step #154] loss: 4.729533229335662\n",
      "[EPOCH #1, step #156] loss: 4.728477508399137\n",
      "[EPOCH #1, step #158] loss: 4.729368461752838\n",
      "[EPOCH #1, step #160] loss: 4.729370327469725\n",
      "[EPOCH #1, step #162] loss: 4.728086641229734\n",
      "[EPOCH #1, step #164] loss: 4.727288650743889\n",
      "[EPOCH #1, step #166] loss: 4.726821071373489\n",
      "[EPOCH #1, step #168] loss: 4.727033485322309\n",
      "[EPOCH #1, step #170] loss: 4.727371079182764\n",
      "[EPOCH #1, step #172] loss: 4.725979399818906\n",
      "[EPOCH #1, step #174] loss: 4.724160679408482\n",
      "[EPOCH #1, step #176] loss: 4.723510825701353\n",
      "[EPOCH #1, step #178] loss: 4.72290817718932\n",
      "[EPOCH #1, step #180] loss: 4.721027234641228\n",
      "[EPOCH #1, step #182] loss: 4.720635690324293\n",
      "[EPOCH #1, step #184] loss: 4.720994374558733\n",
      "[EPOCH #1, step #186] loss: 4.721987658005984\n",
      "[EPOCH #1, step #188] loss: 4.722923119862874\n",
      "[EPOCH #1, step #190] loss: 4.722374891111364\n",
      "[EPOCH #1, step #192] loss: 4.721319416026377\n",
      "[EPOCH #1, step #194] loss: 4.721073744847224\n",
      "[EPOCH #1, step #196] loss: 4.720768458952153\n",
      "[EPOCH #1, step #198] loss: 4.719663102423127\n",
      "[EPOCH #1, step #200] loss: 4.719200032267405\n",
      "[EPOCH #1, step #202] loss: 4.718309430653239\n",
      "[EPOCH #1, step #204] loss: 4.716006909347162\n",
      "[EPOCH #1, step #206] loss: 4.71468092278006\n",
      "[EPOCH #1, step #208] loss: 4.713970877907493\n",
      "[EPOCH #1, step #210] loss: 4.713738511523929\n",
      "[EPOCH #1, step #212] loss: 4.7134982319505\n",
      "[EPOCH #1, step #214] loss: 4.712626479392828\n",
      "[EPOCH #1, step #216] loss: 4.711171486410677\n",
      "[EPOCH #1, step #218] loss: 4.709789093226602\n",
      "[EPOCH #1, step #220] loss: 4.710263142218957\n",
      "[EPOCH #1, step #222] loss: 4.710984788133424\n",
      "[EPOCH #1, step #224] loss: 4.708647836049398\n",
      "[EPOCH #1, step #226] loss: 4.707476002529329\n",
      "[EPOCH #1, step #228] loss: 4.706431759496963\n",
      "[EPOCH #1, step #230] loss: 4.70518734547999\n",
      "[EPOCH #1, step #232] loss: 4.70474463164039\n",
      "[EPOCH #1, step #234] loss: 4.704941660292605\n",
      "[EPOCH #1, step #236] loss: 4.705259604796075\n",
      "[EPOCH #1, step #238] loss: 4.704815507433903\n",
      "[EPOCH #1, step #240] loss: 4.705157329432697\n",
      "[EPOCH #1, step #242] loss: 4.704270829879698\n",
      "[EPOCH #1, step #244] loss: 4.7045765584828905\n",
      "[EPOCH #1, step #246] loss: 4.704217532385699\n",
      "[EPOCH #1, step #248] loss: 4.703352481964601\n",
      "[EPOCH #1, step #250] loss: 4.702749189627598\n",
      "[EPOCH #1, step #252] loss: 4.701664638142341\n",
      "[EPOCH #1, step #254] loss: 4.700786856109021\n",
      "[EPOCH #1, step #256] loss: 4.700210704877683\n",
      "[EPOCH #1, step #258] loss: 4.698712428104003\n",
      "[EPOCH #1, step #260] loss: 4.6988930117581535\n",
      "[EPOCH #1, step #262] loss: 4.698564340859765\n",
      "[EPOCH #1, step #264] loss: 4.6982843165127735\n",
      "[EPOCH #1, step #266] loss: 4.697838011752354\n",
      "[EPOCH #1, step #268] loss: 4.698015161606459\n",
      "[EPOCH #1, step #270] loss: 4.698101779191696\n",
      "[EPOCH #1, step #272] loss: 4.698200161203797\n",
      "[EPOCH #1, step #274] loss: 4.697208929928866\n",
      "[EPOCH #1, step #276] loss: 4.696487044599513\n",
      "[EPOCH #1, step #278] loss: 4.695408075941079\n",
      "[EPOCH #1, step #280] loss: 4.694346190347366\n",
      "[EPOCH #1, step #282] loss: 4.693527767599261\n",
      "[EPOCH #1, step #284] loss: 4.692785729860004\n",
      "[EPOCH #1, step #286] loss: 4.692261120998901\n",
      "[EPOCH #1, step #288] loss: 4.692427834837495\n",
      "[EPOCH #1, step #290] loss: 4.692578689339235\n",
      "[EPOCH #1, step #292] loss: 4.691541248620981\n",
      "[EPOCH #1, step #294] loss: 4.690892086998891\n",
      "[EPOCH #1, step #296] loss: 4.690351723821878\n",
      "[EPOCH #1, step #298] loss: 4.6897088851418385\n",
      "[EPOCH #1, step #300] loss: 4.688881302196718\n",
      "[EPOCH #1, step #302] loss: 4.687663975328502\n",
      "[EPOCH #1, step #304] loss: 4.688380605666364\n",
      "[EPOCH #1, step #306] loss: 4.687633040673569\n",
      "[EPOCH #1, step #308] loss: 4.68687959627812\n",
      "[EPOCH #1, step #310] loss: 4.6868402428949\n",
      "[EPOCH #1, step #312] loss: 4.687322153450963\n",
      "[EPOCH #1, step #314] loss: 4.686850765773229\n",
      "[EPOCH #1, step #316] loss: 4.68704362622571\n",
      "[EPOCH #1, step #318] loss: 4.686810808869365\n",
      "[EPOCH #1, step #320] loss: 4.686827817067179\n",
      "[EPOCH #1, step #322] loss: 4.687082867873342\n",
      "[EPOCH #1, step #324] loss: 4.685805634718675\n",
      "[EPOCH #1, step #326] loss: 4.686591800199736\n",
      "[EPOCH #1, step #328] loss: 4.685666049505077\n",
      "[EPOCH #1, step #330] loss: 4.685380327737583\n",
      "[EPOCH #1, step #332] loss: 4.684638170866637\n",
      "[EPOCH #1, step #334] loss: 4.684124721697907\n",
      "[EPOCH #1, step #336] loss: 4.685043616535402\n",
      "[EPOCH #1, step #338] loss: 4.684501785795949\n",
      "[EPOCH #1, step #340] loss: 4.685258127027942\n",
      "[EPOCH #1, step #342] loss: 4.684649618999604\n",
      "[EPOCH #1, step #344] loss: 4.684462526570195\n",
      "[EPOCH #1, step #346] loss: 4.6840038904195564\n",
      "[EPOCH #1, step #348] loss: 4.6833448874574675\n",
      "[EPOCH #1, step #350] loss: 4.682652106651893\n",
      "[EPOCH #1, step #352] loss: 4.6825668372783715\n",
      "[EPOCH #1, step #354] loss: 4.682380701790393\n",
      "[EPOCH #1, step #356] loss: 4.68219179532775\n",
      "[EPOCH #1, step #358] loss: 4.682188598558431\n",
      "[EPOCH #1, step #360] loss: 4.682708108854426\n",
      "[EPOCH #1, step #362] loss: 4.681704396387105\n",
      "[EPOCH #1, step #364] loss: 4.68137032887707\n",
      "[EPOCH #1, step #366] loss: 4.681264138026848\n",
      "[EPOCH #1, step #368] loss: 4.680818467282345\n",
      "[EPOCH #1, step #370] loss: 4.6804670117936045\n",
      "[EPOCH #1, step #372] loss: 4.680777453864867\n",
      "[EPOCH #1, step #374] loss: 4.680313254038492\n",
      "[EPOCH #1, step #376] loss: 4.679818816147053\n",
      "[EPOCH #1, step #378] loss: 4.679738083741281\n",
      "[EPOCH #1, step #380] loss: 4.6793177472011935\n",
      "[EPOCH #1, step #382] loss: 4.679465423364864\n",
      "[EPOCH #1, step #384] loss: 4.6787246691716184\n",
      "[EPOCH #1, step #386] loss: 4.678676830705746\n",
      "[EPOCH #1, step #388] loss: 4.67849813328917\n",
      "[EPOCH #1, step #390] loss: 4.678226997785251\n",
      "[EPOCH #1, step #392] loss: 4.677433245661301\n",
      "[EPOCH #1, step #394] loss: 4.676782282092903\n",
      "[EPOCH #1, step #396] loss: 4.676318907317347\n",
      "[EPOCH #1, step #398] loss: 4.67621193912095\n",
      "[EPOCH #1, step #400] loss: 4.675872030995433\n",
      "[EPOCH #1, step #402] loss: 4.676341349078763\n",
      "[EPOCH #1, step #404] loss: 4.6764219837424195\n",
      "[EPOCH #1, step #406] loss: 4.675539877842334\n",
      "[EPOCH #1, step #408] loss: 4.675310261966547\n",
      "[EPOCH #1, step #410] loss: 4.675377189097904\n",
      "[EPOCH #1, step #412] loss: 4.674455384076652\n",
      "[EPOCH #1, step #414] loss: 4.674575963077775\n",
      "[EPOCH #1, step #416] loss: 4.673964843475561\n",
      "[EPOCH #1, step #418] loss: 4.673923978373089\n",
      "[EPOCH #1, step #420] loss: 4.6738766242092975\n",
      "[EPOCH #1, step #422] loss: 4.673441942138311\n",
      "[EPOCH #1, step #424] loss: 4.672702156515682\n",
      "[EPOCH #1, step #426] loss: 4.67328598683556\n",
      "[EPOCH #1, step #428] loss: 4.672363271246423\n",
      "[EPOCH #1, step #430] loss: 4.672567843284519\n",
      "[EPOCH #1, step #432] loss: 4.671838913448413\n",
      "[EPOCH #1, step #434] loss: 4.671694636070865\n",
      "[EPOCH #1, step #436] loss: 4.671653746467433\n",
      "[EPOCH #1, step #438] loss: 4.671529476626316\n",
      "[EPOCH #1, step #440] loss: 4.671026359610006\n",
      "[EPOCH #1, step #442] loss: 4.670401872415305\n",
      "[EPOCH #1, step #444] loss: 4.669464866766769\n",
      "[EPOCH #1, step #446] loss: 4.668868709197247\n",
      "[EPOCH #1, step #448] loss: 4.668419233674727\n",
      "[EPOCH #1, step #450] loss: 4.66857397688466\n",
      "[EPOCH #1, step #452] loss: 4.669049801942266\n",
      "[EPOCH #1, step #454] loss: 4.668353988312103\n",
      "[EPOCH #1, step #456] loss: 4.668369971427667\n",
      "[EPOCH #1, step #458] loss: 4.668213422521786\n",
      "[EPOCH #1, step #460] loss: 4.66768451781697\n",
      "[EPOCH #1, step #462] loss: 4.667680702250668\n",
      "[EPOCH #1, step #464] loss: 4.667455765508836\n",
      "[EPOCH #1, step #466] loss: 4.667791389040569\n",
      "[EPOCH #1, step #468] loss: 4.667824023313868\n",
      "[EPOCH #1, step #470] loss: 4.6672868779257355\n",
      "[EPOCH #1, step #472] loss: 4.666780822624868\n",
      "[EPOCH #1, step #474] loss: 4.66700960058915\n",
      "[EPOCH #1, step #476] loss: 4.666038626145017\n",
      "[EPOCH #1, step #478] loss: 4.665834601089701\n",
      "[EPOCH #1, step #480] loss: 4.665149723417794\n",
      "[EPOCH #1, step #482] loss: 4.6655498933101045\n",
      "[EPOCH #1, step #484] loss: 4.665725207574589\n",
      "[EPOCH #1, step #486] loss: 4.665237843867934\n",
      "[EPOCH #1, step #488] loss: 4.6650783172170565\n",
      "[EPOCH #1, step #490] loss: 4.664292634626027\n",
      "[EPOCH #1, step #492] loss: 4.6639989974774405\n",
      "[EPOCH #1, step #494] loss: 4.663232572150953\n",
      "[EPOCH #1, step #496] loss: 4.662603373498744\n",
      "[EPOCH #1, step #498] loss: 4.662429659543391\n",
      "[EPOCH #1, step #500] loss: 4.662645404686233\n",
      "[EPOCH #1, step #502] loss: 4.662752435885177\n",
      "[EPOCH #1, step #504] loss: 4.662057515890291\n",
      "[EPOCH #1, step #506] loss: 4.661783327482626\n",
      "[EPOCH #1, step #508] loss: 4.66179395049869\n",
      "[EPOCH #1, step #510] loss: 4.661197973090842\n",
      "[EPOCH #1, step #512] loss: 4.661260894864624\n",
      "[EPOCH #1, step #514] loss: 4.6607018331879555\n",
      "[EPOCH #1, step #516] loss: 4.660261127427653\n",
      "[EPOCH #1, step #518] loss: 4.659739517292765\n",
      "[EPOCH #1, step #520] loss: 4.6593194831584555\n",
      "[EPOCH #1, step #522] loss: 4.659340118131036\n",
      "[EPOCH #1, step #524] loss: 4.659529932112921\n",
      "[EPOCH #1, step #526] loss: 4.659412983925112\n",
      "[EPOCH #1, step #528] loss: 4.659224250591545\n",
      "[EPOCH #1, step #530] loss: 4.659257162313228\n",
      "[EPOCH #1, step #532] loss: 4.659100019014799\n",
      "[EPOCH #1, step #534] loss: 4.658772997989833\n",
      "[EPOCH #1, step #536] loss: 4.657953727622716\n",
      "[EPOCH #1, step #538] loss: 4.657670403234592\n",
      "[EPOCH #1, step #540] loss: 4.657384607135258\n",
      "[EPOCH #1, step #542] loss: 4.657378079043787\n",
      "[EPOCH #1, step #544] loss: 4.6573010269655\n",
      "[EPOCH #1, step #546] loss: 4.657121782982807\n",
      "[EPOCH #1, step #548] loss: 4.657036561566406\n",
      "[EPOCH #1, step #550] loss: 4.656629130541737\n",
      "[EPOCH #1, step #552] loss: 4.655763425188944\n",
      "[EPOCH #1, step #554] loss: 4.655494030101879\n",
      "[EPOCH #1, step #556] loss: 4.655182733141969\n",
      "[EPOCH #1, step #558] loss: 4.654732380015692\n",
      "[EPOCH #1, step #560] loss: 4.654947717772023\n",
      "[EPOCH #1, step #562] loss: 4.654487025462501\n",
      "[EPOCH #1, step #564] loss: 4.654108782574139\n",
      "[EPOCH #1, step #566] loss: 4.654028094424563\n",
      "[EPOCH #1, step #568] loss: 4.653221668384196\n",
      "[EPOCH #1, step #570] loss: 4.65281331685295\n",
      "[EPOCH #1, step #572] loss: 4.652367299763944\n",
      "[EPOCH #1, step #574] loss: 4.652403343864109\n",
      "[EPOCH #1, step #576] loss: 4.651923507710263\n",
      "[EPOCH #1, step #578] loss: 4.651870893073205\n",
      "[EPOCH #1, step #580] loss: 4.6517984025859995\n",
      "[EPOCH #1, step #582] loss: 4.651557991884994\n",
      "[EPOCH #1, step #584] loss: 4.651220399090367\n",
      "[EPOCH #1, step #586] loss: 4.651211290993114\n",
      "[EPOCH #1, step #588] loss: 4.651257035687743\n",
      "[EPOCH #1, step #590] loss: 4.651263549243133\n",
      "[EPOCH #1, step #592] loss: 4.651082563159036\n",
      "[EPOCH #1, step #594] loss: 4.650885991489186\n",
      "[EPOCH #1, step #596] loss: 4.651015605958462\n",
      "[EPOCH #1, step #598] loss: 4.6506463052434395\n",
      "[EPOCH #1, step #600] loss: 4.65063281305221\n",
      "[EPOCH #1, step #602] loss: 4.650325817848319\n",
      "[EPOCH #1, step #604] loss: 4.6499984820027\n",
      "[EPOCH #1, step #606] loss: 4.649702308598225\n",
      "[EPOCH #1, step #608] loss: 4.649031158347044\n",
      "[EPOCH #1, step #610] loss: 4.648648460250829\n",
      "[EPOCH #1, step #612] loss: 4.648457803306144\n",
      "[EPOCH #1, step #614] loss: 4.648310678373507\n",
      "[EPOCH #1, step #616] loss: 4.64822407479804\n",
      "[EPOCH #1, step #618] loss: 4.648120803093486\n",
      "[EPOCH #1, step #620] loss: 4.6481719992203026\n",
      "[EPOCH #1, step #622] loss: 4.647714099761571\n",
      "[EPOCH #1, step #624] loss: 4.646992884063721\n",
      "[EPOCH #1, step #626] loss: 4.64683204784728\n",
      "[EPOCH #1, step #628] loss: 4.646516777942215\n",
      "[EPOCH #1, step #630] loss: 4.646541465481185\n",
      "[EPOCH #1, step #632] loss: 4.646527984131004\n",
      "[EPOCH #1, step #634] loss: 4.646551377003587\n",
      "[EPOCH #1, step #636] loss: 4.646574840815139\n",
      "[EPOCH #1, step #638] loss: 4.646406510253095\n",
      "[EPOCH #1, step #640] loss: 4.64597382746323\n",
      "[EPOCH #1, step #642] loss: 4.6455506692592685\n",
      "[EPOCH #1, step #644] loss: 4.645401650805806\n",
      "[EPOCH #1, step #646] loss: 4.645039448598069\n",
      "[EPOCH #1, step #648] loss: 4.645196037410036\n",
      "[EPOCH #1, step #650] loss: 4.644860143851941\n",
      "[EPOCH #1, step #652] loss: 4.644545341888942\n",
      "[EPOCH #1, step #654] loss: 4.644472973947306\n",
      "[EPOCH #1, step #656] loss: 4.644403771904143\n",
      "[EPOCH #1, step #658] loss: 4.64408716265457\n",
      "[EPOCH #1, step #660] loss: 4.6439595280184145\n",
      "[EPOCH #1, step #662] loss: 4.643714745120225\n",
      "[EPOCH #1, step #664] loss: 4.643493135351884\n",
      "[EPOCH #1, step #666] loss: 4.643283124091565\n",
      "[EPOCH #1, step #668] loss: 4.642944217619162\n",
      "[EPOCH #1, step #670] loss: 4.642544305804234\n",
      "[EPOCH #1, step #672] loss: 4.642763437523353\n",
      "[EPOCH #1, step #674] loss: 4.642773466463442\n",
      "[EPOCH #1, step #676] loss: 4.642728289261878\n",
      "[EPOCH #1, step #678] loss: 4.642617448264615\n",
      "[EPOCH #1, step #680] loss: 4.642452878573917\n",
      "[EPOCH #1, step #682] loss: 4.642318940616457\n",
      "[EPOCH #1, step #684] loss: 4.641725216468755\n",
      "[EPOCH #1, step #686] loss: 4.641703193364705\n",
      "[EPOCH #1, step #688] loss: 4.641377738321467\n",
      "[EPOCH #1, step #690] loss: 4.641382650770085\n",
      "[EPOCH #1, step #692] loss: 4.640801101535946\n",
      "[EPOCH #1, step #694] loss: 4.640512594044638\n",
      "[EPOCH #1, step #696] loss: 4.64081508494176\n",
      "[EPOCH #1, step #698] loss: 4.640723619338269\n",
      "[EPOCH #1, step #700] loss: 4.6406129171776875\n",
      "[EPOCH #1, step #702] loss: 4.640599260289503\n",
      "[EPOCH #1, step #704] loss: 4.640363111563608\n",
      "[EPOCH #1, step #706] loss: 4.640290747943323\n",
      "[EPOCH #1, step #708] loss: 4.639677164752998\n",
      "[EPOCH #1, step #710] loss: 4.639750173490929\n",
      "[EPOCH #1, step #712] loss: 4.639968329693124\n",
      "[EPOCH #1, step #714] loss: 4.639545880831205\n",
      "[EPOCH #1, step #716] loss: 4.639271549932461\n",
      "[EPOCH #1, step #718] loss: 4.638946826998481\n",
      "[EPOCH #1, step #720] loss: 4.638734986811835\n",
      "[EPOCH #1, step #722] loss: 4.63852927035151\n",
      "[EPOCH #1, step #724] loss: 4.638515686166698\n",
      "[EPOCH #1, step #726] loss: 4.638273563804784\n",
      "[EPOCH #1, step #728] loss: 4.638246107820947\n",
      "[EPOCH #1, step #730] loss: 4.638026534287939\n",
      "[EPOCH #1, step #732] loss: 4.638093463070233\n",
      "[EPOCH #1, step #734] loss: 4.637766803689554\n",
      "[EPOCH #1, step #736] loss: 4.637445583110587\n",
      "[EPOCH #1, step #738] loss: 4.637433117878131\n",
      "[EPOCH #1, step #740] loss: 4.637369266566639\n",
      "[EPOCH #1, step #742] loss: 4.6371342593535765\n",
      "[EPOCH #1, step #744] loss: 4.636619671559174\n",
      "[EPOCH #1, step #746] loss: 4.636354581738412\n",
      "[EPOCH #1, step #748] loss: 4.636066052242019\n",
      "[EPOCH #1, step #750] loss: 4.635560344284606\n",
      "[EPOCH #1, step #752] loss: 4.635540014877421\n",
      "[EPOCH #1, step #754] loss: 4.6352115574262\n",
      "[EPOCH #1, step #756] loss: 4.635010263248983\n",
      "[EPOCH #1, step #758] loss: 4.634773624430218\n",
      "[EPOCH #1, step #760] loss: 4.634648061143271\n",
      "[EPOCH #1, step #762] loss: 4.6346053452198035\n",
      "[EPOCH #1, step #764] loss: 4.634227110046187\n",
      "[EPOCH #1, step #766] loss: 4.6339442030547335\n",
      "[EPOCH #1, step #768] loss: 4.63369617660582\n",
      "[EPOCH #1, step #770] loss: 4.63364200864166\n",
      "[EPOCH #1, step #772] loss: 4.6335120003713826\n",
      "[EPOCH #1, step #774] loss: 4.633231615251111\n",
      "[EPOCH #1, step #776] loss: 4.633021058263005\n",
      "[EPOCH #1, step #778] loss: 4.632753439524973\n",
      "[EPOCH #1, step #780] loss: 4.632392721505843\n",
      "[EPOCH #1, step #782] loss: 4.631871933529051\n",
      "[EPOCH #1, step #784] loss: 4.63193836394389\n",
      "[EPOCH #1, step #786] loss: 4.631625015520535\n",
      "[EPOCH #1, step #788] loss: 4.631542335747161\n",
      "[EPOCH #1, step #790] loss: 4.631380079368273\n",
      "[EPOCH #1, step #792] loss: 4.631423350090037\n",
      "[EPOCH #1, step #794] loss: 4.631132970965883\n",
      "[EPOCH #1, step #796] loss: 4.631102281950947\n",
      "[EPOCH #1, step #798] loss: 4.6308487890957295\n",
      "[EPOCH #1, step #800] loss: 4.6304900553937856\n",
      "[EPOCH #1, step #802] loss: 4.629891806492028\n",
      "[EPOCH #1, step #804] loss: 4.629805952865885\n",
      "[EPOCH #1, step #806] loss: 4.629565179274133\n",
      "[EPOCH #1, step #808] loss: 4.629588506719827\n",
      "[EPOCH #1, step #810] loss: 4.6293016514854575\n",
      "[EPOCH #1, step #812] loss: 4.629341781799204\n",
      "[EPOCH #1, step #814] loss: 4.6293335891208764\n",
      "[EPOCH #1, step #816] loss: 4.6292657642061\n",
      "[EPOCH #1, step #818] loss: 4.6290825510781906\n",
      "[EPOCH #1, step #820] loss: 4.628719800863138\n",
      "[EPOCH #1, step #822] loss: 4.628484052122726\n",
      "[EPOCH #1, step #824] loss: 4.628515066089052\n",
      "[EPOCH #1, step #826] loss: 4.6284272512184526\n",
      "[EPOCH #1, step #828] loss: 4.628346361498493\n",
      "[EPOCH #1, step #830] loss: 4.627967378866515\n",
      "[EPOCH #1, step #832] loss: 4.627791492306456\n",
      "[EPOCH #1, step #834] loss: 4.627447262495577\n",
      "[EPOCH #1, step #836] loss: 4.627534008367942\n",
      "[EPOCH #1, step #838] loss: 4.627687954362726\n",
      "[EPOCH #1, step #840] loss: 4.627596192354255\n",
      "[EPOCH #1, step #842] loss: 4.627480240479058\n",
      "[EPOCH #1, step #844] loss: 4.626969771977713\n",
      "[EPOCH #1, step #846] loss: 4.626853625354969\n",
      "[EPOCH #1, step #848] loss: 4.627032596174764\n",
      "[EPOCH #1, step #850] loss: 4.6269822714611735\n",
      "[EPOCH #1, step #852] loss: 4.627120558010875\n",
      "[EPOCH #1, step #854] loss: 4.627141888378657\n",
      "[EPOCH #1, step #856] loss: 4.626727528722311\n",
      "[EPOCH #1, step #858] loss: 4.626709920285883\n",
      "[EPOCH #1, step #860] loss: 4.626660732442078\n",
      "[EPOCH #1, step #862] loss: 4.626241786178818\n",
      "[EPOCH #1, step #864] loss: 4.626325868044286\n",
      "[EPOCH #1, step #866] loss: 4.626541251542246\n",
      "[EPOCH #1, step #868] loss: 4.626556047498837\n",
      "[EPOCH #1, step #870] loss: 4.626067224517892\n",
      "[EPOCH #1, step #872] loss: 4.625653628494595\n",
      "[EPOCH #1, step #874] loss: 4.6250903549194335\n",
      "[EPOCH #1, step #876] loss: 4.625028670581743\n",
      "[EPOCH #1, step #878] loss: 4.624669539372397\n",
      "[EPOCH #1, step #880] loss: 4.624414805521624\n",
      "[EPOCH #1, step #882] loss: 4.62415460173973\n",
      "[EPOCH #1, step #884] loss: 4.624189423706572\n",
      "[EPOCH #1, step #886] loss: 4.624234861290038\n",
      "[EPOCH #1, step #888] loss: 4.624106350309803\n",
      "[EPOCH #1, step #890] loss: 4.62414793267127\n",
      "[EPOCH #1, step #892] loss: 4.624157828152647\n",
      "[EPOCH #1, step #894] loss: 4.62404198833018\n",
      "[EPOCH #1, step #896] loss: 4.62400043342956\n",
      "[EPOCH #1, step #898] loss: 4.62377229654484\n",
      "[EPOCH #1, step #900] loss: 4.623745814544644\n",
      "[EPOCH #1, step #902] loss: 4.62369367793813\n",
      "[EPOCH #1, step #904] loss: 4.623617505236884\n",
      "[EPOCH #1, step #906] loss: 4.6236769756119465\n",
      "[EPOCH #1, step #908] loss: 4.623619730060774\n",
      "[EPOCH #1, step #910] loss: 4.623565920883162\n",
      "[EPOCH #1, step #912] loss: 4.623237459448055\n",
      "[EPOCH #1, step #914] loss: 4.623069587040469\n",
      "[EPOCH #1, step #916] loss: 4.623212276654259\n",
      "[EPOCH #1, step #918] loss: 4.622912078479688\n",
      "[EPOCH #1, step #920] loss: 4.622541842838581\n",
      "[EPOCH #1, step #922] loss: 4.622266669702168\n",
      "[EPOCH #1, step #924] loss: 4.622089861792487\n",
      "[EPOCH #1, step #926] loss: 4.621706677614807\n",
      "[EPOCH #1, step #928] loss: 4.621494673040121\n",
      "[EPOCH #1, step #930] loss: 4.6211672228722005\n",
      "[EPOCH #1, step #932] loss: 4.62115581447033\n",
      "[EPOCH #1, step #934] loss: 4.62078660363182\n",
      "[EPOCH #1, step #936] loss: 4.620564773217591\n",
      "[EPOCH #1, step #938] loss: 4.620384365098037\n",
      "[EPOCH #1, step #940] loss: 4.620308810668848\n",
      "[EPOCH #1, step #942] loss: 4.6200334477146505\n",
      "[EPOCH #1, step #944] loss: 4.620003147730752\n",
      "[EPOCH #1, step #946] loss: 4.619775455378177\n",
      "[EPOCH #1, step #948] loss: 4.619650580986031\n",
      "[EPOCH #1, step #950] loss: 4.619552524558628\n",
      "[EPOCH #1, step #952] loss: 4.619558053151758\n",
      "[EPOCH #1, step #954] loss: 4.619599163095365\n",
      "[EPOCH #1, step #956] loss: 4.619274881317075\n",
      "[EPOCH #1, step #958] loss: 4.619385912719186\n",
      "[EPOCH #1, step #960] loss: 4.618937502294376\n",
      "[EPOCH #1, step #962] loss: 4.618785067882122\n",
      "[EPOCH #1, step #964] loss: 4.618665211929558\n",
      "[EPOCH #1, step #966] loss: 4.618348220758133\n",
      "[EPOCH #1, step #968] loss: 4.618114954177809\n",
      "[EPOCH #1, step #970] loss: 4.61808064251555\n",
      "[EPOCH #1, step #972] loss: 4.61779584227951\n",
      "[EPOCH #1, step #974] loss: 4.617453233278715\n",
      "[EPOCH #1, step #976] loss: 4.617290466078228\n",
      "[EPOCH #1, step #978] loss: 4.6169397458358\n",
      "[EPOCH #1, step #980] loss: 4.6168668389198855\n",
      "[EPOCH #1, step #982] loss: 4.61669359275065\n",
      "[EPOCH #1, step #984] loss: 4.616339409532886\n",
      "[EPOCH #1, step #986] loss: 4.616290580900245\n",
      "[EPOCH #1, step #988] loss: 4.6159121823865314\n",
      "[EPOCH #1, step #990] loss: 4.615550036868221\n",
      "[EPOCH #1, step #992] loss: 4.615162643901291\n",
      "[EPOCH #1, step #994] loss: 4.615302066707132\n",
      "[EPOCH #1, step #996] loss: 4.6150443790669184\n",
      "[EPOCH #1, step #998] loss: 4.61481399412031\n",
      "[EPOCH #1, step #1000] loss: 4.614503418410813\n",
      "[EPOCH #1, step #1002] loss: 4.614090750723751\n",
      "[EPOCH #1, step #1004] loss: 4.614208781304051\n",
      "[EPOCH #1, step #1006] loss: 4.614077135728156\n",
      "[EPOCH #1, step #1008] loss: 4.613934104813575\n",
      "[EPOCH #1, step #1010] loss: 4.613738571736744\n",
      "[EPOCH #1, step #1012] loss: 4.613533404125278\n",
      "[EPOCH #1, step #1014] loss: 4.613389185261844\n",
      "[EPOCH #1, step #1016] loss: 4.613297862875309\n",
      "[EPOCH #1, step #1018] loss: 4.613205190495724\n",
      "[EPOCH #1, step #1020] loss: 4.61295842704063\n",
      "[EPOCH #1, step #1022] loss: 4.612852659626324\n",
      "[EPOCH #1, step #1024] loss: 4.612814355245451\n",
      "[EPOCH #1, step #1026] loss: 4.612741178291566\n",
      "[EPOCH #1, step #1028] loss: 4.6125492766948435\n",
      "[EPOCH #1, step #1030] loss: 4.612379072246681\n",
      "[EPOCH #1, step #1032] loss: 4.612148725744139\n",
      "[EPOCH #1, step #1034] loss: 4.612043340194629\n",
      "[EPOCH #1, step #1036] loss: 4.611994067406355\n",
      "[EPOCH #1, step #1038] loss: 4.611950630632241\n",
      "[EPOCH #1, step #1040] loss: 4.611791034482284\n",
      "[EPOCH #1, step #1042] loss: 4.611814695945293\n",
      "[EPOCH #1, step #1044] loss: 4.611861392755828\n",
      "[EPOCH #1, step #1046] loss: 4.6119031919790885\n",
      "[EPOCH #1, step #1048] loss: 4.611867225545151\n",
      "[EPOCH #1, step #1050] loss: 4.611611147135581\n",
      "[EPOCH #1, step #1052] loss: 4.611428018082694\n",
      "[EPOCH #1, step #1054] loss: 4.611337154171478\n",
      "[EPOCH #1, step #1056] loss: 4.611322300098026\n",
      "[EPOCH #1, step #1058] loss: 4.611001855365728\n",
      "[EPOCH #1, step #1060] loss: 4.610813975446523\n",
      "[EPOCH #1, step #1062] loss: 4.610530384060132\n",
      "[EPOCH #1, step #1064] loss: 4.610167971239403\n",
      "[EPOCH #1, step #1066] loss: 4.610273745535911\n",
      "[EPOCH #1, step #1068] loss: 4.610016404857582\n",
      "[EPOCH #1, step #1070] loss: 4.609751758789148\n",
      "[EPOCH #1, step #1072] loss: 4.609521793408141\n",
      "[EPOCH #1, step #1074] loss: 4.609515397271444\n",
      "[EPOCH #1, step #1076] loss: 4.609441205983707\n",
      "[EPOCH #1, step #1078] loss: 4.609035535252017\n",
      "[EPOCH #1, step #1080] loss: 4.608866365169839\n",
      "[EPOCH #1, step #1082] loss: 4.608723389915262\n",
      "[EPOCH #1, step #1084] loss: 4.6085143528775685\n",
      "[EPOCH #1, step #1086] loss: 4.608357343085885\n",
      "[EPOCH #1, step #1088] loss: 4.6082616478505765\n",
      "[EPOCH #1, step #1090] loss: 4.608258877622218\n",
      "[EPOCH #1, step #1092] loss: 4.608022871305391\n",
      "[EPOCH #1, step #1094] loss: 4.607911198759732\n",
      "[EPOCH #1, step #1096] loss: 4.607921553397026\n",
      "[EPOCH #1, step #1098] loss: 4.607694442755966\n",
      "[EPOCH #1, step #1100] loss: 4.607453935261102\n",
      "[EPOCH #1, step #1102] loss: 4.607316873987912\n",
      "[EPOCH #1, step #1104] loss: 4.6070205615117\n",
      "[EPOCH #1, step #1106] loss: 4.6068579614323975\n",
      "[EPOCH #1, step #1108] loss: 4.6065113357427\n",
      "[EPOCH #1, step #1110] loss: 4.606260129720858\n",
      "[EPOCH #1, step #1112] loss: 4.606188770253061\n",
      "[EPOCH #1, step #1114] loss: 4.605967051672828\n",
      "[EPOCH #1, step #1116] loss: 4.605807335545362\n",
      "[EPOCH #1, step #1118] loss: 4.605743942141427\n",
      "[EPOCH #1, step #1120] loss: 4.605650248429693\n",
      "[EPOCH #1, step #1122] loss: 4.605327494104102\n",
      "[EPOCH #1, step #1124] loss: 4.605124222649469\n",
      "[EPOCH #1, step #1126] loss: 4.605173929666352\n",
      "[EPOCH #1, step #1128] loss: 4.60489820779373\n",
      "[EPOCH #1, step #1130] loss: 4.6046035549052515\n",
      "[EPOCH #1, step #1132] loss: 4.604465764492671\n",
      "[EPOCH #1, step #1134] loss: 4.60414531976641\n",
      "[EPOCH #1, step #1136] loss: 4.60414241884713\n",
      "[EPOCH #1, step #1138] loss: 4.604061251883971\n",
      "[EPOCH #1, step #1140] loss: 4.60407340119117\n",
      "[EPOCH #1, step #1142] loss: 4.603871002180578\n",
      "[EPOCH #1, step #1144] loss: 4.6038821107956\n",
      "[EPOCH #1, step #1146] loss: 4.60376266447897\n",
      "[EPOCH #1, step #1148] loss: 4.603570282407384\n",
      "[EPOCH #1, step #1150] loss: 4.60351448448506\n",
      "[EPOCH #1, step #1152] loss: 4.603262944316616\n",
      "[EPOCH #1, step #1154] loss: 4.603104310221486\n",
      "[EPOCH #1, step #1156] loss: 4.602926190391943\n",
      "[EPOCH #1, step #1158] loss: 4.602835844054728\n",
      "[EPOCH #1, step #1160] loss: 4.6024376189883265\n",
      "[EPOCH #1, step #1162] loss: 4.602356286266192\n",
      "[EPOCH #1, step #1164] loss: 4.602187944584139\n",
      "[EPOCH #1, step #1166] loss: 4.602219819818282\n",
      "[EPOCH #1, step #1168] loss: 4.602117140747932\n",
      "[EPOCH #1, step #1170] loss: 4.601962889289367\n",
      "[EPOCH #1, step #1172] loss: 4.6017158960340785\n",
      "[EPOCH #1, step #1174] loss: 4.601563698383088\n",
      "[EPOCH #1, step #1176] loss: 4.601546503470526\n",
      "[EPOCH #1, step #1178] loss: 4.601644890100093\n",
      "[EPOCH #1, step #1180] loss: 4.601412919292401\n",
      "[EPOCH #1, step #1182] loss: 4.601221764178368\n",
      "[EPOCH #1, step #1184] loss: 4.600898173690346\n",
      "[EPOCH #1, step #1186] loss: 4.600714156882745\n",
      "[EPOCH #1, step #1188] loss: 4.600816626424445\n",
      "[EPOCH #1, step #1190] loss: 4.600839124018159\n",
      "[EPOCH #1, step #1192] loss: 4.600674746722008\n",
      "[EPOCH #1, step #1194] loss: 4.6005921958380664\n",
      "[EPOCH #1, step #1196] loss: 4.600356164930657\n",
      "[EPOCH #1, step #1198] loss: 4.600204918362679\n",
      "[EPOCH #1, step #1200] loss: 4.599977741034998\n",
      "[EPOCH #1, step #1202] loss: 4.599860163995452\n",
      "[EPOCH #1, step #1204] loss: 4.599675484020186\n",
      "[EPOCH #1, step #1206] loss: 4.599651461113753\n",
      "[EPOCH #1, step #1208] loss: 4.599626726114227\n",
      "[EPOCH #1, step #1210] loss: 4.599526173806604\n",
      "[EPOCH #1, step #1212] loss: 4.599634092697307\n",
      "[EPOCH #1, step #1214] loss: 4.5996688571977025\n",
      "[EPOCH #1, step #1216] loss: 4.599379767525265\n",
      "[EPOCH #1, step #1218] loss: 4.59920943810179\n",
      "[EPOCH #1, step #1220] loss: 4.599186463008446\n",
      "[EPOCH #1, step #1222] loss: 4.598944383279671\n",
      "[EPOCH #1, step #1224] loss: 4.598921467528052\n",
      "[EPOCH #1, step #1226] loss: 4.598821337279403\n",
      "[EPOCH #1, step #1228] loss: 4.598449283736425\n",
      "[EPOCH #1, step #1230] loss: 4.5985104822705765\n",
      "[EPOCH #1, step #1232] loss: 4.598335178712472\n",
      "[EPOCH #1, step #1234] loss: 4.598106968065022\n",
      "[EPOCH #1, step #1236] loss: 4.59792753680637\n",
      "[EPOCH #1, step #1238] loss: 4.597773883302333\n",
      "[EPOCH #1, step #1240] loss: 4.597635258406425\n",
      "[EPOCH #1, step #1242] loss: 4.597551669598777\n",
      "[EPOCH #1, step #1244] loss: 4.597339998957622\n",
      "[EPOCH #1, step #1246] loss: 4.597219409040193\n",
      "[EPOCH #1, step #1248] loss: 4.5972383222740305\n",
      "[EPOCH #1, step #1250] loss: 4.597047723835702\n",
      "[EPOCH #1, step #1252] loss: 4.5968278141280505\n",
      "[EPOCH #1, step #1254] loss: 4.5965091154394875\n",
      "[EPOCH #1, step #1256] loss: 4.596350807563217\n",
      "[EPOCH #1, step #1258] loss: 4.596104693848335\n",
      "[EPOCH #1, step #1260] loss: 4.595962634831549\n",
      "[EPOCH #1, step #1262] loss: 4.596031715260171\n",
      "[EPOCH #1, step #1264] loss: 4.59604090098807\n",
      "[EPOCH #1, step #1266] loss: 4.595880447830284\n",
      "[EPOCH #1, step #1268] loss: 4.59589388729363\n",
      "[EPOCH #1, step #1270] loss: 4.595732752682185\n",
      "[EPOCH #1, step #1272] loss: 4.595637558579726\n",
      "[EPOCH #1, step #1274] loss: 4.595614014793845\n",
      "[EPOCH #1, step #1276] loss: 4.595352735571685\n",
      "[EPOCH #1, step #1278] loss: 4.5953202005286435\n",
      "[EPOCH #1, step #1280] loss: 4.595177944147615\n",
      "[EPOCH #1, step #1282] loss: 4.595028268482568\n",
      "[EPOCH #1, step #1284] loss: 4.5950685163416285\n",
      "[EPOCH #1, step #1286] loss: 4.5949914527661875\n",
      "[EPOCH #1, step #1288] loss: 4.594997182199435\n",
      "[EPOCH #1, step #1290] loss: 4.594920513336276\n",
      "[EPOCH #1, step #1292] loss: 4.594877540650335\n",
      "[EPOCH #1, step #1294] loss: 4.594858298430571\n",
      "[EPOCH #1, step #1296] loss: 4.59484515193801\n",
      "[EPOCH #1, step #1298] loss: 4.594895522166803\n",
      "[EPOCH #1, step #1300] loss: 4.594781855451978\n",
      "[EPOCH #1, step #1302] loss: 4.594557625645413\n",
      "[EPOCH #1, step #1304] loss: 4.594536654154459\n",
      "[EPOCH #1, step #1306] loss: 4.594259719950786\n",
      "[EPOCH #1, step #1308] loss: 4.594099437124953\n",
      "[EPOCH #1, step #1310] loss: 4.5940239529496925\n",
      "[EPOCH #1, step #1312] loss: 4.593835678158692\n",
      "[EPOCH #1, step #1314] loss: 4.593719783391336\n",
      "[EPOCH #1, step #1316] loss: 4.593518632962655\n",
      "[EPOCH #1, step #1318] loss: 4.593345643897415\n",
      "[EPOCH #1, step #1320] loss: 4.593230085784427\n",
      "[EPOCH #1, step #1322] loss: 4.593121503396009\n",
      "[EPOCH #1, step #1324] loss: 4.592884501331257\n",
      "[EPOCH #1, step #1326] loss: 4.592913834241969\n",
      "[EPOCH #1, step #1328] loss: 4.592855732632903\n",
      "[EPOCH #1, step #1330] loss: 4.5926433290242255\n",
      "[EPOCH #1, step #1332] loss: 4.592426838771079\n",
      "[EPOCH #1, step #1334] loss: 4.592225639829028\n",
      "[EPOCH #1, step #1336] loss: 4.591922325613611\n",
      "[EPOCH #1, step #1338] loss: 4.591692503216197\n",
      "[EPOCH #1, step #1340] loss: 4.591388886584709\n",
      "[EPOCH #1, step #1342] loss: 4.59136750568495\n",
      "[EPOCH #1, step #1344] loss: 4.591221500595263\n",
      "[EPOCH #1, step #1346] loss: 4.591364154659914\n",
      "[EPOCH #1, step #1348] loss: 4.591343128212829\n",
      "[EPOCH #1, step #1350] loss: 4.5912191551936985\n",
      "[EPOCH #1, step #1352] loss: 4.591215488563532\n",
      "[EPOCH #1, step #1354] loss: 4.590899058549606\n",
      "[EPOCH #1, step #1356] loss: 4.590680643487873\n",
      "[EPOCH #1, step #1358] loss: 4.590463334098294\n",
      "[EPOCH #1, step #1360] loss: 4.590631872130877\n",
      "[EPOCH #1, step #1362] loss: 4.5906719993512946\n",
      "[EPOCH #1, step #1364] loss: 4.590677551297477\n",
      "[EPOCH #1, step #1366] loss: 4.590693437189802\n",
      "[EPOCH #1, step #1368] loss: 4.590515392727013\n",
      "[EPOCH #1, step #1370] loss: 4.590330261631137\n",
      "[EPOCH #1, step #1372] loss: 4.590108777342794\n",
      "[EPOCH #1, step #1374] loss: 4.590020839344372\n",
      "[EPOCH #1, step #1376] loss: 4.589812577246927\n",
      "[EPOCH #1, step #1378] loss: 4.589860713162052\n",
      "[EPOCH #1, step #1380] loss: 4.5897735126807495\n",
      "[EPOCH #1, step #1382] loss: 4.58967860746625\n",
      "[EPOCH #1, step #1384] loss: 4.589660646596971\n",
      "[EPOCH #1, step #1386] loss: 4.589437612974996\n",
      "[EPOCH #1, step #1388] loss: 4.5893882208371695\n",
      "[EPOCH #1, step #1390] loss: 4.589132504014126\n",
      "[EPOCH #1, step #1392] loss: 4.588934283588571\n",
      "[EPOCH #1, step #1394] loss: 4.588695958523767\n",
      "[EPOCH #1, step #1396] loss: 4.588531379795279\n",
      "[EPOCH #1, step #1398] loss: 4.588483336655219\n",
      "[EPOCH #1, step #1400] loss: 4.588273709710371\n",
      "[EPOCH #1, step #1402] loss: 4.588098295909204\n",
      "[EPOCH #1, step #1404] loss: 4.587940403405458\n",
      "[EPOCH #1, step #1406] loss: 4.587732612281805\n",
      "[EPOCH #1, step #1408] loss: 4.58752222480801\n",
      "[EPOCH #1, step #1410] loss: 4.587401391765397\n",
      "[EPOCH #1, step #1412] loss: 4.5874406604672355\n",
      "[EPOCH #1, step #1414] loss: 4.5871859830175605\n",
      "[EPOCH #1, step #1416] loss: 4.587082443223844\n",
      "[EPOCH #1, step #1418] loss: 4.586902955789815\n",
      "[EPOCH #1, step #1420] loss: 4.586915638982033\n",
      "[EPOCH #1, step #1422] loss: 4.586744874443542\n",
      "[EPOCH #1, step #1424] loss: 4.586804449181808\n",
      "[EPOCH #1, step #1426] loss: 4.586767193982045\n",
      "[EPOCH #1, step #1428] loss: 4.5867976321466655\n",
      "[EPOCH #1, step #1430] loss: 4.586787310119778\n",
      "[EPOCH #1, step #1432] loss: 4.58669837937019\n",
      "[EPOCH #1, step #1434] loss: 4.5866796151270846\n",
      "[EPOCH #1, step #1436] loss: 4.58648431243047\n",
      "[EPOCH #1, step #1438] loss: 4.5864410953773564\n",
      "[EPOCH #1, step #1440] loss: 4.586191552947744\n",
      "[EPOCH #1, step #1442] loss: 4.586121871416881\n",
      "[EPOCH #1, step #1444] loss: 4.586070636894464\n",
      "[EPOCH #1, step #1446] loss: 4.5858721568325755\n",
      "[EPOCH #1, step #1448] loss: 4.585754603497154\n",
      "[EPOCH #1, step #1450] loss: 4.585710307633277\n",
      "[EPOCH #1, step #1452] loss: 4.585765149785336\n",
      "[EPOCH #1, step #1454] loss: 4.585633647892483\n",
      "[EPOCH #1, step #1456] loss: 4.585310359973161\n",
      "[EPOCH #1, step #1458] loss: 4.585282136513159\n",
      "[EPOCH #1, step #1460] loss: 4.585229773133034\n",
      "[EPOCH #1, step #1462] loss: 4.58524294349321\n",
      "[EPOCH #1, step #1464] loss: 4.585074178350664\n",
      "[EPOCH #1, step #1466] loss: 4.584989078693351\n",
      "[EPOCH #1, step #1468] loss: 4.584819161380692\n",
      "[EPOCH #1, step #1470] loss: 4.584840860113978\n",
      "[EPOCH #1, step #1472] loss: 4.5846922282693505\n",
      "[EPOCH #1, step #1474] loss: 4.584562541832359\n",
      "[EPOCH #1, step #1476] loss: 4.584277830659818\n",
      "[EPOCH #1, step #1478] loss: 4.584145560790108\n",
      "[EPOCH #1, step #1480] loss: 4.583945135584077\n",
      "[EPOCH #1, step #1482] loss: 4.583971464063216\n",
      "[EPOCH #1, step #1484] loss: 4.584036436947909\n",
      "[EPOCH #1, step #1486] loss: 4.584102014927713\n",
      "[EPOCH #1, step #1488] loss: 4.583957976667252\n",
      "[EPOCH #1, step #1490] loss: 4.583887195203386\n",
      "[EPOCH #1, step #1492] loss: 4.583800398896225\n",
      "[EPOCH #1, step #1494] loss: 4.583553859302431\n",
      "[EPOCH #1, step #1496] loss: 4.583420068642738\n",
      "[EPOCH #1, step #1498] loss: 4.5832183089074965\n",
      "[EPOCH #1, step #1500] loss: 4.583288290912036\n",
      "[EPOCH #1, step #1502] loss: 4.583193865920097\n",
      "[EPOCH #1, step #1504] loss: 4.582971901434204\n",
      "[EPOCH #1, step #1506] loss: 4.5828891218143655\n",
      "[EPOCH #1, step #1508] loss: 4.5827039297721965\n",
      "[EPOCH #1, step #1510] loss: 4.582517902871618\n",
      "[EPOCH #1, step #1512] loss: 4.582281797513691\n",
      "[EPOCH #1, step #1514] loss: 4.582215977029832\n",
      "[EPOCH #1, step #1516] loss: 4.58215028200797\n",
      "[EPOCH #1, step #1518] loss: 4.5819927655685255\n",
      "[EPOCH #1, step #1520] loss: 4.581759742496047\n",
      "[EPOCH #1, step #1522] loss: 4.581668286439391\n",
      "[EPOCH #1, step #1524] loss: 4.58155852114568\n",
      "[EPOCH #1, step #1526] loss: 4.581667958167775\n",
      "[EPOCH #1, step #1528] loss: 4.581541420354649\n",
      "[EPOCH #1, step #1530] loss: 4.58155862028345\n",
      "[EPOCH #1, step #1532] loss: 4.581491710466259\n",
      "[EPOCH #1, step #1534] loss: 4.581329250025051\n",
      "[EPOCH #1, step #1536] loss: 4.581177061846602\n",
      "[EPOCH #1, step #1538] loss: 4.5810663078883165\n",
      "[EPOCH #1, step #1540] loss: 4.580986282563998\n",
      "[EPOCH #1, step #1542] loss: 4.581004638140339\n",
      "[EPOCH #1, step #1544] loss: 4.580995541174435\n",
      "[EPOCH #1, step #1546] loss: 4.580784234217697\n",
      "[EPOCH #1, step #1548] loss: 4.580760893319945\n",
      "[EPOCH #1, step #1550] loss: 4.580679634015534\n",
      "[EPOCH #1, step #1552] loss: 4.580664520484743\n",
      "[EPOCH #1, step #1554] loss: 4.580578472836608\n",
      "[EPOCH #1, step #1556] loss: 4.5804047609033995\n",
      "[EPOCH #1, step #1558] loss: 4.580311623194647\n",
      "[EPOCH #1, step #1560] loss: 4.580205184538192\n",
      "[EPOCH #1, step #1562] loss: 4.580143165832442\n",
      "[EPOCH #1, step #1564] loss: 4.580042688838971\n",
      "[EPOCH #1, step #1566] loss: 4.580076197263307\n",
      "[EPOCH #1, step #1568] loss: 4.579867392151579\n",
      "[EPOCH #1, step #1570] loss: 4.579840260353307\n",
      "[EPOCH #1, step #1572] loss: 4.579713568290328\n",
      "[EPOCH #1, step #1574] loss: 4.579718749636696\n",
      "[EPOCH #1, step #1576] loss: 4.579495371635306\n",
      "[EPOCH #1, step #1578] loss: 4.57950131480946\n",
      "[EPOCH #1, step #1580] loss: 4.579311401311692\n",
      "[EPOCH #1, step #1582] loss: 4.579288166381104\n",
      "[EPOCH #1, step #1584] loss: 4.579189484653413\n",
      "[EPOCH #1, step #1586] loss: 4.579000470618745\n",
      "[EPOCH #1, step #1588] loss: 4.578715044721408\n",
      "[EPOCH #1, step #1590] loss: 4.578743905559566\n",
      "[EPOCH #1, step #1592] loss: 4.5786693926809425\n",
      "[EPOCH #1, step #1594] loss: 4.578629913748618\n",
      "[EPOCH #1, step #1596] loss: 4.578585079482144\n",
      "[EPOCH #1, step #1598] loss: 4.5785413095547005\n",
      "[EPOCH #1, step #1600] loss: 4.5784778115453015\n",
      "[EPOCH #1, step #1602] loss: 4.578414412788206\n",
      "[EPOCH #1, step #1604] loss: 4.578411943667403\n",
      "[EPOCH #1, step #1606] loss: 4.5783009389653\n",
      "[EPOCH #1, step #1608] loss: 4.578251511803171\n",
      "[EPOCH #1, step #1610] loss: 4.578049785524033\n",
      "[EPOCH #1, step #1612] loss: 4.57788614774445\n",
      "[EPOCH #1, step #1614] loss: 4.5777514115206595\n",
      "[EPOCH #1, step #1616] loss: 4.577746314624982\n",
      "[EPOCH #1, step #1618] loss: 4.57761199861636\n",
      "[EPOCH #1, step #1620] loss: 4.577375459641604\n",
      "[EPOCH #1, step #1622] loss: 4.577279517184931\n",
      "[EPOCH #1, step #1624] loss: 4.577217656942515\n",
      "[EPOCH #1, step #1626] loss: 4.577084020522012\n",
      "[EPOCH #1, step #1628] loss: 4.576877791578599\n",
      "[EPOCH #1, step #1630] loss: 4.576804707197964\n",
      "[EPOCH #1, step #1632] loss: 4.576659011023218\n",
      "[EPOCH #1, step #1634] loss: 4.576548318542107\n",
      "[EPOCH #1, step #1636] loss: 4.576339194158555\n",
      "[EPOCH #1, step #1638] loss: 4.57629549219668\n",
      "[EPOCH #1, step #1640] loss: 4.576276884073168\n",
      "[EPOCH #1, step #1642] loss: 4.576225062513671\n",
      "[EPOCH #1, step #1644] loss: 4.576113491000376\n",
      "[EPOCH #1, step #1646] loss: 4.5761366249506015\n",
      "[EPOCH #1, step #1648] loss: 4.5761397797241585\n",
      "[EPOCH #1, step #1650] loss: 4.576148687024177\n",
      "[EPOCH #1, step #1652] loss: 4.576039208402363\n",
      "[EPOCH #1, step #1654] loss: 4.575933013005559\n",
      "[EPOCH #1, step #1656] loss: 4.575841838362423\n",
      "[EPOCH #1, step #1658] loss: 4.5757235848379105\n",
      "[EPOCH #1, step #1660] loss: 4.57554446323172\n",
      "[EPOCH #1, step #1662] loss: 4.575567506983993\n",
      "[EPOCH #1, step #1664] loss: 4.575577721008667\n",
      "[EPOCH #1, step #1666] loss: 4.575437931651188\n",
      "[EPOCH #1, step #1668] loss: 4.5754243703285615\n",
      "[EPOCH #1, step #1670] loss: 4.575260216001263\n",
      "[EPOCH #1, step #1672] loss: 4.5751735487172365\n",
      "[EPOCH #1, step #1674] loss: 4.575087851623991\n",
      "[EPOCH #1, step #1676] loss: 4.574928709631814\n",
      "[EPOCH #1, step #1678] loss: 4.574792204192879\n",
      "[EPOCH #1, step #1680] loss: 4.574613452028618\n",
      "[EPOCH #1, step #1682] loss: 4.574657530566446\n",
      "[EPOCH #1, step #1684] loss: 4.574460104594245\n",
      "[EPOCH #1, step #1686] loss: 4.574307235091412\n",
      "[EPOCH #1, step #1688] loss: 4.574230386068865\n",
      "[EPOCH #1, step #1690] loss: 4.574100661038083\n",
      "[EPOCH #1, step #1692] loss: 4.573931004827852\n",
      "[EPOCH #1, step #1694] loss: 4.573841522014247\n",
      "[EPOCH #1, step #1696] loss: 4.573740507057293\n",
      "[EPOCH #1, step #1698] loss: 4.573743762655634\n",
      "[EPOCH #1, step #1700] loss: 4.573669692616404\n",
      "[EPOCH #1, step #1702] loss: 4.573404886970363\n",
      "[EPOCH #1, step #1704] loss: 4.573256139671348\n",
      "[EPOCH #1, step #1706] loss: 4.573325468352753\n",
      "[EPOCH #1, step #1708] loss: 4.573051034113759\n",
      "[EPOCH #1, step #1710] loss: 4.572856964945166\n",
      "[EPOCH #1, step #1712] loss: 4.572681364370102\n",
      "[EPOCH #1, step #1714] loss: 4.572558353524166\n",
      "[EPOCH #1, step #1716] loss: 4.572308031014391\n",
      "[EPOCH #1, step #1718] loss: 4.572210601487085\n",
      "[EPOCH #1, step #1720] loss: 4.5722452919542755\n",
      "[EPOCH #1, step #1722] loss: 4.572167408168835\n",
      "[EPOCH #1, step #1724] loss: 4.571915497987167\n",
      "[EPOCH #1, step #1726] loss: 4.571830086854406\n",
      "[EPOCH #1, step #1728] loss: 4.571839673730112\n",
      "[EPOCH #1, step #1730] loss: 4.571721609171797\n",
      "[EPOCH #1, step #1732] loss: 4.571704931638864\n",
      "[EPOCH #1, step #1734] loss: 4.571720637505611\n",
      "[EPOCH #1, step #1736] loss: 4.571672148037451\n",
      "[EPOCH #1, step #1738] loss: 4.571749305231534\n",
      "[EPOCH #1, step #1740] loss: 4.5715404578422\n",
      "[EPOCH #1, step #1742] loss: 4.571481665864596\n",
      "[EPOCH #1, step #1744] loss: 4.5712636253553685\n",
      "[EPOCH #1, step #1746] loss: 4.571074967529\n",
      "[EPOCH #1, step #1748] loss: 4.570832078834477\n",
      "[EPOCH #1, step #1750] loss: 4.570674483671248\n",
      "[EPOCH #1, step #1752] loss: 4.570685704911837\n",
      "[EPOCH #1, step #1754] loss: 4.570710080029958\n",
      "[EPOCH #1, step #1756] loss: 4.570668702785034\n",
      "[EPOCH #1, step #1758] loss: 4.570550448247965\n",
      "[EPOCH #1, step #1760] loss: 4.570567539373221\n",
      "[EPOCH #1, step #1762] loss: 4.570439021423738\n",
      "[EPOCH #1, step #1764] loss: 4.570443263337565\n",
      "[EPOCH #1, step #1766] loss: 4.570425830726645\n",
      "[EPOCH #1, step #1768] loss: 4.5704328462591\n",
      "[EPOCH #1, step #1770] loss: 4.570415882926953\n",
      "[EPOCH #1, step #1772] loss: 4.570380910398996\n",
      "[EPOCH #1, step #1774] loss: 4.570327644079504\n",
      "[EPOCH #1, step #1776] loss: 4.570266820771277\n",
      "[EPOCH #1, step #1778] loss: 4.570124549769229\n",
      "[EPOCH #1, step #1780] loss: 4.570070849584101\n",
      "[EPOCH #1, step #1782] loss: 4.570022576589365\n",
      "[EPOCH #1, step #1784] loss: 4.569896786687087\n",
      "[EPOCH #1, step #1786] loss: 4.569936964721338\n",
      "[EPOCH #1, step #1788] loss: 4.569914386550963\n",
      "[EPOCH #1, step #1790] loss: 4.569938046515554\n",
      "[EPOCH #1, step #1792] loss: 4.569779720508514\n",
      "[EPOCH #1, step #1794] loss: 4.56967371034755\n",
      "[EPOCH #1, step #1796] loss: 4.569644594431321\n",
      "[EPOCH #1, step #1798] loss: 4.569680158531354\n",
      "[EPOCH #1, step #1800] loss: 4.569651412540248\n",
      "[EPOCH #1, step #1802] loss: 4.569451540493661\n",
      "[EPOCH #1, step #1804] loss: 4.569384675276907\n",
      "[EPOCH #1, step #1806] loss: 4.569217570526005\n",
      "[EPOCH #1, step #1808] loss: 4.569163675925032\n",
      "[EPOCH #1, step #1810] loss: 4.5690318654743365\n",
      "[EPOCH #1, step #1812] loss: 4.569105543619483\n",
      "[EPOCH #1, step #1814] loss: 4.569063913066853\n",
      "[EPOCH #1, step #1816] loss: 4.569039147701904\n",
      "[EPOCH #1, step #1818] loss: 4.568904728598487\n",
      "[EPOCH #1, step #1820] loss: 4.568679791240493\n",
      "[EPOCH #1, step #1822] loss: 4.568451393585708\n",
      "[EPOCH #1, step #1824] loss: 4.56821932152526\n",
      "[EPOCH #1, step #1826] loss: 4.568229468585235\n",
      "[EPOCH #1, step #1828] loss: 4.567936198316421\n",
      "[EPOCH #1, step #1830] loss: 4.567819421894234\n",
      "[EPOCH #1, step #1832] loss: 4.567621083319414\n",
      "[EPOCH #1, step #1834] loss: 4.567578640295959\n",
      "[EPOCH #1, step #1836] loss: 4.56750166487551\n",
      "[EPOCH #1, step #1838] loss: 4.567312454516114\n",
      "[EPOCH #1, step #1840] loss: 4.567161861805085\n",
      "[EPOCH #1, step #1842] loss: 4.566893750406515\n",
      "[EPOCH #1, step #1844] loss: 4.566824725649867\n",
      "[EPOCH #1, step #1846] loss: 4.566730061672027\n",
      "[EPOCH #1, step #1848] loss: 4.566610160939045\n",
      "[EPOCH #1, step #1850] loss: 4.566605598845397\n",
      "[EPOCH #1, step #1852] loss: 4.566392689348232\n",
      "[EPOCH #1, step #1854] loss: 4.566439966885548\n",
      "[EPOCH #1, step #1856] loss: 4.566371321228799\n",
      "[EPOCH #1, step #1858] loss: 4.566259933838991\n",
      "[EPOCH #1, step #1860] loss: 4.566115387276511\n",
      "[EPOCH #1, step #1862] loss: 4.566089370848348\n",
      "[EPOCH #1, step #1864] loss: 4.565848950598259\n",
      "[EPOCH #1, step #1866] loss: 4.5655923737647\n",
      "[EPOCH #1, step #1868] loss: 4.56546833146505\n",
      "[EPOCH #1, step #1870] loss: 4.565490851104101\n",
      "[EPOCH #1, step #1872] loss: 4.565373730799506\n",
      "[EPOCH #1, step #1874] loss: 4.565280205790202\n",
      "[EPOCH #1, step #1876] loss: 4.5652236943557405\n",
      "[EPOCH #1, step #1878] loss: 4.565002651275504\n",
      "[EPOCH #1, step #1880] loss: 4.564992054924567\n",
      "[EPOCH #1, step #1882] loss: 4.564902792316261\n",
      "[EPOCH #1, step #1884] loss: 4.5649028939973135\n",
      "[EPOCH #1, step #1886] loss: 4.564806066370541\n",
      "[EPOCH #1, step #1888] loss: 4.564699467809949\n",
      "[EPOCH #1, step #1890] loss: 4.564539139766279\n",
      "[EPOCH #1, step #1892] loss: 4.564354140356742\n",
      "[EPOCH #1, step #1894] loss: 4.56434775732438\n",
      "[EPOCH #1, step #1896] loss: 4.564302132264149\n",
      "[EPOCH #1, step #1898] loss: 4.564111502437983\n",
      "[EPOCH #1, step #1900] loss: 4.564027055572799\n",
      "[EPOCH #1, step #1902] loss: 4.5639451044957635\n",
      "[EPOCH #1, step #1904] loss: 4.563932757990879\n",
      "[EPOCH #1, step #1906] loss: 4.563831098553518\n",
      "[EPOCH #1, step #1908] loss: 4.563727294418311\n",
      "[EPOCH #1, step #1910] loss: 4.56364827827282\n",
      "[EPOCH #1, step #1912] loss: 4.5636540598353355\n",
      "[EPOCH #1, step #1914] loss: 4.563555665925028\n",
      "[EPOCH #1, step #1916] loss: 4.563486378579694\n",
      "[EPOCH #1, step #1918] loss: 4.563226365869153\n",
      "[EPOCH #1, step #1920] loss: 4.563373271280872\n",
      "[EPOCH #1, step #1922] loss: 4.563153490819351\n",
      "[EPOCH #1, step #1924] loss: 4.563155118768865\n",
      "[EPOCH #1, step #1926] loss: 4.5631323028984125\n",
      "[EPOCH #1, step #1928] loss: 4.56315267907949\n",
      "[EPOCH #1, step #1930] loss: 4.563031380563002\n",
      "[EPOCH #1, step #1932] loss: 4.56290123441709\n",
      "[EPOCH #1, step #1934] loss: 4.562781200113222\n",
      "[EPOCH #1, step #1936] loss: 4.562689552001874\n",
      "[EPOCH #1, step #1938] loss: 4.562582137721171\n",
      "[EPOCH #1, step #1940] loss: 4.5624474238514106\n",
      "[EPOCH #1, step #1942] loss: 4.5623829829833955\n",
      "[EPOCH #1, step #1944] loss: 4.56230872823524\n",
      "[EPOCH #1, step #1946] loss: 4.562076940374859\n",
      "[EPOCH #1, step #1948] loss: 4.5619696971390296\n",
      "[EPOCH #1, step #1950] loss: 4.561874526027286\n",
      "[EPOCH #1, step #1952] loss: 4.561932673407895\n",
      "[EPOCH #1, step #1954] loss: 4.5618482997045495\n",
      "[EPOCH #1, step #1956] loss: 4.561645938598446\n",
      "[EPOCH #1, step #1958] loss: 4.5614633078231925\n",
      "[EPOCH #1, step #1960] loss: 4.5614016745419965\n",
      "[EPOCH #1, step #1962] loss: 4.561214215282998\n",
      "[EPOCH #1, step #1964] loss: 4.561132399483795\n",
      "[EPOCH #1, step #1966] loss: 4.561010709073135\n",
      "[EPOCH #1, step #1968] loss: 4.560990750335569\n",
      "[EPOCH #1, step #1970] loss: 4.560878114734187\n",
      "[EPOCH #1, step #1972] loss: 4.5607119962399345\n",
      "[EPOCH #1, step #1974] loss: 4.560681917214695\n",
      "[EPOCH #1, step #1976] loss: 4.560604899352889\n",
      "[EPOCH #1, step #1978] loss: 4.560578462389637\n",
      "[EPOCH #1, step #1980] loss: 4.560402144451806\n",
      "[EPOCH #1, step #1982] loss: 4.560279067092872\n",
      "[EPOCH #1, step #1984] loss: 4.560200724133316\n",
      "[EPOCH #1, step #1986] loss: 4.560148144584237\n",
      "[EPOCH #1, step #1988] loss: 4.560146206644326\n",
      "[EPOCH #1, step #1990] loss: 4.559987721043218\n",
      "[EPOCH #1, step #1992] loss: 4.559973565286808\n",
      "[EPOCH #1, step #1994] loss: 4.55982791475186\n",
      "[EPOCH #1, step #1996] loss: 4.559834755356454\n",
      "[EPOCH #1, step #1998] loss: 4.559782949669949\n",
      "[EPOCH #1, step #2000] loss: 4.559626994402274\n",
      "[EPOCH #1, step #2002] loss: 4.5595403207997\n",
      "[EPOCH #1, step #2004] loss: 4.559449536663636\n",
      "[EPOCH #1, step #2006] loss: 4.5592807036581835\n",
      "[EPOCH #1, step #2008] loss: 4.559165062151957\n",
      "[EPOCH #1, step #2010] loss: 4.559087751873212\n",
      "[EPOCH #1, step #2012] loss: 4.55898495829052\n",
      "[EPOCH #1, step #2014] loss: 4.558792985816748\n",
      "[EPOCH #1, step #2016] loss: 4.558644371571727\n",
      "[EPOCH #1, step #2018] loss: 4.5584558129842\n",
      "[EPOCH #1, step #2020] loss: 4.5583360359139045\n",
      "[EPOCH #1, step #2022] loss: 4.558183076531358\n",
      "[EPOCH #1, step #2024] loss: 4.5580666436089405\n",
      "[EPOCH #1, step #2026] loss: 4.558030250805261\n",
      "[EPOCH #1, step #2028] loss: 4.557869819304577\n",
      "[EPOCH #1, step #2030] loss: 4.557759728706394\n",
      "[EPOCH #1, step #2032] loss: 4.557562189153773\n",
      "[EPOCH #1, step #2034] loss: 4.557463754892935\n",
      "[EPOCH #1, step #2036] loss: 4.557461717284422\n",
      "[EPOCH #1, step #2038] loss: 4.557287663089814\n",
      "[EPOCH #1, step #2040] loss: 4.5571510143457585\n",
      "[EPOCH #1, step #2042] loss: 4.557265936598962\n",
      "[EPOCH #1, step #2044] loss: 4.5573203616153934\n",
      "[EPOCH #1, step #2046] loss: 4.557115199103493\n",
      "[EPOCH #1, step #2048] loss: 4.557021561804604\n",
      "[EPOCH #1, step #2050] loss: 4.556908662118079\n",
      "[EPOCH #1, step #2052] loss: 4.5567995364992315\n",
      "[EPOCH #1, step #2054] loss: 4.556700306217166\n",
      "[EPOCH #1, step #2056] loss: 4.556757712514875\n",
      "[EPOCH #1, step #2058] loss: 4.5567091962213135\n",
      "[EPOCH #1, step #2060] loss: 4.556628504174245\n",
      "[EPOCH #1, step #2062] loss: 4.5566456314434545\n",
      "[EPOCH #1, step #2064] loss: 4.556590360128851\n",
      "[EPOCH #1, step #2066] loss: 4.556595494104689\n",
      "[EPOCH #1, step #2068] loss: 4.556509389457869\n",
      "[EPOCH #1, step #2070] loss: 4.556449235780754\n",
      "[EPOCH #1, step #2072] loss: 4.556377249182679\n",
      "[EPOCH #1, step #2074] loss: 4.556310845570392\n",
      "[EPOCH #1, step #2076] loss: 4.556288135355921\n",
      "[EPOCH #1, step #2078] loss: 4.556297521283845\n",
      "[EPOCH #1, step #2080] loss: 4.556209943891431\n",
      "[EPOCH #1, step #2082] loss: 4.556083563824314\n",
      "[EPOCH #1, step #2084] loss: 4.556041746574055\n",
      "[EPOCH #1, step #2086] loss: 4.555865565840571\n",
      "[EPOCH #1, step #2088] loss: 4.555817094358204\n",
      "[EPOCH #1, step #2090] loss: 4.555740556438707\n",
      "[EPOCH #1, step #2092] loss: 4.555805595195641\n",
      "[EPOCH #1, step #2094] loss: 4.555756995672258\n",
      "[EPOCH #1, step #2096] loss: 4.555741078409059\n",
      "[EPOCH #1, step #2098] loss: 4.555673473616223\n",
      "[EPOCH #1, step #2100] loss: 4.555741989856559\n",
      "[EPOCH #1, step #2102] loss: 4.555757990123769\n",
      "[EPOCH #1, step #2104] loss: 4.555642457359477\n",
      "[EPOCH #1, step #2106] loss: 4.555460018516436\n",
      "[EPOCH #1, step #2108] loss: 4.555419337189893\n",
      "[EPOCH #1, step #2110] loss: 4.555411149516934\n",
      "[EPOCH #1, step #2112] loss: 4.555370792076591\n",
      "[EPOCH #1, step #2114] loss: 4.555353193981992\n",
      "[EPOCH #1, step #2116] loss: 4.555241684560006\n",
      "[EPOCH #1, step #2118] loss: 4.555124220728818\n",
      "[EPOCH #1, step #2120] loss: 4.554967246264683\n",
      "[EPOCH #1, step #2122] loss: 4.554852870630309\n",
      "[EPOCH #1, step #2124] loss: 4.554756116530474\n",
      "[EPOCH #1, step #2126] loss: 4.554712104528344\n",
      "[EPOCH #1, step #2128] loss: 4.5545817218850395\n",
      "[EPOCH #1, step #2130] loss: 4.554547300029953\n",
      "[EPOCH #1, step #2132] loss: 4.554524587344185\n",
      "[EPOCH #1, step #2134] loss: 4.554455642342846\n",
      "[EPOCH #1, step #2136] loss: 4.554392414334002\n",
      "[EPOCH #1, step #2138] loss: 4.55428130362512\n",
      "[EPOCH #1, step #2140] loss: 4.554212582885301\n",
      "[EPOCH #1, step #2142] loss: 4.55422139468173\n",
      "[EPOCH #1, step #2144] loss: 4.554104576911126\n",
      "[EPOCH #1, step #2146] loss: 4.554124345766093\n",
      "[EPOCH #1, step #2148] loss: 4.553991233653498\n",
      "[EPOCH #1, step #2150] loss: 4.554006113223506\n",
      "[EPOCH #1, step #2152] loss: 4.553788962277821\n",
      "[EPOCH #1, step #2154] loss: 4.553712643714073\n",
      "[EPOCH #1, step #2156] loss: 4.553586106085921\n",
      "[EPOCH #1, step #2158] loss: 4.553451989524174\n",
      "[EPOCH #1, step #2160] loss: 4.55342196888639\n",
      "[EPOCH #1, step #2162] loss: 4.553436044956653\n",
      "[EPOCH #1, step #2164] loss: 4.553202219868367\n",
      "[EPOCH #1, step #2166] loss: 4.552996977386686\n",
      "[EPOCH #1, step #2168] loss: 4.552875833832322\n",
      "[EPOCH #1, step #2170] loss: 4.552793809643872\n",
      "[EPOCH #1, step #2172] loss: 4.552704197110748\n",
      "[EPOCH #1, step #2174] loss: 4.5527032523319635\n",
      "[EPOCH #1, step #2176] loss: 4.552641650871449\n",
      "[EPOCH #1, step #2178] loss: 4.552462171009234\n",
      "[EPOCH #1, step #2180] loss: 4.552356996103363\n",
      "[EPOCH #1, step #2182] loss: 4.552227855547819\n",
      "[EPOCH #1, step #2184] loss: 4.552205598818083\n",
      "[EPOCH #1, step #2186] loss: 4.552143418085755\n",
      "[EPOCH #1, step #2188] loss: 4.552063047531916\n",
      "[EPOCH #1, step #2190] loss: 4.552034862723757\n",
      "[EPOCH #1, step #2192] loss: 4.551965057496551\n",
      "[EPOCH #1, step #2194] loss: 4.55184739749361\n",
      "[EPOCH #1, step #2196] loss: 4.551806545170752\n",
      "[EPOCH #1, step #2198] loss: 4.551733159433446\n",
      "[EPOCH #1, step #2200] loss: 4.551717617142368\n",
      "[EPOCH #1, step #2202] loss: 4.551718094756914\n",
      "[EPOCH #1, step #2204] loss: 4.551705083392915\n",
      "[EPOCH #1, step #2206] loss: 4.551604646657677\n",
      "[EPOCH #1, step #2208] loss: 4.551549520790442\n",
      "[EPOCH #1, step #2210] loss: 4.55151071358676\n",
      "[EPOCH #1, step #2212] loss: 4.551574268116974\n",
      "[EPOCH #1, step #2214] loss: 4.551530980525651\n",
      "[EPOCH #1, step #2216] loss: 4.5514047641607895\n",
      "[EPOCH #1, step #2218] loss: 4.551289285502492\n",
      "[EPOCH #1, step #2220] loss: 4.551226575543138\n",
      "[EPOCH #1, step #2222] loss: 4.551149976398298\n",
      "[EPOCH #1, step #2224] loss: 4.55114790798573\n",
      "[EPOCH #1, step #2226] loss: 4.551117881617313\n",
      "[EPOCH #1, step #2228] loss: 4.551095611046867\n",
      "[EPOCH #1, step #2230] loss: 4.55117040490527\n",
      "[EPOCH #1, step #2232] loss: 4.5511098516612005\n",
      "[EPOCH #1, step #2234] loss: 4.551044229426373\n",
      "[EPOCH #1, step #2236] loss: 4.550899610489567\n",
      "[EPOCH #1, step #2238] loss: 4.550889224972028\n",
      "[EPOCH #1, step #2240] loss: 4.550788493839453\n",
      "[EPOCH #1, step #2242] loss: 4.5506997858898695\n",
      "[EPOCH #1, step #2244] loss: 4.550712957785762\n",
      "[EPOCH #1, step #2246] loss: 4.550652455583275\n",
      "[EPOCH #1, step #2248] loss: 4.550604509003378\n",
      "[EPOCH #1, step #2250] loss: 4.550442386552632\n",
      "[EPOCH #1, step #2252] loss: 4.55032277075492\n",
      "[EPOCH #1, step #2254] loss: 4.550245418041086\n",
      "[EPOCH #1, step #2256] loss: 4.550160219566876\n",
      "[EPOCH #1, step #2258] loss: 4.550005030663682\n",
      "[EPOCH #1, step #2260] loss: 4.549911562081514\n",
      "[EPOCH #1, step #2262] loss: 4.549854961054756\n",
      "[EPOCH #1, step #2264] loss: 4.549757623777747\n",
      "[EPOCH #1, step #2266] loss: 4.549602040962653\n",
      "[EPOCH #1, step #2268] loss: 4.549558057192403\n",
      "[EPOCH #1, step #2270] loss: 4.549385647582672\n",
      "[EPOCH #1, step #2272] loss: 4.54929111500013\n",
      "[EPOCH #1, step #2274] loss: 4.5493249970740015\n",
      "[EPOCH #1, step #2276] loss: 4.54930738487629\n",
      "[EPOCH #1, step #2278] loss: 4.549255916140172\n",
      "[EPOCH #1, step #2280] loss: 4.54917619333932\n",
      "[EPOCH #1, step #2282] loss: 4.54918026516848\n",
      "[EPOCH #1, step #2284] loss: 4.549088880217571\n",
      "[EPOCH #1, step #2286] loss: 4.548941726918089\n",
      "[EPOCH #1, step #2288] loss: 4.548886480577234\n",
      "[EPOCH #1, step #2290] loss: 4.548746582723923\n",
      "[EPOCH #1, step #2292] loss: 4.548680759443241\n",
      "[EPOCH #1, step #2294] loss: 4.548587764374311\n",
      "[EPOCH #1, step #2296] loss: 4.548528038526443\n",
      "[EPOCH #1, step #2298] loss: 4.548500955804839\n",
      "[EPOCH #1, step #2300] loss: 4.548284240102623\n",
      "[EPOCH #1, step #2302] loss: 4.548204508544154\n",
      "[EPOCH #1, step #2304] loss: 4.54815862121913\n",
      "[EPOCH #1, step #2306] loss: 4.548145715219921\n",
      "[EPOCH #1, step #2308] loss: 4.548055408423264\n",
      "[EPOCH #1, step #2310] loss: 4.547888409776741\n",
      "[EPOCH #1, step #2312] loss: 4.547861061722903\n",
      "[EPOCH #1, step #2314] loss: 4.547754393797971\n",
      "[EPOCH #1, step #2316] loss: 4.5477914674353075\n",
      "[EPOCH #1, step #2318] loss: 4.547780629117307\n",
      "[EPOCH #1, step #2320] loss: 4.547733774271467\n",
      "[EPOCH #1, step #2322] loss: 4.547644594441289\n",
      "[EPOCH #1, step #2324] loss: 4.5476584889811855\n",
      "[EPOCH #1, step #2326] loss: 4.547514368343312\n",
      "[EPOCH #1, step #2328] loss: 4.5473675215996705\n",
      "[EPOCH #1, step #2330] loss: 4.547321614560422\n",
      "[EPOCH #1, step #2332] loss: 4.5472845395542345\n",
      "[EPOCH #1, step #2334] loss: 4.547235107217662\n",
      "[EPOCH #1, step #2336] loss: 4.547099810458678\n",
      "[EPOCH #1, step #2338] loss: 4.547096692175374\n",
      "[EPOCH #1, step #2340] loss: 4.547073046705856\n",
      "[EPOCH #1, step #2342] loss: 4.54694136400992\n",
      "[EPOCH #1, step #2344] loss: 4.546851245172497\n",
      "[EPOCH #1, step #2346] loss: 4.546836745347883\n",
      "[EPOCH #1, step #2348] loss: 4.546807830716356\n",
      "[EPOCH #1, step #2350] loss: 4.546758669987479\n",
      "[EPOCH #1, step #2352] loss: 4.5466414239626465\n",
      "[EPOCH #1, step #2354] loss: 4.5465994237587966\n",
      "[EPOCH #1, step #2356] loss: 4.5464966678498\n",
      "[EPOCH #1, step #2358] loss: 4.546421069217567\n",
      "[EPOCH #1, step #2360] loss: 4.546314265757282\n",
      "[EPOCH #1, step #2362] loss: 4.546314706653042\n",
      "[EPOCH #1, step #2364] loss: 4.546350776115886\n",
      "[EPOCH #1, step #2366] loss: 4.546220094884656\n",
      "[EPOCH #1, step #2368] loss: 4.5461711722519285\n",
      "[EPOCH #1, step #2370] loss: 4.546077338515933\n",
      "[EPOCH #1, step #2372] loss: 4.545997663434427\n",
      "[EPOCH #1, step #2374] loss: 4.545937310469778\n",
      "[EPOCH #1, step #2376] loss: 4.545902707168521\n",
      "[EPOCH #1, step #2378] loss: 4.5458182685261965\n",
      "[EPOCH #1, step #2380] loss: 4.545748364770177\n",
      "[EPOCH #1, step #2382] loss: 4.545590607798805\n",
      "[EPOCH #1, step #2384] loss: 4.545480862703464\n",
      "[EPOCH #1, step #2386] loss: 4.545389654050076\n",
      "[EPOCH #1, step #2388] loss: 4.545321184703624\n",
      "[EPOCH #1, step #2390] loss: 4.5452437851529695\n",
      "[EPOCH #1, step #2392] loss: 4.54535328569743\n",
      "[EPOCH #1, step #2394] loss: 4.545229496886187\n",
      "[EPOCH #1, step #2396] loss: 4.545077919114566\n",
      "[EPOCH #1, step #2398] loss: 4.54499355660423\n",
      "[EPOCH #1, step #2400] loss: 4.544891251965594\n",
      "[EPOCH #1, step #2402] loss: 4.544886549561906\n",
      "[EPOCH #1, step #2404] loss: 4.544775342445612\n",
      "[EPOCH #1, step #2406] loss: 4.544708231548378\n",
      "[EPOCH #1, step #2408] loss: 4.544696811424246\n",
      "[EPOCH #1, step #2410] loss: 4.54448908169405\n",
      "[EPOCH #1, step #2412] loss: 4.5444493137648925\n",
      "[EPOCH #1, step #2414] loss: 4.544322896053085\n",
      "[EPOCH #1, step #2416] loss: 4.544326866861706\n",
      "[EPOCH #1, step #2418] loss: 4.544187052972359\n",
      "[EPOCH #1, step #2420] loss: 4.5442193932200405\n",
      "[EPOCH #1, step #2422] loss: 4.54418703272871\n",
      "[EPOCH #1, step #2424] loss: 4.544152192774507\n",
      "[EPOCH #1, step #2426] loss: 4.5440319455794755\n",
      "[EPOCH #1, step #2428] loss: 4.544007208672213\n",
      "[EPOCH #1, step #2430] loss: 4.544055578395792\n",
      "[EPOCH #1, step #2432] loss: 4.544003582079127\n",
      "[EPOCH #1, step #2434] loss: 4.5438549854427395\n",
      "[EPOCH #1, step #2436] loss: 4.54383376625947\n",
      "[EPOCH #1, step #2438] loss: 4.543760599978589\n",
      "[EPOCH #1, step #2440] loss: 4.543679740965147\n",
      "[EPOCH #1, step #2442] loss: 4.543568602752061\n",
      "[EPOCH #1, step #2444] loss: 4.54347268594067\n",
      "[EPOCH #1, step #2446] loss: 4.543469877207966\n",
      "[EPOCH #1, step #2448] loss: 4.543389654490547\n",
      "[EPOCH #1, step #2450] loss: 4.5433455838421715\n",
      "[EPOCH #1, step #2452] loss: 4.543274791785954\n",
      "[EPOCH #1, step #2454] loss: 4.543219281019844\n",
      "[EPOCH #1, step #2456] loss: 4.543075607518719\n",
      "[EPOCH #1, step #2458] loss: 4.542960513845213\n",
      "[EPOCH #1, step #2460] loss: 4.542813016650445\n",
      "[EPOCH #1, step #2462] loss: 4.542678925161482\n",
      "[EPOCH #1, step #2464] loss: 4.542463590649031\n",
      "[EPOCH #1, step #2466] loss: 4.542372832280883\n",
      "[EPOCH #1, step #2468] loss: 4.542316727620863\n",
      "[EPOCH #1, step #2470] loss: 4.542154758481157\n",
      "[EPOCH #1, step #2472] loss: 4.5420373148441895\n",
      "[EPOCH #1, step #2474] loss: 4.541931229793664\n",
      "[EPOCH #1, step #2476] loss: 4.541741560388102\n",
      "[EPOCH #1, step #2478] loss: 4.541675923090877\n",
      "[EPOCH #1, step #2480] loss: 4.5416599477024535\n",
      "[EPOCH #1, step #2482] loss: 4.541582076049264\n",
      "[EPOCH #1, step #2484] loss: 4.541479889008121\n",
      "[EPOCH #1, step #2486] loss: 4.5413926948636325\n",
      "[EPOCH #1, step #2488] loss: 4.541297393421803\n",
      "[EPOCH #1, step #2490] loss: 4.541354678616089\n",
      "[EPOCH #1, step #2492] loss: 4.541260867342622\n",
      "[EPOCH #1, step #2494] loss: 4.541254388163228\n",
      "[EPOCH #1, step #2496] loss: 4.541105639137265\n",
      "[EPOCH #1, step #2498] loss: 4.541000041259485\n",
      "[EPOCH #1, elapsed time: 515.201[sec]] loss: 4.540982821655273\n",
      "[EPOCH #2, step #0] loss: 4.520101547241211\n",
      "[EPOCH #2, step #2] loss: 4.431180318196614\n",
      "[EPOCH #2, step #4] loss: 4.4629161834716795\n",
      "[EPOCH #2, step #6] loss: 4.441469056265695\n",
      "[EPOCH #2, step #8] loss: 4.419726212819417\n",
      "[EPOCH #2, step #10] loss: 4.418269330804998\n",
      "[EPOCH #2, step #12] loss: 4.406678163088285\n",
      "[EPOCH #2, step #14] loss: 4.406754207611084\n",
      "[EPOCH #2, step #16] loss: 4.409600370070514\n",
      "[EPOCH #2, step #18] loss: 4.410100610632646\n",
      "[EPOCH #2, step #20] loss: 4.416214715866816\n",
      "[EPOCH #2, step #22] loss: 4.427010577657948\n",
      "[EPOCH #2, step #24] loss: 4.42234052658081\n",
      "[EPOCH #2, step #26] loss: 4.418484758447717\n",
      "[EPOCH #2, step #28] loss: 4.406919347828832\n",
      "[EPOCH #2, step #30] loss: 4.408618957765641\n",
      "[EPOCH #2, step #32] loss: 4.410001075629032\n",
      "[EPOCH #2, step #34] loss: 4.406708472115653\n",
      "[EPOCH #2, step #36] loss: 4.410117136465536\n",
      "[EPOCH #2, step #38] loss: 4.416402046497051\n",
      "[EPOCH #2, step #40] loss: 4.4169579831565295\n",
      "[EPOCH #2, step #42] loss: 4.418800121129945\n",
      "[EPOCH #2, step #44] loss: 4.4172371546427405\n",
      "[EPOCH #2, step #46] loss: 4.421963194583324\n",
      "[EPOCH #2, step #48] loss: 4.425731328068947\n",
      "[EPOCH #2, step #50] loss: 4.42991023905137\n",
      "[EPOCH #2, step #52] loss: 4.428954916180305\n",
      "[EPOCH #2, step #54] loss: 4.426758445392956\n",
      "[EPOCH #2, step #56] loss: 4.427801726157205\n",
      "[EPOCH #2, step #58] loss: 4.424746222415213\n",
      "[EPOCH #2, step #60] loss: 4.428052597358579\n",
      "[EPOCH #2, step #62] loss: 4.429406574794224\n",
      "[EPOCH #2, step #64] loss: 4.429688050196721\n",
      "[EPOCH #2, step #66] loss: 4.428629135017965\n",
      "[EPOCH #2, step #68] loss: 4.42787852494613\n",
      "[EPOCH #2, step #70] loss: 4.42543757129723\n",
      "[EPOCH #2, step #72] loss: 4.431244275341295\n",
      "[EPOCH #2, step #74] loss: 4.426942291259766\n",
      "[EPOCH #2, step #76] loss: 4.427174363817487\n",
      "[EPOCH #2, step #78] loss: 4.428454254246965\n",
      "[EPOCH #2, step #80] loss: 4.430326873873487\n",
      "[EPOCH #2, step #82] loss: 4.430864678808005\n",
      "[EPOCH #2, step #84] loss: 4.432822064792409\n",
      "[EPOCH #2, step #86] loss: 4.434267477057446\n",
      "[EPOCH #2, step #88] loss: 4.435599118136288\n",
      "[EPOCH #2, step #90] loss: 4.436780180249896\n",
      "[EPOCH #2, step #92] loss: 4.43700147444202\n",
      "[EPOCH #2, step #94] loss: 4.437660799528423\n",
      "[EPOCH #2, step #96] loss: 4.435777383981292\n",
      "[EPOCH #2, step #98] loss: 4.437755603982945\n",
      "[EPOCH #2, step #100] loss: 4.43786620621634\n",
      "[EPOCH #2, step #102] loss: 4.4369204947092005\n",
      "[EPOCH #2, step #104] loss: 4.436428147270566\n",
      "[EPOCH #2, step #106] loss: 4.436143465131242\n",
      "[EPOCH #2, step #108] loss: 4.434549161053579\n",
      "[EPOCH #2, step #110] loss: 4.433783166043393\n",
      "[EPOCH #2, step #112] loss: 4.4331151320871\n",
      "[EPOCH #2, step #114] loss: 4.4330147701761\n",
      "[EPOCH #2, step #116] loss: 4.433259417868068\n",
      "[EPOCH #2, step #118] loss: 4.431243022950757\n",
      "[EPOCH #2, step #120] loss: 4.4307758867248035\n",
      "[EPOCH #2, step #122] loss: 4.430215944119586\n",
      "[EPOCH #2, step #124] loss: 4.4282529754638675\n",
      "[EPOCH #2, step #126] loss: 4.426920222485159\n",
      "[EPOCH #2, step #128] loss: 4.427344263061997\n",
      "[EPOCH #2, step #130] loss: 4.428023290998153\n",
      "[EPOCH #2, step #132] loss: 4.4269127917469\n",
      "[EPOCH #2, step #134] loss: 4.427807846775761\n",
      "[EPOCH #2, step #136] loss: 4.4293743015205775\n",
      "[EPOCH #2, step #138] loss: 4.428080936130002\n",
      "[EPOCH #2, step #140] loss: 4.428498014490655\n",
      "[EPOCH #2, step #142] loss: 4.42812626178448\n",
      "[EPOCH #2, step #144] loss: 4.428438186645508\n",
      "[EPOCH #2, step #146] loss: 4.427474417654024\n",
      "[EPOCH #2, step #148] loss: 4.427107657362151\n",
      "[EPOCH #2, step #150] loss: 4.427597964836272\n",
      "[EPOCH #2, step #152] loss: 4.428349794126024\n",
      "[EPOCH #2, step #154] loss: 4.428292619028399\n",
      "[EPOCH #2, step #156] loss: 4.430843760253517\n",
      "[EPOCH #2, step #158] loss: 4.4303388595581055\n",
      "[EPOCH #2, step #160] loss: 4.4293257375681625\n",
      "[EPOCH #2, step #162] loss: 4.4294821265285\n",
      "[EPOCH #2, step #164] loss: 4.430831276286732\n",
      "[EPOCH #2, step #166] loss: 4.431292419662019\n",
      "[EPOCH #2, step #168] loss: 4.432541550969231\n",
      "[EPOCH #2, step #170] loss: 4.431353870191072\n",
      "[EPOCH #2, step #172] loss: 4.432147304446711\n",
      "[EPOCH #2, step #174] loss: 4.431996895926339\n",
      "[EPOCH #2, step #176] loss: 4.432639763180146\n",
      "[EPOCH #2, step #178] loss: 4.431599470490184\n",
      "[EPOCH #2, step #180] loss: 4.431940076100892\n",
      "[EPOCH #2, step #182] loss: 4.432795446427142\n",
      "[EPOCH #2, step #184] loss: 4.433383946805387\n",
      "[EPOCH #2, step #186] loss: 4.43349039108358\n",
      "[EPOCH #2, step #188] loss: 4.4335972013927645\n",
      "[EPOCH #2, step #190] loss: 4.434481698180993\n",
      "[EPOCH #2, step #192] loss: 4.435530689713868\n",
      "[EPOCH #2, step #194] loss: 4.43480447377914\n",
      "[EPOCH #2, step #196] loss: 4.435217586265603\n",
      "[EPOCH #2, step #198] loss: 4.4345044083331695\n",
      "[EPOCH #2, step #200] loss: 4.434550451402047\n",
      "[EPOCH #2, step #202] loss: 4.434779463143184\n",
      "[EPOCH #2, step #204] loss: 4.434926263297476\n",
      "[EPOCH #2, step #206] loss: 4.43530940088097\n",
      "[EPOCH #2, step #208] loss: 4.4344373205632115\n",
      "[EPOCH #2, step #210] loss: 4.4343649222387524\n",
      "[EPOCH #2, step #212] loss: 4.4336336991037\n",
      "[EPOCH #2, step #214] loss: 4.433075935896053\n",
      "[EPOCH #2, step #216] loss: 4.433197239027595\n",
      "[EPOCH #2, step #218] loss: 4.4328009331063045\n",
      "[EPOCH #2, step #220] loss: 4.434288868537316\n",
      "[EPOCH #2, step #222] loss: 4.433713192362422\n",
      "[EPOCH #2, step #224] loss: 4.434499977959527\n",
      "[EPOCH #2, step #226] loss: 4.434094389104633\n",
      "[EPOCH #2, step #228] loss: 4.433729740209454\n",
      "[EPOCH #2, step #230] loss: 4.433927909636394\n",
      "[EPOCH #2, step #232] loss: 4.434397390472019\n",
      "[EPOCH #2, step #234] loss: 4.434398385311695\n",
      "[EPOCH #2, step #236] loss: 4.43475857465076\n",
      "[EPOCH #2, step #238] loss: 4.435663777914007\n",
      "[EPOCH #2, step #240] loss: 4.435324419583522\n",
      "[EPOCH #2, step #242] loss: 4.435338703202612\n",
      "[EPOCH #2, step #244] loss: 4.435263106287742\n",
      "[EPOCH #2, step #246] loss: 4.435600230568333\n",
      "[EPOCH #2, step #248] loss: 4.434897223629626\n",
      "[EPOCH #2, step #250] loss: 4.43503859698535\n",
      "[EPOCH #2, step #252] loss: 4.43624954826747\n",
      "[EPOCH #2, step #254] loss: 4.435622276979334\n",
      "[EPOCH #2, step #256] loss: 4.435544227347764\n",
      "[EPOCH #2, step #258] loss: 4.435421439211341\n",
      "[EPOCH #2, step #260] loss: 4.435099335009111\n",
      "[EPOCH #2, step #262] loss: 4.434618681555919\n",
      "[EPOCH #2, step #264] loss: 4.434731019218013\n",
      "[EPOCH #2, step #266] loss: 4.434773121880235\n",
      "[EPOCH #2, step #268] loss: 4.434935904790034\n",
      "[EPOCH #2, step #270] loss: 4.434512280889983\n",
      "[EPOCH #2, step #272] loss: 4.43377482061421\n",
      "[EPOCH #2, step #274] loss: 4.432567277388139\n",
      "[EPOCH #2, step #276] loss: 4.431829247664028\n",
      "[EPOCH #2, step #278] loss: 4.431018020944356\n",
      "[EPOCH #2, step #280] loss: 4.4307140082226955\n",
      "[EPOCH #2, step #282] loss: 4.430001883961708\n",
      "[EPOCH #2, step #284] loss: 4.429052838107996\n",
      "[EPOCH #2, step #286] loss: 4.428566525622112\n",
      "[EPOCH #2, step #288] loss: 4.427949709050796\n",
      "[EPOCH #2, step #290] loss: 4.426892138019051\n",
      "[EPOCH #2, step #292] loss: 4.426075022375217\n",
      "[EPOCH #2, step #294] loss: 4.425975679947158\n",
      "[EPOCH #2, step #296] loss: 4.425329873056123\n",
      "[EPOCH #2, step #298] loss: 4.425205806425981\n",
      "[EPOCH #2, step #300] loss: 4.425801419736539\n",
      "[EPOCH #2, step #302] loss: 4.4265592436585885\n",
      "[EPOCH #2, step #304] loss: 4.426121184864982\n",
      "[EPOCH #2, step #306] loss: 4.426172099594961\n",
      "[EPOCH #2, step #308] loss: 4.4253595237978836\n",
      "[EPOCH #2, step #310] loss: 4.424753613793965\n",
      "[EPOCH #2, step #312] loss: 4.424434405927079\n",
      "[EPOCH #2, step #314] loss: 4.424069188133118\n",
      "[EPOCH #2, step #316] loss: 4.423504988853864\n",
      "[EPOCH #2, step #318] loss: 4.42371365119671\n",
      "[EPOCH #2, step #320] loss: 4.424290174264403\n",
      "[EPOCH #2, step #322] loss: 4.4239404090786865\n",
      "[EPOCH #2, step #324] loss: 4.423658266801101\n",
      "[EPOCH #2, step #326] loss: 4.42380251432413\n",
      "[EPOCH #2, step #328] loss: 4.423340475667936\n",
      "[EPOCH #2, step #330] loss: 4.423731726101878\n",
      "[EPOCH #2, step #332] loss: 4.4234826085087775\n",
      "[EPOCH #2, step #334] loss: 4.422704723699769\n",
      "[EPOCH #2, step #336] loss: 4.4228418191745655\n",
      "[EPOCH #2, step #338] loss: 4.422813177812064\n",
      "[EPOCH #2, step #340] loss: 4.4221372660304095\n",
      "[EPOCH #2, step #342] loss: 4.422220274588804\n",
      "[EPOCH #2, step #344] loss: 4.421088240803152\n",
      "[EPOCH #2, step #346] loss: 4.420765221290698\n",
      "[EPOCH #2, step #348] loss: 4.42124037346389\n",
      "[EPOCH #2, step #350] loss: 4.421540360844713\n",
      "[EPOCH #2, step #352] loss: 4.421822870757019\n",
      "[EPOCH #2, step #354] loss: 4.421670512078514\n",
      "[EPOCH #2, step #356] loss: 4.422008999255525\n",
      "[EPOCH #2, step #358] loss: 4.421908953727786\n",
      "[EPOCH #2, step #360] loss: 4.421947327347013\n",
      "[EPOCH #2, step #362] loss: 4.421918439471032\n",
      "[EPOCH #2, step #364] loss: 4.422065152207466\n",
      "[EPOCH #2, step #366] loss: 4.422546211315436\n",
      "[EPOCH #2, step #368] loss: 4.42245020646713\n",
      "[EPOCH #2, step #370] loss: 4.421981180453236\n",
      "[EPOCH #2, step #372] loss: 4.422179571105712\n",
      "[EPOCH #2, step #374] loss: 4.422013291676839\n",
      "[EPOCH #2, step #376] loss: 4.421350296991889\n",
      "[EPOCH #2, step #378] loss: 4.4213109733561415\n",
      "[EPOCH #2, step #380] loss: 4.421270743442646\n",
      "[EPOCH #2, step #382] loss: 4.422047582681123\n",
      "[EPOCH #2, step #384] loss: 4.4225422636255045\n",
      "[EPOCH #2, step #386] loss: 4.421770258467327\n",
      "[EPOCH #2, step #388] loss: 4.421591679050867\n",
      "[EPOCH #2, step #390] loss: 4.421310702858069\n",
      "[EPOCH #2, step #392] loss: 4.4209953858955515\n",
      "[EPOCH #2, step #394] loss: 4.4212757001949266\n",
      "[EPOCH #2, step #396] loss: 4.421543081701553\n",
      "[EPOCH #2, step #398] loss: 4.422043874449001\n",
      "[EPOCH #2, step #400] loss: 4.421725991360862\n",
      "[EPOCH #2, step #402] loss: 4.421645952513437\n",
      "[EPOCH #2, step #404] loss: 4.422009788324804\n",
      "[EPOCH #2, step #406] loss: 4.421511074834725\n",
      "[EPOCH #2, step #408] loss: 4.4213611817301635\n",
      "[EPOCH #2, step #410] loss: 4.421630365135026\n",
      "[EPOCH #2, step #412] loss: 4.421411874507877\n",
      "[EPOCH #2, step #414] loss: 4.421252504601536\n",
      "[EPOCH #2, step #416] loss: 4.420650461594835\n",
      "[EPOCH #2, step #418] loss: 4.420642288317259\n",
      "[EPOCH #2, step #420] loss: 4.420976217455649\n",
      "[EPOCH #2, step #422] loss: 4.421104147079143\n",
      "[EPOCH #2, step #424] loss: 4.420887533075669\n",
      "[EPOCH #2, step #426] loss: 4.4208923782062755\n",
      "[EPOCH #2, step #428] loss: 4.420593489577998\n",
      "[EPOCH #2, step #430] loss: 4.420686134329528\n",
      "[EPOCH #2, step #432] loss: 4.42103938527801\n",
      "[EPOCH #2, step #434] loss: 4.421162965928001\n",
      "[EPOCH #2, step #436] loss: 4.4209874783828\n",
      "[EPOCH #2, step #438] loss: 4.420620988876238\n",
      "[EPOCH #2, step #440] loss: 4.420574494229963\n",
      "[EPOCH #2, step #442] loss: 4.420372955416987\n",
      "[EPOCH #2, step #444] loss: 4.42048210508368\n",
      "[EPOCH #2, step #446] loss: 4.4201943036960545\n",
      "[EPOCH #2, step #448] loss: 4.420152405057028\n",
      "[EPOCH #2, step #450] loss: 4.4201217004306566\n",
      "[EPOCH #2, step #452] loss: 4.420177140772737\n",
      "[EPOCH #2, step #454] loss: 4.419938605172294\n",
      "[EPOCH #2, step #456] loss: 4.41975330523902\n",
      "[EPOCH #2, step #458] loss: 4.419910163920949\n",
      "[EPOCH #2, step #460] loss: 4.419625876006749\n",
      "[EPOCH #2, step #462] loss: 4.419458603498487\n",
      "[EPOCH #2, step #464] loss: 4.419533778775123\n",
      "[EPOCH #2, step #466] loss: 4.419009640589516\n",
      "[EPOCH #2, step #468] loss: 4.4190466185368455\n",
      "[EPOCH #2, step #470] loss: 4.419113151080542\n",
      "[EPOCH #2, step #472] loss: 4.4185044931810955\n",
      "[EPOCH #2, step #474] loss: 4.4187320207294665\n",
      "[EPOCH #2, step #476] loss: 4.418388325713216\n",
      "[EPOCH #2, step #478] loss: 4.418082702134995\n",
      "[EPOCH #2, step #480] loss: 4.4178474539282915\n",
      "[EPOCH #2, step #482] loss: 4.417787031604143\n",
      "[EPOCH #2, step #484] loss: 4.417187032011366\n",
      "[EPOCH #2, step #486] loss: 4.416970915862912\n",
      "[EPOCH #2, step #488] loss: 4.416956001507968\n",
      "[EPOCH #2, step #490] loss: 4.416990915286808\n",
      "[EPOCH #2, step #492] loss: 4.416707119158267\n",
      "[EPOCH #2, step #494] loss: 4.416754667686694\n",
      "[EPOCH #2, step #496] loss: 4.416470406760632\n",
      "[EPOCH #2, step #498] loss: 4.416379200432726\n",
      "[EPOCH #2, step #500] loss: 4.416552418006394\n",
      "[EPOCH #2, step #502] loss: 4.416745950877074\n",
      "[EPOCH #2, step #504] loss: 4.416244266056778\n",
      "[EPOCH #2, step #506] loss: 4.416057699530787\n",
      "[EPOCH #2, step #508] loss: 4.416083588815625\n",
      "[EPOCH #2, step #510] loss: 4.4161217758566895\n",
      "[EPOCH #2, step #512] loss: 4.416307058018318\n",
      "[EPOCH #2, step #514] loss: 4.415847426479303\n",
      "[EPOCH #2, step #516] loss: 4.41560854902581\n",
      "[EPOCH #2, step #518] loss: 4.4158628166066425\n",
      "[EPOCH #2, step #520] loss: 4.41597635503465\n",
      "[EPOCH #2, step #522] loss: 4.416514478723588\n",
      "[EPOCH #2, step #524] loss: 4.4166968772524875\n",
      "[EPOCH #2, step #526] loss: 4.416444319701512\n",
      "[EPOCH #2, step #528] loss: 4.416640902737363\n",
      "[EPOCH #2, step #530] loss: 4.416472170088026\n",
      "[EPOCH #2, step #532] loss: 4.416514728574771\n",
      "[EPOCH #2, step #534] loss: 4.416428093598268\n",
      "[EPOCH #2, step #536] loss: 4.4163339950518905\n",
      "[EPOCH #2, step #538] loss: 4.416779883496173\n",
      "[EPOCH #2, step #540] loss: 4.416873597834335\n",
      "[EPOCH #2, step #542] loss: 4.416967565842096\n",
      "[EPOCH #2, step #544] loss: 4.416990260027964\n",
      "[EPOCH #2, step #546] loss: 4.416789988495096\n",
      "[EPOCH #2, step #548] loss: 4.417011901111985\n",
      "[EPOCH #2, step #550] loss: 4.416865415451964\n",
      "[EPOCH #2, step #552] loss: 4.416913143764999\n",
      "[EPOCH #2, step #554] loss: 4.41714928858989\n",
      "[EPOCH #2, step #556] loss: 4.4167940586644825\n",
      "[EPOCH #2, step #558] loss: 4.416815746662229\n",
      "[EPOCH #2, step #560] loss: 4.4169926150384855\n",
      "[EPOCH #2, step #562] loss: 4.417046984813141\n",
      "[EPOCH #2, step #564] loss: 4.416871986557952\n",
      "[EPOCH #2, step #566] loss: 4.416707582995164\n",
      "[EPOCH #2, step #568] loss: 4.416740306022926\n",
      "[EPOCH #2, step #570] loss: 4.416798835460442\n",
      "[EPOCH #2, step #572] loss: 4.416539729785753\n",
      "[EPOCH #2, step #574] loss: 4.416681962220565\n",
      "[EPOCH #2, step #576] loss: 4.416511527380456\n",
      "[EPOCH #2, step #578] loss: 4.416816377886836\n",
      "[EPOCH #2, step #580] loss: 4.416640917733696\n",
      "[EPOCH #2, step #582] loss: 4.416365383828688\n",
      "[EPOCH #2, step #584] loss: 4.416037303565914\n",
      "[EPOCH #2, step #586] loss: 4.415852948794779\n",
      "[EPOCH #2, step #588] loss: 4.415504151167813\n",
      "[EPOCH #2, step #590] loss: 4.415881497404095\n",
      "[EPOCH #2, step #592] loss: 4.415854610037844\n",
      "[EPOCH #2, step #594] loss: 4.415683829684218\n",
      "[EPOCH #2, step #596] loss: 4.415761655299508\n",
      "[EPOCH #2, step #598] loss: 4.41542971870537\n",
      "[EPOCH #2, step #600] loss: 4.415534908878625\n",
      "[EPOCH #2, step #602] loss: 4.415380868547987\n",
      "[EPOCH #2, step #604] loss: 4.415208780272933\n",
      "[EPOCH #2, step #606] loss: 4.4154546131215735\n",
      "[EPOCH #2, step #608] loss: 4.415482373073183\n",
      "[EPOCH #2, step #610] loss: 4.415352313296104\n",
      "[EPOCH #2, step #612] loss: 4.415414271891409\n",
      "[EPOCH #2, step #614] loss: 4.415579301942655\n",
      "[EPOCH #2, step #616] loss: 4.415452823453436\n",
      "[EPOCH #2, step #618] loss: 4.415413356559149\n",
      "[EPOCH #2, step #620] loss: 4.415391525016699\n",
      "[EPOCH #2, step #622] loss: 4.415159807924666\n",
      "[EPOCH #2, step #624] loss: 4.415017944335937\n",
      "[EPOCH #2, step #626] loss: 4.41504757027877\n",
      "[EPOCH #2, step #628] loss: 4.414897684452075\n",
      "[EPOCH #2, step #630] loss: 4.414629089851198\n",
      "[EPOCH #2, step #632] loss: 4.414477804825769\n",
      "[EPOCH #2, step #634] loss: 4.414201988009956\n",
      "[EPOCH #2, step #636] loss: 4.414235925000728\n",
      "[EPOCH #2, step #638] loss: 4.413976812586538\n",
      "[EPOCH #2, step #640] loss: 4.414133376152765\n",
      "[EPOCH #2, step #642] loss: 4.413920891970906\n",
      "[EPOCH #2, step #644] loss: 4.413671628818956\n",
      "[EPOCH #2, step #646] loss: 4.413756175240188\n",
      "[EPOCH #2, step #648] loss: 4.413644803874481\n",
      "[EPOCH #2, step #650] loss: 4.413553162470757\n",
      "[EPOCH #2, step #652] loss: 4.413591103020708\n",
      "[EPOCH #2, step #654] loss: 4.413336127768946\n",
      "[EPOCH #2, step #656] loss: 4.413328349318134\n",
      "[EPOCH #2, step #658] loss: 4.413318945171258\n",
      "[EPOCH #2, step #660] loss: 4.413404661660476\n",
      "[EPOCH #2, step #662] loss: 4.413351250989405\n",
      "[EPOCH #2, step #664] loss: 4.413310857643759\n",
      "[EPOCH #2, step #666] loss: 4.413309391113235\n",
      "[EPOCH #2, step #668] loss: 4.41319064637827\n",
      "[EPOCH #2, step #670] loss: 4.412987872491828\n",
      "[EPOCH #2, step #672] loss: 4.413285763444376\n",
      "[EPOCH #2, step #674] loss: 4.413122066921658\n",
      "[EPOCH #2, step #676] loss: 4.4131903275077065\n",
      "[EPOCH #2, step #678] loss: 4.4130432328349185\n",
      "[EPOCH #2, step #680] loss: 4.413056423310492\n",
      "[EPOCH #2, step #682] loss: 4.412970043031573\n",
      "[EPOCH #2, step #684] loss: 4.412868785858154\n",
      "[EPOCH #2, step #686] loss: 4.412853824693216\n",
      "[EPOCH #2, step #688] loss: 4.412942433737878\n",
      "[EPOCH #2, step #690] loss: 4.4132500771330685\n",
      "[EPOCH #2, step #692] loss: 4.413269468200155\n",
      "[EPOCH #2, step #694] loss: 4.413288482830679\n",
      "[EPOCH #2, step #696] loss: 4.41342933140321\n",
      "[EPOCH #2, step #698] loss: 4.413063944324063\n",
      "[EPOCH #2, step #700] loss: 4.412940179060257\n",
      "[EPOCH #2, step #702] loss: 4.412914525733394\n",
      "[EPOCH #2, step #704] loss: 4.412702445442795\n",
      "[EPOCH #2, step #706] loss: 4.412872015671144\n",
      "[EPOCH #2, step #708] loss: 4.412858291808909\n",
      "[EPOCH #2, step #710] loss: 4.413084310486012\n",
      "[EPOCH #2, step #712] loss: 4.413054919008787\n",
      "[EPOCH #2, step #714] loss: 4.413233014086743\n",
      "[EPOCH #2, step #716] loss: 4.413217292503665\n",
      "[EPOCH #2, step #718] loss: 4.4133056791833445\n",
      "[EPOCH #2, step #720] loss: 4.413525756619013\n",
      "[EPOCH #2, step #722] loss: 4.413509472442036\n",
      "[EPOCH #2, step #724] loss: 4.413552478264118\n",
      "[EPOCH #2, step #726] loss: 4.413033389651628\n",
      "[EPOCH #2, step #728] loss: 4.4130386645574795\n",
      "[EPOCH #2, step #730] loss: 4.413221106809728\n",
      "[EPOCH #2, step #732] loss: 4.4133173433852875\n",
      "[EPOCH #2, step #734] loss: 4.413660159078585\n",
      "[EPOCH #2, step #736] loss: 4.413533582777958\n",
      "[EPOCH #2, step #738] loss: 4.413467484010895\n",
      "[EPOCH #2, step #740] loss: 4.413448773254106\n",
      "[EPOCH #2, step #742] loss: 4.4136057926860826\n",
      "[EPOCH #2, step #744] loss: 4.413507916623314\n",
      "[EPOCH #2, step #746] loss: 4.413555320167797\n",
      "[EPOCH #2, step #748] loss: 4.413121015907766\n",
      "[EPOCH #2, step #750] loss: 4.413088215016494\n",
      "[EPOCH #2, step #752] loss: 4.412703462172631\n",
      "[EPOCH #2, step #754] loss: 4.412342343740906\n",
      "[EPOCH #2, step #756] loss: 4.412455361616973\n",
      "[EPOCH #2, step #758] loss: 4.412344582624272\n",
      "[EPOCH #2, step #760] loss: 4.412315566655684\n",
      "[EPOCH #2, step #762] loss: 4.412087751091073\n",
      "[EPOCH #2, step #764] loss: 4.412119776906531\n",
      "[EPOCH #2, step #766] loss: 4.411955477827687\n",
      "[EPOCH #2, step #768] loss: 4.411590950076695\n",
      "[EPOCH #2, step #770] loss: 4.411437724814805\n",
      "[EPOCH #2, step #772] loss: 4.411146757982064\n",
      "[EPOCH #2, step #774] loss: 4.411107847152218\n",
      "[EPOCH #2, step #776] loss: 4.411058326485534\n",
      "[EPOCH #2, step #778] loss: 4.411156948172847\n",
      "[EPOCH #2, step #780] loss: 4.4111850948553535\n",
      "[EPOCH #2, step #782] loss: 4.411173465455933\n",
      "[EPOCH #2, step #784] loss: 4.4111193128452175\n",
      "[EPOCH #2, step #786] loss: 4.411071994859048\n",
      "[EPOCH #2, step #788] loss: 4.410959796760472\n",
      "[EPOCH #2, step #790] loss: 4.4107084997685915\n",
      "[EPOCH #2, step #792] loss: 4.410353499432981\n",
      "[EPOCH #2, step #794] loss: 4.410079752724125\n",
      "[EPOCH #2, step #796] loss: 4.4098758344518645\n",
      "[EPOCH #2, step #798] loss: 4.409643643490215\n",
      "[EPOCH #2, step #800] loss: 4.409446543670921\n",
      "[EPOCH #2, step #802] loss: 4.408961005109927\n",
      "[EPOCH #2, step #804] loss: 4.40926909535568\n",
      "[EPOCH #2, step #806] loss: 4.409581053803549\n",
      "[EPOCH #2, step #808] loss: 4.409449236059955\n",
      "[EPOCH #2, step #810] loss: 4.409377813633214\n",
      "[EPOCH #2, step #812] loss: 4.409258190849464\n",
      "[EPOCH #2, step #814] loss: 4.409364437618138\n",
      "[EPOCH #2, step #816] loss: 4.409050373581661\n",
      "[EPOCH #2, step #818] loss: 4.408949479951963\n",
      "[EPOCH #2, step #820] loss: 4.40853802556678\n",
      "[EPOCH #2, step #822] loss: 4.409012913848766\n",
      "[EPOCH #2, step #824] loss: 4.408973758581913\n",
      "[EPOCH #2, step #826] loss: 4.408872852302376\n",
      "[EPOCH #2, step #828] loss: 4.408846332310768\n",
      "[EPOCH #2, step #830] loss: 4.409020503075114\n",
      "[EPOCH #2, step #832] loss: 4.409053591834683\n",
      "[EPOCH #2, step #834] loss: 4.409055998225412\n",
      "[EPOCH #2, step #836] loss: 4.408951158876772\n",
      "[EPOCH #2, step #838] loss: 4.408887004397623\n",
      "[EPOCH #2, step #840] loss: 4.408925207843395\n",
      "[EPOCH #2, step #842] loss: 4.40875611412709\n",
      "[EPOCH #2, step #844] loss: 4.408766076691757\n",
      "[EPOCH #2, step #846] loss: 4.408548376495469\n",
      "[EPOCH #2, step #848] loss: 4.408753381601071\n",
      "[EPOCH #2, step #850] loss: 4.4085082251372825\n",
      "[EPOCH #2, step #852] loss: 4.408689879029465\n",
      "[EPOCH #2, step #854] loss: 4.408338660524603\n",
      "[EPOCH #2, step #856] loss: 4.408174051843418\n",
      "[EPOCH #2, step #858] loss: 4.408135258138388\n",
      "[EPOCH #2, step #860] loss: 4.408188066692995\n",
      "[EPOCH #2, step #862] loss: 4.408337939517661\n",
      "[EPOCH #2, step #864] loss: 4.408563561522203\n",
      "[EPOCH #2, step #866] loss: 4.408894851820967\n",
      "[EPOCH #2, step #868] loss: 4.408974656850202\n",
      "[EPOCH #2, step #870] loss: 4.40897139068586\n",
      "[EPOCH #2, step #872] loss: 4.408651491211042\n",
      "[EPOCH #2, step #874] loss: 4.408747083936419\n",
      "[EPOCH #2, step #876] loss: 4.408674432043338\n",
      "[EPOCH #2, step #878] loss: 4.408820888423811\n",
      "[EPOCH #2, step #880] loss: 4.408833057195726\n",
      "[EPOCH #2, step #882] loss: 4.409051470146308\n",
      "[EPOCH #2, step #884] loss: 4.408926537077305\n",
      "[EPOCH #2, step #886] loss: 4.408641871631884\n",
      "[EPOCH #2, step #888] loss: 4.408513268147867\n",
      "[EPOCH #2, step #890] loss: 4.408449238264467\n",
      "[EPOCH #2, step #892] loss: 4.408392227510322\n",
      "[EPOCH #2, step #894] loss: 4.408121094090979\n",
      "[EPOCH #2, step #896] loss: 4.408158811039749\n",
      "[EPOCH #2, step #898] loss: 4.4082036819288275\n",
      "[EPOCH #2, step #900] loss: 4.408297013230912\n",
      "[EPOCH #2, step #902] loss: 4.408606427320478\n",
      "[EPOCH #2, step #904] loss: 4.408829931396147\n",
      "[EPOCH #2, step #906] loss: 4.408626130029411\n",
      "[EPOCH #2, step #908] loss: 4.408418409895189\n",
      "[EPOCH #2, step #910] loss: 4.4085089053333\n",
      "[EPOCH #2, step #912] loss: 4.408327108406159\n",
      "[EPOCH #2, step #914] loss: 4.408519435319744\n",
      "[EPOCH #2, step #916] loss: 4.408435054017778\n",
      "[EPOCH #2, step #918] loss: 4.40829312944049\n",
      "[EPOCH #2, step #920] loss: 4.408470939735636\n",
      "[EPOCH #2, step #922] loss: 4.408441687198527\n",
      "[EPOCH #2, step #924] loss: 4.408246321807036\n",
      "[EPOCH #2, step #926] loss: 4.408040036918533\n",
      "[EPOCH #2, step #928] loss: 4.408097427294252\n",
      "[EPOCH #2, step #930] loss: 4.407935975829718\n",
      "[EPOCH #2, step #932] loss: 4.408024115322232\n",
      "[EPOCH #2, step #934] loss: 4.407906743421911\n",
      "[EPOCH #2, step #936] loss: 4.4078063908865\n",
      "[EPOCH #2, step #938] loss: 4.407794034265228\n",
      "[EPOCH #2, step #940] loss: 4.407793072914343\n",
      "[EPOCH #2, step #942] loss: 4.407906324967094\n",
      "[EPOCH #2, step #944] loss: 4.407672482445126\n",
      "[EPOCH #2, step #946] loss: 4.407710565307198\n",
      "[EPOCH #2, step #948] loss: 4.407696394824881\n",
      "[EPOCH #2, step #950] loss: 4.407698552064464\n",
      "[EPOCH #2, step #952] loss: 4.40777146628621\n",
      "[EPOCH #2, step #954] loss: 4.407726879020012\n",
      "[EPOCH #2, step #956] loss: 4.407964639653731\n",
      "[EPOCH #2, step #958] loss: 4.407921928807518\n",
      "[EPOCH #2, step #960] loss: 4.407842916454906\n",
      "[EPOCH #2, step #962] loss: 4.407799406338827\n",
      "[EPOCH #2, step #964] loss: 4.407759759339644\n",
      "[EPOCH #2, step #966] loss: 4.407550255539745\n",
      "[EPOCH #2, step #968] loss: 4.407610827801275\n",
      "[EPOCH #2, step #970] loss: 4.407777438571353\n",
      "[EPOCH #2, step #972] loss: 4.4076880801005816\n",
      "[EPOCH #2, step #974] loss: 4.407888240325145\n",
      "[EPOCH #2, step #976] loss: 4.407622453012144\n",
      "[EPOCH #2, step #978] loss: 4.407817024254336\n",
      "[EPOCH #2, step #980] loss: 4.407847403993908\n",
      "[EPOCH #2, step #982] loss: 4.407545576745593\n",
      "[EPOCH #2, step #984] loss: 4.407403665145642\n",
      "[EPOCH #2, step #986] loss: 4.407246617198353\n",
      "[EPOCH #2, step #988] loss: 4.407074433125186\n",
      "[EPOCH #2, step #990] loss: 4.4071531810625775\n",
      "[EPOCH #2, step #992] loss: 4.406844711015592\n",
      "[EPOCH #2, step #994] loss: 4.406804714490421\n",
      "[EPOCH #2, step #996] loss: 4.406662525360659\n",
      "[EPOCH #2, step #998] loss: 4.406762416656311\n",
      "[EPOCH #2, step #1000] loss: 4.406719438798659\n",
      "[EPOCH #2, step #1002] loss: 4.406730475002605\n",
      "[EPOCH #2, step #1004] loss: 4.406692643426544\n",
      "[EPOCH #2, step #1006] loss: 4.406682174989459\n",
      "[EPOCH #2, step #1008] loss: 4.406528017802092\n",
      "[EPOCH #2, step #1010] loss: 4.40667338989137\n",
      "[EPOCH #2, step #1012] loss: 4.406625590931628\n",
      "[EPOCH #2, step #1014] loss: 4.406639096302352\n",
      "[EPOCH #2, step #1016] loss: 4.406757134369107\n",
      "[EPOCH #2, step #1018] loss: 4.406741565296296\n",
      "[EPOCH #2, step #1020] loss: 4.406735141877446\n",
      "[EPOCH #2, step #1022] loss: 4.4067641274087705\n",
      "[EPOCH #2, step #1024] loss: 4.406796861974205\n",
      "[EPOCH #2, step #1026] loss: 4.4067292840246335\n",
      "[EPOCH #2, step #1028] loss: 4.406622129001006\n",
      "[EPOCH #2, step #1030] loss: 4.406625958118013\n",
      "[EPOCH #2, step #1032] loss: 4.4065475325385925\n",
      "[EPOCH #2, step #1034] loss: 4.406366762446896\n",
      "[EPOCH #2, step #1036] loss: 4.406327590648096\n",
      "[EPOCH #2, step #1038] loss: 4.40624237014653\n",
      "[EPOCH #2, step #1040] loss: 4.4061418582795335\n",
      "[EPOCH #2, step #1042] loss: 4.406208798160717\n",
      "[EPOCH #2, step #1044] loss: 4.406400415885962\n",
      "[EPOCH #2, step #1046] loss: 4.406591690941549\n",
      "[EPOCH #2, step #1048] loss: 4.406434641438967\n",
      "[EPOCH #2, step #1050] loss: 4.406349781464669\n",
      "[EPOCH #2, step #1052] loss: 4.406386222821242\n",
      "[EPOCH #2, step #1054] loss: 4.406427071557791\n",
      "[EPOCH #2, step #1056] loss: 4.406393816703428\n",
      "[EPOCH #2, step #1058] loss: 4.406392906160147\n",
      "[EPOCH #2, step #1060] loss: 4.4063079871735855\n",
      "[EPOCH #2, step #1062] loss: 4.406061102058791\n",
      "[EPOCH #2, step #1064] loss: 4.406088420259001\n",
      "[EPOCH #2, step #1066] loss: 4.40617978293536\n",
      "[EPOCH #2, step #1068] loss: 4.405927076866071\n",
      "[EPOCH #2, step #1070] loss: 4.405742517038553\n",
      "[EPOCH #2, step #1072] loss: 4.405861618134427\n",
      "[EPOCH #2, step #1074] loss: 4.405789652092512\n",
      "[EPOCH #2, step #1076] loss: 4.4055948554085935\n",
      "[EPOCH #2, step #1078] loss: 4.405624391858063\n",
      "[EPOCH #2, step #1080] loss: 4.405554684084064\n",
      "[EPOCH #2, step #1082] loss: 4.405581243150452\n",
      "[EPOCH #2, step #1084] loss: 4.405486334963328\n",
      "[EPOCH #2, step #1086] loss: 4.405416627268015\n",
      "[EPOCH #2, step #1088] loss: 4.405529621218847\n",
      "[EPOCH #2, step #1090] loss: 4.405660783993225\n",
      "[EPOCH #2, step #1092] loss: 4.405593079377617\n",
      "[EPOCH #2, step #1094] loss: 4.405376499210862\n",
      "[EPOCH #2, step #1096] loss: 4.405196051654102\n",
      "[EPOCH #2, step #1098] loss: 4.405249133123063\n",
      "[EPOCH #2, step #1100] loss: 4.404903446945897\n",
      "[EPOCH #2, step #1102] loss: 4.404935948760101\n",
      "[EPOCH #2, step #1104] loss: 4.404868287306566\n",
      "[EPOCH #2, step #1106] loss: 4.404951689581453\n",
      "[EPOCH #2, step #1108] loss: 4.404845670282142\n",
      "[EPOCH #2, step #1110] loss: 4.404728072275459\n",
      "[EPOCH #2, step #1112] loss: 4.404414818721747\n",
      "[EPOCH #2, step #1114] loss: 4.40426619106344\n",
      "[EPOCH #2, step #1116] loss: 4.403966479356948\n",
      "[EPOCH #2, step #1118] loss: 4.403916760786395\n",
      "[EPOCH #2, step #1120] loss: 4.403929456018317\n",
      "[EPOCH #2, step #1122] loss: 4.403920771920671\n",
      "[EPOCH #2, step #1124] loss: 4.403583062489828\n",
      "[EPOCH #2, step #1126] loss: 4.403321720587012\n",
      "[EPOCH #2, step #1128] loss: 4.403266638121424\n",
      "[EPOCH #2, step #1130] loss: 4.403247697494601\n",
      "[EPOCH #2, step #1132] loss: 4.403169880687717\n",
      "[EPOCH #2, step #1134] loss: 4.403161988699488\n",
      "[EPOCH #2, step #1136] loss: 4.403323976639183\n",
      "[EPOCH #2, step #1138] loss: 4.4032933559200025\n",
      "[EPOCH #2, step #1140] loss: 4.403316388727801\n",
      "[EPOCH #2, step #1142] loss: 4.403291150758064\n",
      "[EPOCH #2, step #1144] loss: 4.4032003065384115\n",
      "[EPOCH #2, step #1146] loss: 4.403170376106668\n",
      "[EPOCH #2, step #1148] loss: 4.403205367769751\n",
      "[EPOCH #2, step #1150] loss: 4.40318170865653\n",
      "[EPOCH #2, step #1152] loss: 4.403309707939366\n",
      "[EPOCH #2, step #1154] loss: 4.403314833207564\n",
      "[EPOCH #2, step #1156] loss: 4.403184230923137\n",
      "[EPOCH #2, step #1158] loss: 4.403191046842323\n",
      "[EPOCH #2, step #1160] loss: 4.403330621793288\n",
      "[EPOCH #2, step #1162] loss: 4.403286177847576\n",
      "[EPOCH #2, step #1164] loss: 4.403155294917684\n",
      "[EPOCH #2, step #1166] loss: 4.403010736496507\n",
      "[EPOCH #2, step #1168] loss: 4.402820433379239\n",
      "[EPOCH #2, step #1170] loss: 4.402968402605195\n",
      "[EPOCH #2, step #1172] loss: 4.403046085198314\n",
      "[EPOCH #2, step #1174] loss: 4.403086027185967\n",
      "[EPOCH #2, step #1176] loss: 4.4031610432173505\n",
      "[EPOCH #2, step #1178] loss: 4.403132775559882\n",
      "[EPOCH #2, step #1180] loss: 4.403046393172404\n",
      "[EPOCH #2, step #1182] loss: 4.40312931644443\n",
      "[EPOCH #2, step #1184] loss: 4.403408466210345\n",
      "[EPOCH #2, step #1186] loss: 4.40324741225335\n",
      "[EPOCH #2, step #1188] loss: 4.402897414086344\n",
      "[EPOCH #2, step #1190] loss: 4.402772973506617\n",
      "[EPOCH #2, step #1192] loss: 4.402680005731447\n",
      "[EPOCH #2, step #1194] loss: 4.402623123504128\n",
      "[EPOCH #2, step #1196] loss: 4.4025731576714\n",
      "[EPOCH #2, step #1198] loss: 4.4026553219213795\n",
      "[EPOCH #2, step #1200] loss: 4.402588427017174\n",
      "[EPOCH #2, step #1202] loss: 4.402429643315468\n",
      "[EPOCH #2, step #1204] loss: 4.4022430269549995\n",
      "[EPOCH #2, step #1206] loss: 4.402321632182983\n",
      "[EPOCH #2, step #1208] loss: 4.402250692211369\n",
      "[EPOCH #2, step #1210] loss: 4.402144854173101\n",
      "[EPOCH #2, step #1212] loss: 4.402052171350213\n",
      "[EPOCH #2, step #1214] loss: 4.401946605085836\n",
      "[EPOCH #2, step #1216] loss: 4.401953619233682\n",
      "[EPOCH #2, step #1218] loss: 4.401977315703995\n",
      "[EPOCH #2, step #1220] loss: 4.40194524457277\n",
      "[EPOCH #2, step #1222] loss: 4.401903868306395\n",
      "[EPOCH #2, step #1224] loss: 4.401734127122529\n",
      "[EPOCH #2, step #1226] loss: 4.401621706619823\n",
      "[EPOCH #2, step #1228] loss: 4.401602634867582\n",
      "[EPOCH #2, step #1230] loss: 4.4015079501389875\n",
      "[EPOCH #2, step #1232] loss: 4.401528476411892\n",
      "[EPOCH #2, step #1234] loss: 4.40155288294742\n",
      "[EPOCH #2, step #1236] loss: 4.401427799015153\n",
      "[EPOCH #2, step #1238] loss: 4.4011872443968105\n",
      "[EPOCH #2, step #1240] loss: 4.40104186390793\n",
      "[EPOCH #2, step #1242] loss: 4.401161956480215\n",
      "[EPOCH #2, step #1244] loss: 4.401282044299635\n",
      "[EPOCH #2, step #1246] loss: 4.401179367958687\n",
      "[EPOCH #2, step #1248] loss: 4.4010862933816295\n",
      "[EPOCH #2, step #1250] loss: 4.400890482224816\n",
      "[EPOCH #2, step #1252] loss: 4.400846058905648\n",
      "[EPOCH #2, step #1254] loss: 4.400719310562924\n",
      "[EPOCH #2, step #1256] loss: 4.400648727052821\n",
      "[EPOCH #2, step #1258] loss: 4.400668186645265\n",
      "[EPOCH #2, step #1260] loss: 4.400714868595447\n",
      "[EPOCH #2, step #1262] loss: 4.400506686333712\n",
      "[EPOCH #2, step #1264] loss: 4.400432520704307\n",
      "[EPOCH #2, step #1266] loss: 4.400426235364569\n",
      "[EPOCH #2, step #1268] loss: 4.4003547592553875\n",
      "[EPOCH #2, step #1270] loss: 4.4004576444438275\n",
      "[EPOCH #2, step #1272] loss: 4.400469577321834\n",
      "[EPOCH #2, step #1274] loss: 4.4006533035577515\n",
      "[EPOCH #2, step #1276] loss: 4.400640590730457\n",
      "[EPOCH #2, step #1278] loss: 4.400653838738508\n",
      "[EPOCH #2, step #1280] loss: 4.4007533998065025\n",
      "[EPOCH #2, step #1282] loss: 4.400630122502149\n",
      "[EPOCH #2, step #1284] loss: 4.400650786611356\n",
      "[EPOCH #2, step #1286] loss: 4.400622363783356\n",
      "[EPOCH #2, step #1288] loss: 4.400745594196305\n",
      "[EPOCH #2, step #1290] loss: 4.400828710182243\n",
      "[EPOCH #2, step #1292] loss: 4.400622919190327\n",
      "[EPOCH #2, step #1294] loss: 4.400319438183169\n",
      "[EPOCH #2, step #1296] loss: 4.400252172003917\n",
      "[EPOCH #2, step #1298] loss: 4.400013525730101\n",
      "[EPOCH #2, step #1300] loss: 4.400181660736459\n",
      "[EPOCH #2, step #1302] loss: 4.400079091877911\n",
      "[EPOCH #2, step #1304] loss: 4.400273082265452\n",
      "[EPOCH #2, step #1306] loss: 4.400368197965731\n",
      "[EPOCH #2, step #1308] loss: 4.400483672358408\n",
      "[EPOCH #2, step #1310] loss: 4.400408313076156\n",
      "[EPOCH #2, step #1312] loss: 4.4003803532085435\n",
      "[EPOCH #2, step #1314] loss: 4.400476528573852\n",
      "[EPOCH #2, step #1316] loss: 4.400591179377759\n",
      "[EPOCH #2, step #1318] loss: 4.400515896881775\n",
      "[EPOCH #2, step #1320] loss: 4.400566471426168\n",
      "[EPOCH #2, step #1322] loss: 4.400576667900979\n",
      "[EPOCH #2, step #1324] loss: 4.40043789053863\n",
      "[EPOCH #2, step #1326] loss: 4.400330133157829\n",
      "[EPOCH #2, step #1328] loss: 4.400225926019627\n",
      "[EPOCH #2, step #1330] loss: 4.400220067047876\n",
      "[EPOCH #2, step #1332] loss: 4.4001044078778255\n",
      "[EPOCH #2, step #1334] loss: 4.399970243932603\n",
      "[EPOCH #2, step #1336] loss: 4.4000386357218275\n",
      "[EPOCH #2, step #1338] loss: 4.400241220059726\n",
      "[EPOCH #2, step #1340] loss: 4.400096144452191\n",
      "[EPOCH #2, step #1342] loss: 4.400113544265281\n",
      "[EPOCH #2, step #1344] loss: 4.3999901991351384\n",
      "[EPOCH #2, step #1346] loss: 4.400167705221537\n",
      "[EPOCH #2, step #1348] loss: 4.400234165149233\n",
      "[EPOCH #2, step #1350] loss: 4.400249031894212\n",
      "[EPOCH #2, step #1352] loss: 4.40009049072322\n",
      "[EPOCH #2, step #1354] loss: 4.399808187942224\n",
      "[EPOCH #2, step #1356] loss: 4.399865627288818\n",
      "[EPOCH #2, step #1358] loss: 4.39991057348918\n",
      "[EPOCH #2, step #1360] loss: 4.399847622394211\n",
      "[EPOCH #2, step #1362] loss: 4.400027062380393\n",
      "[EPOCH #2, step #1364] loss: 4.4000210346319735\n",
      "[EPOCH #2, step #1366] loss: 4.399995695698759\n",
      "[EPOCH #2, step #1368] loss: 4.39997439666592\n",
      "[EPOCH #2, step #1370] loss: 4.3998293247125515\n",
      "[EPOCH #2, step #1372] loss: 4.399721256764544\n",
      "[EPOCH #2, step #1374] loss: 4.399790902918036\n",
      "[EPOCH #2, step #1376] loss: 4.399715673603177\n",
      "[EPOCH #2, step #1378] loss: 4.399631726387361\n",
      "[EPOCH #2, step #1380] loss: 4.399561174186911\n",
      "[EPOCH #2, step #1382] loss: 4.399463873537054\n",
      "[EPOCH #2, step #1384] loss: 4.3995463639820525\n",
      "[EPOCH #2, step #1386] loss: 4.399504582144497\n",
      "[EPOCH #2, step #1388] loss: 4.399565562144555\n",
      "[EPOCH #2, step #1390] loss: 4.399494506402636\n",
      "[EPOCH #2, step #1392] loss: 4.39958967241724\n",
      "[EPOCH #2, step #1394] loss: 4.399674392002885\n",
      "[EPOCH #2, step #1396] loss: 4.399896477663098\n",
      "[EPOCH #2, step #1398] loss: 4.400114908143399\n",
      "[EPOCH #2, step #1400] loss: 4.400068386548932\n",
      "[EPOCH #2, step #1402] loss: 4.399779651475989\n",
      "[EPOCH #2, step #1404] loss: 4.3998202273005695\n",
      "[EPOCH #2, step #1406] loss: 4.399680133228532\n",
      "[EPOCH #2, step #1408] loss: 4.399618358287175\n",
      "[EPOCH #2, step #1410] loss: 4.399644836815091\n",
      "[EPOCH #2, step #1412] loss: 4.399510571573613\n",
      "[EPOCH #2, step #1414] loss: 4.399401315614949\n",
      "[EPOCH #2, step #1416] loss: 4.39921526750714\n",
      "[EPOCH #2, step #1418] loss: 4.399212782445132\n",
      "[EPOCH #2, step #1420] loss: 4.399130594050859\n",
      "[EPOCH #2, step #1422] loss: 4.399057907364524\n",
      "[EPOCH #2, step #1424] loss: 4.39893380282218\n",
      "[EPOCH #2, step #1426] loss: 4.398927044684685\n",
      "[EPOCH #2, step #1428] loss: 4.398823534883428\n",
      "[EPOCH #2, step #1430] loss: 4.3987242017568695\n",
      "[EPOCH #2, step #1432] loss: 4.39864760383558\n",
      "[EPOCH #2, step #1434] loss: 4.398473660538836\n",
      "[EPOCH #2, step #1436] loss: 4.39848088587666\n",
      "[EPOCH #2, step #1438] loss: 4.398516433283718\n",
      "[EPOCH #2, step #1440] loss: 4.398414249804018\n",
      "[EPOCH #2, step #1442] loss: 4.398294574092513\n",
      "[EPOCH #2, step #1444] loss: 4.3983179075907675\n",
      "[EPOCH #2, step #1446] loss: 4.398157221247759\n",
      "[EPOCH #2, step #1448] loss: 4.3981369616164265\n",
      "[EPOCH #2, step #1450] loss: 4.398354509959625\n",
      "[EPOCH #2, step #1452] loss: 4.398294606047996\n",
      "[EPOCH #2, step #1454] loss: 4.398310654761455\n",
      "[EPOCH #2, step #1456] loss: 4.398276350640564\n",
      "[EPOCH #2, step #1458] loss: 4.398154624117981\n",
      "[EPOCH #2, step #1460] loss: 4.398151591573163\n",
      "[EPOCH #2, step #1462] loss: 4.398155704884194\n",
      "[EPOCH #2, step #1464] loss: 4.398040081453812\n",
      "[EPOCH #2, step #1466] loss: 4.398011057085949\n",
      "[EPOCH #2, step #1468] loss: 4.3980402991916145\n",
      "[EPOCH #2, step #1470] loss: 4.398026892638061\n",
      "[EPOCH #2, step #1472] loss: 4.397892032281445\n",
      "[EPOCH #2, step #1474] loss: 4.397792951292911\n",
      "[EPOCH #2, step #1476] loss: 4.397584419599157\n",
      "[EPOCH #2, step #1478] loss: 4.397453111760757\n",
      "[EPOCH #2, step #1480] loss: 4.3972614151170974\n",
      "[EPOCH #2, step #1482] loss: 4.397317838443748\n",
      "[EPOCH #2, step #1484] loss: 4.397210383495498\n",
      "[EPOCH #2, step #1486] loss: 4.39716478538898\n",
      "[EPOCH #2, step #1488] loss: 4.397025283188368\n",
      "[EPOCH #2, step #1490] loss: 4.3970208094473415\n",
      "[EPOCH #2, step #1492] loss: 4.396981196202293\n",
      "[EPOCH #2, step #1494] loss: 4.3969879944587635\n",
      "[EPOCH #2, step #1496] loss: 4.396956929861106\n",
      "[EPOCH #2, step #1498] loss: 4.396983131080408\n",
      "[EPOCH #2, step #1500] loss: 4.3968948364893174\n",
      "[EPOCH #2, step #1502] loss: 4.396796817551116\n",
      "[EPOCH #2, step #1504] loss: 4.396788020149814\n",
      "[EPOCH #2, step #1506] loss: 4.396675082018776\n",
      "[EPOCH #2, step #1508] loss: 4.396790121785057\n",
      "[EPOCH #2, step #1510] loss: 4.396775851890316\n",
      "[EPOCH #2, step #1512] loss: 4.396699130732101\n",
      "[EPOCH #2, step #1514] loss: 4.396790252975111\n",
      "[EPOCH #2, step #1516] loss: 4.396762585435639\n",
      "[EPOCH #2, step #1518] loss: 4.396750065902097\n",
      "[EPOCH #2, step #1520] loss: 4.396620626593796\n",
      "[EPOCH #2, step #1522] loss: 4.396555512341244\n",
      "[EPOCH #2, step #1524] loss: 4.396613892727211\n",
      "[EPOCH #2, step #1526] loss: 4.396555263541771\n",
      "[EPOCH #2, step #1528] loss: 4.396541013212435\n",
      "[EPOCH #2, step #1530] loss: 4.396693002313356\n",
      "[EPOCH #2, step #1532] loss: 4.396673568929634\n",
      "[EPOCH #2, step #1534] loss: 4.396622122537818\n",
      "[EPOCH #2, step #1536] loss: 4.396529459224246\n",
      "[EPOCH #2, step #1538] loss: 4.396647161752702\n",
      "[EPOCH #2, step #1540] loss: 4.396759423089446\n",
      "[EPOCH #2, step #1542] loss: 4.396574279973048\n",
      "[EPOCH #2, step #1544] loss: 4.396480045503783\n",
      "[EPOCH #2, step #1546] loss: 4.396565609927631\n",
      "[EPOCH #2, step #1548] loss: 4.396495288999101\n",
      "[EPOCH #2, step #1550] loss: 4.396304137010408\n",
      "[EPOCH #2, step #1552] loss: 4.396237756541216\n",
      "[EPOCH #2, step #1554] loss: 4.3961059254465376\n",
      "[EPOCH #2, step #1556] loss: 4.396175236294647\n",
      "[EPOCH #2, step #1558] loss: 4.3961570435720345\n",
      "[EPOCH #2, step #1560] loss: 4.396010686311104\n",
      "[EPOCH #2, step #1562] loss: 4.395898384538432\n",
      "[EPOCH #2, step #1564] loss: 4.395840073545901\n",
      "[EPOCH #2, step #1566] loss: 4.395818807398657\n",
      "[EPOCH #2, step #1568] loss: 4.395697297679154\n",
      "[EPOCH #2, step #1570] loss: 4.395705827400989\n",
      "[EPOCH #2, step #1572] loss: 4.3955854287362754\n",
      "[EPOCH #2, step #1574] loss: 4.395455962892562\n",
      "[EPOCH #2, step #1576] loss: 4.395521175732852\n",
      "[EPOCH #2, step #1578] loss: 4.395505502602962\n",
      "[EPOCH #2, step #1580] loss: 4.395569585984753\n",
      "[EPOCH #2, step #1582] loss: 4.395568938023368\n",
      "[EPOCH #2, step #1584] loss: 4.395560032937805\n",
      "[EPOCH #2, step #1586] loss: 4.395595571569625\n",
      "[EPOCH #2, step #1588] loss: 4.395579272354827\n",
      "[EPOCH #2, step #1590] loss: 4.39570502797288\n",
      "[EPOCH #2, step #1592] loss: 4.395672857723967\n",
      "[EPOCH #2, step #1594] loss: 4.395599990952351\n",
      "[EPOCH #2, step #1596] loss: 4.39562000478889\n",
      "[EPOCH #2, step #1598] loss: 4.3956194046216135\n",
      "[EPOCH #2, step #1600] loss: 4.395628191991421\n",
      "[EPOCH #2, step #1602] loss: 4.395562061873809\n",
      "[EPOCH #2, step #1604] loss: 4.395384279886882\n",
      "[EPOCH #2, step #1606] loss: 4.39530143488544\n",
      "[EPOCH #2, step #1608] loss: 4.395272829282202\n",
      "[EPOCH #2, step #1610] loss: 4.395208855729011\n",
      "[EPOCH #2, step #1612] loss: 4.395203809152829\n",
      "[EPOCH #2, step #1614] loss: 4.395116296419787\n",
      "[EPOCH #2, step #1616] loss: 4.394968946701078\n",
      "[EPOCH #2, step #1618] loss: 4.395010719629186\n",
      "[EPOCH #2, step #1620] loss: 4.394840620242101\n",
      "[EPOCH #2, step #1622] loss: 4.39495068659991\n",
      "[EPOCH #2, step #1624] loss: 4.395010818481445\n",
      "[EPOCH #2, step #1626] loss: 4.3949007246489895\n",
      "[EPOCH #2, step #1628] loss: 4.394859034419426\n",
      "[EPOCH #2, step #1630] loss: 4.394769383236939\n",
      "[EPOCH #2, step #1632] loss: 4.394635163416398\n",
      "[EPOCH #2, step #1634] loss: 4.394674899016681\n",
      "[EPOCH #2, step #1636] loss: 4.394540405739497\n",
      "[EPOCH #2, step #1638] loss: 4.394422595132918\n",
      "[EPOCH #2, step #1640] loss: 4.39431717452445\n",
      "[EPOCH #2, step #1642] loss: 4.394059115332651\n",
      "[EPOCH #2, step #1644] loss: 4.393857082842331\n",
      "[EPOCH #2, step #1646] loss: 4.393807408009143\n",
      "[EPOCH #2, step #1648] loss: 4.393866553749006\n",
      "[EPOCH #2, step #1650] loss: 4.393916701057476\n",
      "[EPOCH #2, step #1652] loss: 4.393988514985017\n",
      "[EPOCH #2, step #1654] loss: 4.3938679827664195\n",
      "[EPOCH #2, step #1656] loss: 4.393895613693946\n",
      "[EPOCH #2, step #1658] loss: 4.393783538581522\n",
      "[EPOCH #2, step #1660] loss: 4.393712339452919\n",
      "[EPOCH #2, step #1662] loss: 4.3936378616062335\n",
      "[EPOCH #2, step #1664] loss: 4.393559761161919\n",
      "[EPOCH #2, step #1666] loss: 4.393445056144106\n",
      "[EPOCH #2, step #1668] loss: 4.393323319482546\n",
      "[EPOCH #2, step #1670] loss: 4.393266826243831\n",
      "[EPOCH #2, step #1672] loss: 4.393178710378862\n",
      "[EPOCH #2, step #1674] loss: 4.393145293620095\n",
      "[EPOCH #2, step #1676] loss: 4.393106741498607\n",
      "[EPOCH #2, step #1678] loss: 4.393042506171948\n",
      "[EPOCH #2, step #1680] loss: 4.393116760651034\n",
      "[EPOCH #2, step #1682] loss: 4.392982834234028\n",
      "[EPOCH #2, step #1684] loss: 4.392789718729806\n",
      "[EPOCH #2, step #1686] loss: 4.39272592840777\n",
      "[EPOCH #2, step #1688] loss: 4.392544931035694\n",
      "[EPOCH #2, step #1690] loss: 4.3923993508256025\n",
      "[EPOCH #2, step #1692] loss: 4.392495052813075\n",
      "[EPOCH #2, step #1694] loss: 4.392496189747588\n",
      "[EPOCH #2, step #1696] loss: 4.392467604430901\n",
      "[EPOCH #2, step #1698] loss: 4.392242026651797\n",
      "[EPOCH #2, step #1700] loss: 4.392160891365262\n",
      "[EPOCH #2, step #1702] loss: 4.392201799682779\n",
      "[EPOCH #2, step #1704] loss: 4.3922076188923675\n",
      "[EPOCH #2, step #1706] loss: 4.392338599387707\n",
      "[EPOCH #2, step #1708] loss: 4.392446336863403\n",
      "[EPOCH #2, step #1710] loss: 4.392375294748348\n",
      "[EPOCH #2, step #1712] loss: 4.392317581510794\n",
      "[EPOCH #2, step #1714] loss: 4.392017870811262\n",
      "[EPOCH #2, step #1716] loss: 4.391991152621732\n",
      "[EPOCH #2, step #1718] loss: 4.392014716516199\n",
      "[EPOCH #2, step #1720] loss: 4.392012097127627\n",
      "[EPOCH #2, step #1722] loss: 4.391968701097485\n",
      "[EPOCH #2, step #1724] loss: 4.391867226310398\n",
      "[EPOCH #2, step #1726] loss: 4.391764275242155\n",
      "[EPOCH #2, step #1728] loss: 4.391734898745359\n",
      "[EPOCH #2, step #1730] loss: 4.391578807643492\n",
      "[EPOCH #2, step #1732] loss: 4.391478333800635\n",
      "[EPOCH #2, step #1734] loss: 4.391397969248659\n",
      "[EPOCH #2, step #1736] loss: 4.391389733425968\n",
      "[EPOCH #2, step #1738] loss: 4.39136121024888\n",
      "[EPOCH #2, step #1740] loss: 4.391287536062364\n",
      "[EPOCH #2, step #1742] loss: 4.391293859180881\n",
      "[EPOCH #2, step #1744] loss: 4.391362838963724\n",
      "[EPOCH #2, step #1746] loss: 4.3913089182285425\n",
      "[EPOCH #2, step #1748] loss: 4.391347203819053\n",
      "[EPOCH #2, step #1750] loss: 4.391280483069385\n",
      "[EPOCH #2, step #1752] loss: 4.391163390769458\n",
      "[EPOCH #2, step #1754] loss: 4.391127833885345\n",
      "[EPOCH #2, step #1756] loss: 4.391136683451566\n",
      "[EPOCH #2, step #1758] loss: 4.39103447706473\n",
      "[EPOCH #2, step #1760] loss: 4.390935881039015\n",
      "[EPOCH #2, step #1762] loss: 4.390858468722702\n",
      "[EPOCH #2, step #1764] loss: 4.390802533092985\n",
      "[EPOCH #2, step #1766] loss: 4.390704313644124\n",
      "[EPOCH #2, step #1768] loss: 4.390668632082511\n",
      "[EPOCH #2, step #1770] loss: 4.390526189701763\n",
      "[EPOCH #2, step #1772] loss: 4.390364565421684\n",
      "[EPOCH #2, step #1774] loss: 4.390220919595638\n",
      "[EPOCH #2, step #1776] loss: 4.390111451747381\n",
      "[EPOCH #2, step #1778] loss: 4.390085281985338\n",
      "[EPOCH #2, step #1780] loss: 4.390092010112507\n",
      "[EPOCH #2, step #1782] loss: 4.390113644163995\n",
      "[EPOCH #2, step #1784] loss: 4.390142267157717\n",
      "[EPOCH #2, step #1786] loss: 4.39021074938654\n",
      "[EPOCH #2, step #1788] loss: 4.390137115092568\n",
      "[EPOCH #2, step #1790] loss: 4.39014078451228\n",
      "[EPOCH #2, step #1792] loss: 4.390071820228776\n",
      "[EPOCH #2, step #1794] loss: 4.390076180088819\n",
      "[EPOCH #2, step #1796] loss: 4.390201007649842\n",
      "[EPOCH #2, step #1798] loss: 4.3902358650962405\n",
      "[EPOCH #2, step #1800] loss: 4.390264588948557\n",
      "[EPOCH #2, step #1802] loss: 4.390234240808556\n",
      "[EPOCH #2, step #1804] loss: 4.390131734546862\n",
      "[EPOCH #2, step #1806] loss: 4.390108671383628\n",
      "[EPOCH #2, step #1808] loss: 4.390005042106161\n",
      "[EPOCH #2, step #1810] loss: 4.389930908618086\n",
      "[EPOCH #2, step #1812] loss: 4.389894659323253\n",
      "[EPOCH #2, step #1814] loss: 4.389923816344626\n",
      "[EPOCH #2, step #1816] loss: 4.389791418840811\n",
      "[EPOCH #2, step #1818] loss: 4.38989835024011\n",
      "[EPOCH #2, step #1820] loss: 4.389839210855902\n",
      "[EPOCH #2, step #1822] loss: 4.3896257360754465\n",
      "[EPOCH #2, step #1824] loss: 4.389537373634234\n",
      "[EPOCH #2, step #1826] loss: 4.389531035337151\n",
      "[EPOCH #2, step #1828] loss: 4.389468110148668\n",
      "[EPOCH #2, step #1830] loss: 4.389405245340828\n",
      "[EPOCH #2, step #1832] loss: 4.389442020339258\n",
      "[EPOCH #2, step #1834] loss: 4.389456077752386\n",
      "[EPOCH #2, step #1836] loss: 4.389226862835248\n",
      "[EPOCH #2, step #1838] loss: 4.389199687580235\n",
      "[EPOCH #2, step #1840] loss: 4.3890043504208345\n",
      "[EPOCH #2, step #1842] loss: 4.389165042084453\n",
      "[EPOCH #2, step #1844] loss: 4.389179959594395\n",
      "[EPOCH #2, step #1846] loss: 4.389166984031442\n",
      "[EPOCH #2, step #1848] loss: 4.389132176301749\n",
      "[EPOCH #2, step #1850] loss: 4.389201778904416\n",
      "[EPOCH #2, step #1852] loss: 4.389210876489548\n",
      "[EPOCH #2, step #1854] loss: 4.389099522613773\n",
      "[EPOCH #2, step #1856] loss: 4.389007035544564\n",
      "[EPOCH #2, step #1858] loss: 4.388923689394372\n",
      "[EPOCH #2, step #1860] loss: 4.3887951831417915\n",
      "[EPOCH #2, step #1862] loss: 4.388734088299803\n",
      "[EPOCH #2, step #1864] loss: 4.38863784570157\n",
      "[EPOCH #2, step #1866] loss: 4.3885512405624345\n",
      "[EPOCH #2, step #1868] loss: 4.388523356936206\n",
      "[EPOCH #2, step #1870] loss: 4.38853472022052\n",
      "[EPOCH #2, step #1872] loss: 4.388735200145699\n",
      "[EPOCH #2, step #1874] loss: 4.388796476745606\n",
      "[EPOCH #2, step #1876] loss: 4.388661188970047\n",
      "[EPOCH #2, step #1878] loss: 4.3886061755185946\n",
      "[EPOCH #2, step #1880] loss: 4.3885519771991675\n",
      "[EPOCH #2, step #1882] loss: 4.388572474837746\n",
      "[EPOCH #2, step #1884] loss: 4.388478422038435\n",
      "[EPOCH #2, step #1886] loss: 4.3884517296193195\n",
      "[EPOCH #2, step #1888] loss: 4.388411498940641\n",
      "[EPOCH #2, step #1890] loss: 4.388356049435282\n",
      "[EPOCH #2, step #1892] loss: 4.388322394925705\n",
      "[EPOCH #2, step #1894] loss: 4.388277246077646\n",
      "[EPOCH #2, step #1896] loss: 4.388296600888765\n",
      "[EPOCH #2, step #1898] loss: 4.38822090180062\n",
      "[EPOCH #2, step #1900] loss: 4.388225788696386\n",
      "[EPOCH #2, step #1902] loss: 4.388217529872187\n",
      "[EPOCH #2, step #1904] loss: 4.388185534139318\n",
      "[EPOCH #2, step #1906] loss: 4.388249052714851\n",
      "[EPOCH #2, step #1908] loss: 4.388319441883772\n",
      "[EPOCH #2, step #1910] loss: 4.388403403453338\n",
      "[EPOCH #2, step #1912] loss: 4.388476131477097\n",
      "[EPOCH #2, step #1914] loss: 4.388280088160742\n",
      "[EPOCH #2, step #1916] loss: 4.3881603212610285\n",
      "[EPOCH #2, step #1918] loss: 4.3879647722090205\n",
      "[EPOCH #2, step #1920] loss: 4.387958185552372\n",
      "[EPOCH #2, step #1922] loss: 4.3878892505782625\n",
      "[EPOCH #2, step #1924] loss: 4.387874154920702\n",
      "[EPOCH #2, step #1926] loss: 4.387704407296529\n",
      "[EPOCH #2, step #1928] loss: 4.387697681112571\n",
      "[EPOCH #2, step #1930] loss: 4.387701170608702\n",
      "[EPOCH #2, step #1932] loss: 4.387660772764677\n",
      "[EPOCH #2, step #1934] loss: 4.38750941821155\n",
      "[EPOCH #2, step #1936] loss: 4.387387898759738\n",
      "[EPOCH #2, step #1938] loss: 4.387284833909065\n",
      "[EPOCH #2, step #1940] loss: 4.38724503917586\n",
      "[EPOCH #2, step #1942] loss: 4.387355579890606\n",
      "[EPOCH #2, step #1944] loss: 4.387327602651248\n",
      "[EPOCH #2, step #1946] loss: 4.3871217623819865\n",
      "[EPOCH #2, step #1948] loss: 4.387105308843919\n",
      "[EPOCH #2, step #1950] loss: 4.3870622690123575\n",
      "[EPOCH #2, step #1952] loss: 4.387009531793629\n",
      "[EPOCH #2, step #1954] loss: 4.387014360013215\n",
      "[EPOCH #2, step #1956] loss: 4.386945835594992\n",
      "[EPOCH #2, step #1958] loss: 4.386950166204744\n",
      "[EPOCH #2, step #1960] loss: 4.386887572481583\n",
      "[EPOCH #2, step #1962] loss: 4.3867319219281224\n",
      "[EPOCH #2, step #1964] loss: 4.3867339192455965\n",
      "[EPOCH #2, step #1966] loss: 4.386802821942375\n",
      "[EPOCH #2, step #1968] loss: 4.3869064904034225\n",
      "[EPOCH #2, step #1970] loss: 4.386863010054249\n",
      "[EPOCH #2, step #1972] loss: 4.386795293602071\n",
      "[EPOCH #2, step #1974] loss: 4.3868871251842645\n",
      "[EPOCH #2, step #1976] loss: 4.386864558216533\n",
      "[EPOCH #2, step #1978] loss: 4.38692081980539\n",
      "[EPOCH #2, step #1980] loss: 4.386787146886511\n",
      "[EPOCH #2, step #1982] loss: 4.386790532387692\n",
      "[EPOCH #2, step #1984] loss: 4.386730309637728\n",
      "[EPOCH #2, step #1986] loss: 4.386839741849395\n",
      "[EPOCH #2, step #1988] loss: 4.386882149316127\n",
      "[EPOCH #2, step #1990] loss: 4.386838006230828\n",
      "[EPOCH #2, step #1992] loss: 4.386793361497296\n",
      "[EPOCH #2, step #1994] loss: 4.386751570737451\n",
      "[EPOCH #2, step #1996] loss: 4.3866709464182545\n",
      "[EPOCH #2, step #1998] loss: 4.386718614271964\n",
      "[EPOCH #2, step #2000] loss: 4.386592097427772\n",
      "[EPOCH #2, step #2002] loss: 4.386557313601969\n",
      "[EPOCH #2, step #2004] loss: 4.386539929882249\n",
      "[EPOCH #2, step #2006] loss: 4.386587806762482\n",
      "[EPOCH #2, step #2008] loss: 4.386618245116747\n",
      "[EPOCH #2, step #2010] loss: 4.386579904788647\n",
      "[EPOCH #2, step #2012] loss: 4.38652007377272\n",
      "[EPOCH #2, step #2014] loss: 4.386519970195761\n",
      "[EPOCH #2, step #2016] loss: 4.386505900483706\n",
      "[EPOCH #2, step #2018] loss: 4.38646172957588\n",
      "[EPOCH #2, step #2020] loss: 4.386290526637814\n",
      "[EPOCH #2, step #2022] loss: 4.386404093338367\n",
      "[EPOCH #2, step #2024] loss: 4.386505310859209\n",
      "[EPOCH #2, step #2026] loss: 4.38653445631986\n",
      "[EPOCH #2, step #2028] loss: 4.386567523292628\n",
      "[EPOCH #2, step #2030] loss: 4.386459684207603\n",
      "[EPOCH #2, step #2032] loss: 4.386471805581892\n",
      "[EPOCH #2, step #2034] loss: 4.386411084824171\n",
      "[EPOCH #2, step #2036] loss: 4.386470930682835\n",
      "[EPOCH #2, step #2038] loss: 4.386560820550998\n",
      "[EPOCH #2, step #2040] loss: 4.386519967545953\n",
      "[EPOCH #2, step #2042] loss: 4.38658566225176\n",
      "[EPOCH #2, step #2044] loss: 4.386506749132152\n",
      "[EPOCH #2, step #2046] loss: 4.3863457130931565\n",
      "[EPOCH #2, step #2048] loss: 4.3863104549834295\n",
      "[EPOCH #2, step #2050] loss: 4.3862256559728126\n",
      "[EPOCH #2, step #2052] loss: 4.386181761448522\n",
      "[EPOCH #2, step #2054] loss: 4.386150506523114\n",
      "[EPOCH #2, step #2056] loss: 4.386164830040457\n",
      "[EPOCH #2, step #2058] loss: 4.3861329186130575\n",
      "[EPOCH #2, step #2060] loss: 4.386070509664877\n",
      "[EPOCH #2, step #2062] loss: 4.386049734985603\n",
      "[EPOCH #2, step #2064] loss: 4.386016436292819\n",
      "[EPOCH #2, step #2066] loss: 4.386007011369282\n",
      "[EPOCH #2, step #2068] loss: 4.385918504549951\n",
      "[EPOCH #2, step #2070] loss: 4.385784381026522\n",
      "[EPOCH #2, step #2072] loss: 4.385775222907133\n",
      "[EPOCH #2, step #2074] loss: 4.385793090038989\n",
      "[EPOCH #2, step #2076] loss: 4.385759832541071\n",
      "[EPOCH #2, step #2078] loss: 4.385667402965624\n",
      "[EPOCH #2, step #2080] loss: 4.385641103521785\n",
      "[EPOCH #2, step #2082] loss: 4.385576328290408\n",
      "[EPOCH #2, step #2084] loss: 4.385538222177995\n",
      "[EPOCH #2, step #2086] loss: 4.3854128899731935\n",
      "[EPOCH #2, step #2088] loss: 4.385397273995421\n",
      "[EPOCH #2, step #2090] loss: 4.3853929212707925\n",
      "[EPOCH #2, step #2092] loss: 4.385360005365509\n",
      "[EPOCH #2, step #2094] loss: 4.385323393600937\n",
      "[EPOCH #2, step #2096] loss: 4.385395302451902\n",
      "[EPOCH #2, step #2098] loss: 4.3853521106242\n",
      "[EPOCH #2, step #2100] loss: 4.385379788309321\n",
      "[EPOCH #2, step #2102] loss: 4.385329063994171\n",
      "[EPOCH #2, step #2104] loss: 4.385302692920749\n",
      "[EPOCH #2, step #2106] loss: 4.385301477036659\n",
      "[EPOCH #2, step #2108] loss: 4.385329937471378\n",
      "[EPOCH #2, step #2110] loss: 4.3853404444012805\n",
      "[EPOCH #2, step #2112] loss: 4.385256957962679\n",
      "[EPOCH #2, step #2114] loss: 4.385260678239466\n",
      "[EPOCH #2, step #2116] loss: 4.385260474079332\n",
      "[EPOCH #2, step #2118] loss: 4.385321296234635\n",
      "[EPOCH #2, step #2120] loss: 4.385243731925422\n",
      "[EPOCH #2, step #2122] loss: 4.385122839370203\n",
      "[EPOCH #2, step #2124] loss: 4.385040730195887\n",
      "[EPOCH #2, step #2126] loss: 4.38494461940161\n",
      "[EPOCH #2, step #2128] loss: 4.384871503902523\n",
      "[EPOCH #2, step #2130] loss: 4.384887038570567\n",
      "[EPOCH #2, step #2132] loss: 4.384733882131456\n",
      "[EPOCH #2, step #2134] loss: 4.384724442573566\n",
      "[EPOCH #2, step #2136] loss: 4.384685491143291\n",
      "[EPOCH #2, step #2138] loss: 4.3845820634261425\n",
      "[EPOCH #2, step #2140] loss: 4.384463657680264\n",
      "[EPOCH #2, step #2142] loss: 4.384325643371212\n",
      "[EPOCH #2, step #2144] loss: 4.384299242913306\n",
      "[EPOCH #2, step #2146] loss: 4.384307932398404\n",
      "[EPOCH #2, step #2148] loss: 4.384263887578468\n",
      "[EPOCH #2, step #2150] loss: 4.3842010460361\n",
      "[EPOCH #2, step #2152] loss: 4.384164734657454\n",
      "[EPOCH #2, step #2154] loss: 4.384069435225839\n",
      "[EPOCH #2, step #2156] loss: 4.384050597622818\n",
      "[EPOCH #2, step #2158] loss: 4.3839787016758605\n",
      "[EPOCH #2, step #2160] loss: 4.383880181866407\n",
      "[EPOCH #2, step #2162] loss: 4.383914797602569\n",
      "[EPOCH #2, step #2164] loss: 4.383841836590139\n",
      "[EPOCH #2, step #2166] loss: 4.383757558680923\n",
      "[EPOCH #2, step #2168] loss: 4.3837799157564294\n",
      "[EPOCH #2, step #2170] loss: 4.383848964316897\n",
      "[EPOCH #2, step #2172] loss: 4.383812556988733\n",
      "[EPOCH #2, step #2174] loss: 4.383817428501173\n",
      "[EPOCH #2, step #2176] loss: 4.383827414926609\n",
      "[EPOCH #2, step #2178] loss: 4.383778891359901\n",
      "[EPOCH #2, step #2180] loss: 4.383776183086817\n",
      "[EPOCH #2, step #2182] loss: 4.383771092750805\n",
      "[EPOCH #2, step #2184] loss: 4.38367595628961\n",
      "[EPOCH #2, step #2186] loss: 4.383584225292232\n",
      "[EPOCH #2, step #2188] loss: 4.383572237303182\n",
      "[EPOCH #2, step #2190] loss: 4.383488296863969\n",
      "[EPOCH #2, step #2192] loss: 4.383463703004182\n",
      "[EPOCH #2, step #2194] loss: 4.3833828374302195\n",
      "[EPOCH #2, step #2196] loss: 4.38333336439901\n",
      "[EPOCH #2, step #2198] loss: 4.383244173805407\n",
      "[EPOCH #2, step #2200] loss: 4.3833064326694045\n",
      "[EPOCH #2, step #2202] loss: 4.3832157310766355\n",
      "[EPOCH #2, step #2204] loss: 4.383263857402499\n",
      "[EPOCH #2, step #2206] loss: 4.383294853950227\n",
      "[EPOCH #2, step #2208] loss: 4.383201961120149\n",
      "[EPOCH #2, step #2210] loss: 4.38311512747376\n",
      "[EPOCH #2, step #2212] loss: 4.383177117314535\n",
      "[EPOCH #2, step #2214] loss: 4.383186198464903\n",
      "[EPOCH #2, step #2216] loss: 4.383145831111965\n",
      "[EPOCH #2, step #2218] loss: 4.38324393512645\n",
      "[EPOCH #2, step #2220] loss: 4.383182293549683\n",
      "[EPOCH #2, step #2222] loss: 4.38319186160439\n",
      "[EPOCH #2, step #2224] loss: 4.383196372557222\n",
      "[EPOCH #2, step #2226] loss: 4.383074745448866\n",
      "[EPOCH #2, step #2228] loss: 4.383104562652437\n",
      "[EPOCH #2, step #2230] loss: 4.383045061026812\n",
      "[EPOCH #2, step #2232] loss: 4.382958435232843\n",
      "[EPOCH #2, step #2234] loss: 4.382978605530672\n",
      "[EPOCH #2, step #2236] loss: 4.3829288970800615\n",
      "[EPOCH #2, step #2238] loss: 4.382831461890674\n",
      "[EPOCH #2, step #2240] loss: 4.3827503892472155\n",
      "[EPOCH #2, step #2242] loss: 4.3826635741677284\n",
      "[EPOCH #2, step #2244] loss: 4.3826948499360965\n",
      "[EPOCH #2, step #2246] loss: 4.382648547596543\n",
      "[EPOCH #2, step #2248] loss: 4.3825622783760965\n",
      "[EPOCH #2, step #2250] loss: 4.382581854226694\n",
      "[EPOCH #2, step #2252] loss: 4.3825846689836325\n",
      "[EPOCH #2, step #2254] loss: 4.382538987051886\n",
      "[EPOCH #2, step #2256] loss: 4.38255559082944\n",
      "[EPOCH #2, step #2258] loss: 4.382510451628814\n",
      "[EPOCH #2, step #2260] loss: 4.382502928501418\n",
      "[EPOCH #2, step #2262] loss: 4.382484136826688\n",
      "[EPOCH #2, step #2264] loss: 4.382471709009301\n",
      "[EPOCH #2, step #2266] loss: 4.382492231597025\n",
      "[EPOCH #2, step #2268] loss: 4.382410141905797\n",
      "[EPOCH #2, step #2270] loss: 4.382398809645471\n",
      "[EPOCH #2, step #2272] loss: 4.382296869527177\n",
      "[EPOCH #2, step #2274] loss: 4.382344400175326\n",
      "[EPOCH #2, step #2276] loss: 4.382341833453876\n",
      "[EPOCH #2, step #2278] loss: 4.3821775757361525\n",
      "[EPOCH #2, step #2280] loss: 4.382065083362825\n",
      "[EPOCH #2, step #2282] loss: 4.381997324425141\n",
      "[EPOCH #2, step #2284] loss: 4.381962211283343\n",
      "[EPOCH #2, step #2286] loss: 4.382011224882645\n",
      "[EPOCH #2, step #2288] loss: 4.38192689986456\n",
      "[EPOCH #2, step #2290] loss: 4.381840594103116\n",
      "[EPOCH #2, step #2292] loss: 4.381870402657617\n",
      "[EPOCH #2, step #2294] loss: 4.381799153720631\n",
      "[EPOCH #2, step #2296] loss: 4.381669909735062\n",
      "[EPOCH #2, step #2298] loss: 4.381656751056089\n",
      "[EPOCH #2, step #2300] loss: 4.3815916406232756\n",
      "[EPOCH #2, step #2302] loss: 4.381516741712456\n",
      "[EPOCH #2, step #2304] loss: 4.381503691021636\n",
      "[EPOCH #2, step #2306] loss: 4.38151317806434\n",
      "[EPOCH #2, step #2308] loss: 4.381383805834612\n",
      "[EPOCH #2, step #2310] loss: 4.381336505564711\n",
      "[EPOCH #2, step #2312] loss: 4.381315386145856\n",
      "[EPOCH #2, step #2314] loss: 4.381270817495062\n",
      "[EPOCH #2, step #2316] loss: 4.381252185507072\n",
      "[EPOCH #2, step #2318] loss: 4.381240106132741\n",
      "[EPOCH #2, step #2320] loss: 4.381249478334808\n",
      "[EPOCH #2, step #2322] loss: 4.3812307707975\n",
      "[EPOCH #2, step #2324] loss: 4.381188308551748\n",
      "[EPOCH #2, step #2326] loss: 4.381136312062616\n",
      "[EPOCH #2, step #2328] loss: 4.381113198142341\n",
      "[EPOCH #2, step #2330] loss: 4.381086384263074\n",
      "[EPOCH #2, step #2332] loss: 4.381132924193944\n",
      "[EPOCH #2, step #2334] loss: 4.381035450439127\n",
      "[EPOCH #2, step #2336] loss: 4.3810000544079974\n",
      "[EPOCH #2, step #2338] loss: 4.380987683337384\n",
      "[EPOCH #2, step #2340] loss: 4.380998645466344\n",
      "[EPOCH #2, step #2342] loss: 4.380910646900172\n",
      "[EPOCH #2, step #2344] loss: 4.380908390809732\n",
      "[EPOCH #2, step #2346] loss: 4.380860565975468\n",
      "[EPOCH #2, step #2348] loss: 4.380776013552559\n",
      "[EPOCH #2, step #2350] loss: 4.380772150713857\n",
      "[EPOCH #2, step #2352] loss: 4.380711236914016\n",
      "[EPOCH #2, step #2354] loss: 4.38068161618178\n",
      "[EPOCH #2, step #2356] loss: 4.380712434706366\n",
      "[EPOCH #2, step #2358] loss: 4.380553172977863\n",
      "[EPOCH #2, step #2360] loss: 4.380522097636461\n",
      "[EPOCH #2, step #2362] loss: 4.380554412312292\n",
      "[EPOCH #2, step #2364] loss: 4.38052910951933\n",
      "[EPOCH #2, step #2366] loss: 4.380522362935075\n",
      "[EPOCH #2, step #2368] loss: 4.380538886687382\n",
      "[EPOCH #2, step #2370] loss: 4.380454196350728\n",
      "[EPOCH #2, step #2372] loss: 4.380510563092849\n",
      "[EPOCH #2, step #2374] loss: 4.380436553352758\n",
      "[EPOCH #2, step #2376] loss: 4.38039183566487\n",
      "[EPOCH #2, step #2378] loss: 4.380343672707082\n",
      "[EPOCH #2, step #2380] loss: 4.380243021617763\n",
      "[EPOCH #2, step #2382] loss: 4.380176580407467\n",
      "[EPOCH #2, step #2384] loss: 4.380068267766284\n",
      "[EPOCH #2, step #2386] loss: 4.380066709434532\n",
      "[EPOCH #2, step #2388] loss: 4.380081622457843\n",
      "[EPOCH #2, step #2390] loss: 4.380032805207882\n",
      "[EPOCH #2, step #2392] loss: 4.380021911857021\n",
      "[EPOCH #2, step #2394] loss: 4.380097091173081\n",
      "[EPOCH #2, step #2396] loss: 4.379999419575589\n",
      "[EPOCH #2, step #2398] loss: 4.380001017231403\n",
      "[EPOCH #2, step #2400] loss: 4.3799553133953815\n",
      "[EPOCH #2, step #2402] loss: 4.379981516402709\n",
      "[EPOCH #2, step #2404] loss: 4.379922181206781\n",
      "[EPOCH #2, step #2406] loss: 4.379905320125147\n",
      "[EPOCH #2, step #2408] loss: 4.379937910449708\n",
      "[EPOCH #2, step #2410] loss: 4.379964329898728\n",
      "[EPOCH #2, step #2412] loss: 4.380053844681134\n",
      "[EPOCH #2, step #2414] loss: 4.380015523290536\n",
      "[EPOCH #2, step #2416] loss: 4.379972823257036\n",
      "[EPOCH #2, step #2418] loss: 4.379970074291316\n",
      "[EPOCH #2, step #2420] loss: 4.379990941086667\n",
      "[EPOCH #2, step #2422] loss: 4.379989629336935\n",
      "[EPOCH #2, step #2424] loss: 4.379887297915429\n",
      "[EPOCH #2, step #2426] loss: 4.379812621952429\n",
      "[EPOCH #2, step #2428] loss: 4.379885582631387\n",
      "[EPOCH #2, step #2430] loss: 4.379859217593132\n",
      "[EPOCH #2, step #2432] loss: 4.37976133602909\n",
      "[EPOCH #2, step #2434] loss: 4.3796328626863765\n",
      "[EPOCH #2, step #2436] loss: 4.379626926849453\n",
      "[EPOCH #2, step #2438] loss: 4.379548129902457\n",
      "[EPOCH #2, step #2440] loss: 4.379506421548233\n",
      "[EPOCH #2, step #2442] loss: 4.379516138583483\n",
      "[EPOCH #2, step #2444] loss: 4.37952416796382\n",
      "[EPOCH #2, step #2446] loss: 4.3795803367823645\n",
      "[EPOCH #2, step #2448] loss: 4.3794160138446\n",
      "[EPOCH #2, step #2450] loss: 4.379383769243506\n",
      "[EPOCH #2, step #2452] loss: 4.3794165594549606\n",
      "[EPOCH #2, step #2454] loss: 4.379356817800984\n",
      "[EPOCH #2, step #2456] loss: 4.379325939864708\n",
      "[EPOCH #2, step #2458] loss: 4.379323396609052\n",
      "[EPOCH #2, step #2460] loss: 4.379287762353208\n",
      "[EPOCH #2, step #2462] loss: 4.379248194901671\n",
      "[EPOCH #2, step #2464] loss: 4.37921523757687\n",
      "[EPOCH #2, step #2466] loss: 4.379138751893958\n",
      "[EPOCH #2, step #2468] loss: 4.378996757516594\n",
      "[EPOCH #2, step #2470] loss: 4.378922157951426\n",
      "[EPOCH #2, step #2472] loss: 4.378922182107039\n",
      "[EPOCH #2, step #2474] loss: 4.3788593386640455\n",
      "[EPOCH #2, step #2476] loss: 4.378817798150948\n",
      "[EPOCH #2, step #2478] loss: 4.3788012431099705\n",
      "[EPOCH #2, step #2480] loss: 4.3786860409882316\n",
      "[EPOCH #2, step #2482] loss: 4.378685886707221\n",
      "[EPOCH #2, step #2484] loss: 4.378714629006338\n",
      "[EPOCH #2, step #2486] loss: 4.378653096443302\n",
      "[EPOCH #2, step #2488] loss: 4.378619441166131\n",
      "[EPOCH #2, step #2490] loss: 4.378563298608634\n",
      "[EPOCH #2, step #2492] loss: 4.37856589155507\n",
      "[EPOCH #2, step #2494] loss: 4.378587859235928\n",
      "[EPOCH #2, step #2496] loss: 4.378607842747479\n",
      "[EPOCH #2, step #2498] loss: 4.378523983636729\n",
      "[EPOCH #2, elapsed time: 1032.903[sec]] loss: 4.378519610595703\n",
      "[EPOCH #3, step #0] loss: 4.397319316864014\n",
      "[EPOCH #3, step #2] loss: 4.368198712666829\n",
      "[EPOCH #3, step #4] loss: 4.341111755371093\n",
      "[EPOCH #3, step #6] loss: 4.322695527757917\n",
      "[EPOCH #3, step #8] loss: 4.328955332438151\n",
      "[EPOCH #3, step #10] loss: 4.332513028925115\n",
      "[EPOCH #3, step #12] loss: 4.325703400831956\n",
      "[EPOCH #3, step #14] loss: 4.32088483174642\n",
      "[EPOCH #3, step #16] loss: 4.325644689447739\n",
      "[EPOCH #3, step #18] loss: 4.321109395278127\n",
      "[EPOCH #3, step #20] loss: 4.3207221031188965\n",
      "[EPOCH #3, step #22] loss: 4.323040423185929\n",
      "[EPOCH #3, step #24] loss: 4.323400382995605\n",
      "[EPOCH #3, step #26] loss: 4.324331725085223\n",
      "[EPOCH #3, step #28] loss: 4.329954065125564\n",
      "[EPOCH #3, step #30] loss: 4.329622868568666\n",
      "[EPOCH #3, step #32] loss: 4.328456112832734\n",
      "[EPOCH #3, step #34] loss: 4.330195590427944\n",
      "[EPOCH #3, step #36] loss: 4.328978718938054\n",
      "[EPOCH #3, step #38] loss: 4.3319998887869025\n",
      "[EPOCH #3, step #40] loss: 4.330157966148563\n",
      "[EPOCH #3, step #42] loss: 4.32915072108424\n",
      "[EPOCH #3, step #44] loss: 4.321416134304471\n",
      "[EPOCH #3, step #46] loss: 4.31979904783533\n",
      "[EPOCH #3, step #48] loss: 4.3238218949765574\n",
      "[EPOCH #3, step #50] loss: 4.329953128216314\n",
      "[EPOCH #3, step #52] loss: 4.334110071074288\n",
      "[EPOCH #3, step #54] loss: 4.335083085840399\n",
      "[EPOCH #3, step #56] loss: 4.3420154169986125\n",
      "[EPOCH #3, step #58] loss: 4.342446359537416\n",
      "[EPOCH #3, step #60] loss: 4.343185620229752\n",
      "[EPOCH #3, step #62] loss: 4.344971225375221\n",
      "[EPOCH #3, step #64] loss: 4.347875228294959\n",
      "[EPOCH #3, step #66] loss: 4.347990769058911\n",
      "[EPOCH #3, step #68] loss: 4.346116646476414\n",
      "[EPOCH #3, step #70] loss: 4.346762281068614\n",
      "[EPOCH #3, step #72] loss: 4.342559703408855\n",
      "[EPOCH #3, step #74] loss: 4.342134278615315\n",
      "[EPOCH #3, step #76] loss: 4.3395994112089085\n",
      "[EPOCH #3, step #78] loss: 4.337949245790892\n",
      "[EPOCH #3, step #80] loss: 4.34063556459215\n",
      "[EPOCH #3, step #82] loss: 4.339790568294295\n",
      "[EPOCH #3, step #84] loss: 4.338260033551385\n",
      "[EPOCH #3, step #86] loss: 4.339594616287056\n",
      "[EPOCH #3, step #88] loss: 4.339712116155732\n",
      "[EPOCH #3, step #90] loss: 4.33872540966495\n",
      "[EPOCH #3, step #92] loss: 4.338329622822423\n",
      "[EPOCH #3, step #94] loss: 4.336573500382273\n",
      "[EPOCH #3, step #96] loss: 4.33817548850148\n",
      "[EPOCH #3, step #98] loss: 4.3391699453797\n",
      "[EPOCH #3, step #100] loss: 4.336790726916624\n",
      "[EPOCH #3, step #102] loss: 4.337701403978959\n",
      "[EPOCH #3, step #104] loss: 4.340663296835763\n",
      "[EPOCH #3, step #106] loss: 4.340279267213055\n",
      "[EPOCH #3, step #108] loss: 4.340259814481123\n",
      "[EPOCH #3, step #110] loss: 4.339565500482783\n",
      "[EPOCH #3, step #112] loss: 4.338171507404969\n",
      "[EPOCH #3, step #114] loss: 4.3384607315063475\n",
      "[EPOCH #3, step #116] loss: 4.338130800133078\n",
      "[EPOCH #3, step #118] loss: 4.337727634846663\n",
      "[EPOCH #3, step #120] loss: 4.336818931516537\n",
      "[EPOCH #3, step #122] loss: 4.336524091115812\n",
      "[EPOCH #3, step #124] loss: 4.337262950897217\n",
      "[EPOCH #3, step #126] loss: 4.337070457578644\n",
      "[EPOCH #3, step #128] loss: 4.336474377979604\n",
      "[EPOCH #3, step #130] loss: 4.335482357112506\n",
      "[EPOCH #3, step #132] loss: 4.335978368171175\n",
      "[EPOCH #3, step #134] loss: 4.33733838752464\n",
      "[EPOCH #3, step #136] loss: 4.3354110787384705\n",
      "[EPOCH #3, step #138] loss: 4.33636259175033\n",
      "[EPOCH #3, step #140] loss: 4.33557524917819\n",
      "[EPOCH #3, step #142] loss: 4.335334361016334\n",
      "[EPOCH #3, step #144] loss: 4.3341766784931055\n",
      "[EPOCH #3, step #146] loss: 4.334489251480622\n",
      "[EPOCH #3, step #148] loss: 4.332618844589131\n",
      "[EPOCH #3, step #150] loss: 4.331488268264872\n",
      "[EPOCH #3, step #152] loss: 4.331013916364682\n",
      "[EPOCH #3, step #154] loss: 4.329858881427396\n",
      "[EPOCH #3, step #156] loss: 4.329653490880492\n",
      "[EPOCH #3, step #158] loss: 4.330075791796798\n",
      "[EPOCH #3, step #160] loss: 4.330137273539668\n",
      "[EPOCH #3, step #162] loss: 4.33050571921413\n",
      "[EPOCH #3, step #164] loss: 4.331112624659683\n",
      "[EPOCH #3, step #166] loss: 4.3317502073185175\n",
      "[EPOCH #3, step #168] loss: 4.3318090156690605\n",
      "[EPOCH #3, step #170] loss: 4.332982860810575\n",
      "[EPOCH #3, step #172] loss: 4.33236492851566\n",
      "[EPOCH #3, step #174] loss: 4.332119334084647\n",
      "[EPOCH #3, step #176] loss: 4.333088707789189\n",
      "[EPOCH #3, step #178] loss: 4.332641225953342\n",
      "[EPOCH #3, step #180] loss: 4.331189397948881\n",
      "[EPOCH #3, step #182] loss: 4.3306065960660005\n",
      "[EPOCH #3, step #184] loss: 4.330216577890757\n",
      "[EPOCH #3, step #186] loss: 4.330552024637314\n",
      "[EPOCH #3, step #188] loss: 4.330623891618517\n",
      "[EPOCH #3, step #190] loss: 4.3299126625061035\n",
      "[EPOCH #3, step #192] loss: 4.328595900164984\n",
      "[EPOCH #3, step #194] loss: 4.327983738825871\n",
      "[EPOCH #3, step #196] loss: 4.328403693165271\n",
      "[EPOCH #3, step #198] loss: 4.328479407420709\n",
      "[EPOCH #3, step #200] loss: 4.327804475281369\n",
      "[EPOCH #3, step #202] loss: 4.327195266197467\n",
      "[EPOCH #3, step #204] loss: 4.32794146886686\n",
      "[EPOCH #3, step #206] loss: 4.327549151176416\n",
      "[EPOCH #3, step #208] loss: 4.3269342463552665\n",
      "[EPOCH #3, step #210] loss: 4.328097036099547\n",
      "[EPOCH #3, step #212] loss: 4.327890483426376\n",
      "[EPOCH #3, step #214] loss: 4.327302156492721\n",
      "[EPOCH #3, step #216] loss: 4.326506961875247\n",
      "[EPOCH #3, step #218] loss: 4.325989644821376\n",
      "[EPOCH #3, step #220] loss: 4.32511164807626\n",
      "[EPOCH #3, step #222] loss: 4.324718511692612\n",
      "[EPOCH #3, step #224] loss: 4.323992771572537\n",
      "[EPOCH #3, step #226] loss: 4.323378575530872\n",
      "[EPOCH #3, step #228] loss: 4.3236260747284865\n",
      "[EPOCH #3, step #230] loss: 4.323556043368914\n",
      "[EPOCH #3, step #232] loss: 4.324203513722563\n",
      "[EPOCH #3, step #234] loss: 4.32398590331382\n",
      "[EPOCH #3, step #236] loss: 4.324574983572658\n",
      "[EPOCH #3, step #238] loss: 4.325525110236771\n",
      "[EPOCH #3, step #240] loss: 4.326140192039775\n",
      "[EPOCH #3, step #242] loss: 4.325948585698634\n",
      "[EPOCH #3, step #244] loss: 4.325557654244559\n",
      "[EPOCH #3, step #246] loss: 4.324676434520768\n",
      "[EPOCH #3, step #248] loss: 4.32449197769165\n",
      "[EPOCH #3, step #250] loss: 4.323192149994383\n",
      "[EPOCH #3, step #252] loss: 4.323419733009791\n",
      "[EPOCH #3, step #254] loss: 4.323321204091989\n",
      "[EPOCH #3, step #256] loss: 4.322911726361583\n",
      "[EPOCH #3, step #258] loss: 4.323597219459799\n",
      "[EPOCH #3, step #260] loss: 4.323927736830437\n",
      "[EPOCH #3, step #262] loss: 4.323855184330233\n",
      "[EPOCH #3, step #264] loss: 4.32349287788823\n",
      "[EPOCH #3, step #266] loss: 4.32319070337417\n",
      "[EPOCH #3, step #268] loss: 4.32301174929594\n",
      "[EPOCH #3, step #270] loss: 4.323653235206745\n",
      "[EPOCH #3, step #272] loss: 4.322642083569761\n",
      "[EPOCH #3, step #274] loss: 4.323661679354581\n",
      "[EPOCH #3, step #276] loss: 4.322689633076802\n",
      "[EPOCH #3, step #278] loss: 4.323026258885647\n",
      "[EPOCH #3, step #280] loss: 4.322268172091012\n",
      "[EPOCH #3, step #282] loss: 4.322290654738463\n",
      "[EPOCH #3, step #284] loss: 4.321175980149654\n",
      "[EPOCH #3, step #286] loss: 4.321179933248912\n",
      "[EPOCH #3, step #288] loss: 4.3212227903847875\n",
      "[EPOCH #3, step #290] loss: 4.321133310442528\n",
      "[EPOCH #3, step #292] loss: 4.320822087571075\n",
      "[EPOCH #3, step #294] loss: 4.32101327443527\n",
      "[EPOCH #3, step #296] loss: 4.320753137671987\n",
      "[EPOCH #3, step #298] loss: 4.320508196202409\n",
      "[EPOCH #3, step #300] loss: 4.319679979470085\n",
      "[EPOCH #3, step #302] loss: 4.318838948857273\n",
      "[EPOCH #3, step #304] loss: 4.31837420385392\n",
      "[EPOCH #3, step #306] loss: 4.318258628783086\n",
      "[EPOCH #3, step #308] loss: 4.318368272873962\n",
      "[EPOCH #3, step #310] loss: 4.31874576642199\n",
      "[EPOCH #3, step #312] loss: 4.319211842533879\n",
      "[EPOCH #3, step #314] loss: 4.318868877774193\n",
      "[EPOCH #3, step #316] loss: 4.319193366198133\n",
      "[EPOCH #3, step #318] loss: 4.319965597230439\n",
      "[EPOCH #3, step #320] loss: 4.319856968995567\n",
      "[EPOCH #3, step #322] loss: 4.32033047513696\n",
      "[EPOCH #3, step #324] loss: 4.319936436873216\n",
      "[EPOCH #3, step #326] loss: 4.319510525519695\n",
      "[EPOCH #3, step #328] loss: 4.31977534801402\n",
      "[EPOCH #3, step #330] loss: 4.319866664460059\n",
      "[EPOCH #3, step #332] loss: 4.318974683950613\n",
      "[EPOCH #3, step #334] loss: 4.318821884269145\n",
      "[EPOCH #3, step #336] loss: 4.318522856568016\n",
      "[EPOCH #3, step #338] loss: 4.319336186468074\n",
      "[EPOCH #3, step #340] loss: 4.319124350449906\n",
      "[EPOCH #3, step #342] loss: 4.319632355047732\n",
      "[EPOCH #3, step #344] loss: 4.319882669310639\n",
      "[EPOCH #3, step #346] loss: 4.320653673551268\n",
      "[EPOCH #3, step #348] loss: 4.320757481976703\n",
      "[EPOCH #3, step #350] loss: 4.320445942403245\n",
      "[EPOCH #3, step #352] loss: 4.320606496448895\n",
      "[EPOCH #3, step #354] loss: 4.3212000605086205\n",
      "[EPOCH #3, step #356] loss: 4.321581152640805\n",
      "[EPOCH #3, step #358] loss: 4.321705900526976\n",
      "[EPOCH #3, step #360] loss: 4.321220043953766\n",
      "[EPOCH #3, step #362] loss: 4.320978955460646\n",
      "[EPOCH #3, step #364] loss: 4.321540921354947\n",
      "[EPOCH #3, step #366] loss: 4.321732736738241\n",
      "[EPOCH #3, step #368] loss: 4.321452410886605\n",
      "[EPOCH #3, step #370] loss: 4.320622504560774\n",
      "[EPOCH #3, step #372] loss: 4.320270021863022\n",
      "[EPOCH #3, step #374] loss: 4.320513267517089\n",
      "[EPOCH #3, step #376] loss: 4.32056397278682\n",
      "[EPOCH #3, step #378] loss: 4.320830280988386\n",
      "[EPOCH #3, step #380] loss: 4.3209512227476425\n",
      "[EPOCH #3, step #382] loss: 4.3212374918765875\n",
      "[EPOCH #3, step #384] loss: 4.321344129141275\n",
      "[EPOCH #3, step #386] loss: 4.32152610971022\n",
      "[EPOCH #3, step #388] loss: 4.321017037320566\n",
      "[EPOCH #3, step #390] loss: 4.320823342598918\n",
      "[EPOCH #3, step #392] loss: 4.320669352553273\n",
      "[EPOCH #3, step #394] loss: 4.320647737044323\n",
      "[EPOCH #3, step #396] loss: 4.320788055583272\n",
      "[EPOCH #3, step #398] loss: 4.320513555579317\n",
      "[EPOCH #3, step #400] loss: 4.32052568426156\n",
      "[EPOCH #3, step #402] loss: 4.320584745620084\n",
      "[EPOCH #3, step #404] loss: 4.320377706598353\n",
      "[EPOCH #3, step #406] loss: 4.320610868842947\n",
      "[EPOCH #3, step #408] loss: 4.320061835216718\n",
      "[EPOCH #3, step #410] loss: 4.319991992337861\n",
      "[EPOCH #3, step #412] loss: 4.319793006121102\n",
      "[EPOCH #3, step #414] loss: 4.320001854953996\n",
      "[EPOCH #3, step #416] loss: 4.320090583188357\n",
      "[EPOCH #3, step #418] loss: 4.320380436207628\n",
      "[EPOCH #3, step #420] loss: 4.320021460571652\n",
      "[EPOCH #3, step #422] loss: 4.320064903151059\n",
      "[EPOCH #3, step #424] loss: 4.319814395904541\n",
      "[EPOCH #3, step #426] loss: 4.319876086795637\n",
      "[EPOCH #3, step #428] loss: 4.320173468067374\n",
      "[EPOCH #3, step #430] loss: 4.319628032896746\n",
      "[EPOCH #3, step #432] loss: 4.319500757970656\n",
      "[EPOCH #3, step #434] loss: 4.319352752860936\n",
      "[EPOCH #3, step #436] loss: 4.319118907849903\n",
      "[EPOCH #3, step #438] loss: 4.318998290085847\n",
      "[EPOCH #3, step #440] loss: 4.318975456177243\n",
      "[EPOCH #3, step #442] loss: 4.31930172793214\n",
      "[EPOCH #3, step #444] loss: 4.3191855602050095\n",
      "[EPOCH #3, step #446] loss: 4.319156588057277\n",
      "[EPOCH #3, step #448] loss: 4.319340222662435\n",
      "[EPOCH #3, step #450] loss: 4.318922483736025\n",
      "[EPOCH #3, step #452] loss: 4.319090794780133\n",
      "[EPOCH #3, step #454] loss: 4.319353841425299\n",
      "[EPOCH #3, step #456] loss: 4.318745413807341\n",
      "[EPOCH #3, step #458] loss: 4.318105969813395\n",
      "[EPOCH #3, step #460] loss: 4.317844998034893\n",
      "[EPOCH #3, step #462] loss: 4.318087252375889\n",
      "[EPOCH #3, step #464] loss: 4.318515909871747\n",
      "[EPOCH #3, step #466] loss: 4.318110464982353\n",
      "[EPOCH #3, step #468] loss: 4.3181836111967495\n",
      "[EPOCH #3, step #470] loss: 4.3178678135963\n",
      "[EPOCH #3, step #472] loss: 4.317486223660362\n",
      "[EPOCH #3, step #474] loss: 4.317305682332893\n",
      "[EPOCH #3, step #476] loss: 4.317453836245107\n",
      "[EPOCH #3, step #478] loss: 4.317657140202214\n",
      "[EPOCH #3, step #480] loss: 4.317589417812482\n",
      "[EPOCH #3, step #482] loss: 4.317770366589722\n",
      "[EPOCH #3, step #484] loss: 4.317904183299271\n",
      "[EPOCH #3, step #486] loss: 4.3181112943488715\n",
      "[EPOCH #3, step #488] loss: 4.317981100764986\n",
      "[EPOCH #3, step #490] loss: 4.317377581858587\n",
      "[EPOCH #3, step #492] loss: 4.316877602080061\n",
      "[EPOCH #3, step #494] loss: 4.316810104100391\n",
      "[EPOCH #3, step #496] loss: 4.317066773082649\n",
      "[EPOCH #3, step #498] loss: 4.317158418093511\n",
      "[EPOCH #3, step #500] loss: 4.3171302572695796\n",
      "[EPOCH #3, step #502] loss: 4.317193492032424\n",
      "[EPOCH #3, step #504] loss: 4.317260630768124\n",
      "[EPOCH #3, step #506] loss: 4.316769137185001\n",
      "[EPOCH #3, step #508] loss: 4.316923507772869\n",
      "[EPOCH #3, step #510] loss: 4.316584254897504\n",
      "[EPOCH #3, step #512] loss: 4.3168283159505085\n",
      "[EPOCH #3, step #514] loss: 4.31761972464404\n",
      "[EPOCH #3, step #516] loss: 4.318011680470214\n",
      "[EPOCH #3, step #518] loss: 4.3180913327976\n",
      "[EPOCH #3, step #520] loss: 4.317829216281649\n",
      "[EPOCH #3, step #522] loss: 4.317620569159834\n",
      "[EPOCH #3, step #524] loss: 4.317719243367513\n",
      "[EPOCH #3, step #526] loss: 4.317938401984082\n",
      "[EPOCH #3, step #528] loss: 4.3182191740587665\n",
      "[EPOCH #3, step #530] loss: 4.318100469485065\n",
      "[EPOCH #3, step #532] loss: 4.3175904665834235\n",
      "[EPOCH #3, step #534] loss: 4.317575526014667\n",
      "[EPOCH #3, step #536] loss: 4.317701579472206\n",
      "[EPOCH #3, step #538] loss: 4.317567999597383\n",
      "[EPOCH #3, step #540] loss: 4.317494435583598\n",
      "[EPOCH #3, step #542] loss: 4.317114020359889\n",
      "[EPOCH #3, step #544] loss: 4.31689115839267\n",
      "[EPOCH #3, step #546] loss: 4.31711199078743\n",
      "[EPOCH #3, step #548] loss: 4.317528823685777\n",
      "[EPOCH #3, step #550] loss: 4.317433732823754\n",
      "[EPOCH #3, step #552] loss: 4.317464585330129\n",
      "[EPOCH #3, step #554] loss: 4.3178253740877715\n",
      "[EPOCH #3, step #556] loss: 4.317999017088803\n",
      "[EPOCH #3, step #558] loss: 4.31805328904836\n",
      "[EPOCH #3, step #560] loss: 4.3180262953202355\n",
      "[EPOCH #3, step #562] loss: 4.3177432548089305\n",
      "[EPOCH #3, step #564] loss: 4.317565690943625\n",
      "[EPOCH #3, step #566] loss: 4.317486917741294\n",
      "[EPOCH #3, step #568] loss: 4.317264130329206\n",
      "[EPOCH #3, step #570] loss: 4.31700344803872\n",
      "[EPOCH #3, step #572] loss: 4.316744180249918\n",
      "[EPOCH #3, step #574] loss: 4.316854968692946\n",
      "[EPOCH #3, step #576] loss: 4.317505873675156\n",
      "[EPOCH #3, step #578] loss: 4.317371605591453\n",
      "[EPOCH #3, step #580] loss: 4.317184128982885\n",
      "[EPOCH #3, step #582] loss: 4.316985424754959\n",
      "[EPOCH #3, step #584] loss: 4.317086555611374\n",
      "[EPOCH #3, step #586] loss: 4.316990142380359\n",
      "[EPOCH #3, step #588] loss: 4.31658189446493\n",
      "[EPOCH #3, step #590] loss: 4.31679372254967\n",
      "[EPOCH #3, step #592] loss: 4.316767930582442\n",
      "[EPOCH #3, step #594] loss: 4.316544490301308\n",
      "[EPOCH #3, step #596] loss: 4.316696068749356\n",
      "[EPOCH #3, step #598] loss: 4.31698523459331\n",
      "[EPOCH #3, step #600] loss: 4.316745118571201\n",
      "[EPOCH #3, step #602] loss: 4.316725388490541\n",
      "[EPOCH #3, step #604] loss: 4.316465348645675\n",
      "[EPOCH #3, step #606] loss: 4.316094570536983\n",
      "[EPOCH #3, step #608] loss: 4.316476326270644\n",
      "[EPOCH #3, step #610] loss: 4.316411144019344\n",
      "[EPOCH #3, step #612] loss: 4.316053924124937\n",
      "[EPOCH #3, step #614] loss: 4.316030000671138\n",
      "[EPOCH #3, step #616] loss: 4.315877150098161\n",
      "[EPOCH #3, step #618] loss: 4.3157792022039505\n",
      "[EPOCH #3, step #620] loss: 4.315788676005439\n",
      "[EPOCH #3, step #622] loss: 4.315748127467368\n",
      "[EPOCH #3, step #624] loss: 4.315902481079101\n",
      "[EPOCH #3, step #626] loss: 4.316211387871555\n",
      "[EPOCH #3, step #628] loss: 4.316427249787154\n",
      "[EPOCH #3, step #630] loss: 4.316067272434144\n",
      "[EPOCH #3, step #632] loss: 4.315878135144804\n",
      "[EPOCH #3, step #634] loss: 4.3159249395836055\n",
      "[EPOCH #3, step #636] loss: 4.316430886836029\n",
      "[EPOCH #3, step #638] loss: 4.316226886844785\n",
      "[EPOCH #3, step #640] loss: 4.316315011933516\n",
      "[EPOCH #3, step #642] loss: 4.3163396959912905\n",
      "[EPOCH #3, step #644] loss: 4.316127170888028\n",
      "[EPOCH #3, step #646] loss: 4.316095997782358\n",
      "[EPOCH #3, step #648] loss: 4.316154969307968\n",
      "[EPOCH #3, step #650] loss: 4.316159540607083\n",
      "[EPOCH #3, step #652] loss: 4.316397320804333\n",
      "[EPOCH #3, step #654] loss: 4.316540706430683\n",
      "[EPOCH #3, step #656] loss: 4.316794714789776\n",
      "[EPOCH #3, step #658] loss: 4.317013431572227\n",
      "[EPOCH #3, step #660] loss: 4.317013353875834\n",
      "[EPOCH #3, step #662] loss: 4.31674151470938\n",
      "[EPOCH #3, step #664] loss: 4.316389428045516\n",
      "[EPOCH #3, step #666] loss: 4.316504506335623\n",
      "[EPOCH #3, step #668] loss: 4.316580599911722\n",
      "[EPOCH #3, step #670] loss: 4.316374911636602\n",
      "[EPOCH #3, step #672] loss: 4.31645988107259\n",
      "[EPOCH #3, step #674] loss: 4.316327436235216\n",
      "[EPOCH #3, step #676] loss: 4.315959764196038\n",
      "[EPOCH #3, step #678] loss: 4.3162186753240706\n",
      "[EPOCH #3, step #680] loss: 4.316245028626026\n",
      "[EPOCH #3, step #682] loss: 4.315960701876818\n",
      "[EPOCH #3, step #684] loss: 4.3161066681799225\n",
      "[EPOCH #3, step #686] loss: 4.316187584417529\n",
      "[EPOCH #3, step #688] loss: 4.316397086283292\n",
      "[EPOCH #3, step #690] loss: 4.316636458837175\n",
      "[EPOCH #3, step #692] loss: 4.3167140514819655\n",
      "[EPOCH #3, step #694] loss: 4.316775332251899\n",
      "[EPOCH #3, step #696] loss: 4.316434113845935\n",
      "[EPOCH #3, step #698] loss: 4.316494458734733\n",
      "[EPOCH #3, step #700] loss: 4.316252597558515\n",
      "[EPOCH #3, step #702] loss: 4.315988638323028\n",
      "[EPOCH #3, step #704] loss: 4.315891623666101\n",
      "[EPOCH #3, step #706] loss: 4.316050930104128\n",
      "[EPOCH #3, step #708] loss: 4.315892266621879\n",
      "[EPOCH #3, step #710] loss: 4.315971141123067\n",
      "[EPOCH #3, step #712] loss: 4.315951302543095\n",
      "[EPOCH #3, step #714] loss: 4.31604293676523\n",
      "[EPOCH #3, step #716] loss: 4.316371641065952\n",
      "[EPOCH #3, step #718] loss: 4.316262414292931\n",
      "[EPOCH #3, step #720] loss: 4.316179703409563\n",
      "[EPOCH #3, step #722] loss: 4.316436526851206\n",
      "[EPOCH #3, step #724] loss: 4.316353149414063\n",
      "[EPOCH #3, step #726] loss: 4.316235786946964\n",
      "[EPOCH #3, step #728] loss: 4.316173752966568\n",
      "[EPOCH #3, step #730] loss: 4.316383854308957\n",
      "[EPOCH #3, step #732] loss: 4.316461370554009\n",
      "[EPOCH #3, step #734] loss: 4.316260128280743\n",
      "[EPOCH #3, step #736] loss: 4.316297307913792\n",
      "[EPOCH #3, step #738] loss: 4.316298523517036\n",
      "[EPOCH #3, step #740] loss: 4.316057059768079\n",
      "[EPOCH #3, step #742] loss: 4.315926418971887\n",
      "[EPOCH #3, step #744] loss: 4.315895966395436\n",
      "[EPOCH #3, step #746] loss: 4.315600877784821\n",
      "[EPOCH #3, step #748] loss: 4.315245417631834\n",
      "[EPOCH #3, step #750] loss: 4.314960983240811\n",
      "[EPOCH #3, step #752] loss: 4.314842424861305\n",
      "[EPOCH #3, step #754] loss: 4.315002826665411\n",
      "[EPOCH #3, step #756] loss: 4.314934986900622\n",
      "[EPOCH #3, step #758] loss: 4.314868092693987\n",
      "[EPOCH #3, step #760] loss: 4.314831359001089\n",
      "[EPOCH #3, step #762] loss: 4.315023555980598\n",
      "[EPOCH #3, step #764] loss: 4.3149052763296885\n",
      "[EPOCH #3, step #766] loss: 4.314761585175913\n",
      "[EPOCH #3, step #768] loss: 4.314480209226571\n",
      "[EPOCH #3, step #770] loss: 4.314494300599846\n",
      "[EPOCH #3, step #772] loss: 4.314563305843694\n",
      "[EPOCH #3, step #774] loss: 4.314279369846467\n",
      "[EPOCH #3, step #776] loss: 4.314576348436078\n",
      "[EPOCH #3, step #778] loss: 4.314255898662954\n",
      "[EPOCH #3, step #780] loss: 4.313862135101982\n",
      "[EPOCH #3, step #782] loss: 4.313523411294053\n",
      "[EPOCH #3, step #784] loss: 4.313306956078597\n",
      "[EPOCH #3, step #786] loss: 4.31363388362046\n",
      "[EPOCH #3, step #788] loss: 4.313467737084257\n",
      "[EPOCH #3, step #790] loss: 4.313216597633025\n",
      "[EPOCH #3, step #792] loss: 4.313444687769963\n",
      "[EPOCH #3, step #794] loss: 4.313450663944461\n",
      "[EPOCH #3, step #796] loss: 4.313248731859654\n",
      "[EPOCH #3, step #798] loss: 4.313218310717796\n",
      "[EPOCH #3, step #800] loss: 4.313239236895957\n",
      "[EPOCH #3, step #802] loss: 4.313370097767223\n",
      "[EPOCH #3, step #804] loss: 4.3134746273111855\n",
      "[EPOCH #3, step #806] loss: 4.313589740124481\n",
      "[EPOCH #3, step #808] loss: 4.313467381763812\n",
      "[EPOCH #3, step #810] loss: 4.3134597209173124\n",
      "[EPOCH #3, step #812] loss: 4.313119367479838\n",
      "[EPOCH #3, step #814] loss: 4.313166424101847\n",
      "[EPOCH #3, step #816] loss: 4.3133285795606335\n",
      "[EPOCH #3, step #818] loss: 4.313452084276993\n",
      "[EPOCH #3, step #820] loss: 4.313389838540452\n",
      "[EPOCH #3, step #822] loss: 4.313122443411214\n",
      "[EPOCH #3, step #824] loss: 4.3134384565642385\n",
      "[EPOCH #3, step #826] loss: 4.313502607495494\n",
      "[EPOCH #3, step #828] loss: 4.31343034783376\n",
      "[EPOCH #3, step #830] loss: 4.3133924509500625\n",
      "[EPOCH #3, step #832] loss: 4.313210212597612\n",
      "[EPOCH #3, step #834] loss: 4.313393539725664\n",
      "[EPOCH #3, step #836] loss: 4.313098168002805\n",
      "[EPOCH #3, step #838] loss: 4.312941221571366\n",
      "[EPOCH #3, step #840] loss: 4.312569569464104\n",
      "[EPOCH #3, step #842] loss: 4.31250269133001\n",
      "[EPOCH #3, step #844] loss: 4.312489430015609\n",
      "[EPOCH #3, step #846] loss: 4.31242921360831\n",
      "[EPOCH #3, step #848] loss: 4.312528610229492\n",
      "[EPOCH #3, step #850] loss: 4.312317769199645\n",
      "[EPOCH #3, step #852] loss: 4.312277516614371\n",
      "[EPOCH #3, step #854] loss: 4.312326667183324\n",
      "[EPOCH #3, step #856] loss: 4.312296294275772\n",
      "[EPOCH #3, step #858] loss: 4.311871258843348\n",
      "[EPOCH #3, step #860] loss: 4.31183598681194\n",
      "[EPOCH #3, step #862] loss: 4.31195396730549\n",
      "[EPOCH #3, step #864] loss: 4.311747010181405\n",
      "[EPOCH #3, step #866] loss: 4.311510782890804\n",
      "[EPOCH #3, step #868] loss: 4.311433016773746\n",
      "[EPOCH #3, step #870] loss: 4.311389029641924\n",
      "[EPOCH #3, step #872] loss: 4.311281756983317\n",
      "[EPOCH #3, step #874] loss: 4.311283121926444\n",
      "[EPOCH #3, step #876] loss: 4.311343344206685\n",
      "[EPOCH #3, step #878] loss: 4.311090735174012\n",
      "[EPOCH #3, step #880] loss: 4.311117633382254\n",
      "[EPOCH #3, step #882] loss: 4.310998961987609\n",
      "[EPOCH #3, step #884] loss: 4.310913211477678\n",
      "[EPOCH #3, step #886] loss: 4.310751123116546\n",
      "[EPOCH #3, step #888] loss: 4.3106246182299035\n",
      "[EPOCH #3, step #890] loss: 4.310364966708268\n",
      "[EPOCH #3, step #892] loss: 4.310304572841221\n",
      "[EPOCH #3, step #894] loss: 4.31039755517544\n",
      "[EPOCH #3, step #896] loss: 4.310211711636887\n",
      "[EPOCH #3, step #898] loss: 4.310478548849782\n",
      "[EPOCH #3, step #900] loss: 4.310564514799467\n",
      "[EPOCH #3, step #902] loss: 4.310586633080264\n",
      "[EPOCH #3, step #904] loss: 4.3105655469947095\n",
      "[EPOCH #3, step #906] loss: 4.310362552991907\n",
      "[EPOCH #3, step #908] loss: 4.3103589686361214\n",
      "[EPOCH #3, step #910] loss: 4.310360904844349\n",
      "[EPOCH #3, step #912] loss: 4.310468871648315\n",
      "[EPOCH #3, step #914] loss: 4.310456800721382\n",
      "[EPOCH #3, step #916] loss: 4.3104058362405455\n",
      "[EPOCH #3, step #918] loss: 4.31059985529222\n",
      "[EPOCH #3, step #920] loss: 4.310661262590903\n",
      "[EPOCH #3, step #922] loss: 4.310658575266697\n",
      "[EPOCH #3, step #924] loss: 4.310514586165144\n",
      "[EPOCH #3, step #926] loss: 4.310586067207937\n",
      "[EPOCH #3, step #928] loss: 4.31074046831983\n",
      "[EPOCH #3, step #930] loss: 4.310646127809387\n",
      "[EPOCH #3, step #932] loss: 4.310767760486337\n",
      "[EPOCH #3, step #934] loss: 4.310687725046739\n",
      "[EPOCH #3, step #936] loss: 4.310285567473958\n",
      "[EPOCH #3, step #938] loss: 4.3102019996054\n",
      "[EPOCH #3, step #940] loss: 4.310129582438535\n",
      "[EPOCH #3, step #942] loss: 4.310082221461\n",
      "[EPOCH #3, step #944] loss: 4.309977359620351\n",
      "[EPOCH #3, step #946] loss: 4.310083346734455\n",
      "[EPOCH #3, step #948] loss: 4.3102674642528696\n",
      "[EPOCH #3, step #950] loss: 4.310104127185954\n",
      "[EPOCH #3, step #952] loss: 4.310033437212517\n",
      "[EPOCH #3, step #954] loss: 4.3102333116281715\n",
      "[EPOCH #3, step #956] loss: 4.310140570129347\n",
      "[EPOCH #3, step #958] loss: 4.310142793794618\n",
      "[EPOCH #3, step #960] loss: 4.31033685204886\n",
      "[EPOCH #3, step #962] loss: 4.310315808279368\n",
      "[EPOCH #3, step #964] loss: 4.310177411934255\n",
      "[EPOCH #3, step #966] loss: 4.3101357079834335\n",
      "[EPOCH #3, step #968] loss: 4.310087216035747\n",
      "[EPOCH #3, step #970] loss: 4.30990187122451\n",
      "[EPOCH #3, step #972] loss: 4.309767058794937\n",
      "[EPOCH #3, step #974] loss: 4.309882080616095\n",
      "[EPOCH #3, step #976] loss: 4.3095516755856655\n",
      "[EPOCH #3, step #978] loss: 4.309525527554708\n",
      "[EPOCH #3, step #980] loss: 4.30937436815431\n",
      "[EPOCH #3, step #982] loss: 4.30929406103986\n",
      "[EPOCH #3, step #984] loss: 4.309204607203527\n",
      "[EPOCH #3, step #986] loss: 4.309231621636929\n",
      "[EPOCH #3, step #988] loss: 4.309099479681079\n",
      "[EPOCH #3, step #990] loss: 4.308857655549266\n",
      "[EPOCH #3, step #992] loss: 4.308764504762094\n",
      "[EPOCH #3, step #994] loss: 4.308818667138641\n",
      "[EPOCH #3, step #996] loss: 4.308646009343796\n",
      "[EPOCH #3, step #998] loss: 4.308382079646632\n",
      "[EPOCH #3, step #1000] loss: 4.308152643236128\n",
      "[EPOCH #3, step #1002] loss: 4.308119170568282\n",
      "[EPOCH #3, step #1004] loss: 4.307772732729935\n",
      "[EPOCH #3, step #1006] loss: 4.307658308193961\n",
      "[EPOCH #3, step #1008] loss: 4.307506318843755\n",
      "[EPOCH #3, step #1010] loss: 4.30720006629574\n",
      "[EPOCH #3, step #1012] loss: 4.3071061216243285\n",
      "[EPOCH #3, step #1014] loss: 4.307406071254185\n",
      "[EPOCH #3, step #1016] loss: 4.307287093343519\n",
      "[EPOCH #3, step #1018] loss: 4.307018330567485\n",
      "[EPOCH #3, step #1020] loss: 4.306825740563881\n",
      "[EPOCH #3, step #1022] loss: 4.306990799200034\n",
      "[EPOCH #3, step #1024] loss: 4.306860912136916\n",
      "[EPOCH #3, step #1026] loss: 4.306843549622533\n",
      "[EPOCH #3, step #1028] loss: 4.306808169080402\n",
      "[EPOCH #3, step #1030] loss: 4.3065984885052035\n",
      "[EPOCH #3, step #1032] loss: 4.306353039570722\n",
      "[EPOCH #3, step #1034] loss: 4.306566505155701\n",
      "[EPOCH #3, step #1036] loss: 4.306648255314179\n",
      "[EPOCH #3, step #1038] loss: 4.306499164074631\n",
      "[EPOCH #3, step #1040] loss: 4.306359802268081\n",
      "[EPOCH #3, step #1042] loss: 4.306382192975608\n",
      "[EPOCH #3, step #1044] loss: 4.306325974989165\n",
      "[EPOCH #3, step #1046] loss: 4.3061333891313875\n",
      "[EPOCH #3, step #1048] loss: 4.306215093974504\n",
      "[EPOCH #3, step #1050] loss: 4.3059420449522765\n",
      "[EPOCH #3, step #1052] loss: 4.305855044612178\n",
      "[EPOCH #3, step #1054] loss: 4.30592502033541\n",
      "[EPOCH #3, step #1056] loss: 4.30613863727566\n",
      "[EPOCH #3, step #1058] loss: 4.30613638704064\n",
      "[EPOCH #3, step #1060] loss: 4.305906906541399\n",
      "[EPOCH #3, step #1062] loss: 4.305969256224323\n",
      "[EPOCH #3, step #1064] loss: 4.305812922106103\n",
      "[EPOCH #3, step #1066] loss: 4.305831855999161\n",
      "[EPOCH #3, step #1068] loss: 4.30596537228719\n",
      "[EPOCH #3, step #1070] loss: 4.305956865002573\n",
      "[EPOCH #3, step #1072] loss: 4.305778574521366\n",
      "[EPOCH #3, step #1074] loss: 4.305802815459495\n",
      "[EPOCH #3, step #1076] loss: 4.305677514886457\n",
      "[EPOCH #3, step #1078] loss: 4.305343295160104\n",
      "[EPOCH #3, step #1080] loss: 4.30532950499232\n",
      "[EPOCH #3, step #1082] loss: 4.305463084223528\n",
      "[EPOCH #3, step #1084] loss: 4.305380385702107\n",
      "[EPOCH #3, step #1086] loss: 4.30514148231373\n",
      "[EPOCH #3, step #1088] loss: 4.305285962159968\n",
      "[EPOCH #3, step #1090] loss: 4.305361285327663\n",
      "[EPOCH #3, step #1092] loss: 4.305386760097409\n",
      "[EPOCH #3, step #1094] loss: 4.30566218345677\n",
      "[EPOCH #3, step #1096] loss: 4.305268048153428\n",
      "[EPOCH #3, step #1098] loss: 4.305356784122873\n",
      "[EPOCH #3, step #1100] loss: 4.305248350581724\n",
      "[EPOCH #3, step #1102] loss: 4.305114385115484\n",
      "[EPOCH #3, step #1104] loss: 4.305146411533269\n",
      "[EPOCH #3, step #1106] loss: 4.305061019856109\n",
      "[EPOCH #3, step #1108] loss: 4.305120339148533\n",
      "[EPOCH #3, step #1110] loss: 4.3049777654996335\n",
      "[EPOCH #3, step #1112] loss: 4.305035769350338\n",
      "[EPOCH #3, step #1114] loss: 4.305114835901645\n",
      "[EPOCH #3, step #1116] loss: 4.305009695922251\n",
      "[EPOCH #3, step #1118] loss: 4.305049295399847\n",
      "[EPOCH #3, step #1120] loss: 4.304913548887255\n",
      "[EPOCH #3, step #1122] loss: 4.304962987042068\n",
      "[EPOCH #3, step #1124] loss: 4.304994895087348\n",
      "[EPOCH #3, step #1126] loss: 4.3049466874394255\n",
      "[EPOCH #3, step #1128] loss: 4.304947738630567\n",
      "[EPOCH #3, step #1130] loss: 4.3050558139116335\n",
      "[EPOCH #3, step #1132] loss: 4.305030736056241\n",
      "[EPOCH #3, step #1134] loss: 4.305050410048027\n",
      "[EPOCH #3, step #1136] loss: 4.305173865933112\n",
      "[EPOCH #3, step #1138] loss: 4.3054171186251216\n",
      "[EPOCH #3, step #1140] loss: 4.305486544927518\n",
      "[EPOCH #3, step #1142] loss: 4.305263716196242\n",
      "[EPOCH #3, step #1144] loss: 4.305312010502711\n",
      "[EPOCH #3, step #1146] loss: 4.305321803171944\n",
      "[EPOCH #3, step #1148] loss: 4.305308631651291\n",
      "[EPOCH #3, step #1150] loss: 4.305311116832531\n",
      "[EPOCH #3, step #1152] loss: 4.305318431242176\n",
      "[EPOCH #3, step #1154] loss: 4.305118613222461\n",
      "[EPOCH #3, step #1156] loss: 4.30492839260975\n",
      "[EPOCH #3, step #1158] loss: 4.304819790426259\n",
      "[EPOCH #3, step #1160] loss: 4.304808967386618\n",
      "[EPOCH #3, step #1162] loss: 4.304852363064898\n",
      "[EPOCH #3, step #1164] loss: 4.30485763959107\n",
      "[EPOCH #3, step #1166] loss: 4.304767398621756\n",
      "[EPOCH #3, step #1168] loss: 4.304527634734063\n",
      "[EPOCH #3, step #1170] loss: 4.304567192885532\n",
      "[EPOCH #3, step #1172] loss: 4.304455456729639\n",
      "[EPOCH #3, step #1174] loss: 4.304592387828421\n",
      "[EPOCH #3, step #1176] loss: 4.3046970343123885\n",
      "[EPOCH #3, step #1178] loss: 4.304575048936839\n",
      "[EPOCH #3, step #1180] loss: 4.30463944765393\n",
      "[EPOCH #3, step #1182] loss: 4.3047116461791735\n",
      "[EPOCH #3, step #1184] loss: 4.304608708796119\n",
      "[EPOCH #3, step #1186] loss: 4.304788944594681\n",
      "[EPOCH #3, step #1188] loss: 4.304813089442915\n",
      "[EPOCH #3, step #1190] loss: 4.30473821169183\n",
      "[EPOCH #3, step #1192] loss: 4.304620559737149\n",
      "[EPOCH #3, step #1194] loss: 4.304514810729725\n",
      "[EPOCH #3, step #1196] loss: 4.304328851134158\n",
      "[EPOCH #3, step #1198] loss: 4.304248144071832\n",
      "[EPOCH #3, step #1200] loss: 4.30408619960877\n",
      "[EPOCH #3, step #1202] loss: 4.3040503706420745\n",
      "[EPOCH #3, step #1204] loss: 4.303898074913817\n",
      "[EPOCH #3, step #1206] loss: 4.3036834875610905\n",
      "[EPOCH #3, step #1208] loss: 4.303730391982767\n",
      "[EPOCH #3, step #1210] loss: 4.303558680363473\n",
      "[EPOCH #3, step #1212] loss: 4.30358499863476\n",
      "[EPOCH #3, step #1214] loss: 4.303276212715808\n",
      "[EPOCH #3, step #1216] loss: 4.303212249288817\n",
      "[EPOCH #3, step #1218] loss: 4.303037354357424\n",
      "[EPOCH #3, step #1220] loss: 4.3028554939813635\n",
      "[EPOCH #3, step #1222] loss: 4.302979456432901\n",
      "[EPOCH #3, step #1224] loss: 4.302878041948591\n",
      "[EPOCH #3, step #1226] loss: 4.302853531437185\n",
      "[EPOCH #3, step #1228] loss: 4.302837141749333\n",
      "[EPOCH #3, step #1230] loss: 4.302842083256192\n",
      "[EPOCH #3, step #1232] loss: 4.302753059049979\n",
      "[EPOCH #3, step #1234] loss: 4.302763175192149\n",
      "[EPOCH #3, step #1236] loss: 4.30273406854443\n",
      "[EPOCH #3, step #1238] loss: 4.302688207233789\n",
      "[EPOCH #3, step #1240] loss: 4.302716336646069\n",
      "[EPOCH #3, step #1242] loss: 4.302609022835619\n",
      "[EPOCH #3, step #1244] loss: 4.30286453392611\n",
      "[EPOCH #3, step #1246] loss: 4.30293030727359\n",
      "[EPOCH #3, step #1248] loss: 4.302921326471577\n",
      "[EPOCH #3, step #1250] loss: 4.303035938863655\n",
      "[EPOCH #3, step #1252] loss: 4.302802823775307\n",
      "[EPOCH #3, step #1254] loss: 4.3027126954371235\n",
      "[EPOCH #3, step #1256] loss: 4.302568782011289\n",
      "[EPOCH #3, step #1258] loss: 4.302606647406617\n",
      "[EPOCH #3, step #1260] loss: 4.302418605190341\n",
      "[EPOCH #3, step #1262] loss: 4.302303589815576\n",
      "[EPOCH #3, step #1264] loss: 4.302049517348821\n",
      "[EPOCH #3, step #1266] loss: 4.302012412305318\n",
      "[EPOCH #3, step #1268] loss: 4.302276188975906\n",
      "[EPOCH #3, step #1270] loss: 4.302368114165299\n",
      "[EPOCH #3, step #1272] loss: 4.302245197760619\n",
      "[EPOCH #3, step #1274] loss: 4.302218943203197\n",
      "[EPOCH #3, step #1276] loss: 4.302218403215043\n",
      "[EPOCH #3, step #1278] loss: 4.302195758536237\n",
      "[EPOCH #3, step #1280] loss: 4.302345100182467\n",
      "[EPOCH #3, step #1282] loss: 4.302315008798244\n",
      "[EPOCH #3, step #1284] loss: 4.302212101383432\n",
      "[EPOCH #3, step #1286] loss: 4.302254277046281\n",
      "[EPOCH #3, step #1288] loss: 4.302361496110772\n",
      "[EPOCH #3, step #1290] loss: 4.302329810251107\n",
      "[EPOCH #3, step #1292] loss: 4.302252093489928\n",
      "[EPOCH #3, step #1294] loss: 4.30226851098786\n",
      "[EPOCH #3, step #1296] loss: 4.302053607787733\n",
      "[EPOCH #3, step #1298] loss: 4.301957860554614\n",
      "[EPOCH #3, step #1300] loss: 4.301951453101168\n",
      "[EPOCH #3, step #1302] loss: 4.301984461654816\n",
      "[EPOCH #3, step #1304] loss: 4.301802632269731\n",
      "[EPOCH #3, step #1306] loss: 4.3018486009085555\n",
      "[EPOCH #3, step #1308] loss: 4.301685583145223\n",
      "[EPOCH #3, step #1310] loss: 4.301816089566112\n",
      "[EPOCH #3, step #1312] loss: 4.301831289728634\n",
      "[EPOCH #3, step #1314] loss: 4.301775206359167\n",
      "[EPOCH #3, step #1316] loss: 4.301849726573391\n",
      "[EPOCH #3, step #1318] loss: 4.301813883705515\n",
      "[EPOCH #3, step #1320] loss: 4.301967992645426\n",
      "[EPOCH #3, step #1322] loss: 4.301689133532373\n",
      "[EPOCH #3, step #1324] loss: 4.301615531489534\n",
      "[EPOCH #3, step #1326] loss: 4.301601339123172\n",
      "[EPOCH #3, step #1328] loss: 4.30162991465618\n",
      "[EPOCH #3, step #1330] loss: 4.3016946496684065\n",
      "[EPOCH #3, step #1332] loss: 4.301625829483456\n",
      "[EPOCH #3, step #1334] loss: 4.301549102840352\n",
      "[EPOCH #3, step #1336] loss: 4.301592610715126\n",
      "[EPOCH #3, step #1338] loss: 4.30142662291922\n",
      "[EPOCH #3, step #1340] loss: 4.301422295509925\n",
      "[EPOCH #3, step #1342] loss: 4.301435250447742\n",
      "[EPOCH #3, step #1344] loss: 4.301301074116647\n",
      "[EPOCH #3, step #1346] loss: 4.3011784783627425\n",
      "[EPOCH #3, step #1348] loss: 4.301220171004423\n",
      "[EPOCH #3, step #1350] loss: 4.301287471056867\n",
      "[EPOCH #3, step #1352] loss: 4.3011610058617435\n",
      "[EPOCH #3, step #1354] loss: 4.300964942453533\n",
      "[EPOCH #3, step #1356] loss: 4.3009061989049835\n",
      "[EPOCH #3, step #1358] loss: 4.300759319350332\n",
      "[EPOCH #3, step #1360] loss: 4.300697837131676\n",
      "[EPOCH #3, step #1362] loss: 4.300681137286471\n",
      "[EPOCH #3, step #1364] loss: 4.30061455262013\n",
      "[EPOCH #3, step #1366] loss: 4.3004464168311385\n",
      "[EPOCH #3, step #1368] loss: 4.300123913606821\n",
      "[EPOCH #3, step #1370] loss: 4.300137607308741\n",
      "[EPOCH #3, step #1372] loss: 4.300054147134102\n",
      "[EPOCH #3, step #1374] loss: 4.299960665442727\n",
      "[EPOCH #3, step #1376] loss: 4.299922694308046\n",
      "[EPOCH #3, step #1378] loss: 4.299843172131456\n",
      "[EPOCH #3, step #1380] loss: 4.299769123769342\n",
      "[EPOCH #3, step #1382] loss: 4.299816956144265\n",
      "[EPOCH #3, step #1384] loss: 4.299808235202886\n",
      "[EPOCH #3, step #1386] loss: 4.299833340875176\n",
      "[EPOCH #3, step #1388] loss: 4.299682742318633\n",
      "[EPOCH #3, step #1390] loss: 4.299522020935069\n",
      "[EPOCH #3, step #1392] loss: 4.299536477210106\n",
      "[EPOCH #3, step #1394] loss: 4.2993570766995886\n",
      "[EPOCH #3, step #1396] loss: 4.299549278055845\n",
      "[EPOCH #3, step #1398] loss: 4.299358432850214\n",
      "[EPOCH #3, step #1400] loss: 4.299357406927976\n",
      "[EPOCH #3, step #1402] loss: 4.299382031176316\n",
      "[EPOCH #3, step #1404] loss: 4.2994267217629325\n",
      "[EPOCH #3, step #1406] loss: 4.299382015077798\n",
      "[EPOCH #3, step #1408] loss: 4.2992703302097794\n",
      "[EPOCH #3, step #1410] loss: 4.29909858139249\n",
      "[EPOCH #3, step #1412] loss: 4.299178454020474\n",
      "[EPOCH #3, step #1414] loss: 4.299232295376673\n",
      "[EPOCH #3, step #1416] loss: 4.2992953919321195\n",
      "[EPOCH #3, step #1418] loss: 4.299245759918288\n",
      "[EPOCH #3, step #1420] loss: 4.299072006065857\n",
      "[EPOCH #3, step #1422] loss: 4.298956986472209\n",
      "[EPOCH #3, step #1424] loss: 4.2990101253777215\n",
      "[EPOCH #3, step #1426] loss: 4.2989538872083255\n",
      "[EPOCH #3, step #1428] loss: 4.2989279412989685\n",
      "[EPOCH #3, step #1430] loss: 4.298814961995219\n",
      "[EPOCH #3, step #1432] loss: 4.298801642568979\n",
      "[EPOCH #3, step #1434] loss: 4.298636436960838\n",
      "[EPOCH #3, step #1436] loss: 4.298508992689554\n",
      "[EPOCH #3, step #1438] loss: 4.298625888884109\n",
      "[EPOCH #3, step #1440] loss: 4.29858356230297\n",
      "[EPOCH #3, step #1442] loss: 4.298613908632877\n",
      "[EPOCH #3, step #1444] loss: 4.298361646427828\n",
      "[EPOCH #3, step #1446] loss: 4.2984831893369915\n",
      "[EPOCH #3, step #1448] loss: 4.298469366247363\n",
      "[EPOCH #3, step #1450] loss: 4.29833954979353\n",
      "[EPOCH #3, step #1452] loss: 4.298451030328203\n",
      "[EPOCH #3, step #1454] loss: 4.298367903896214\n",
      "[EPOCH #3, step #1456] loss: 4.298219374294713\n",
      "[EPOCH #3, step #1458] loss: 4.298139618716067\n",
      "[EPOCH #3, step #1460] loss: 4.298117247123901\n",
      "[EPOCH #3, step #1462] loss: 4.298022024613574\n",
      "[EPOCH #3, step #1464] loss: 4.29817735603645\n",
      "[EPOCH #3, step #1466] loss: 4.2982522043632505\n",
      "[EPOCH #3, step #1468] loss: 4.298148939607905\n",
      "[EPOCH #3, step #1470] loss: 4.298075967869088\n",
      "[EPOCH #3, step #1472] loss: 4.297980844124757\n",
      "[EPOCH #3, step #1474] loss: 4.297969031738023\n",
      "[EPOCH #3, step #1476] loss: 4.297854307461366\n",
      "[EPOCH #3, step #1478] loss: 4.297763572180892\n",
      "[EPOCH #3, step #1480] loss: 4.297764198430722\n",
      "[EPOCH #3, step #1482] loss: 4.297896828429569\n",
      "[EPOCH #3, step #1484] loss: 4.297752564523357\n",
      "[EPOCH #3, step #1486] loss: 4.297546799582126\n",
      "[EPOCH #3, step #1488] loss: 4.297661221691872\n",
      "[EPOCH #3, step #1490] loss: 4.297815536349352\n",
      "[EPOCH #3, step #1492] loss: 4.297924001346238\n",
      "[EPOCH #3, step #1494] loss: 4.297749348866899\n",
      "[EPOCH #3, step #1496] loss: 4.297684458150972\n",
      "[EPOCH #3, step #1498] loss: 4.297697822279418\n",
      "[EPOCH #3, step #1500] loss: 4.297484421237638\n",
      "[EPOCH #3, step #1502] loss: 4.29746297781101\n",
      "[EPOCH #3, step #1504] loss: 4.2974416427042\n",
      "[EPOCH #3, step #1506] loss: 4.297349378140304\n",
      "[EPOCH #3, step #1508] loss: 4.297374288478065\n",
      "[EPOCH #3, step #1510] loss: 4.29734172653789\n",
      "[EPOCH #3, step #1512] loss: 4.297468192200745\n",
      "[EPOCH #3, step #1514] loss: 4.297313607250503\n",
      "[EPOCH #3, step #1516] loss: 4.297154565979637\n",
      "[EPOCH #3, step #1518] loss: 4.297365947174665\n",
      "[EPOCH #3, step #1520] loss: 4.297318001000368\n",
      "[EPOCH #3, step #1522] loss: 4.297364133952561\n",
      "[EPOCH #3, step #1524] loss: 4.297442464515811\n",
      "[EPOCH #3, step #1526] loss: 4.297504783379305\n",
      "[EPOCH #3, step #1528] loss: 4.2975910626805005\n",
      "[EPOCH #3, step #1530] loss: 4.297566017241824\n",
      "[EPOCH #3, step #1532] loss: 4.2974139835347716\n",
      "[EPOCH #3, step #1534] loss: 4.297356771879165\n",
      "[EPOCH #3, step #1536] loss: 4.297325168248501\n",
      "[EPOCH #3, step #1538] loss: 4.297346812397571\n",
      "[EPOCH #3, step #1540] loss: 4.297497980011353\n",
      "[EPOCH #3, step #1542] loss: 4.297539436732185\n",
      "[EPOCH #3, step #1544] loss: 4.297385721144939\n",
      "[EPOCH #3, step #1546] loss: 4.297449199322662\n",
      "[EPOCH #3, step #1548] loss: 4.297364862446942\n",
      "[EPOCH #3, step #1550] loss: 4.297441779683283\n",
      "[EPOCH #3, step #1552] loss: 4.297364359430091\n",
      "[EPOCH #3, step #1554] loss: 4.297349253191442\n",
      "[EPOCH #3, step #1556] loss: 4.297309299884274\n",
      "[EPOCH #3, step #1558] loss: 4.2970947755920035\n",
      "[EPOCH #3, step #1560] loss: 4.297123235895571\n",
      "[EPOCH #3, step #1562] loss: 4.297025613577337\n",
      "[EPOCH #3, step #1564] loss: 4.297043446153879\n",
      "[EPOCH #3, step #1566] loss: 4.297066916001398\n",
      "[EPOCH #3, step #1568] loss: 4.296929263551954\n",
      "[EPOCH #3, step #1570] loss: 4.2969332929631845\n",
      "[EPOCH #3, step #1572] loss: 4.296786794371463\n",
      "[EPOCH #3, step #1574] loss: 4.2966711139678955\n",
      "[EPOCH #3, step #1576] loss: 4.296589058716991\n",
      "[EPOCH #3, step #1578] loss: 4.296588417093085\n",
      "[EPOCH #3, step #1580] loss: 4.296401811505631\n",
      "[EPOCH #3, step #1582] loss: 4.296354316565633\n",
      "[EPOCH #3, step #1584] loss: 4.296286738332514\n",
      "[EPOCH #3, step #1586] loss: 4.2962083009781145\n",
      "[EPOCH #3, step #1588] loss: 4.296046702497181\n",
      "[EPOCH #3, step #1590] loss: 4.296026790164538\n",
      "[EPOCH #3, step #1592] loss: 4.295921359657867\n",
      "[EPOCH #3, step #1594] loss: 4.2960219283089\n",
      "[EPOCH #3, step #1596] loss: 4.296071179299782\n",
      "[EPOCH #3, step #1598] loss: 4.295911014340981\n",
      "[EPOCH #3, step #1600] loss: 4.295828480187391\n",
      "[EPOCH #3, step #1602] loss: 4.295761463126018\n",
      "[EPOCH #3, step #1604] loss: 4.295682968751664\n",
      "[EPOCH #3, step #1606] loss: 4.295597794935729\n",
      "[EPOCH #3, step #1608] loss: 4.295717292575825\n",
      "[EPOCH #3, step #1610] loss: 4.295672507407427\n",
      "[EPOCH #3, step #1612] loss: 4.295552281068115\n",
      "[EPOCH #3, step #1614] loss: 4.295491243220704\n",
      "[EPOCH #3, step #1616] loss: 4.295376460789602\n",
      "[EPOCH #3, step #1618] loss: 4.295357816524164\n",
      "[EPOCH #3, step #1620] loss: 4.295231461892667\n",
      "[EPOCH #3, step #1622] loss: 4.295305946471143\n",
      "[EPOCH #3, step #1624] loss: 4.295185816691472\n",
      "[EPOCH #3, step #1626] loss: 4.295042309500208\n",
      "[EPOCH #3, step #1628] loss: 4.295121714994174\n",
      "[EPOCH #3, step #1630] loss: 4.295011893289357\n",
      "[EPOCH #3, step #1632] loss: 4.2950030633152005\n",
      "[EPOCH #3, step #1634] loss: 4.295048794615159\n",
      "[EPOCH #3, step #1636] loss: 4.295055781622521\n",
      "[EPOCH #3, step #1638] loss: 4.294965327258805\n",
      "[EPOCH #3, step #1640] loss: 4.294840437112096\n",
      "[EPOCH #3, step #1642] loss: 4.294842093874606\n",
      "[EPOCH #3, step #1644] loss: 4.294779965218077\n",
      "[EPOCH #3, step #1646] loss: 4.294658391491601\n",
      "[EPOCH #3, step #1648] loss: 4.294828527547722\n",
      "[EPOCH #3, step #1650] loss: 4.294642558609046\n",
      "[EPOCH #3, step #1652] loss: 4.294564683438789\n",
      "[EPOCH #3, step #1654] loss: 4.294598860178829\n",
      "[EPOCH #3, step #1656] loss: 4.294480991277194\n",
      "[EPOCH #3, step #1658] loss: 4.2945596970586335\n",
      "[EPOCH #3, step #1660] loss: 4.294619357420837\n",
      "[EPOCH #3, step #1662] loss: 4.29440460316904\n",
      "[EPOCH #3, step #1664] loss: 4.294477096334234\n",
      "[EPOCH #3, step #1666] loss: 4.294432363041018\n",
      "[EPOCH #3, step #1668] loss: 4.294249582747773\n",
      "[EPOCH #3, step #1670] loss: 4.294044827787125\n",
      "[EPOCH #3, step #1672] loss: 4.294164695072801\n",
      "[EPOCH #3, step #1674] loss: 4.29418912446321\n",
      "[EPOCH #3, step #1676] loss: 4.29404708737764\n",
      "[EPOCH #3, step #1678] loss: 4.29393432834732\n",
      "[EPOCH #3, step #1680] loss: 4.294019554570486\n",
      "[EPOCH #3, step #1682] loss: 4.293877740028141\n",
      "[EPOCH #3, step #1684] loss: 4.293894418410802\n",
      "[EPOCH #3, step #1686] loss: 4.293821096137634\n",
      "[EPOCH #3, step #1688] loss: 4.293940206051155\n",
      "[EPOCH #3, step #1690] loss: 4.2938889953809545\n",
      "[EPOCH #3, step #1692] loss: 4.293789911889776\n",
      "[EPOCH #3, step #1694] loss: 4.29388820048982\n",
      "[EPOCH #3, step #1696] loss: 4.293731147567174\n",
      "[EPOCH #3, step #1698] loss: 4.293818650209181\n",
      "[EPOCH #3, step #1700] loss: 4.2937194552581355\n",
      "[EPOCH #3, step #1702] loss: 4.293821599017293\n",
      "[EPOCH #3, step #1704] loss: 4.293702026406342\n",
      "[EPOCH #3, step #1706] loss: 4.293556258492674\n",
      "[EPOCH #3, step #1708] loss: 4.29359642018206\n",
      "[EPOCH #3, step #1710] loss: 4.2934565233390005\n",
      "[EPOCH #3, step #1712] loss: 4.293398663514689\n",
      "[EPOCH #3, step #1714] loss: 4.293357834245999\n",
      "[EPOCH #3, step #1716] loss: 4.293355333992542\n",
      "[EPOCH #3, step #1718] loss: 4.293348440117029\n",
      "[EPOCH #3, step #1720] loss: 4.293327056551173\n",
      "[EPOCH #3, step #1722] loss: 4.293220472003718\n",
      "[EPOCH #3, step #1724] loss: 4.293129350828088\n",
      "[EPOCH #3, step #1726] loss: 4.292952090933203\n",
      "[EPOCH #3, step #1728] loss: 4.292849668813896\n",
      "[EPOCH #3, step #1730] loss: 4.292850078013093\n",
      "[EPOCH #3, step #1732] loss: 4.292810862833409\n",
      "[EPOCH #3, step #1734] loss: 4.292872905868618\n",
      "[EPOCH #3, step #1736] loss: 4.292718299175037\n",
      "[EPOCH #3, step #1738] loss: 4.292648489410813\n",
      "[EPOCH #3, step #1740] loss: 4.292646797685223\n",
      "[EPOCH #3, step #1742] loss: 4.292541442762759\n",
      "[EPOCH #3, step #1744] loss: 4.292533280036511\n",
      "[EPOCH #3, step #1746] loss: 4.292549327502062\n",
      "[EPOCH #3, step #1748] loss: 4.292527362508321\n",
      "[EPOCH #3, step #1750] loss: 4.2925136982543615\n",
      "[EPOCH #3, step #1752] loss: 4.292519521658854\n",
      "[EPOCH #3, step #1754] loss: 4.292452650396233\n",
      "[EPOCH #3, step #1756] loss: 4.292498819652985\n",
      "[EPOCH #3, step #1758] loss: 4.292334459666978\n",
      "[EPOCH #3, step #1760] loss: 4.292338475119587\n",
      "[EPOCH #3, step #1762] loss: 4.2924531715131256\n",
      "[EPOCH #3, step #1764] loss: 4.292398068722517\n",
      "[EPOCH #3, step #1766] loss: 4.2924010124865\n",
      "[EPOCH #3, step #1768] loss: 4.292268579325614\n",
      "[EPOCH #3, step #1770] loss: 4.292340758289744\n",
      "[EPOCH #3, step #1772] loss: 4.29234402084243\n",
      "[EPOCH #3, step #1774] loss: 4.2923107336608455\n",
      "[EPOCH #3, step #1776] loss: 4.292280697996991\n",
      "[EPOCH #3, step #1778] loss: 4.292104599246689\n",
      "[EPOCH #3, step #1780] loss: 4.292004785130762\n",
      "[EPOCH #3, step #1782] loss: 4.2919571319458125\n",
      "[EPOCH #3, step #1784] loss: 4.291903282747883\n",
      "[EPOCH #3, step #1786] loss: 4.291653480887346\n",
      "[EPOCH #3, step #1788] loss: 4.291714904826736\n",
      "[EPOCH #3, step #1790] loss: 4.291581202192322\n",
      "[EPOCH #3, step #1792] loss: 4.291562253379715\n",
      "[EPOCH #3, step #1794] loss: 4.2914901660345395\n",
      "[EPOCH #3, step #1796] loss: 4.291378554995351\n",
      "[EPOCH #3, step #1798] loss: 4.2913249384502095\n",
      "[EPOCH #3, step #1800] loss: 4.291281972971974\n",
      "[EPOCH #3, step #1802] loss: 4.291311285592289\n",
      "[EPOCH #3, step #1804] loss: 4.29130044009877\n",
      "[EPOCH #3, step #1806] loss: 4.291388560871954\n",
      "[EPOCH #3, step #1808] loss: 4.291466256806441\n",
      "[EPOCH #3, step #1810] loss: 4.291442903348058\n",
      "[EPOCH #3, step #1812] loss: 4.291318447499136\n",
      "[EPOCH #3, step #1814] loss: 4.291228631311212\n",
      "[EPOCH #3, step #1816] loss: 4.291196059314641\n",
      "[EPOCH #3, step #1818] loss: 4.291222550437763\n",
      "[EPOCH #3, step #1820] loss: 4.291213902191171\n",
      "[EPOCH #3, step #1822] loss: 4.290986977235603\n",
      "[EPOCH #3, step #1824] loss: 4.290955270610444\n",
      "[EPOCH #3, step #1826] loss: 4.291040260934021\n",
      "[EPOCH #3, step #1828] loss: 4.291076809672986\n",
      "[EPOCH #3, step #1830] loss: 4.2908934936179675\n",
      "[EPOCH #3, step #1832] loss: 4.290915109522019\n",
      "[EPOCH #3, step #1834] loss: 4.290867271605239\n",
      "[EPOCH #3, step #1836] loss: 4.290723167947926\n",
      "[EPOCH #3, step #1838] loss: 4.290749355008124\n",
      "[EPOCH #3, step #1840] loss: 4.290676954710762\n",
      "[EPOCH #3, step #1842] loss: 4.290559278773796\n",
      "[EPOCH #3, step #1844] loss: 4.290574077022108\n",
      "[EPOCH #3, step #1846] loss: 4.2904893631669205\n",
      "[EPOCH #3, step #1848] loss: 4.290324062705363\n",
      "[EPOCH #3, step #1850] loss: 4.290200215555538\n",
      "[EPOCH #3, step #1852] loss: 4.290165709342689\n",
      "[EPOCH #3, step #1854] loss: 4.290123207806898\n",
      "[EPOCH #3, step #1856] loss: 4.290143335642324\n",
      "[EPOCH #3, step #1858] loss: 4.29000819748996\n",
      "[EPOCH #3, step #1860] loss: 4.290015515942909\n",
      "[EPOCH #3, step #1862] loss: 4.290082905283553\n",
      "[EPOCH #3, step #1864] loss: 4.289972371134617\n",
      "[EPOCH #3, step #1866] loss: 4.289977208139045\n",
      "[EPOCH #3, step #1868] loss: 4.289917749794745\n",
      "[EPOCH #3, step #1870] loss: 4.289823618118677\n",
      "[EPOCH #3, step #1872] loss: 4.289800579383675\n",
      "[EPOCH #3, step #1874] loss: 4.289741223017375\n",
      "[EPOCH #3, step #1876] loss: 4.289801470193066\n",
      "[EPOCH #3, step #1878] loss: 4.289674002036825\n",
      "[EPOCH #3, step #1880] loss: 4.289739956696068\n",
      "[EPOCH #3, step #1882] loss: 4.289624604277325\n",
      "[EPOCH #3, step #1884] loss: 4.289549161394964\n",
      "[EPOCH #3, step #1886] loss: 4.289422133389642\n",
      "[EPOCH #3, step #1888] loss: 4.2892536151465945\n",
      "[EPOCH #3, step #1890] loss: 4.289140594503603\n",
      "[EPOCH #3, step #1892] loss: 4.289029345207597\n",
      "[EPOCH #3, step #1894] loss: 4.2889985581501175\n",
      "[EPOCH #3, step #1896] loss: 4.288752984296791\n",
      "[EPOCH #3, step #1898] loss: 4.28867025975493\n",
      "[EPOCH #3, step #1900] loss: 4.288601692321111\n",
      "[EPOCH #3, step #1902] loss: 4.288437418479138\n",
      "[EPOCH #3, step #1904] loss: 4.288399970562752\n",
      "[EPOCH #3, step #1906] loss: 4.288336674455704\n",
      "[EPOCH #3, step #1908] loss: 4.288221876430161\n",
      "[EPOCH #3, step #1910] loss: 4.288215125311987\n",
      "[EPOCH #3, step #1912] loss: 4.288256772949131\n",
      "[EPOCH #3, step #1914] loss: 4.288385372186766\n",
      "[EPOCH #3, step #1916] loss: 4.288396598649759\n",
      "[EPOCH #3, step #1918] loss: 4.288304675923715\n",
      "[EPOCH #3, step #1920] loss: 4.2883953245154025\n",
      "[EPOCH #3, step #1922] loss: 4.288356203866761\n",
      "[EPOCH #3, step #1924] loss: 4.288311712958596\n",
      "[EPOCH #3, step #1926] loss: 4.288272666263036\n",
      "[EPOCH #3, step #1928] loss: 4.288191632077507\n",
      "[EPOCH #3, step #1930] loss: 4.288168423290638\n",
      "[EPOCH #3, step #1932] loss: 4.288090281454442\n",
      "[EPOCH #3, step #1934] loss: 4.288205696938882\n",
      "[EPOCH #3, step #1936] loss: 4.288214027604678\n",
      "[EPOCH #3, step #1938] loss: 4.288214827272435\n",
      "[EPOCH #3, step #1940] loss: 4.288126764317139\n",
      "[EPOCH #3, step #1942] loss: 4.2880330590077795\n",
      "[EPOCH #3, step #1944] loss: 4.287829787197946\n",
      "[EPOCH #3, step #1946] loss: 4.2879096875144205\n",
      "[EPOCH #3, step #1948] loss: 4.287809340632102\n",
      "[EPOCH #3, step #1950] loss: 4.287805652325121\n",
      "[EPOCH #3, step #1952] loss: 4.287811475598501\n",
      "[EPOCH #3, step #1954] loss: 4.287780953917052\n",
      "[EPOCH #3, step #1956] loss: 4.287744884900311\n",
      "[EPOCH #3, step #1958] loss: 4.287763871362832\n",
      "[EPOCH #3, step #1960] loss: 4.287761396440178\n",
      "[EPOCH #3, step #1962] loss: 4.287731069845626\n",
      "[EPOCH #3, step #1964] loss: 4.287684229251386\n",
      "[EPOCH #3, step #1966] loss: 4.287703977360844\n",
      "[EPOCH #3, step #1968] loss: 4.2875237193671865\n",
      "[EPOCH #3, step #1970] loss: 4.287558738499472\n",
      "[EPOCH #3, step #1972] loss: 4.287416987653559\n",
      "[EPOCH #3, step #1974] loss: 4.287420368436017\n",
      "[EPOCH #3, step #1976] loss: 4.287231460266422\n",
      "[EPOCH #3, step #1978] loss: 4.287089266639698\n",
      "[EPOCH #3, step #1980] loss: 4.286979264890467\n",
      "[EPOCH #3, step #1982] loss: 4.286880912771143\n",
      "[EPOCH #3, step #1984] loss: 4.286830187684643\n",
      "[EPOCH #3, step #1986] loss: 4.286833765762322\n",
      "[EPOCH #3, step #1988] loss: 4.286776965080403\n",
      "[EPOCH #3, step #1990] loss: 4.2866426500946\n",
      "[EPOCH #3, step #1992] loss: 4.2865697221425805\n",
      "[EPOCH #3, step #1994] loss: 4.286527964465302\n",
      "[EPOCH #3, step #1996] loss: 4.286557501056043\n",
      "[EPOCH #3, step #1998] loss: 4.286259911667412\n",
      "[EPOCH #3, step #2000] loss: 4.286211102917932\n",
      "[EPOCH #3, step #2002] loss: 4.286234086713253\n",
      "[EPOCH #3, step #2004] loss: 4.286159584112001\n",
      "[EPOCH #3, step #2006] loss: 4.286118583650689\n",
      "[EPOCH #3, step #2008] loss: 4.286063204006273\n",
      "[EPOCH #3, step #2010] loss: 4.2859628204087725\n",
      "[EPOCH #3, step #2012] loss: 4.285865922739301\n",
      "[EPOCH #3, step #2014] loss: 4.285824594426687\n",
      "[EPOCH #3, step #2016] loss: 4.285683551990696\n",
      "[EPOCH #3, step #2018] loss: 4.285714602458589\n",
      "[EPOCH #3, step #2020] loss: 4.285575322972731\n",
      "[EPOCH #3, step #2022] loss: 4.2856955623721\n",
      "[EPOCH #3, step #2024] loss: 4.285560109527022\n",
      "[EPOCH #3, step #2026] loss: 4.285590091008062\n",
      "[EPOCH #3, step #2028] loss: 4.28545368322896\n",
      "[EPOCH #3, step #2030] loss: 4.285353791942625\n",
      "[EPOCH #3, step #2032] loss: 4.285470947630582\n",
      "[EPOCH #3, step #2034] loss: 4.285279285819876\n",
      "[EPOCH #3, step #2036] loss: 4.28537239218437\n",
      "[EPOCH #3, step #2038] loss: 4.285323985246188\n",
      "[EPOCH #3, step #2040] loss: 4.2853923188536145\n",
      "[EPOCH #3, step #2042] loss: 4.285382286334119\n",
      "[EPOCH #3, step #2044] loss: 4.2852863387548545\n",
      "[EPOCH #3, step #2046] loss: 4.285188695076447\n",
      "[EPOCH #3, step #2048] loss: 4.285169251084968\n",
      "[EPOCH #3, step #2050] loss: 4.28519838964108\n",
      "[EPOCH #3, step #2052] loss: 4.285015012250549\n",
      "[EPOCH #3, step #2054] loss: 4.28501050338838\n",
      "[EPOCH #3, step #2056] loss: 4.2850638181188465\n",
      "[EPOCH #3, step #2058] loss: 4.284967164613484\n",
      "[EPOCH #3, step #2060] loss: 4.284912388513297\n",
      "[EPOCH #3, step #2062] loss: 4.284769639192249\n",
      "[EPOCH #3, step #2064] loss: 4.284807460706402\n",
      "[EPOCH #3, step #2066] loss: 4.284812044970471\n",
      "[EPOCH #3, step #2068] loss: 4.284707559946944\n",
      "[EPOCH #3, step #2070] loss: 4.2846271639455065\n",
      "[EPOCH #3, step #2072] loss: 4.284553822376161\n",
      "[EPOCH #3, step #2074] loss: 4.284578906254596\n",
      "[EPOCH #3, step #2076] loss: 4.284597891021016\n",
      "[EPOCH #3, step #2078] loss: 4.284460708921835\n",
      "[EPOCH #3, step #2080] loss: 4.284267590058532\n",
      "[EPOCH #3, step #2082] loss: 4.284264238076432\n",
      "[EPOCH #3, step #2084] loss: 4.284254798386023\n",
      "[EPOCH #3, step #2086] loss: 4.284266286794226\n",
      "[EPOCH #3, step #2088] loss: 4.2842497385078895\n",
      "[EPOCH #3, step #2090] loss: 4.284271920941652\n",
      "[EPOCH #3, step #2092] loss: 4.2842418264670625\n",
      "[EPOCH #3, step #2094] loss: 4.284170582755369\n",
      "[EPOCH #3, step #2096] loss: 4.284194674103045\n",
      "[EPOCH #3, step #2098] loss: 4.284137854864621\n",
      "[EPOCH #3, step #2100] loss: 4.284217384870367\n",
      "[EPOCH #3, step #2102] loss: 4.284145358172248\n",
      "[EPOCH #3, step #2104] loss: 4.2840224712308395\n",
      "[EPOCH #3, step #2106] loss: 4.284007859784503\n",
      "[EPOCH #3, step #2108] loss: 4.284067280544608\n",
      "[EPOCH #3, step #2110] loss: 4.284055207477029\n",
      "[EPOCH #3, step #2112] loss: 4.2840315669394196\n",
      "[EPOCH #3, step #2114] loss: 4.283943620866636\n",
      "[EPOCH #3, step #2116] loss: 4.2839123061877915\n",
      "[EPOCH #3, step #2118] loss: 4.283894294272509\n",
      "[EPOCH #3, step #2120] loss: 4.283882323215166\n",
      "[EPOCH #3, step #2122] loss: 4.283846763749197\n",
      "[EPOCH #3, step #2124] loss: 4.283780104468851\n",
      "[EPOCH #3, step #2126] loss: 4.2837992588791085\n",
      "[EPOCH #3, step #2128] loss: 4.283789261118691\n",
      "[EPOCH #3, step #2130] loss: 4.28381106865792\n",
      "[EPOCH #3, step #2132] loss: 4.283816211725477\n",
      "[EPOCH #3, step #2134] loss: 4.283816221670468\n",
      "[EPOCH #3, step #2136] loss: 4.283825229025355\n",
      "[EPOCH #3, step #2138] loss: 4.283753189543465\n",
      "[EPOCH #3, step #2140] loss: 4.2837732102146\n",
      "[EPOCH #3, step #2142] loss: 4.283783058668485\n",
      "[EPOCH #3, step #2144] loss: 4.283830161861607\n",
      "[EPOCH #3, step #2146] loss: 4.283728022546506\n",
      "[EPOCH #3, step #2148] loss: 4.283531665358226\n",
      "[EPOCH #3, step #2150] loss: 4.283548429144199\n",
      "[EPOCH #3, step #2152] loss: 4.283455739284857\n",
      "[EPOCH #3, step #2154] loss: 4.283413153955942\n",
      "[EPOCH #3, step #2156] loss: 4.283324262860422\n",
      "[EPOCH #3, step #2158] loss: 4.283452689288777\n",
      "[EPOCH #3, step #2160] loss: 4.283276010696008\n",
      "[EPOCH #3, step #2162] loss: 4.283264239739336\n",
      "[EPOCH #3, step #2164] loss: 4.283223496848937\n",
      "[EPOCH #3, step #2166] loss: 4.283290397202193\n",
      "[EPOCH #3, step #2168] loss: 4.283192289870365\n",
      "[EPOCH #3, step #2170] loss: 4.28312392478602\n",
      "[EPOCH #3, step #2172] loss: 4.283163475321012\n",
      "[EPOCH #3, step #2174] loss: 4.283144566546912\n",
      "[EPOCH #3, step #2176] loss: 4.283068572155204\n",
      "[EPOCH #3, step #2178] loss: 4.282971885894733\n",
      "[EPOCH #3, step #2180] loss: 4.282987672874874\n",
      "[EPOCH #3, step #2182] loss: 4.2829478052412755\n",
      "[EPOCH #3, step #2184] loss: 4.282991034815459\n",
      "[EPOCH #3, step #2186] loss: 4.283028754568776\n",
      "[EPOCH #3, step #2188] loss: 4.283004140352865\n",
      "[EPOCH #3, step #2190] loss: 4.282927517692883\n",
      "[EPOCH #3, step #2192] loss: 4.282967521564851\n",
      "[EPOCH #3, step #2194] loss: 4.282919441431693\n",
      "[EPOCH #3, step #2196] loss: 4.282781818643396\n",
      "[EPOCH #3, step #2198] loss: 4.282809468061613\n",
      "[EPOCH #3, step #2200] loss: 4.2827554879974965\n",
      "[EPOCH #3, step #2202] loss: 4.282718978791793\n",
      "[EPOCH #3, step #2204] loss: 4.282772303600701\n",
      "[EPOCH #3, step #2206] loss: 4.282781238192933\n",
      "[EPOCH #3, step #2208] loss: 4.282780134391871\n",
      "[EPOCH #3, step #2210] loss: 4.282806076436278\n",
      "[EPOCH #3, step #2212] loss: 4.282791175833666\n",
      "[EPOCH #3, step #2214] loss: 4.282819709820887\n",
      "[EPOCH #3, step #2216] loss: 4.282749166776857\n",
      "[EPOCH #3, step #2218] loss: 4.282681929737451\n",
      "[EPOCH #3, step #2220] loss: 4.282696332978752\n",
      "[EPOCH #3, step #2222] loss: 4.28273028336395\n",
      "[EPOCH #3, step #2224] loss: 4.282820098748368\n",
      "[EPOCH #3, step #2226] loss: 4.282768933185292\n",
      "[EPOCH #3, step #2228] loss: 4.282724770904925\n",
      "[EPOCH #3, step #2230] loss: 4.282745450240836\n",
      "[EPOCH #3, step #2232] loss: 4.2828051468421675\n",
      "[EPOCH #3, step #2234] loss: 4.28271416721728\n",
      "[EPOCH #3, step #2236] loss: 4.282680304497014\n",
      "[EPOCH #3, step #2238] loss: 4.28270772633248\n",
      "[EPOCH #3, step #2240] loss: 4.282684698534672\n",
      "[EPOCH #3, step #2242] loss: 4.282605648891128\n",
      "[EPOCH #3, step #2244] loss: 4.282531305835613\n",
      "[EPOCH #3, step #2246] loss: 4.282521755683671\n",
      "[EPOCH #3, step #2248] loss: 4.282570247175218\n",
      "[EPOCH #3, step #2250] loss: 4.282539903190813\n",
      "[EPOCH #3, step #2252] loss: 4.282469512619445\n",
      "[EPOCH #3, step #2254] loss: 4.28235765338737\n",
      "[EPOCH #3, step #2256] loss: 4.282347743786907\n",
      "[EPOCH #3, step #2258] loss: 4.282307978170326\n",
      "[EPOCH #3, step #2260] loss: 4.282361025938888\n",
      "[EPOCH #3, step #2262] loss: 4.282385894591651\n",
      "[EPOCH #3, step #2264] loss: 4.2823907325862525\n",
      "[EPOCH #3, step #2266] loss: 4.282402834074056\n",
      "[EPOCH #3, step #2268] loss: 4.282370122427875\n",
      "[EPOCH #3, step #2270] loss: 4.282297508982193\n",
      "[EPOCH #3, step #2272] loss: 4.282058806341753\n",
      "[EPOCH #3, step #2274] loss: 4.2820529840280726\n",
      "[EPOCH #3, step #2276] loss: 4.282017323025287\n",
      "[EPOCH #3, step #2278] loss: 4.281979178482631\n",
      "[EPOCH #3, step #2280] loss: 4.2818073874134495\n",
      "[EPOCH #3, step #2282] loss: 4.281743917974839\n",
      "[EPOCH #3, step #2284] loss: 4.281622177036377\n",
      "[EPOCH #3, step #2286] loss: 4.281458758036018\n",
      "[EPOCH #3, step #2288] loss: 4.281456782235595\n",
      "[EPOCH #3, step #2290] loss: 4.28140944187842\n",
      "[EPOCH #3, step #2292] loss: 4.281404979250809\n",
      "[EPOCH #3, step #2294] loss: 4.281342411145146\n",
      "[EPOCH #3, step #2296] loss: 4.281232104075178\n",
      "[EPOCH #3, step #2298] loss: 4.281156216460033\n",
      "[EPOCH #3, step #2300] loss: 4.281068769655555\n",
      "[EPOCH #3, step #2302] loss: 4.280971574617685\n",
      "[EPOCH #3, step #2304] loss: 4.280937714380194\n",
      "[EPOCH #3, step #2306] loss: 4.280977792640656\n",
      "[EPOCH #3, step #2308] loss: 4.2809795163420175\n",
      "[EPOCH #3, step #2310] loss: 4.280898192701583\n",
      "[EPOCH #3, step #2312] loss: 4.280876477250175\n",
      "[EPOCH #3, step #2314] loss: 4.280920847136825\n",
      "[EPOCH #3, step #2316] loss: 4.280952497551448\n",
      "[EPOCH #3, step #2318] loss: 4.281023386893163\n",
      "[EPOCH #3, step #2320] loss: 4.280995005012226\n",
      "[EPOCH #3, step #2322] loss: 4.2809168775175035\n",
      "[EPOCH #3, step #2324] loss: 4.280924393130887\n",
      "[EPOCH #3, step #2326] loss: 4.280877240287115\n",
      "[EPOCH #3, step #2328] loss: 4.280799645525849\n",
      "[EPOCH #3, step #2330] loss: 4.2808265633871505\n",
      "[EPOCH #3, step #2332] loss: 4.280761831878747\n",
      "[EPOCH #3, step #2334] loss: 4.280808245293334\n",
      "[EPOCH #3, step #2336] loss: 4.2807921795035\n",
      "[EPOCH #3, step #2338] loss: 4.280778548818411\n",
      "[EPOCH #3, step #2340] loss: 4.280778710020245\n",
      "[EPOCH #3, step #2342] loss: 4.280689892278387\n",
      "[EPOCH #3, step #2344] loss: 4.280698935156947\n",
      "[EPOCH #3, step #2346] loss: 4.280653706397409\n",
      "[EPOCH #3, step #2348] loss: 4.280627177085406\n",
      "[EPOCH #3, step #2350] loss: 4.280496098243749\n",
      "[EPOCH #3, step #2352] loss: 4.280395761632534\n",
      "[EPOCH #3, step #2354] loss: 4.280346532789259\n",
      "[EPOCH #3, step #2356] loss: 4.280396100470309\n",
      "[EPOCH #3, step #2358] loss: 4.28037402052756\n",
      "[EPOCH #3, step #2360] loss: 4.280420376152772\n",
      "[EPOCH #3, step #2362] loss: 4.2803901959111315\n",
      "[EPOCH #3, step #2364] loss: 4.280398296755414\n",
      "[EPOCH #3, step #2366] loss: 4.280433803356091\n",
      "[EPOCH #3, step #2368] loss: 4.2804600971064115\n",
      "[EPOCH #3, step #2370] loss: 4.280440316326957\n",
      "[EPOCH #3, step #2372] loss: 4.280418704342751\n",
      "[EPOCH #3, step #2374] loss: 4.280324157413683\n",
      "[EPOCH #3, step #2376] loss: 4.280303528291817\n",
      "[EPOCH #3, step #2378] loss: 4.280361583324078\n",
      "[EPOCH #3, step #2380] loss: 4.280357613277155\n",
      "[EPOCH #3, step #2382] loss: 4.280302135873331\n",
      "[EPOCH #3, step #2384] loss: 4.280233493241124\n",
      "[EPOCH #3, step #2386] loss: 4.280253725667154\n",
      "[EPOCH #3, step #2388] loss: 4.280181524193701\n",
      "[EPOCH #3, step #2390] loss: 4.280231176585345\n",
      "[EPOCH #3, step #2392] loss: 4.280177824277037\n",
      "[EPOCH #3, step #2394] loss: 4.280172623672167\n",
      "[EPOCH #3, step #2396] loss: 4.2801083679939245\n",
      "[EPOCH #3, step #2398] loss: 4.280043699086433\n",
      "[EPOCH #3, step #2400] loss: 4.279937694192876\n",
      "[EPOCH #3, step #2402] loss: 4.279803578177939\n",
      "[EPOCH #3, step #2404] loss: 4.279700772430198\n",
      "[EPOCH #3, step #2406] loss: 4.279683409759401\n",
      "[EPOCH #3, step #2408] loss: 4.279638888678544\n",
      "[EPOCH #3, step #2410] loss: 4.279481887619607\n",
      "[EPOCH #3, step #2412] loss: 4.27950363854773\n",
      "[EPOCH #3, step #2414] loss: 4.279492313274439\n",
      "[EPOCH #3, step #2416] loss: 4.279510971135624\n",
      "[EPOCH #3, step #2418] loss: 4.279576186061448\n",
      "[EPOCH #3, step #2420] loss: 4.279577263324333\n",
      "[EPOCH #3, step #2422] loss: 4.279479293650062\n",
      "[EPOCH #3, step #2424] loss: 4.279356109186546\n",
      "[EPOCH #3, step #2426] loss: 4.279298249713472\n",
      "[EPOCH #3, step #2428] loss: 4.2792835557544215\n",
      "[EPOCH #3, step #2430] loss: 4.279218125274671\n",
      "[EPOCH #3, step #2432] loss: 4.2791178147096085\n",
      "[EPOCH #3, step #2434] loss: 4.279107483701295\n",
      "[EPOCH #3, step #2436] loss: 4.279195569703263\n",
      "[EPOCH #3, step #2438] loss: 4.279154650643987\n",
      "[EPOCH #3, step #2440] loss: 4.278999455880553\n",
      "[EPOCH #3, step #2442] loss: 4.278920977709969\n",
      "[EPOCH #3, step #2444] loss: 4.278945693794204\n",
      "[EPOCH #3, step #2446] loss: 4.278853085852663\n",
      "[EPOCH #3, step #2448] loss: 4.278782876767446\n",
      "[EPOCH #3, step #2450] loss: 4.278829253872576\n",
      "[EPOCH #3, step #2452] loss: 4.27869914795883\n",
      "[EPOCH #3, step #2454] loss: 4.278681657270599\n",
      "[EPOCH #3, step #2456] loss: 4.278650465368691\n",
      "[EPOCH #3, step #2458] loss: 4.278699159234362\n",
      "[EPOCH #3, step #2460] loss: 4.278647727667922\n",
      "[EPOCH #3, step #2462] loss: 4.278717565575219\n",
      "[EPOCH #3, step #2464] loss: 4.278776308972744\n",
      "[EPOCH #3, step #2466] loss: 4.27871559069231\n",
      "[EPOCH #3, step #2468] loss: 4.278707830371911\n",
      "[EPOCH #3, step #2470] loss: 4.278698399114203\n",
      "[EPOCH #3, step #2472] loss: 4.27860978109558\n",
      "[EPOCH #3, step #2474] loss: 4.278555095075357\n",
      "[EPOCH #3, step #2476] loss: 4.278482806841404\n",
      "[EPOCH #3, step #2478] loss: 4.278526756348943\n",
      "[EPOCH #3, step #2480] loss: 4.278364452225986\n",
      "[EPOCH #3, step #2482] loss: 4.278418267200514\n",
      "[EPOCH #3, step #2484] loss: 4.278473335300653\n",
      "[EPOCH #3, step #2486] loss: 4.278468354715609\n",
      "[EPOCH #3, step #2488] loss: 4.278475301339474\n",
      "[EPOCH #3, step #2490] loss: 4.278428260220528\n",
      "[EPOCH #3, step #2492] loss: 4.278349857372402\n",
      "[EPOCH #3, step #2494] loss: 4.278283397372595\n",
      "[EPOCH #3, step #2496] loss: 4.278176756730116\n",
      "[EPOCH #3, step #2498] loss: 4.2782004522580825\n",
      "[EPOCH #3, elapsed time: 1546.656[sec]] loss: 4.278207158374786\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'coco2014_clf'\n",
    "model.train(dataloader.dataset.trainloader, epochs=epochs, lr=learning_rate, wd=weight_decay, output_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba45fa-de0f-4164-a9f4-8aab6c875907",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3a83ec-4044-46fe-bdaf-bc2963063a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = model.predict(dataloader.dataset.trainloader)\n",
    "train_predictions, train_labels = train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50fcb205-1cff-48c0-9ad2-1881319b9056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.1291375,\n",
      " 'classification_report': {'0': {'f1-score': 0.012511170688114387,\n",
      "                                 'precision': 0.058823529411764705,\n",
      "                                 'recall': 0.007,\n",
      "                                 'support': 1000},\n",
      "                           '1': {'f1-score': 0.04040404040404041,\n",
      "                                 'precision': 0.04603580562659847,\n",
      "                                 'recall': 0.036,\n",
      "                                 'support': 1000},\n",
      "                           '10': {'f1-score': 0.0380517503805175,\n",
      "                                  'precision': 0.07961783439490445,\n",
      "                                  'recall': 0.025,\n",
      "                                  'support': 1000},\n",
      "                           '12': {'f1-score': 0.30813240702901507,\n",
      "                                  'precision': 0.1936312275295326,\n",
      "                                  'recall': 0.754,\n",
      "                                  'support': 1000},\n",
      "                           '13': {'f1-score': 0.07002457002457002,\n",
      "                                  'precision': 0.09076433121019108,\n",
      "                                  'recall': 0.057,\n",
      "                                  'support': 1000},\n",
      "                           '14': {'f1-score': 0.04073033707865169,\n",
      "                                  'precision': 0.06839622641509434,\n",
      "                                  'recall': 0.029,\n",
      "                                  'support': 1000},\n",
      "                           '15': {'f1-score': 0.01740391588107324,\n",
      "                                  'precision': 0.0316622691292876,\n",
      "                                  'recall': 0.012,\n",
      "                                  'support': 1000},\n",
      "                           '16': {'f1-score': 0.04035433070866142,\n",
      "                                  'precision': 0.03972868217054264,\n",
      "                                  'recall': 0.041,\n",
      "                                  'support': 1000},\n",
      "                           '17': {'f1-score': 0.015267175572519083,\n",
      "                                  'precision': 0.02097902097902098,\n",
      "                                  'recall': 0.012,\n",
      "                                  'support': 1000},\n",
      "                           '18': {'f1-score': 0.04202586206896552,\n",
      "                                  'precision': 0.0455607476635514,\n",
      "                                  'recall': 0.039,\n",
      "                                  'support': 1000},\n",
      "                           '19': {'f1-score': 0.08438356164383562,\n",
      "                                  'precision': 0.09333333333333334,\n",
      "                                  'recall': 0.077,\n",
      "                                  'support': 1000},\n",
      "                           '2': {'f1-score': 0.04169079328314997,\n",
      "                                 'precision': 0.04951856946354883,\n",
      "                                 'recall': 0.036,\n",
      "                                 'support': 1000},\n",
      "                           '20': {'f1-score': 0.045667447306791564,\n",
      "                                  'precision': 0.05508474576271186,\n",
      "                                  'recall': 0.039,\n",
      "                                  'support': 1000},\n",
      "                           '21': {'f1-score': 0.10441767068273092,\n",
      "                                  'precision': 0.1224764468371467,\n",
      "                                  'recall': 0.091,\n",
      "                                  'support': 1000},\n",
      "                           '22': {'f1-score': 0.1338854382332643,\n",
      "                                  'precision': 0.08694353152076487,\n",
      "                                  'recall': 0.291,\n",
      "                                  'support': 1000},\n",
      "                           '23': {'f1-score': 0.21741557354172483,\n",
      "                                  'precision': 0.12633798248459294,\n",
      "                                  'recall': 0.779,\n",
      "                                  'support': 1000},\n",
      "                           '24': {'f1-score': 0.12105263157894738,\n",
      "                                  'precision': 0.1078125,\n",
      "                                  'recall': 0.138,\n",
      "                                  'support': 1000},\n",
      "                           '26': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '27': {'f1-score': 0.01597444089456869,\n",
      "                                  'precision': 0.03968253968253968,\n",
      "                                  'recall': 0.01,\n",
      "                                  'support': 1000},\n",
      "                           '3': {'f1-score': 0.12096455395967849,\n",
      "                                 'precision': 0.06966155714068903,\n",
      "                                 'recall': 0.459,\n",
      "                                 'support': 1000},\n",
      "                           '30': {'f1-score': 0.02819383259911894,\n",
      "                                  'precision': 0.11851851851851852,\n",
      "                                  'recall': 0.016,\n",
      "                                  'support': 1000},\n",
      "                           '31': {'f1-score': 0.09097472924187724,\n",
      "                                  'precision': 0.16363636363636364,\n",
      "                                  'recall': 0.063,\n",
      "                                  'support': 1000},\n",
      "                           '32': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '33': {'f1-score': 0.13825608798114689,\n",
      "                                  'precision': 0.32234432234432236,\n",
      "                                  'recall': 0.088,\n",
      "                                  'support': 1000},\n",
      "                           '34': {'f1-score': 0.26437640029873033,\n",
      "                                  'precision': 0.21096543504171633,\n",
      "                                  'recall': 0.354,\n",
      "                                  'support': 1000},\n",
      "                           '35': {'f1-score': 0.029992107340173636,\n",
      "                                  'precision': 0.07116104868913857,\n",
      "                                  'recall': 0.019,\n",
      "                                  'support': 1000},\n",
      "                           '36': {'f1-score': 0.8281573498964803,\n",
      "                                  'precision': 0.7067137809187279,\n",
      "                                  'recall': 1.0,\n",
      "                                  'support': 1000},\n",
      "                           '37': {'f1-score': 0.005797101449275362,\n",
      "                                  'precision': 0.08571428571428572,\n",
      "                                  'recall': 0.003,\n",
      "                                  'support': 1000},\n",
      "                           '38': {'f1-score': 0.03840245775729647,\n",
      "                                  'precision': 0.08278145695364239,\n",
      "                                  'recall': 0.025,\n",
      "                                  'support': 1000},\n",
      "                           '39': {'f1-score': 0.49974293059125957,\n",
      "                                  'precision': 0.5142857142857142,\n",
      "                                  'recall': 0.486,\n",
      "                                  'support': 1000},\n",
      "                           '4': {'f1-score': 0.22331806974286725,\n",
      "                                 'precision': 0.13552800342026508,\n",
      "                                 'recall': 0.634,\n",
      "                                 'support': 1000},\n",
      "                           '40': {'f1-score': 0.01679496151154654,\n",
      "                                  'precision': 0.027972027972027972,\n",
      "                                  'recall': 0.012,\n",
      "                                  'support': 1000},\n",
      "                           '41': {'f1-score': 0.028103044496487116,\n",
      "                                  'precision': 0.06405693950177936,\n",
      "                                  'recall': 0.018,\n",
      "                                  'support': 1000},\n",
      "                           '42': {'f1-score': 0.08666291502532357,\n",
      "                                  'precision': 0.0990990990990991,\n",
      "                                  'recall': 0.077,\n",
      "                                  'support': 1000},\n",
      "                           '43': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '45': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '46': {'f1-score': 0.034133333333333335,\n",
      "                                  'precision': 0.036571428571428574,\n",
      "                                  'recall': 0.032,\n",
      "                                  'support': 1000},\n",
      "                           '47': {'f1-score': 0.047619047619047616,\n",
      "                                  'precision': 0.06678700361010831,\n",
      "                                  'recall': 0.037,\n",
      "                                  'support': 1000},\n",
      "                           '48': {'f1-score': 0.011088011088011086,\n",
      "                                  'precision': 0.01805869074492099,\n",
      "                                  'recall': 0.008,\n",
      "                                  'support': 1000},\n",
      "                           '49': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '5': {'f1-score': 0.09693165969316599,\n",
      "                                 'precision': 0.07441113490364026,\n",
      "                                 'recall': 0.139,\n",
      "                                 'support': 1000},\n",
      "                           '50': {'f1-score': 0.03656059580230196,\n",
      "                                  'precision': 0.027635619242579325,\n",
      "                                  'recall': 0.054,\n",
      "                                  'support': 1000},\n",
      "                           '51': {'f1-score': 0.0782608695652174,\n",
      "                                  'precision': 0.10327868852459017,\n",
      "                                  'recall': 0.063,\n",
      "                                  'support': 1000},\n",
      "                           '52': {'f1-score': 0.07643312101910829,\n",
      "                                  'precision': 0.1875,\n",
      "                                  'recall': 0.048,\n",
      "                                  'support': 1000},\n",
      "                           '53': {'f1-score': 0.09189189189189188,\n",
      "                                  'precision': 0.08360655737704918,\n",
      "                                  'recall': 0.102,\n",
      "                                  'support': 1000},\n",
      "                           '54': {'f1-score': 0.28422320353271774,\n",
      "                                  'precision': 0.17780010045203415,\n",
      "                                  'recall': 0.708,\n",
      "                                  'support': 1000},\n",
      "                           '55': {'f1-score': 0.1358273381294964,\n",
      "                                  'precision': 0.09535353535353536,\n",
      "                                  'recall': 0.236,\n",
      "                                  'support': 1000},\n",
      "                           '56': {'f1-score': 0.12608875985068438,\n",
      "                                  'precision': 0.07953950811093669,\n",
      "                                  'recall': 0.304,\n",
      "                                  'support': 1000},\n",
      "                           '57': {'f1-score': 0.007272727272727274,\n",
      "                                  'precision': 0.04,\n",
      "                                  'recall': 0.004,\n",
      "                                  'support': 1000},\n",
      "                           '58': {'f1-score': 0.14181286549707603,\n",
      "                                  'precision': 0.26358695652173914,\n",
      "                                  'recall': 0.097,\n",
      "                                  'support': 1000},\n",
      "                           '59': {'f1-score': 0.0019157088122605365,\n",
      "                                  'precision': 0.022727272727272728,\n",
      "                                  'recall': 0.001,\n",
      "                                  'support': 1000},\n",
      "                           '6': {'f1-score': 0.036881810561609385,\n",
      "                                 'precision': 0.11398963730569948,\n",
      "                                 'recall': 0.022,\n",
      "                                 'support': 1000},\n",
      "                           '60': {'f1-score': 0.020684168655529037,\n",
      "                                  'precision': 0.05058365758754864,\n",
      "                                  'recall': 0.013,\n",
      "                                  'support': 1000},\n",
      "                           '61': {'f1-score': 0.022590361445783132,\n",
      "                                  'precision': 0.04573170731707317,\n",
      "                                  'recall': 0.015,\n",
      "                                  'support': 1000},\n",
      "                           '62': {'f1-score': 0.036845983787767135,\n",
      "                                  'precision': 0.0700280112044818,\n",
      "                                  'recall': 0.025,\n",
      "                                  'support': 1000},\n",
      "                           '63': {'f1-score': 0.014995313964386131,\n",
      "                                  'precision': 0.11940298507462686,\n",
      "                                  'recall': 0.008,\n",
      "                                  'support': 1000},\n",
      "                           '64': {'f1-score': 0.003798670465337132,\n",
      "                                  'precision': 0.03773584905660377,\n",
      "                                  'recall': 0.002,\n",
      "                                  'support': 1000},\n",
      "                           '66': {'f1-score': 0.012892828364222403,\n",
      "                                  'precision': 0.03319502074688797,\n",
      "                                  'recall': 0.008,\n",
      "                                  'support': 1000},\n",
      "                           '69': {'f1-score': 0.1375475563359672,\n",
      "                                  'precision': 0.08056222146040452,\n",
      "                                  'recall': 0.47,\n",
      "                                  'support': 1000},\n",
      "                           '7': {'f1-score': 0.0615570307785154,\n",
      "                                 'precision': 0.0776255707762557,\n",
      "                                 'recall': 0.051,\n",
      "                                 'support': 1000},\n",
      "                           '71': {'f1-score': 0.05683836589698046,\n",
      "                                  'precision': 0.25396825396825395,\n",
      "                                  'recall': 0.032,\n",
      "                                  'support': 1000},\n",
      "                           '72': {'f1-score': 0.003929273084479371,\n",
      "                                  'precision': 0.1111111111111111,\n",
      "                                  'recall': 0.002,\n",
      "                                  'support': 1000},\n",
      "                           '73': {'f1-score': 0.029185867895545316,\n",
      "                                  'precision': 0.06291390728476821,\n",
      "                                  'recall': 0.019,\n",
      "                                  'support': 1000},\n",
      "                           '74': {'f1-score': 0.010195412064570943,\n",
      "                                  'precision': 0.03389830508474576,\n",
      "                                  'recall': 0.006,\n",
      "                                  'support': 1000},\n",
      "                           '75': {'f1-score': 0.026548672566371685,\n",
      "                                  'precision': 0.11538461538461539,\n",
      "                                  'recall': 0.015,\n",
      "                                  'support': 1000},\n",
      "                           '76': {'f1-score': 0.027447392497712716,\n",
      "                                  'precision': 0.16129032258064516,\n",
      "                                  'recall': 0.015,\n",
      "                                  'support': 1000},\n",
      "                           '77': {'f1-score': 0.0358974358974359,\n",
      "                                  'precision': 0.12352941176470589,\n",
      "                                  'recall': 0.021,\n",
      "                                  'support': 1000},\n",
      "                           '78': {'f1-score': 0.06389391199517781,\n",
      "                                  'precision': 0.08042488619119878,\n",
      "                                  'recall': 0.053,\n",
      "                                  'support': 1000},\n",
      "                           '79': {'f1-score': 0.478551751279024,\n",
      "                                  'precision': 0.3945489941596366,\n",
      "                                  'recall': 0.608,\n",
      "                                  'support': 1000},\n",
      "                           '8': {'f1-score': 0.005612722170252572,\n",
      "                                 'precision': 0.043478260869565216,\n",
      "                                 'recall': 0.003,\n",
      "                                 'support': 1000},\n",
      "                           '80': {'f1-score': 0.07887817703768624,\n",
      "                                  'precision': 0.050505050505050504,\n",
      "                                  'recall': 0.18,\n",
      "                                  'support': 1000},\n",
      "                           '81': {'f1-score': 0.01687116564417178,\n",
      "                                  'precision': 0.03618421052631579,\n",
      "                                  'recall': 0.011,\n",
      "                                  'support': 1000},\n",
      "                           '83': {'f1-score': 0.011834319526627219,\n",
      "                                  'precision': 0.42857142857142855,\n",
      "                                  'recall': 0.006,\n",
      "                                  'support': 1000},\n",
      "                           '84': {'f1-score': 0.09026644915715064,\n",
      "                                  'precision': 0.09892729439809297,\n",
      "                                  'recall': 0.083,\n",
      "                                  'support': 1000},\n",
      "                           '85': {'f1-score': 0.00186046511627907,\n",
      "                                  'precision': 0.013333333333333334,\n",
      "                                  'recall': 0.001,\n",
      "                                  'support': 1000},\n",
      "                           '86': {'f1-score': 0.01929824561403509,\n",
      "                                  'precision': 0.07857142857142857,\n",
      "                                  'recall': 0.011,\n",
      "                                  'support': 1000},\n",
      "                           '87': {'f1-score': 0.017333333333333333,\n",
      "                                  'precision': 0.026,\n",
      "                                  'recall': 0.013,\n",
      "                                  'support': 1000},\n",
      "                           '88': {'f1-score': 0.6148170919151552,\n",
      "                                  'precision': 0.44385264092321347,\n",
      "                                  'recall': 1.0,\n",
      "                                  'support': 1000},\n",
      "                           '89': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '9': {'f1-score': 0.10064516129032258,\n",
      "                                 'precision': 0.14181818181818182,\n",
      "                                 'recall': 0.078,\n",
      "                                 'support': 1000},\n",
      "                           'accuracy': 0.1291375,\n",
      "                           'macro avg': {'f1-score': 0.08746109662926091,\n",
      "                                         'precision': 0.10791063338208906,\n",
      "                                         'recall': 0.1291375,\n",
      "                                         'support': 80000},\n",
      "                           'weighted avg': {'f1-score': 0.08746109662926091,\n",
      "                                            'precision': 0.10791063338208906,\n",
      "                                            'recall': 0.1291375,\n",
      "                                            'support': 80000}}}\n"
     ]
    }
   ],
   "source": [
    "train_eval_result = model.evaluate(train_labels, train_predictions)\n",
    "pprint.pprint(train_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7768822-0900-4959-8618-20c89b0f5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model.predict(dataloader.dataset.testloader)\n",
    "test_predictions, test_labels = test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9521acc2-29d8-4dbb-81e2-0e264b4155a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.07965,\n",
      " 'classification_report': {'0': {'f1-score': 0.001785714285714286,\n",
      "                                 'precision': 0.008333333333333333,\n",
      "                                 'recall': 0.001,\n",
      "                                 'support': 1000},\n",
      "                           '1': {'f1-score': 0.019801980198019802,\n",
      "                                 'precision': 0.022004889975550123,\n",
      "                                 'recall': 0.018,\n",
      "                                 'support': 1000},\n",
      "                           '10': {'f1-score': 0.04978749241044323,\n",
      "                                  'precision': 0.0633693972179289,\n",
      "                                  'recall': 0.041,\n",
      "                                  'support': 1000},\n",
      "                           '12': {'f1-score': 0.27219413549039434,\n",
      "                                  'precision': 0.170595690747782,\n",
      "                                  'recall': 0.673,\n",
      "                                  'support': 1000},\n",
      "                           '13': {'f1-score': 0.019429265330904673,\n",
      "                                  'precision': 0.02472952086553323,\n",
      "                                  'recall': 0.016,\n",
      "                                  'support': 1000},\n",
      "                           '14': {'f1-score': 0.020330368487928845,\n",
      "                                  'precision': 0.027874564459930314,\n",
      "                                  'recall': 0.016,\n",
      "                                  'support': 1000},\n",
      "                           '15': {'f1-score': 0.018543046357615896,\n",
      "                                  'precision': 0.027450980392156862,\n",
      "                                  'recall': 0.014,\n",
      "                                  'support': 1000},\n",
      "                           '16': {'f1-score': 0.041407867494824016,\n",
      "                                  'precision': 0.04291845493562232,\n",
      "                                  'recall': 0.04,\n",
      "                                  'support': 1000},\n",
      "                           '17': {'f1-score': 0.028018679119412943,\n",
      "                                  'precision': 0.04208416833667335,\n",
      "                                  'recall': 0.021,\n",
      "                                  'support': 1000},\n",
      "                           '18': {'f1-score': 0.044532409698169226,\n",
      "                                  'precision': 0.04407443682664055,\n",
      "                                  'recall': 0.045,\n",
      "                                  'support': 1000},\n",
      "                           '19': {'f1-score': 0.03781979977753059,\n",
      "                                  'precision': 0.042606516290726815,\n",
      "                                  'recall': 0.034,\n",
      "                                  'support': 1000},\n",
      "                           '2': {'f1-score': 0.01958650707290533,\n",
      "                                 'precision': 0.021479713603818614,\n",
      "                                 'recall': 0.018,\n",
      "                                 'support': 1000},\n",
      "                           '20': {'f1-score': 0.01390644753476612,\n",
      "                                  'precision': 0.018900343642611683,\n",
      "                                  'recall': 0.011,\n",
      "                                  'support': 1000},\n",
      "                           '21': {'f1-score': 0.07031700288184436,\n",
      "                                  'precision': 0.08299319727891157,\n",
      "                                  'recall': 0.061,\n",
      "                                  'support': 1000},\n",
      "                           '22': {'f1-score': 0.09671694764862467,\n",
      "                                  'precision': 0.062143671607753706,\n",
      "                                  'recall': 0.218,\n",
      "                                  'support': 1000},\n",
      "                           '23': {'f1-score': 0.19939577039274925,\n",
      "                                  'precision': 0.11477393013760774,\n",
      "                                  'recall': 0.759,\n",
      "                                  'support': 1000},\n",
      "                           '24': {'f1-score': 0.11953124999999999,\n",
      "                                  'precision': 0.09807692307692308,\n",
      "                                  'recall': 0.153,\n",
      "                                  'support': 1000},\n",
      "                           '26': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '27': {'f1-score': 0.016638935108153077,\n",
      "                                  'precision': 0.04950495049504951,\n",
      "                                  'recall': 0.01,\n",
      "                                  'support': 1000},\n",
      "                           '3': {'f1-score': 0.1090689238210399,\n",
      "                                 'precision': 0.06203576341127923,\n",
      "                                 'recall': 0.451,\n",
      "                                 'support': 1000},\n",
      "                           '30': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '31': {'f1-score': 0.0808664259927798,\n",
      "                                  'precision': 0.14545454545454545,\n",
      "                                  'recall': 0.056,\n",
      "                                  'support': 1000},\n",
      "                           '32': {'f1-score': 0.007207207207207207,\n",
      "                                  'precision': 0.03636363636363636,\n",
      "                                  'recall': 0.004,\n",
      "                                  'support': 1000},\n",
      "                           '33': {'f1-score': 0.03157063930544594,\n",
      "                                  'precision': 0.0749063670411985,\n",
      "                                  'recall': 0.02,\n",
      "                                  'support': 1000},\n",
      "                           '34': {'f1-score': 0.21151756609923938,\n",
      "                                  'precision': 0.16581487791027824,\n",
      "                                  'recall': 0.292,\n",
      "                                  'support': 1000},\n",
      "                           '35': {'f1-score': 0.006568144499178983,\n",
      "                                  'precision': 0.01834862385321101,\n",
      "                                  'recall': 0.004,\n",
      "                                  'support': 1000},\n",
      "                           '36': {'f1-score': 0.06576125804145819,\n",
      "                                  'precision': 0.11528822055137844,\n",
      "                                  'recall': 0.046,\n",
      "                                  'support': 1000},\n",
      "                           '37': {'f1-score': 0.009708737864077669,\n",
      "                                  'precision': 0.16666666666666666,\n",
      "                                  'recall': 0.005,\n",
      "                                  'support': 1000},\n",
      "                           '38': {'f1-score': 0.01245674740484429,\n",
      "                                  'precision': 0.020224719101123594,\n",
      "                                  'recall': 0.009,\n",
      "                                  'support': 1000},\n",
      "                           '39': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '4': {'f1-score': 0.21221401549270405,\n",
      "                                 'precision': 0.12942210503186113,\n",
      "                                 'recall': 0.589,\n",
      "                                 'support': 1000},\n",
      "                           '40': {'f1-score': 0.011494252873563218,\n",
      "                                  'precision': 0.02040816326530612,\n",
      "                                  'recall': 0.008,\n",
      "                                  'support': 1000},\n",
      "                           '41': {'f1-score': 0.02180232558139535,\n",
      "                                  'precision': 0.0398936170212766,\n",
      "                                  'recall': 0.015,\n",
      "                                  'support': 1000},\n",
      "                           '42': {'f1-score': 0.048484848484848485,\n",
      "                                  'precision': 0.053987730061349694,\n",
      "                                  'recall': 0.044,\n",
      "                                  'support': 1000},\n",
      "                           '43': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '45': {'f1-score': 0.0033670033670033673,\n",
      "                                  'precision': 0.010638297872340425,\n",
      "                                  'recall': 0.002,\n",
      "                                  'support': 1000},\n",
      "                           '46': {'f1-score': 0.06228765571913931,\n",
      "                                  'precision': 0.07180156657963446,\n",
      "                                  'recall': 0.055,\n",
      "                                  'support': 1000},\n",
      "                           '47': {'f1-score': 0.011711125569290826,\n",
      "                                  'precision': 0.01675977653631285,\n",
      "                                  'recall': 0.009,\n",
      "                                  'support': 1000},\n",
      "                           '48': {'f1-score': 0.017582417582417582,\n",
      "                                  'precision': 0.03287671232876712,\n",
      "                                  'recall': 0.012,\n",
      "                                  'support': 1000},\n",
      "                           '49': {'f1-score': 0.00437636761487965,\n",
      "                                  'precision': 0.008086253369272238,\n",
      "                                  'recall': 0.003,\n",
      "                                  'support': 1000},\n",
      "                           '5': {'f1-score': 0.06449511400651467,\n",
      "                                 'precision': 0.04782608695652174,\n",
      "                                 'recall': 0.099,\n",
      "                                 'support': 1000},\n",
      "                           '50': {'f1-score': 0.04644616467276566,\n",
      "                                  'precision': 0.035830618892508145,\n",
      "                                  'recall': 0.066,\n",
      "                                  'support': 1000},\n",
      "                           '51': {'f1-score': 0.10775606867969213,\n",
      "                                  'precision': 0.1320754716981132,\n",
      "                                  'recall': 0.091,\n",
      "                                  'support': 1000},\n",
      "                           '52': {'f1-score': 0.06130268199233716,\n",
      "                                  'precision': 0.13114754098360656,\n",
      "                                  'recall': 0.04,\n",
      "                                  'support': 1000},\n",
      "                           '53': {'f1-score': 0.08086506817113305,\n",
      "                                  'precision': 0.07630878438331855,\n",
      "                                  'recall': 0.086,\n",
      "                                  'support': 1000},\n",
      "                           '54': {'f1-score': 0.22783143107989468,\n",
      "                                  'precision': 0.14595050618672667,\n",
      "                                  'recall': 0.519,\n",
      "                                  'support': 1000},\n",
      "                           '55': {'f1-score': 0.09150684931506851,\n",
      "                                  'precision': 0.06301886792452831,\n",
      "                                  'recall': 0.167,\n",
      "                                  'support': 1000},\n",
      "                           '56': {'f1-score': 0.07861369399830938,\n",
      "                                  'precision': 0.04983922829581994,\n",
      "                                  'recall': 0.186,\n",
      "                                  'support': 1000},\n",
      "                           '57': {'f1-score': 0.011884550084889645,\n",
      "                                  'precision': 0.03932584269662921,\n",
      "                                  'recall': 0.007,\n",
      "                                  'support': 1000},\n",
      "                           '58': {'f1-score': 0.137466307277628,\n",
      "                                  'precision': 0.21074380165289255,\n",
      "                                  'recall': 0.102,\n",
      "                                  'support': 1000},\n",
      "                           '59': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '6': {'f1-score': 0.03908794788273616,\n",
      "                                 'precision': 0.10526315789473684,\n",
      "                                 'recall': 0.024,\n",
      "                                 'support': 1000},\n",
      "                           '60': {'f1-score': 0.021224489795918365,\n",
      "                                  'precision': 0.057777777777777775,\n",
      "                                  'recall': 0.013,\n",
      "                                  'support': 1000},\n",
      "                           '61': {'f1-score': 0.021772939346811817,\n",
      "                                  'precision': 0.04895104895104895,\n",
      "                                  'recall': 0.014,\n",
      "                                  'support': 1000},\n",
      "                           '62': {'f1-score': 0.042488619119878605,\n",
      "                                  'precision': 0.0880503144654088,\n",
      "                                  'recall': 0.028,\n",
      "                                  'support': 1000},\n",
      "                           '63': {'f1-score': 0.009267840593141797,\n",
      "                                  'precision': 0.06329113924050633,\n",
      "                                  'recall': 0.005,\n",
      "                                  'support': 1000},\n",
      "                           '64': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '66': {'f1-score': 0.0017937219730941706,\n",
      "                                  'precision': 0.008695652173913044,\n",
      "                                  'recall': 0.001,\n",
      "                                  'support': 1000},\n",
      "                           '69': {'f1-score': 0.1339892924323542,\n",
      "                                  'precision': 0.07832854001015056,\n",
      "                                  'recall': 0.463,\n",
      "                                  'support': 1000},\n",
      "                           '7': {'f1-score': 0.037037037037037035,\n",
      "                                 'precision': 0.04395604395604396,\n",
      "                                 'recall': 0.032,\n",
      "                                 'support': 1000},\n",
      "                           '71': {'f1-score': 0.016100178890876567,\n",
      "                                  'precision': 0.07627118644067797,\n",
      "                                  'recall': 0.009,\n",
      "                                  'support': 1000},\n",
      "                           '72': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '73': {'f1-score': 0.05355776587605204,\n",
      "                                  'precision': 0.11400651465798045,\n",
      "                                  'recall': 0.035,\n",
      "                                  'support': 1000},\n",
      "                           '74': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '75': {'f1-score': 0.007060900264783759,\n",
      "                                  'precision': 0.03007518796992481,\n",
      "                                  'recall': 0.004,\n",
      "                                  'support': 1000},\n",
      "                           '76': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '77': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '78': {'f1-score': 0.04081632653061224,\n",
      "                                  'precision': 0.04712041884816754,\n",
      "                                  'recall': 0.036,\n",
      "                                  'support': 1000},\n",
      "                           '79': {'f1-score': 0.25819070904645475,\n",
      "                                  'precision': 0.25263157894736843,\n",
      "                                  'recall': 0.264,\n",
      "                                  'support': 1000},\n",
      "                           '8': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 1000},\n",
      "                           '80': {'f1-score': 0.06892307692307692,\n",
      "                                  'precision': 0.04335483870967742,\n",
      "                                  'recall': 0.168,\n",
      "                                  'support': 1000},\n",
      "                           '81': {'f1-score': 0.008006405124099279,\n",
      "                                  'precision': 0.020080321285140562,\n",
      "                                  'recall': 0.005,\n",
      "                                  'support': 1000},\n",
      "                           '83': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '84': {'f1-score': 0.03386127799016931,\n",
      "                                  'precision': 0.03730445246690734,\n",
      "                                  'recall': 0.031,\n",
      "                                  'support': 1000},\n",
      "                           '85': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '86': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '87': {'f1-score': 0.010533245556287029,\n",
      "                                  'precision': 0.015414258188824663,\n",
      "                                  'recall': 0.008,\n",
      "                                  'support': 1000},\n",
      "                           '88': {'f1-score': 0.0531535105117017,\n",
      "                                  'precision': 0.0440499671268902,\n",
      "                                  'recall': 0.067,\n",
      "                                  'support': 1000},\n",
      "                           '89': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1000},\n",
      "                           '9': {'f1-score': 0.03773584905660378,\n",
      "                                 'precision': 0.054003724394785846,\n",
      "                                 'recall': 0.029,\n",
      "                                 'support': 1000},\n",
      "                           'accuracy': 0.07965,\n",
      "                           'macro avg': {'f1-score': 0.04775697931300545,\n",
      "                                         'precision': 0.05391981498525735,\n",
      "                                         'recall': 0.07965,\n",
      "                                         'support': 80000},\n",
      "                           'weighted avg': {'f1-score': 0.04775697931300545,\n",
      "                                            'precision': 0.05391981498525736,\n",
      "                                            'recall': 0.07965,\n",
      "                                            'support': 80000}}}\n"
     ]
    }
   ],
   "source": [
    "test_eval_result = model.evaluate(test_labels, test_predictions)\n",
    "pprint.pprint(test_eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
