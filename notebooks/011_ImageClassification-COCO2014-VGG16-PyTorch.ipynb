{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfcf5ad-531c-4505-9fb2-48db18b4568e",
   "metadata": {},
   "source": [
    "# Sample code of Image Classification VGG16 Model with PyTorch\n",
    "\n",
    "This notebook is the sample code of training the image classification model using COCO2014 dataset.  \n",
    "COCO2014 dataset has not classification labels, therefore it makes classification dataset cropping bounding boxes.\n",
    "\n",
    "|Item|Description|\n",
    "|---|---|\n",
    "|DeepLearning Framework|PyTorch|\n",
    "|Dataset|COCO2014 Classification|\n",
    "|Model Architecture|VGG16|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2f3e25-58a2-4dae-9166-a85f649bde47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d563f93f-798e-4334-8094-334e84c69097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "#import json\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "#from PIL import Image\n",
    "#from data_loader.data_loader import DataLoader\n",
    "#from models.pytorch import vgg16\n",
    "import itertools\n",
    "#\n",
    "#import torch\n",
    "#from torch.utils.data import Dataset\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pprint\n",
    "\n",
    "from data_loader.data_loader import DataLoader\n",
    "from models.pytorch import vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0d189-4ff4-4619-b8e4-91151436612c",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f272e2-e60d-4bf3-aff9-a255ce7e68f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2f23ca5cf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53721ad-cdf2-4570-a63d-34eca78d01bf",
   "metadata": {},
   "source": [
    "## Device Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d22f5cd-18ef-4a30-9d92-ce0c0afffd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca638a-c879-4044-bd94-4181db50a2d2",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfee9194-b6b7-438e-a967-eb25c0f544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.001\n",
    "input_tensor_shape = (3, 224, 224)   # CHW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06247d29-f8d4-4ab5-83bf-830195df2ef8",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac110b-b5d8-4dbd-b982-49b82ee743c6",
   "metadata": {},
   "source": [
    "### Download and Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86cf2bd-366a-4e94-8835-4a01ed16deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 300000/300000 [52:19<00:00, 95.56it/s]  \n",
      "100% 84509/84509 [01:04<00:00, 1317.60it/s]\n",
      "100% 84509/84509 [00:00<00:00, 636579.61it/s]\n",
      "100% 291875/291875 [29:36<00:00, 164.27it/s] \n",
      "100% 56834/56834 [00:45<00:00, 1237.10it/s]\n",
      "100% 56834/56834 [00:00<00:00, 643566.80it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '/tmp/dataset'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "dataloader = DataLoader(dataset_name='coco2014_classification_pytorch', resize=input_tensor_shape[1:], dataset_dir=dataset_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b3fa3a-64fa-4d11-81a0-7a5822b2d430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hot dog',\n",
       " 'dog',\n",
       " 'potted plant',\n",
       " 'tv',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'horse',\n",
       " 'sheep',\n",
       " 'cow',\n",
       " 'bottle',\n",
       " 'couch',\n",
       " 'chair',\n",
       " 'dining table',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorcycle',\n",
       " 'airplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'boat',\n",
       " 'person',\n",
       " 'stop sign',\n",
       " 'umbrella',\n",
       " 'tie',\n",
       " 'sports ball',\n",
       " 'sandwich',\n",
       " 'bed',\n",
       " 'cell phone',\n",
       " 'refrigerator',\n",
       " 'clock',\n",
       " 'toothbrush',\n",
       " 'truck',\n",
       " 'traffic light',\n",
       " 'fire hydrant',\n",
       " 'parking meter',\n",
       " 'bench',\n",
       " 'elephant',\n",
       " 'bear',\n",
       " 'zebra',\n",
       " 'giraffe',\n",
       " 'frisbee',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'kite',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'skateboard',\n",
       " 'surfboard',\n",
       " 'tennis racket',\n",
       " 'wine glass',\n",
       " 'cup',\n",
       " 'fork',\n",
       " 'knife',\n",
       " 'spoon',\n",
       " 'bowl',\n",
       " 'banana',\n",
       " 'apple',\n",
       " 'orange',\n",
       " 'broccoli',\n",
       " 'carrot',\n",
       " 'pizza',\n",
       " 'donut',\n",
       " 'cake',\n",
       " 'toilet',\n",
       " 'laptop',\n",
       " 'mouse',\n",
       " 'remote',\n",
       " 'keyboard',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " 'toaster',\n",
       " 'sink',\n",
       " 'book',\n",
       " 'vase',\n",
       " 'scissors',\n",
       " 'teddy bear',\n",
       " 'hair drier',\n",
       " 'backpack',\n",
       " 'handbag',\n",
       " 'suitcase']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.dataset.class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa766c6-60fb-437b-9647-119d398e2491",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = dataloader.dataset.trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e44063-dcd7-473c-a029-2e514d149c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2641/2641 [38:59<00:00,  1.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 5,\n",
       " 87,\n",
       " 3,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 20,\n",
       " 0,\n",
       " 81,\n",
       " 4,\n",
       " 66,\n",
       " 2,\n",
       " 87,\n",
       " 4,\n",
       " 43,\n",
       " 51,\n",
       " 7,\n",
       " 10,\n",
       " 53,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 81,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 6,\n",
       " 0,\n",
       " 43,\n",
       " 0,\n",
       " 51,\n",
       " 14,\n",
       " 23,\n",
       " 0,\n",
       " 71,\n",
       " 4,\n",
       " 1,\n",
       " 18,\n",
       " 40,\n",
       " 23,\n",
       " 80,\n",
       " 66,\n",
       " 23,\n",
       " 0,\n",
       " 63,\n",
       " 0,\n",
       " 0,\n",
       " 50,\n",
       " 8,\n",
       " 0,\n",
       " 33,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 0,\n",
       " 69,\n",
       " 5,\n",
       " 0,\n",
       " 78,\n",
       " 23,\n",
       " 0,\n",
       " 2,\n",
       " 55,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 64,\n",
       " 62,\n",
       " 0,\n",
       " 0,\n",
       " 32,\n",
       " 3,\n",
       " 6,\n",
       " 72,\n",
       " 0,\n",
       " 4,\n",
       " 69,\n",
       " 0,\n",
       " 61,\n",
       " 63,\n",
       " 0,\n",
       " 31,\n",
       " 23,\n",
       " 16,\n",
       " 85,\n",
       " 2,\n",
       " 58,\n",
       " 0,\n",
       " 18,\n",
       " 16,\n",
       " 0,\n",
       " 48,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 45,\n",
       " 0,\n",
       " 0,\n",
       " 23,\n",
       " 0,\n",
       " 47,\n",
       " 58,\n",
       " 0,\n",
       " 43,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 66,\n",
       " 0,\n",
       " 0,\n",
       " 48,\n",
       " 30,\n",
       " 0,\n",
       " 31,\n",
       " 57,\n",
       " 51,\n",
       " 0,\n",
       " 78,\n",
       " 16,\n",
       " 51,\n",
       " 16,\n",
       " 9,\n",
       " 18,\n",
       " 62,\n",
       " 64,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 46,\n",
       " 58,\n",
       " 0,\n",
       " 49,\n",
       " 0,\n",
       " 17,\n",
       " 21,\n",
       " 0,\n",
       " 31,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 60,\n",
       " 0,\n",
       " 38,\n",
       " 51,\n",
       " 22,\n",
       " 5,\n",
       " 0,\n",
       " 10,\n",
       " 20,\n",
       " 0,\n",
       " 6,\n",
       " 32,\n",
       " 58,\n",
       " 0,\n",
       " 66,\n",
       " 0,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 0,\n",
       " 85,\n",
       " 0,\n",
       " 71,\n",
       " 53,\n",
       " 66,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 81,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 13,\n",
       " 0,\n",
       " 56,\n",
       " 0,\n",
       " 17,\n",
       " 41,\n",
       " 56,\n",
       " 64,\n",
       " 30,\n",
       " 0,\n",
       " 6,\n",
       " 81,\n",
       " 56,\n",
       " 66,\n",
       " 51,\n",
       " 0,\n",
       " 61,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 23,\n",
       " 76,\n",
       " 8,\n",
       " 0,\n",
       " 66,\n",
       " 87,\n",
       " 16,\n",
       " 72,\n",
       " 62,\n",
       " 85,\n",
       " 66,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 58,\n",
       " 10,\n",
       " 5,\n",
       " 60,\n",
       " 66,\n",
       " 6,\n",
       " 10,\n",
       " 62,\n",
       " 18,\n",
       " 87,\n",
       " 27,\n",
       " 50,\n",
       " 0,\n",
       " 21,\n",
       " 3,\n",
       " 20,\n",
       " 61,\n",
       " 0,\n",
       " 0,\n",
       " 66,\n",
       " 66,\n",
       " 81,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 53,\n",
       " 0,\n",
       " 47,\n",
       " 3,\n",
       " 64,\n",
       " 0,\n",
       " 23,\n",
       " 78,\n",
       " 73,\n",
       " 69,\n",
       " 33,\n",
       " 0,\n",
       " 24,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 41,\n",
       " 63,\n",
       " 50,\n",
       " 62,\n",
       " 0,\n",
       " 30,\n",
       " 21,\n",
       " 24,\n",
       " 16,\n",
       " 27,\n",
       " 4,\n",
       " 16,\n",
       " 2,\n",
       " 24,\n",
       " 2,\n",
       " 27,\n",
       " 34,\n",
       " 6,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 71,\n",
       " 64,\n",
       " 24,\n",
       " 0,\n",
       " 40,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 66,\n",
       " 0,\n",
       " 32,\n",
       " 4,\n",
       " 21,\n",
       " 58,\n",
       " 15,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 19,\n",
       " 81,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 47,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 45,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 18,\n",
       " 43,\n",
       " 0,\n",
       " 61,\n",
       " 0,\n",
       " 64,\n",
       " 6,\n",
       " 62,\n",
       " 23,\n",
       " 22,\n",
       " 18,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 32,\n",
       " 48,\n",
       " 0,\n",
       " 64,\n",
       " 5,\n",
       " 63,\n",
       " 0,\n",
       " 27,\n",
       " 19,\n",
       " 18,\n",
       " 63,\n",
       " 81,\n",
       " 45,\n",
       " 54,\n",
       " 8,\n",
       " 0,\n",
       " 78,\n",
       " 0,\n",
       " 3,\n",
       " 18,\n",
       " 24,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 5,\n",
       " 16,\n",
       " 64,\n",
       " 0,\n",
       " 1,\n",
       " 39,\n",
       " 3,\n",
       " 0,\n",
       " 15,\n",
       " 23,\n",
       " 17,\n",
       " 0,\n",
       " 66,\n",
       " 66,\n",
       " 0,\n",
       " 71,\n",
       " 0,\n",
       " 69,\n",
       " 87,\n",
       " 0,\n",
       " 19,\n",
       " 4,\n",
       " 24,\n",
       " 0,\n",
       " 14,\n",
       " 58,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 41,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 59,\n",
       " 8,\n",
       " 0,\n",
       " 16,\n",
       " 10,\n",
       " 24,\n",
       " 19,\n",
       " 57,\n",
       " 6,\n",
       " 58,\n",
       " 7,\n",
       " 18,\n",
       " 0,\n",
       " 2,\n",
       " 64,\n",
       " 64,\n",
       " 5,\n",
       " 46,\n",
       " 0,\n",
       " 0,\n",
       " 72,\n",
       " 66,\n",
       " 2,\n",
       " 14,\n",
       " 14,\n",
       " 16,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 63,\n",
       " 0,\n",
       " 57,\n",
       " 0,\n",
       " 16,\n",
       " 58,\n",
       " 2,\n",
       " 18,\n",
       " 19,\n",
       " 85,\n",
       " 48,\n",
       " 7,\n",
       " 72,\n",
       " 10,\n",
       " 0,\n",
       " 53,\n",
       " 21,\n",
       " 6,\n",
       " 64,\n",
       " 4,\n",
       " 62,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 56,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 81,\n",
       " 59,\n",
       " 0,\n",
       " 0,\n",
       " 8,\n",
       " 21,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 27,\n",
       " 71,\n",
       " 86,\n",
       " 43,\n",
       " 57,\n",
       " 3,\n",
       " 58,\n",
       " 58,\n",
       " 19,\n",
       " 27,\n",
       " 27,\n",
       " 78,\n",
       " 0,\n",
       " 66,\n",
       " 31,\n",
       " 86,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 64,\n",
       " 0,\n",
       " 62,\n",
       " 24,\n",
       " 61,\n",
       " 46,\n",
       " 16,\n",
       " 60,\n",
       " 17,\n",
       " 64,\n",
       " 72,\n",
       " 4,\n",
       " 15,\n",
       " 50,\n",
       " 0,\n",
       " 5,\n",
       " 24,\n",
       " 0,\n",
       " 83,\n",
       " 83,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 53,\n",
       " 18,\n",
       " 46,\n",
       " 27,\n",
       " 23,\n",
       " 85,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 80,\n",
       " 41,\n",
       " 58,\n",
       " 57,\n",
       " 0,\n",
       " 0,\n",
       " 58,\n",
       " 43,\n",
       " 0,\n",
       " 78,\n",
       " 60,\n",
       " 16,\n",
       " 5,\n",
       " 0,\n",
       " 76,\n",
       " 3,\n",
       " 26,\n",
       " 50,\n",
       " 7,\n",
       " 0,\n",
       " 63,\n",
       " 6,\n",
       " 16,\n",
       " 66,\n",
       " 16,\n",
       " 53,\n",
       " 81,\n",
       " 13,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 16,\n",
       " 35,\n",
       " 0,\n",
       " 0,\n",
       " 59,\n",
       " 50,\n",
       " 23,\n",
       " 4,\n",
       " 0,\n",
       " 32,\n",
       " 42,\n",
       " 0,\n",
       " 0,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 23,\n",
       " 0,\n",
       " 0,\n",
       " 23,\n",
       " 64,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 4,\n",
       " 72,\n",
       " 8,\n",
       " 0,\n",
       " 51,\n",
       " 16,\n",
       " 66,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 57,\n",
       " 0,\n",
       " 4,\n",
       " 81,\n",
       " 76,\n",
       " 0,\n",
       " 3,\n",
       " 64,\n",
       " 0,\n",
       " 0,\n",
       " 66,\n",
       " 6,\n",
       " 66,\n",
       " 42,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 0,\n",
       " 14,\n",
       " 76,\n",
       " 17,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 23,\n",
       " 0,\n",
       " 66,\n",
       " 2,\n",
       " 19,\n",
       " 37,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 48,\n",
       " 17,\n",
       " 0,\n",
       " 66,\n",
       " 6,\n",
       " 0,\n",
       " 85,\n",
       " 52,\n",
       " 75,\n",
       " 66,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 34,\n",
       " 47,\n",
       " 4,\n",
       " 85,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 0,\n",
       " 6,\n",
       " 18,\n",
       " 0,\n",
       " 0,\n",
       " 81,\n",
       " 66,\n",
       " 23,\n",
       " 2,\n",
       " 16,\n",
       " 61,\n",
       " 3,\n",
       " 66,\n",
       " 0,\n",
       " 0,\n",
       " 66,\n",
       " 47,\n",
       " 20,\n",
       " 61,\n",
       " 0,\n",
       " 75,\n",
       " 23,\n",
       " 66,\n",
       " 24,\n",
       " 0,\n",
       " 6,\n",
       " 24,\n",
       " 0,\n",
       " 57,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 72,\n",
       " 86,\n",
       " 34,\n",
       " 69,\n",
       " 54,\n",
       " 54,\n",
       " 0,\n",
       " 17,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 4,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 14,\n",
       " 0,\n",
       " 27,\n",
       " 0,\n",
       " 23,\n",
       " 69,\n",
       " 35,\n",
       " 21,\n",
       " 27,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 61,\n",
       " 0,\n",
       " 50,\n",
       " 0,\n",
       " 0,\n",
       " 89,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 69,\n",
       " 0,\n",
       " 58,\n",
       " 0,\n",
       " 7,\n",
       " 2,\n",
       " 51,\n",
       " 0,\n",
       " 46,\n",
       " 0,\n",
       " 23,\n",
       " 58,\n",
       " 0,\n",
       " 4,\n",
       " 14,\n",
       " 84,\n",
       " 12,\n",
       " 17,\n",
       " 0,\n",
       " 16,\n",
       " 32,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 75,\n",
       " 0,\n",
       " 0,\n",
       " 64,\n",
       " 69,\n",
       " 32,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 78,\n",
       " 0,\n",
       " 0,\n",
       " 66,\n",
       " 50,\n",
       " 66,\n",
       " 0,\n",
       " 18,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 58,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 72,\n",
       " 5,\n",
       " 0,\n",
       " 66,\n",
       " 0,\n",
       " 23,\n",
       " 0,\n",
       " 14,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 66,\n",
       " 4,\n",
       " 19,\n",
       " 72,\n",
       " 0,\n",
       " 69,\n",
       " 53,\n",
       " 87,\n",
       " 74,\n",
       " 48,\n",
       " 0,\n",
       " 21,\n",
       " 0,\n",
       " 0,\n",
       " 62,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 80,\n",
       " 66,\n",
       " 66,\n",
       " 20,\n",
       " 61,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 64,\n",
       " 45,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 66,\n",
       " 4,\n",
       " 0,\n",
       " 78,\n",
       " 0,\n",
       " 2,\n",
       " 49,\n",
       " 12,\n",
       " 66,\n",
       " 72,\n",
       " 72,\n",
       " 0,\n",
       " 59,\n",
       " 0,\n",
       " 0,\n",
       " 61,\n",
       " 64,\n",
       " 0,\n",
       " 72,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 5,\n",
       " 0,\n",
       " 16,\n",
       " 87,\n",
       " 48,\n",
       " 17,\n",
       " 0,\n",
       " 87,\n",
       " 66,\n",
       " 34,\n",
       " 62,\n",
       " 0,\n",
       " 9,\n",
       " 61,\n",
       " 20,\n",
       " 69,\n",
       " 0,\n",
       " 66,\n",
       " 41,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 27,\n",
       " 0,\n",
       " 5,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 14,\n",
       " 58,\n",
       " 4,\n",
       " 46,\n",
       " 5,\n",
       " 4,\n",
       " 31,\n",
       " 0,\n",
       " 19,\n",
       " 0,\n",
       " 62,\n",
       " 0,\n",
       " 23,\n",
       " 0,\n",
       " 64,\n",
       " 21,\n",
       " 16,\n",
       " 71,\n",
       " 0,\n",
       " 58,\n",
       " 62,\n",
       " 24,\n",
       " 32,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 21,\n",
       " 66,\n",
       " 69,\n",
       " 15,\n",
       " 64,\n",
       " 61,\n",
       " 23,\n",
       " 16,\n",
       " 27,\n",
       " 5,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 24,\n",
       " 0,\n",
       " 0,\n",
       " 72,\n",
       " 0,\n",
       " 32,\n",
       " 12,\n",
       " 6,\n",
       " 51,\n",
       " 0,\n",
       " 80,\n",
       " 85,\n",
       " 12,\n",
       " 0,\n",
       " 66,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 64,\n",
       " 14,\n",
       " 81,\n",
       " 71,\n",
       " 0,\n",
       " 80,\n",
       " 85,\n",
       " 66,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 81,\n",
       " 69,\n",
       " 60,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 38,\n",
       " 66,\n",
       " 41,\n",
       " 64,\n",
       " 74,\n",
       " 0,\n",
       " 72,\n",
       " 0,\n",
       " 66,\n",
       " 69,\n",
       " 66,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 71,\n",
       " 9,\n",
       " 49,\n",
       " 64,\n",
       " 23,\n",
       " 0,\n",
       " 16,\n",
       " 43,\n",
       " 22,\n",
       " 19,\n",
       " 0,\n",
       " 0,\n",
       " 87,\n",
       " 62,\n",
       " 0,\n",
       " 8,\n",
       " 17,\n",
       " 0,\n",
       " 8,\n",
       " 66,\n",
       " 0,\n",
       " 85,\n",
       " 0,\n",
       " 4,\n",
       " 66,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = []\n",
    "i = 0\n",
    "for inputs, labels in tqdm(trainloader):\n",
    "    train_labels += labels.tolist()\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bccf373-81f5-410f-a26f-30ec64268064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(np.array(train_labels, dtype=int)).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e7b42b8-bf32-4027-ae82-19319487d35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5, 87,  3, 18, 20, 81,  4, 66,  2, 43, 51,  7, 10, 53,  6, 14,\n",
       "       23, 71,  1, 40, 80, 63, 50,  8, 33, 69, 78, 55, 24, 64, 62, 32, 72,\n",
       "       61, 31, 16, 85, 58, 48, 59, 45, 47, 30, 57,  9, 46, 49, 17, 21, 60,\n",
       "       38, 22, 13, 56, 41, 76, 27, 73, 34, 15, 19, 54, 39, 86, 83, 26, 35,\n",
       "       42, 12, 37, 52, 75, 89, 84, 74, 36, 77, 79, 88])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.array(train_labels, dtype=int)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d051482-6e8a-4047-bacc-2c9fe9565b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d567088-238b-45ef-93a2-d490ceafcef1",
   "metadata": {},
   "source": [
    "## Training VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "803a91f2-3e38-4378-be27-0267d8892b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [32, 90]                  --\n",
      "├─Sequential: 1-1                        [32, 64, 224, 224]        --\n",
      "│    └─Conv2d: 2-1                       [32, 64, 224, 224]        1,792\n",
      "│    └─BatchNorm2d: 2-2                  [32, 64, 224, 224]        128\n",
      "│    └─ReLU: 2-3                         [32, 64, 224, 224]        --\n",
      "├─Sequential: 1-2                        [32, 64, 112, 112]        --\n",
      "│    └─Conv2d: 2-4                       [32, 64, 224, 224]        36,928\n",
      "│    └─BatchNorm2d: 2-5                  [32, 64, 224, 224]        128\n",
      "│    └─ReLU: 2-6                         [32, 64, 224, 224]        --\n",
      "│    └─MaxPool2d: 2-7                    [32, 64, 112, 112]        --\n",
      "├─Sequential: 1-3                        [32, 128, 112, 112]       --\n",
      "│    └─Conv2d: 2-8                       [32, 128, 112, 112]       73,856\n",
      "│    └─BatchNorm2d: 2-9                  [32, 128, 112, 112]       256\n",
      "│    └─ReLU: 2-10                        [32, 128, 112, 112]       --\n",
      "├─Sequential: 1-4                        [32, 128, 56, 56]         --\n",
      "│    └─Conv2d: 2-11                      [32, 128, 112, 112]       147,584\n",
      "│    └─BatchNorm2d: 2-12                 [32, 128, 112, 112]       256\n",
      "│    └─ReLU: 2-13                        [32, 128, 112, 112]       --\n",
      "│    └─MaxPool2d: 2-14                   [32, 128, 56, 56]         --\n",
      "├─Sequential: 1-5                        [32, 256, 56, 56]         --\n",
      "│    └─Conv2d: 2-15                      [32, 256, 56, 56]         295,168\n",
      "│    └─BatchNorm2d: 2-16                 [32, 256, 56, 56]         512\n",
      "│    └─ReLU: 2-17                        [32, 256, 56, 56]         --\n",
      "├─Sequential: 1-6                        [32, 256, 56, 56]         --\n",
      "│    └─Conv2d: 2-18                      [32, 256, 56, 56]         590,080\n",
      "│    └─BatchNorm2d: 2-19                 [32, 256, 56, 56]         512\n",
      "│    └─ReLU: 2-20                        [32, 256, 56, 56]         --\n",
      "├─Sequential: 1-7                        [32, 256, 28, 28]         --\n",
      "│    └─Conv2d: 2-21                      [32, 256, 56, 56]         590,080\n",
      "│    └─BatchNorm2d: 2-22                 [32, 256, 56, 56]         512\n",
      "│    └─ReLU: 2-23                        [32, 256, 56, 56]         --\n",
      "│    └─MaxPool2d: 2-24                   [32, 256, 28, 28]         --\n",
      "├─Sequential: 1-8                        [32, 512, 28, 28]         --\n",
      "│    └─Conv2d: 2-25                      [32, 512, 28, 28]         1,180,160\n",
      "│    └─BatchNorm2d: 2-26                 [32, 512, 28, 28]         1,024\n",
      "│    └─ReLU: 2-27                        [32, 512, 28, 28]         --\n",
      "├─Sequential: 1-9                        [32, 512, 28, 28]         --\n",
      "│    └─Conv2d: 2-28                      [32, 512, 28, 28]         2,359,808\n",
      "│    └─BatchNorm2d: 2-29                 [32, 512, 28, 28]         1,024\n",
      "│    └─ReLU: 2-30                        [32, 512, 28, 28]         --\n",
      "├─Sequential: 1-10                       [32, 512, 14, 14]         --\n",
      "│    └─Conv2d: 2-31                      [32, 512, 28, 28]         2,359,808\n",
      "│    └─BatchNorm2d: 2-32                 [32, 512, 28, 28]         1,024\n",
      "│    └─ReLU: 2-33                        [32, 512, 28, 28]         --\n",
      "│    └─MaxPool2d: 2-34                   [32, 512, 14, 14]         --\n",
      "├─Sequential: 1-11                       [32, 512, 14, 14]         --\n",
      "│    └─Conv2d: 2-35                      [32, 512, 14, 14]         2,359,808\n",
      "│    └─BatchNorm2d: 2-36                 [32, 512, 14, 14]         1,024\n",
      "│    └─ReLU: 2-37                        [32, 512, 14, 14]         --\n",
      "├─Sequential: 1-12                       [32, 512, 14, 14]         --\n",
      "│    └─Conv2d: 2-38                      [32, 512, 14, 14]         2,359,808\n",
      "│    └─BatchNorm2d: 2-39                 [32, 512, 14, 14]         1,024\n",
      "│    └─ReLU: 2-40                        [32, 512, 14, 14]         --\n",
      "├─Sequential: 1-13                       [32, 512, 7, 7]           --\n",
      "│    └─Conv2d: 2-41                      [32, 512, 14, 14]         2,359,808\n",
      "│    └─BatchNorm2d: 2-42                 [32, 512, 14, 14]         1,024\n",
      "│    └─ReLU: 2-43                        [32, 512, 14, 14]         --\n",
      "│    └─MaxPool2d: 2-44                   [32, 512, 7, 7]           --\n",
      "├─AdaptiveAvgPool2d: 1-14                [32, 512, 7, 7]           --\n",
      "├─Sequential: 1-15                       [32, 4096]                --\n",
      "│    └─Linear: 2-45                      [32, 4096]                102,764,544\n",
      "│    └─ReLU: 2-46                        [32, 4096]                --\n",
      "│    └─Dropout: 2-47                     [32, 4096]                --\n",
      "├─Sequential: 1-16                       [32, 4096]                --\n",
      "│    └─Linear: 2-48                      [32, 4096]                16,781,312\n",
      "│    └─ReLU: 2-49                        [32, 4096]                --\n",
      "│    └─Dropout: 2-50                     [32, 4096]                --\n",
      "├─Sequential: 1-17                       [32, 90]                  --\n",
      "│    └─Linear: 2-51                      [32, 90]                  368,730\n",
      "==========================================================================================\n",
      "Total params: 134,637,722\n",
      "Trainable params: 134,637,722\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 495.36\n",
      "==========================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 6938.45\n",
      "Params size (MB): 538.55\n",
      "Estimated Total Size (MB): 7496.27\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "input_size = (batch_size, 3, 224, 224)\n",
    "#num_classes = len(dataloader.dataset.class_name)\n",
    "num_classes = max(train_labels)+1\n",
    "model = vgg16.VGG16(device, input_size=input_size, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528c4a81-3ed8-4c1a-8f9c-acd8e1b23dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH #0, step #0] loss: 5.011264801025391\n",
      "[EPOCH #0, step #2] loss: 4.808112621307373\n",
      "[EPOCH #0, step #4] loss: 4.779122734069825\n",
      "[EPOCH #0, step #6] loss: 4.801761150360107\n",
      "[EPOCH #0, step #8] loss: 4.745552115970188\n",
      "[EPOCH #0, step #10] loss: 4.742481535131281\n",
      "[EPOCH #0, step #12] loss: 4.743678459754357\n",
      "[EPOCH #0, step #14] loss: 4.73467378616333\n",
      "[EPOCH #0, step #16] loss: 4.747528889599969\n",
      "[EPOCH #0, step #18] loss: 4.738131949776097\n",
      "[EPOCH #0, step #20] loss: 4.74905032203311\n",
      "[EPOCH #0, step #22] loss: 4.7384296085523525\n",
      "[EPOCH #0, step #24] loss: 4.739431266784668\n",
      "[EPOCH #0, step #26] loss: 4.74488549762302\n",
      "[EPOCH #0, step #28] loss: 4.742120249518033\n",
      "[EPOCH #0, step #30] loss: 4.739771981393138\n",
      "[EPOCH #0, step #32] loss: 4.743175087553082\n",
      "[EPOCH #0, step #34] loss: 4.739212935311453\n",
      "[EPOCH #0, step #36] loss: 4.733048310150972\n",
      "[EPOCH #0, step #38] loss: 4.733639729328645\n",
      "[EPOCH #0, step #40] loss: 4.734997237600932\n",
      "[EPOCH #0, step #42] loss: 4.733519620673601\n",
      "[EPOCH #0, step #44] loss: 4.734394529130724\n",
      "[EPOCH #0, step #46] loss: 4.734373549197582\n",
      "[EPOCH #0, step #48] loss: 4.730055283526985\n",
      "[EPOCH #0, step #50] loss: 4.736109668133306\n",
      "[EPOCH #0, step #52] loss: 4.735248619655393\n",
      "[EPOCH #0, step #54] loss: 4.744976035031406\n",
      "[EPOCH #0, step #56] loss: 4.745787871511359\n",
      "[EPOCH #0, step #58] loss: 4.749794071003542\n",
      "[EPOCH #0, step #60] loss: 4.752001223016958\n",
      "[EPOCH #0, step #62] loss: 4.754836445762997\n",
      "[EPOCH #0, step #64] loss: 4.754408418215238\n",
      "[EPOCH #0, step #66] loss: 4.754407092706481\n",
      "[EPOCH #0, step #68] loss: 4.7524653103040615\n",
      "[EPOCH #0, step #70] loss: 4.753318182179625\n",
      "[EPOCH #0, step #72] loss: 4.754698818677093\n",
      "[EPOCH #0, step #74] loss: 4.760144901275635\n",
      "[EPOCH #0, step #76] loss: 4.758397195246313\n",
      "[EPOCH #0, step #78] loss: 4.759420636333997\n",
      "[EPOCH #0, step #80] loss: 4.760049207710925\n",
      "[EPOCH #0, step #82] loss: 4.759763183363949\n",
      "[EPOCH #0, step #84] loss: 4.759762623730828\n",
      "[EPOCH #0, step #86] loss: 4.763300977904221\n",
      "[EPOCH #0, step #88] loss: 4.763928349098463\n",
      "[EPOCH #0, step #90] loss: 4.762793996831873\n",
      "[EPOCH #0, step #92] loss: 4.761715258321455\n",
      "[EPOCH #0, step #94] loss: 4.76250470813952\n",
      "[EPOCH #0, step #96] loss: 4.764436082741649\n",
      "[EPOCH #0, step #98] loss: 4.760993543297354\n",
      "[EPOCH #0, step #100] loss: 4.758397678337475\n",
      "[EPOCH #0, step #102] loss: 4.761925715844608\n",
      "[EPOCH #0, step #104] loss: 4.7605768521626795\n",
      "[EPOCH #0, step #106] loss: 4.7601805178918575\n",
      "[EPOCH #0, step #108] loss: 4.759402349454548\n",
      "[EPOCH #0, step #110] loss: 4.761300383387385\n",
      "[EPOCH #0, step #112] loss: 4.759316452836568\n",
      "[EPOCH #0, step #114] loss: 4.75828104019165\n",
      "[EPOCH #0, step #116] loss: 4.7566986573048124\n",
      "[EPOCH #0, step #118] loss: 4.752424464506261\n",
      "[EPOCH #0, step #120] loss: 4.751301698448244\n",
      "[EPOCH #0, step #122] loss: 4.750091591501624\n",
      "[EPOCH #0, step #124] loss: 4.750464351654053\n",
      "[EPOCH #0, step #126] loss: 4.751509005629171\n",
      "[EPOCH #0, step #128] loss: 4.753229281699011\n",
      "[EPOCH #0, step #130] loss: 4.753740656466884\n",
      "[EPOCH #0, step #132] loss: 4.756953117542697\n",
      "[EPOCH #0, step #134] loss: 4.756702578509295\n",
      "[EPOCH #0, step #136] loss: 4.757462080377731\n",
      "[EPOCH #0, step #138] loss: 4.75573597709052\n",
      "[EPOCH #0, step #140] loss: 4.756132754873722\n",
      "[EPOCH #0, step #142] loss: 4.755886518038237\n",
      "[EPOCH #0, step #144] loss: 4.7558706020486765\n",
      "[EPOCH #0, step #146] loss: 4.754535639367136\n",
      "[EPOCH #0, step #148] loss: 4.751641180691303\n",
      "[EPOCH #0, step #150] loss: 4.753700584765301\n",
      "[EPOCH #0, step #152] loss: 4.75346979128769\n",
      "[EPOCH #0, step #154] loss: 4.751504341248543\n",
      "[EPOCH #0, step #156] loss: 4.751103349552033\n",
      "[EPOCH #0, step #158] loss: 4.751200199127197\n",
      "[EPOCH #0, step #160] loss: 4.7507552597093285\n",
      "[EPOCH #0, step #162] loss: 4.751277821195638\n",
      "[EPOCH #0, step #164] loss: 4.749351868484959\n",
      "[EPOCH #0, step #166] loss: 4.7492061089612765\n",
      "[EPOCH #0, step #168] loss: 4.747788671911115\n",
      "[EPOCH #0, step #170] loss: 4.74537822377612\n",
      "[EPOCH #0, step #172] loss: 4.7451502739349545\n",
      "[EPOCH #0, step #174] loss: 4.74479006086077\n",
      "[EPOCH #0, step #176] loss: 4.745922487334343\n",
      "[EPOCH #0, step #178] loss: 4.746987358817841\n",
      "[EPOCH #0, step #180] loss: 4.747286082631317\n",
      "[EPOCH #0, step #182] loss: 4.7476276048545625\n",
      "[EPOCH #0, step #184] loss: 4.750232913043048\n",
      "[EPOCH #0, step #186] loss: 4.748842412775213\n",
      "[EPOCH #0, step #188] loss: 4.748199341789125\n",
      "[EPOCH #0, step #190] loss: 4.748552507130888\n",
      "[EPOCH #0, step #192] loss: 4.750205153628334\n",
      "[EPOCH #0, step #194] loss: 4.7498489404336\n",
      "[EPOCH #0, step #196] loss: 4.750374314748696\n",
      "[EPOCH #0, step #198] loss: 4.751146999435808\n",
      "[EPOCH #0, step #200] loss: 4.752521678582946\n",
      "[EPOCH #0, step #202] loss: 4.75424072895144\n",
      "[EPOCH #0, step #204] loss: 4.7537352050223\n",
      "[EPOCH #0, step #206] loss: 4.753702847853951\n",
      "[EPOCH #0, step #208] loss: 4.753074125810103\n",
      "[EPOCH #0, step #210] loss: 4.752640755820614\n",
      "[EPOCH #0, step #212] loss: 4.751351878116948\n",
      "[EPOCH #0, step #214] loss: 4.751922181595203\n",
      "[EPOCH #0, step #216] loss: 4.752182692426691\n",
      "[EPOCH #0, step #218] loss: 4.751924932819524\n",
      "[EPOCH #0, step #220] loss: 4.751414516932285\n",
      "[EPOCH #0, step #222] loss: 4.7524627848056396\n",
      "[EPOCH #0, step #224] loss: 4.752483355204264\n",
      "[EPOCH #0, step #226] loss: 4.751602364006547\n",
      "[EPOCH #0, step #228] loss: 4.7515789535888935\n",
      "[EPOCH #0, step #230] loss: 4.751828777841675\n",
      "[EPOCH #0, step #232] loss: 4.752037025828218\n",
      "[EPOCH #0, step #234] loss: 4.752834662985294\n",
      "[EPOCH #0, step #236] loss: 4.751592084828308\n",
      "[EPOCH #0, step #238] loss: 4.75059593072995\n",
      "[EPOCH #0, step #240] loss: 4.7516883240695815\n",
      "[EPOCH #0, step #242] loss: 4.752150200031422\n",
      "[EPOCH #0, step #244] loss: 4.751615158392458\n",
      "[EPOCH #0, step #246] loss: 4.751295925634593\n",
      "[EPOCH #0, step #248] loss: 4.750383124294051\n",
      "[EPOCH #0, step #250] loss: 4.750307132523373\n",
      "[EPOCH #0, step #252] loss: 4.750629017946748\n",
      "[EPOCH #0, step #254] loss: 4.750517820844463\n",
      "[EPOCH #0, step #256] loss: 4.749685784722117\n",
      "[EPOCH #0, step #258] loss: 4.750329435562075\n",
      "[EPOCH #0, step #260] loss: 4.749993558130958\n",
      "[EPOCH #0, step #262] loss: 4.750974947055483\n",
      "[EPOCH #0, step #264] loss: 4.750727347607882\n",
      "[EPOCH #0, step #266] loss: 4.750996228907439\n",
      "[EPOCH #0, step #268] loss: 4.751208314222031\n",
      "[EPOCH #0, step #270] loss: 4.750459396531221\n",
      "[EPOCH #0, step #272] loss: 4.751458692026662\n",
      "[EPOCH #0, step #274] loss: 4.750581793351607\n",
      "[EPOCH #0, step #276] loss: 4.750441353243611\n",
      "[EPOCH #0, step #278] loss: 4.750409415118583\n",
      "[EPOCH #0, step #280] loss: 4.751022640920619\n",
      "[EPOCH #0, step #282] loss: 4.751575286312575\n",
      "[EPOCH #0, step #284] loss: 4.751109554893092\n",
      "[EPOCH #0, step #286] loss: 4.750584580757061\n",
      "[EPOCH #0, step #288] loss: 4.750355060537793\n",
      "[EPOCH #0, step #290] loss: 4.749545434905901\n",
      "[EPOCH #0, step #292] loss: 4.749547245559432\n",
      "[EPOCH #0, step #294] loss: 4.749829686698267\n",
      "[EPOCH #0, step #296] loss: 4.750263745535905\n",
      "[EPOCH #0, step #298] loss: 4.750027637417898\n",
      "[EPOCH #0, step #300] loss: 4.750314048754417\n",
      "[EPOCH #0, step #302] loss: 4.749117843388724\n",
      "[EPOCH #0, step #304] loss: 4.748978330268234\n",
      "[EPOCH #0, step #306] loss: 4.748776993456415\n",
      "[EPOCH #0, step #308] loss: 4.7492670303023745\n",
      "[EPOCH #0, step #310] loss: 4.749258763537147\n",
      "[EPOCH #0, step #312] loss: 4.749671282478795\n",
      "[EPOCH #0, step #314] loss: 4.7484372290353924\n",
      "[EPOCH #0, step #316] loss: 4.748863773767129\n",
      "[EPOCH #0, step #318] loss: 4.748763607587186\n",
      "[EPOCH #0, step #320] loss: 4.7485084192032385\n",
      "[EPOCH #0, step #322] loss: 4.748017427721998\n",
      "[EPOCH #0, step #324] loss: 4.748011719630314\n",
      "[EPOCH #0, step #326] loss: 4.747628505076837\n",
      "[EPOCH #0, step #328] loss: 4.747215676814951\n",
      "[EPOCH #0, step #330] loss: 4.747544110001394\n",
      "[EPOCH #0, step #332] loss: 4.747745019895536\n",
      "[EPOCH #0, step #334] loss: 4.748113737533342\n",
      "[EPOCH #0, step #336] loss: 4.748753890085645\n",
      "[EPOCH #0, step #338] loss: 4.749429649308016\n",
      "[EPOCH #0, step #340] loss: 4.749400652049224\n",
      "[EPOCH #0, step #342] loss: 4.749815832421661\n",
      "[EPOCH #0, step #344] loss: 4.749071462603583\n",
      "[EPOCH #0, step #346] loss: 4.749818943419442\n",
      "[EPOCH #0, step #348] loss: 4.751150837600402\n",
      "[EPOCH #0, step #350] loss: 4.75067116256453\n",
      "[EPOCH #0, step #352] loss: 4.749976766008156\n",
      "[EPOCH #0, step #354] loss: 4.749901756770174\n",
      "[EPOCH #0, step #356] loss: 4.750550012294652\n",
      "[EPOCH #0, step #358] loss: 4.751146562252204\n",
      "[EPOCH #0, step #360] loss: 4.751478528051825\n",
      "[EPOCH #0, step #362] loss: 4.750865973060124\n",
      "[EPOCH #0, step #364] loss: 4.750743927367746\n",
      "[EPOCH #0, step #366] loss: 4.750456046018678\n",
      "[EPOCH #0, step #368] loss: 4.750086222237687\n",
      "[EPOCH #0, step #370] loss: 4.750157962912176\n",
      "[EPOCH #0, step #372] loss: 4.751079987584745\n",
      "[EPOCH #0, step #374] loss: 4.7509915669759115\n",
      "[EPOCH #0, step #376] loss: 4.7511990177852725\n",
      "[EPOCH #0, step #378] loss: 4.751366086874285\n",
      "[EPOCH #0, step #380] loss: 4.75171526711131\n",
      "[EPOCH #0, step #382] loss: 4.751709821018787\n",
      "[EPOCH #0, step #384] loss: 4.751667265458541\n",
      "[EPOCH #0, step #386] loss: 4.751887363364839\n",
      "[EPOCH #0, step #388] loss: 4.751750551336529\n",
      "[EPOCH #0, step #390] loss: 4.750326487109485\n",
      "[EPOCH #0, step #392] loss: 4.750521748423879\n",
      "[EPOCH #0, step #394] loss: 4.750672613216352\n",
      "[EPOCH #0, step #396] loss: 4.751399216784038\n",
      "[EPOCH #0, step #398] loss: 4.751483669854645\n",
      "[EPOCH #0, step #400] loss: 4.75164108442844\n",
      "[EPOCH #0, step #402] loss: 4.7511195994488356\n",
      "[EPOCH #0, step #404] loss: 4.751457325028785\n",
      "[EPOCH #0, step #406] loss: 4.751673017731463\n",
      "[EPOCH #0, step #408] loss: 4.750305146634725\n",
      "[EPOCH #0, step #410] loss: 4.750095606430314\n",
      "[EPOCH #0, step #412] loss: 4.749444035583201\n",
      "[EPOCH #0, step #414] loss: 4.749204886103251\n",
      "[EPOCH #0, step #416] loss: 4.75002606138051\n",
      "[EPOCH #0, step #418] loss: 4.749920956559284\n",
      "[EPOCH #0, step #420] loss: 4.750270993012997\n",
      "[EPOCH #0, step #422] loss: 4.750594930445894\n",
      "[EPOCH #0, step #424] loss: 4.750571423698874\n",
      "[EPOCH #0, step #426] loss: 4.750599876779025\n",
      "[EPOCH #0, step #428] loss: 4.750118065547277\n",
      "[EPOCH #0, step #430] loss: 4.750211996711323\n",
      "[EPOCH #0, step #432] loss: 4.750426420163337\n",
      "[EPOCH #0, step #434] loss: 4.750627691992398\n",
      "[EPOCH #0, step #436] loss: 4.750420503812742\n",
      "[EPOCH #0, step #438] loss: 4.75081654072894\n",
      "[EPOCH #0, step #440] loss: 4.750888328162992\n",
      "[EPOCH #0, step #442] loss: 4.751025633672021\n",
      "[EPOCH #0, step #444] loss: 4.751131858182757\n",
      "[EPOCH #0, step #446] loss: 4.751487377772662\n",
      "[EPOCH #0, step #448] loss: 4.751421882739843\n",
      "[EPOCH #0, step #450] loss: 4.750987543499391\n",
      "[EPOCH #0, step #452] loss: 4.75096794364205\n",
      "[EPOCH #0, step #454] loss: 4.751316245571598\n",
      "[EPOCH #0, step #456] loss: 4.7508380042645815\n",
      "[EPOCH #0, step #458] loss: 4.750981892895335\n",
      "[EPOCH #0, step #460] loss: 4.750690878084063\n",
      "[EPOCH #0, step #462] loss: 4.75107008437616\n",
      "[EPOCH #0, step #464] loss: 4.751126164262013\n",
      "[EPOCH #0, step #466] loss: 4.750600488078671\n",
      "[EPOCH #0, step #468] loss: 4.751089907404202\n",
      "[EPOCH #0, step #470] loss: 4.751485708159753\n",
      "[EPOCH #0, step #472] loss: 4.751456188097061\n",
      "[EPOCH #0, step #474] loss: 4.751459704951236\n",
      "[EPOCH #0, step #476] loss: 4.751693195766872\n",
      "[EPOCH #0, step #478] loss: 4.751739628379679\n",
      "[EPOCH #0, step #480] loss: 4.752061749694253\n",
      "[EPOCH #0, step #482] loss: 4.751853812555349\n",
      "[EPOCH #0, step #484] loss: 4.7511601418564\n",
      "[EPOCH #0, step #486] loss: 4.7509235611196905\n",
      "[EPOCH #0, step #488] loss: 4.751223762098753\n",
      "[EPOCH #0, step #490] loss: 4.751536735449206\n",
      "[EPOCH #0, step #492] loss: 4.75194484273512\n",
      "[EPOCH #0, step #494] loss: 4.75133039301092\n",
      "[EPOCH #0, step #496] loss: 4.75149821851335\n",
      "[EPOCH #0, step #498] loss: 4.751301910690889\n",
      "[EPOCH #0, step #500] loss: 4.751694084403519\n",
      "[EPOCH #0, step #502] loss: 4.752068186849059\n",
      "[EPOCH #0, step #504] loss: 4.752566893738095\n",
      "[EPOCH #0, step #506] loss: 4.752547358149843\n",
      "[EPOCH #0, step #508] loss: 4.752280876069734\n",
      "[EPOCH #0, step #510] loss: 4.7522892895976625\n",
      "[EPOCH #0, step #512] loss: 4.752456855588024\n",
      "[EPOCH #0, step #514] loss: 4.752495322181183\n",
      "[EPOCH #0, step #516] loss: 4.752351131845028\n",
      "[EPOCH #0, step #518] loss: 4.752196909145583\n",
      "[EPOCH #0, step #520] loss: 4.752298282799016\n",
      "[EPOCH #0, step #522] loss: 4.7517935513994205\n",
      "[EPOCH #0, step #524] loss: 4.752113599323091\n",
      "[EPOCH #0, step #526] loss: 4.751698325662052\n",
      "[EPOCH #0, step #528] loss: 4.752105975196132\n",
      "[EPOCH #0, step #530] loss: 4.752102303190644\n",
      "[EPOCH #0, step #532] loss: 4.752212802643624\n",
      "[EPOCH #0, step #534] loss: 4.751943000900411\n",
      "[EPOCH #0, step #536] loss: 4.7514606241407344\n",
      "[EPOCH #0, step #538] loss: 4.7512392245772155\n",
      "[EPOCH #0, step #540] loss: 4.750963103528825\n",
      "[EPOCH #0, step #542] loss: 4.751176368685278\n",
      "[EPOCH #0, step #544] loss: 4.751680095917588\n",
      "[EPOCH #0, step #546] loss: 4.752268638645709\n",
      "[EPOCH #0, step #548] loss: 4.752218677263659\n",
      "[EPOCH #0, step #550] loss: 4.752632964111716\n",
      "[EPOCH #0, step #552] loss: 4.7524745054745114\n",
      "[EPOCH #0, step #554] loss: 4.752501959414095\n",
      "[EPOCH #0, step #556] loss: 4.752811492666522\n",
      "[EPOCH #0, step #558] loss: 4.753300142202906\n",
      "[EPOCH #0, step #560] loss: 4.75268622005687\n",
      "[EPOCH #0, step #562] loss: 4.752775329467882\n",
      "[EPOCH #0, step #564] loss: 4.753100161425835\n",
      "[EPOCH #0, step #566] loss: 4.752964935605488\n",
      "[EPOCH #0, step #568] loss: 4.7528140557461755\n",
      "[EPOCH #0, step #570] loss: 4.752638328305059\n",
      "[EPOCH #0, step #572] loss: 4.752667422069929\n",
      "[EPOCH #0, step #574] loss: 4.752689884019935\n",
      "[EPOCH #0, step #576] loss: 4.753218088860735\n",
      "[EPOCH #0, step #578] loss: 4.7529325304047845\n",
      "[EPOCH #0, step #580] loss: 4.75324876870633\n",
      "[EPOCH #0, step #582] loss: 4.75303167012707\n",
      "[EPOCH #0, step #584] loss: 4.753556006178897\n",
      "[EPOCH #0, step #586] loss: 4.753514159070979\n",
      "[EPOCH #0, step #588] loss: 4.754062884125118\n",
      "[EPOCH #0, step #590] loss: 4.753505552682376\n",
      "[EPOCH #0, step #592] loss: 4.7533383626760966\n",
      "[EPOCH #0, step #594] loss: 4.7536761764718705\n",
      "[EPOCH #0, step #596] loss: 4.75321161088033\n",
      "[EPOCH #0, step #598] loss: 4.753329799250888\n",
      "[EPOCH #0, step #600] loss: 4.753418549523377\n",
      "[EPOCH #0, step #602] loss: 4.7538216410584715\n",
      "[EPOCH #0, step #604] loss: 4.754061276459497\n",
      "[EPOCH #0, step #606] loss: 4.753771990095764\n",
      "[EPOCH #0, step #608] loss: 4.753723361417774\n",
      "[EPOCH #0, step #610] loss: 4.753869930773047\n",
      "[EPOCH #0, step #612] loss: 4.754217808242723\n",
      "[EPOCH #0, step #614] loss: 4.754606808298003\n",
      "[EPOCH #0, step #616] loss: 4.7545146092023804\n",
      "[EPOCH #0, step #618] loss: 4.754880374390782\n",
      "[EPOCH #0, step #620] loss: 4.754966264377662\n",
      "[EPOCH #0, step #622] loss: 4.755546037113685\n",
      "[EPOCH #0, step #624] loss: 4.755205040740967\n",
      "[EPOCH #0, step #626] loss: 4.755020169740279\n",
      "[EPOCH #0, step #628] loss: 4.754936457059341\n",
      "[EPOCH #0, step #630] loss: 4.754906825142693\n",
      "[EPOCH #0, step #632] loss: 4.754971849974862\n",
      "[EPOCH #0, step #634] loss: 4.754986750985694\n",
      "[EPOCH #0, step #636] loss: 4.754966928970122\n",
      "[EPOCH #0, step #638] loss: 4.754562545830096\n",
      "[EPOCH #0, step #640] loss: 4.754071545117359\n",
      "[EPOCH #0, step #642] loss: 4.7538104109222745\n",
      "[EPOCH #0, step #644] loss: 4.753617314035578\n",
      "[EPOCH #0, step #646] loss: 4.75344780165777\n",
      "[EPOCH #0, step #648] loss: 4.753277594209268\n",
      "[EPOCH #0, step #650] loss: 4.7535290227324545\n",
      "[EPOCH #0, step #652] loss: 4.75358131359036\n",
      "[EPOCH #0, step #654] loss: 4.7536192974061455\n",
      "[EPOCH #0, step #656] loss: 4.753449625076224\n",
      "[EPOCH #0, step #658] loss: 4.753261330276049\n",
      "[EPOCH #0, step #660] loss: 4.753307074894523\n",
      "[EPOCH #0, step #662] loss: 4.753494226734743\n",
      "[EPOCH #0, step #664] loss: 4.753513112462553\n",
      "[EPOCH #0, step #666] loss: 4.753678349004514\n",
      "[EPOCH #0, step #668] loss: 4.7535999722901305\n",
      "[EPOCH #0, step #670] loss: 4.754038790832156\n",
      "[EPOCH #0, step #672] loss: 4.754373279050059\n",
      "[EPOCH #0, step #674] loss: 4.754643659944888\n",
      "[EPOCH #0, step #676] loss: 4.755151660614915\n",
      "[EPOCH #0, step #678] loss: 4.755122838561068\n",
      "[EPOCH #0, step #680] loss: 4.754747737346767\n",
      "[EPOCH #0, step #682] loss: 4.754915708678708\n",
      "[EPOCH #0, step #684] loss: 4.754625690070382\n",
      "[EPOCH #0, step #686] loss: 4.754349223390937\n",
      "[EPOCH #0, step #688] loss: 4.754483103233083\n",
      "[EPOCH #0, step #690] loss: 4.75433756473613\n",
      "[EPOCH #0, step #692] loss: 4.75469654944724\n",
      "[EPOCH #0, step #694] loss: 4.754990878208078\n",
      "[EPOCH #0, step #696] loss: 4.754909464755394\n",
      "[EPOCH #0, step #698] loss: 4.75487027420677\n",
      "[EPOCH #0, step #700] loss: 4.755191283287234\n",
      "[EPOCH #0, step #702] loss: 4.755605706449593\n",
      "[EPOCH #0, step #704] loss: 4.755666301943732\n",
      "[EPOCH #0, step #706] loss: 4.755887679028545\n",
      "[EPOCH #0, step #708] loss: 4.756106392116574\n",
      "[EPOCH #0, step #710] loss: 4.756064004535917\n",
      "[EPOCH #0, step #712] loss: 4.7563654804631\n",
      "[EPOCH #0, step #714] loss: 4.756489513637303\n",
      "[EPOCH #0, step #716] loss: 4.756489688548749\n",
      "[EPOCH #0, step #718] loss: 4.756539876669936\n",
      "[EPOCH #0, step #720] loss: 4.756769585708639\n",
      "[EPOCH #0, step #722] loss: 4.756847566259022\n",
      "[EPOCH #0, step #724] loss: 4.756435978988121\n",
      "[EPOCH #0, step #726] loss: 4.7568005689072645\n",
      "[EPOCH #0, step #728] loss: 4.7568758106362505\n",
      "[EPOCH #0, step #730] loss: 4.756732970679996\n",
      "[EPOCH #0, step #732] loss: 4.756644380369095\n",
      "[EPOCH #0, step #734] loss: 4.757020064113902\n",
      "[EPOCH #0, step #736] loss: 4.7567645594417\n",
      "[EPOCH #0, step #738] loss: 4.7568178247857\n",
      "[EPOCH #0, step #740] loss: 4.756841614500553\n",
      "[EPOCH #0, step #742] loss: 4.75634199196365\n",
      "[EPOCH #0, step #744] loss: 4.75648022082028\n",
      "[EPOCH #0, step #746] loss: 4.7565373512635745\n",
      "[EPOCH #0, step #748] loss: 4.756740956503814\n",
      "[EPOCH #0, step #750] loss: 4.756630117185265\n",
      "[EPOCH #0, step #752] loss: 4.756808730552238\n",
      "[EPOCH #0, step #754] loss: 4.756998583812587\n",
      "[EPOCH #0, step #756] loss: 4.756958029229228\n",
      "[EPOCH #0, step #758] loss: 4.757106067948976\n",
      "[EPOCH #0, step #760] loss: 4.757450108772509\n",
      "[EPOCH #0, step #762] loss: 4.757477672196934\n",
      "[EPOCH #0, step #764] loss: 4.757106646992802\n",
      "[EPOCH #0, step #766] loss: 4.75715369444627\n",
      "[EPOCH #0, step #768] loss: 4.757062796033406\n",
      "[EPOCH #0, step #770] loss: 4.756855855192192\n",
      "[EPOCH #0, step #772] loss: 4.757105069820508\n",
      "[EPOCH #0, step #774] loss: 4.757454084580944\n",
      "[EPOCH #0, step #776] loss: 4.757689022496247\n",
      "[EPOCH #0, step #778] loss: 4.757700248615427\n",
      "[EPOCH #0, step #780] loss: 4.757417145825531\n",
      "[EPOCH #0, step #782] loss: 4.757412524363395\n",
      "[EPOCH #0, step #784] loss: 4.7572769171113425\n",
      "[EPOCH #0, step #786] loss: 4.757235312552737\n",
      "[EPOCH #0, step #788] loss: 4.757295014438218\n",
      "[EPOCH #0, step #790] loss: 4.756933949539243\n",
      "[EPOCH #0, step #792] loss: 4.756758161330734\n",
      "[EPOCH #0, step #794] loss: 4.756382236240795\n",
      "[EPOCH #0, step #796] loss: 4.756127198936051\n",
      "[EPOCH #0, step #798] loss: 4.7561123433787476\n",
      "[EPOCH #0, step #800] loss: 4.755776233292102\n",
      "[EPOCH #0, step #802] loss: 4.755879555366108\n",
      "[EPOCH #0, step #804] loss: 4.756200696814875\n",
      "[EPOCH #0, step #806] loss: 4.7563611772719225\n",
      "[EPOCH #0, step #808] loss: 4.756083315765607\n",
      "[EPOCH #0, step #810] loss: 4.755801057403978\n",
      "[EPOCH #0, step #812] loss: 4.755672611433641\n",
      "[EPOCH #0, step #814] loss: 4.755759639974021\n",
      "[EPOCH #0, step #816] loss: 4.755798692948377\n",
      "[EPOCH #0, step #818] loss: 4.756110692635561\n",
      "[EPOCH #0, step #820] loss: 4.756269366674388\n",
      "[EPOCH #0, step #822] loss: 4.756134262641174\n",
      "[EPOCH #0, step #824] loss: 4.75609184438532\n",
      "[EPOCH #0, step #826] loss: 4.756444864722809\n",
      "[EPOCH #0, step #828] loss: 4.756635598020473\n",
      "[EPOCH #0, step #830] loss: 4.756461991586697\n",
      "[EPOCH #0, step #832] loss: 4.756106978848058\n",
      "[EPOCH #0, step #834] loss: 4.756074760916704\n",
      "[EPOCH #0, step #836] loss: 4.755936851638192\n",
      "[EPOCH #0, step #838] loss: 4.75624877022617\n",
      "[EPOCH #0, step #840] loss: 4.7562844863828095\n",
      "[EPOCH #0, step #842] loss: 4.756267032872174\n",
      "[EPOCH #0, step #844] loss: 4.756187998754738\n",
      "[EPOCH #0, step #846] loss: 4.756258560887757\n",
      "[EPOCH #0, step #848] loss: 4.756254084399507\n",
      "[EPOCH #0, step #850] loss: 4.756188164586606\n",
      "[EPOCH #0, step #852] loss: 4.756229778526816\n",
      "[EPOCH #0, step #854] loss: 4.756503186030694\n",
      "[EPOCH #0, step #856] loss: 4.756630409000357\n",
      "[EPOCH #0, step #858] loss: 4.756666489335794\n",
      "[EPOCH #0, step #860] loss: 4.756789088941479\n",
      "[EPOCH #0, step #862] loss: 4.756807892513165\n",
      "[EPOCH #0, step #864] loss: 4.756706763140728\n",
      "[EPOCH #0, step #866] loss: 4.756753000833584\n",
      "[EPOCH #0, step #868] loss: 4.756492468786734\n",
      "[EPOCH #0, step #870] loss: 4.756536481027351\n",
      "[EPOCH #0, step #872] loss: 4.756264790363464\n",
      "[EPOCH #0, step #874] loss: 4.756105299813407\n",
      "[EPOCH #0, step #876] loss: 4.755955246998211\n",
      "[EPOCH #0, step #878] loss: 4.755817527250218\n",
      "[EPOCH #0, step #880] loss: 4.755782492720986\n",
      "[EPOCH #0, step #882] loss: 4.755914263655078\n",
      "[EPOCH #0, step #884] loss: 4.755442786351436\n",
      "[EPOCH #0, step #886] loss: 4.755579839833291\n",
      "[EPOCH #0, step #888] loss: 4.755751228439794\n",
      "[EPOCH #0, step #890] loss: 4.7557869332139076\n",
      "[EPOCH #0, step #892] loss: 4.755685131867549\n",
      "[EPOCH #0, step #894] loss: 4.755680219021589\n",
      "[EPOCH #0, step #896] loss: 4.755787714933738\n",
      "[EPOCH #0, step #898] loss: 4.755705587325558\n",
      "[EPOCH #0, step #900] loss: 4.7558247262444535\n",
      "[EPOCH #0, step #902] loss: 4.7554716926608505\n",
      "[EPOCH #0, step #904] loss: 4.755534812758641\n",
      "[EPOCH #0, step #906] loss: 4.754966808916321\n",
      "[EPOCH #0, step #908] loss: 4.755091159912881\n",
      "[EPOCH #0, step #910] loss: 4.7550094412919846\n",
      "[EPOCH #0, step #912] loss: 4.754962206670253\n",
      "[EPOCH #0, step #914] loss: 4.754706515640509\n",
      "[EPOCH #0, step #916] loss: 4.754855446103095\n",
      "[EPOCH #0, step #918] loss: 4.754729950129661\n",
      "[EPOCH #0, step #920] loss: 4.754843390855157\n",
      "[EPOCH #0, step #922] loss: 4.755329728772296\n",
      "[EPOCH #0, step #924] loss: 4.755693337208516\n",
      "[EPOCH #0, step #926] loss: 4.755723302546957\n",
      "[EPOCH #0, step #928] loss: 4.755648564727751\n",
      "[EPOCH #0, step #930] loss: 4.755659904951184\n",
      "[EPOCH #0, step #932] loss: 4.755926066272895\n",
      "[EPOCH #0, step #934] loss: 4.755890171540612\n",
      "[EPOCH #0, step #936] loss: 4.755807035378992\n",
      "[EPOCH #0, step #938] loss: 4.755703980422503\n",
      "[EPOCH #0, step #940] loss: 4.755741053002547\n",
      "[EPOCH #0, step #942] loss: 4.755329060782051\n",
      "[EPOCH #0, step #944] loss: 4.755275144021978\n",
      "[EPOCH #0, step #946] loss: 4.755512165041383\n",
      "[EPOCH #0, step #948] loss: 4.755153742429454\n",
      "[EPOCH #0, step #950] loss: 4.755507836206478\n",
      "[EPOCH #0, step #952] loss: 4.755366100719568\n",
      "[EPOCH #0, step #954] loss: 4.755481965629218\n",
      "[EPOCH #0, step #956] loss: 4.75544485851515\n",
      "[EPOCH #0, step #958] loss: 4.755486432156051\n",
      "[EPOCH #0, step #960] loss: 4.7556121927394335\n",
      "[EPOCH #0, step #962] loss: 4.7557395967739025\n",
      "[EPOCH #0, step #964] loss: 4.755689402071305\n",
      "[EPOCH #0, step #966] loss: 4.75575353786149\n",
      "[EPOCH #0, step #968] loss: 4.755717189442386\n",
      "[EPOCH #0, step #970] loss: 4.755877003733579\n",
      "[EPOCH #0, step #972] loss: 4.755831858114496\n",
      "[EPOCH #0, step #974] loss: 4.755846232878856\n",
      "[EPOCH #0, step #976] loss: 4.755618193498531\n",
      "[EPOCH #0, step #978] loss: 4.75551599818182\n",
      "[EPOCH #0, step #980] loss: 4.755385865980454\n",
      "[EPOCH #0, step #982] loss: 4.755424913773095\n",
      "[EPOCH #0, step #984] loss: 4.755383237364328\n",
      "[EPOCH #0, step #986] loss: 4.755480649381906\n",
      "[EPOCH #0, step #988] loss: 4.755278097968492\n",
      "[EPOCH #0, step #990] loss: 4.755431747821458\n",
      "[EPOCH #0, step #992] loss: 4.75535611875828\n",
      "[EPOCH #0, step #994] loss: 4.7551019984873095\n",
      "[EPOCH #0, step #996] loss: 4.754695004182929\n",
      "[EPOCH #0, step #998] loss: 4.754842435991442\n",
      "[EPOCH #0, step #1000] loss: 4.7549000072193435\n",
      "[EPOCH #0, step #1002] loss: 4.75494492636364\n",
      "[EPOCH #0, step #1004] loss: 4.754689572462395\n",
      "[EPOCH #0, step #1006] loss: 4.754722602791204\n",
      "[EPOCH #0, step #1008] loss: 4.754763977496901\n",
      "[EPOCH #0, step #1010] loss: 4.754698595354513\n",
      "[EPOCH #0, step #1012] loss: 4.754754836090932\n",
      "[EPOCH #0, step #1014] loss: 4.7546626631262265\n",
      "[EPOCH #0, step #1016] loss: 4.754959343691725\n",
      "[EPOCH #0, step #1018] loss: 4.754938441941969\n",
      "[EPOCH #0, step #1020] loss: 4.754984709004581\n",
      "[EPOCH #0, step #1022] loss: 4.754973377067439\n",
      "[EPOCH #0, step #1024] loss: 4.7549784460300355\n",
      "[EPOCH #0, step #1026] loss: 4.754709591071585\n",
      "[EPOCH #0, step #1028] loss: 4.754536701013326\n",
      "[EPOCH #0, step #1030] loss: 4.754303427602803\n",
      "[EPOCH #0, step #1032] loss: 4.754333134196043\n",
      "[EPOCH #0, step #1034] loss: 4.754288569740627\n",
      "[EPOCH #0, step #1036] loss: 4.754301931497102\n",
      "[EPOCH #0, step #1038] loss: 4.754124133832415\n",
      "[EPOCH #0, step #1040] loss: 4.754134575754948\n",
      "[EPOCH #0, step #1042] loss: 4.753952881290951\n",
      "[EPOCH #0, step #1044] loss: 4.753851737245989\n",
      "[EPOCH #0, step #1046] loss: 4.753514355893577\n",
      "[EPOCH #0, step #1048] loss: 4.753502887129443\n",
      "[EPOCH #0, step #1050] loss: 4.753588691878387\n",
      "[EPOCH #0, step #1052] loss: 4.753828811736075\n",
      "[EPOCH #0, step #1054] loss: 4.753672213802971\n",
      "[EPOCH #0, step #1056] loss: 4.753611484336312\n",
      "[EPOCH #0, step #1058] loss: 4.753359964368927\n",
      "[EPOCH #0, step #1060] loss: 4.753145676306348\n",
      "[EPOCH #0, step #1062] loss: 4.7532061749993915\n",
      "[EPOCH #0, step #1064] loss: 4.753205742634518\n",
      "[EPOCH #0, step #1066] loss: 4.753278514214957\n",
      "[EPOCH #0, step #1068] loss: 4.753271595800559\n",
      "[EPOCH #0, step #1070] loss: 4.753283579921633\n",
      "[EPOCH #0, step #1072] loss: 4.75331060306446\n",
      "[EPOCH #0, step #1074] loss: 4.753015530608421\n",
      "[EPOCH #0, step #1076] loss: 4.753109646374801\n",
      "[EPOCH #0, step #1078] loss: 4.7529885099373885\n",
      "[EPOCH #0, step #1080] loss: 4.752645912488007\n",
      "[EPOCH #0, step #1082] loss: 4.75272175655911\n",
      "[EPOCH #0, step #1084] loss: 4.7526636334608225\n",
      "[EPOCH #0, step #1086] loss: 4.752619778002306\n",
      "[EPOCH #0, step #1088] loss: 4.7528703517055595\n",
      "[EPOCH #0, step #1090] loss: 4.752668169198181\n",
      "[EPOCH #0, step #1092] loss: 4.7526154823512625\n",
      "[EPOCH #0, step #1094] loss: 4.752323766821596\n",
      "[EPOCH #0, step #1096] loss: 4.75232696185464\n",
      "[EPOCH #0, step #1098] loss: 4.752365176519336\n",
      "[EPOCH #0, step #1100] loss: 4.752445599905044\n",
      "[EPOCH #0, step #1102] loss: 4.752438554737855\n",
      "[EPOCH #0, step #1104] loss: 4.752406156980074\n",
      "[EPOCH #0, step #1106] loss: 4.752303344878186\n",
      "[EPOCH #0, step #1108] loss: 4.752338162190856\n",
      "[EPOCH #0, step #1110] loss: 4.752452817722873\n",
      "[EPOCH #0, step #1112] loss: 4.752209211831037\n",
      "[EPOCH #0, step #1114] loss: 4.752086353302002\n",
      "[EPOCH #0, step #1116] loss: 4.752264443741596\n",
      "[EPOCH #0, step #1118] loss: 4.752042524083796\n",
      "[EPOCH #0, step #1120] loss: 4.752045417448754\n",
      "[EPOCH #0, step #1122] loss: 4.752083537627413\n",
      "[EPOCH #0, step #1124] loss: 4.752351518842909\n",
      "[EPOCH #0, step #1126] loss: 4.752362285553087\n",
      "[EPOCH #0, step #1128] loss: 4.752516928133023\n",
      "[EPOCH #0, step #1130] loss: 4.75254311970543\n",
      "[EPOCH #0, step #1132] loss: 4.752506339434281\n",
      "[EPOCH #0, step #1134] loss: 4.7525986583222375\n",
      "[EPOCH #0, step #1136] loss: 4.752602945426733\n",
      "[EPOCH #0, step #1138] loss: 4.752603017205832\n",
      "[EPOCH #0, step #1140] loss: 4.752458312864579\n",
      "[EPOCH #0, step #1142] loss: 4.752255863196372\n",
      "[EPOCH #0, step #1144] loss: 4.752223925194886\n",
      "[EPOCH #0, step #1146] loss: 4.752324890446019\n",
      "[EPOCH #0, step #1148] loss: 4.752336510375233\n",
      "[EPOCH #0, step #1150] loss: 4.752554379371225\n",
      "[EPOCH #0, step #1152] loss: 4.752265067485138\n",
      "[EPOCH #0, step #1154] loss: 4.752445188745276\n",
      "[EPOCH #0, step #1156] loss: 4.752200575047974\n",
      "[EPOCH #0, step #1158] loss: 4.752156846784537\n",
      "[EPOCH #0, step #1160] loss: 4.752012795358768\n",
      "[EPOCH #0, step #1162] loss: 4.752380618737468\n",
      "[EPOCH #0, step #1164] loss: 4.752330316699114\n",
      "[EPOCH #0, step #1166] loss: 4.752350281729285\n",
      "[EPOCH #0, step #1168] loss: 4.752530332504938\n",
      "[EPOCH #0, step #1170] loss: 4.752399668950082\n",
      "[EPOCH #0, step #1172] loss: 4.752473342144276\n",
      "[EPOCH #0, step #1174] loss: 4.752781990538252\n",
      "[EPOCH #0, step #1176] loss: 4.752554428810372\n",
      "[EPOCH #0, step #1178] loss: 4.752565248827493\n",
      "[EPOCH #0, step #1180] loss: 4.75277811102096\n",
      "[EPOCH #0, step #1182] loss: 4.752422094143555\n",
      "[EPOCH #0, step #1184] loss: 4.752387102344368\n",
      "[EPOCH #0, step #1186] loss: 4.752436571595241\n",
      "[EPOCH #0, step #1188] loss: 4.752278954768201\n",
      "[EPOCH #0, step #1190] loss: 4.752354727584109\n",
      "[EPOCH #0, step #1192] loss: 4.752311301651065\n",
      "[EPOCH #0, step #1194] loss: 4.752449723965952\n",
      "[EPOCH #0, step #1196] loss: 4.752518145562015\n",
      "[EPOCH #0, step #1198] loss: 4.752431204957301\n",
      "[EPOCH #0, step #1200] loss: 4.752521918675584\n",
      "[EPOCH #0, step #1202] loss: 4.75263716237107\n",
      "[EPOCH #0, step #1204] loss: 4.752477624901103\n",
      "[EPOCH #0, step #1206] loss: 4.752402245553941\n",
      "[EPOCH #0, step #1208] loss: 4.75238741873906\n",
      "[EPOCH #0, step #1210] loss: 4.752629055256296\n",
      "[EPOCH #0, step #1212] loss: 4.752630275040742\n",
      "[EPOCH #0, step #1214] loss: 4.752594604021237\n",
      "[EPOCH #0, step #1216] loss: 4.752446143276494\n",
      "[EPOCH #0, step #1218] loss: 4.7523356622315704\n",
      "[EPOCH #0, step #1220] loss: 4.752536185152896\n",
      "[EPOCH #0, step #1222] loss: 4.752492291363945\n",
      "[EPOCH #0, step #1224] loss: 4.752533582570601\n",
      "[EPOCH #0, step #1226] loss: 4.75226216522192\n",
      "[EPOCH #0, step #1228] loss: 4.752151249093288\n",
      "[EPOCH #0, step #1230] loss: 4.751950055578683\n",
      "[EPOCH #0, step #1232] loss: 4.752005305216744\n",
      "[EPOCH #0, step #1234] loss: 4.752182719195902\n",
      "[EPOCH #0, step #1236] loss: 4.752035394636199\n",
      "[EPOCH #0, step #1238] loss: 4.752079372929411\n",
      "[EPOCH #0, step #1240] loss: 4.75196872729625\n",
      "[EPOCH #0, step #1242] loss: 4.75200503810413\n",
      "[EPOCH #0, step #1244] loss: 4.752181339263916\n",
      "[EPOCH #0, step #1246] loss: 4.752011238333695\n",
      "[EPOCH #0, step #1248] loss: 4.752090727452567\n",
      "[EPOCH #0, step #1250] loss: 4.752031270643886\n",
      "[EPOCH #0, step #1252] loss: 4.7520388765327475\n",
      "[EPOCH #0, step #1254] loss: 4.752146994830127\n",
      "[EPOCH #0, step #1256] loss: 4.752014959436992\n",
      "[EPOCH #0, step #1258] loss: 4.751993160762893\n",
      "[EPOCH #0, step #1260] loss: 4.751888894165063\n",
      "[EPOCH #0, step #1262] loss: 4.751778624120065\n",
      "[EPOCH #0, step #1264] loss: 4.751676318202565\n",
      "[EPOCH #0, step #1266] loss: 4.751501075094294\n",
      "[EPOCH #0, step #1268] loss: 4.751188667483739\n",
      "[EPOCH #0, step #1270] loss: 4.7510749871668905\n",
      "[EPOCH #0, step #1272] loss: 4.7509911695242115\n",
      "[EPOCH #0, step #1274] loss: 4.751059072157916\n",
      "[EPOCH #0, step #1276] loss: 4.75111494684182\n",
      "[EPOCH #0, step #1278] loss: 4.751412341704678\n",
      "[EPOCH #0, step #1280] loss: 4.751849730251172\n",
      "[EPOCH #0, step #1282] loss: 4.751746931767333\n",
      "[EPOCH #0, step #1284] loss: 4.751535119527973\n",
      "[EPOCH #0, step #1286] loss: 4.751450549750339\n",
      "[EPOCH #0, step #1288] loss: 4.751496695658326\n",
      "[EPOCH #0, step #1290] loss: 4.751649999507724\n",
      "[EPOCH #0, step #1292] loss: 4.751407605589404\n",
      "[EPOCH #0, step #1294] loss: 4.751338864845659\n",
      "[EPOCH #0, step #1296] loss: 4.751391411195283\n",
      "[EPOCH #0, step #1298] loss: 4.751466949321932\n",
      "[EPOCH #0, step #1300] loss: 4.751484048815529\n",
      "[EPOCH #0, step #1302] loss: 4.751373874345195\n",
      "[EPOCH #0, step #1304] loss: 4.751288157495959\n",
      "[EPOCH #0, step #1306] loss: 4.751131141650923\n",
      "[EPOCH #0, step #1308] loss: 4.75124475043452\n",
      "[EPOCH #0, step #1310] loss: 4.7511532886804835\n",
      "[EPOCH #0, step #1312] loss: 4.75122866300073\n",
      "[EPOCH #0, step #1314] loss: 4.75128437557148\n",
      "[EPOCH #0, step #1316] loss: 4.751070065306458\n",
      "[EPOCH #0, step #1318] loss: 4.751114316742198\n",
      "[EPOCH #0, step #1320] loss: 4.751421992657854\n",
      "[EPOCH #0, step #1322] loss: 4.75163608091128\n",
      "[EPOCH #0, step #1324] loss: 4.751919963044941\n",
      "[EPOCH #0, step #1326] loss: 4.75193323402721\n",
      "[EPOCH #0, step #1328] loss: 4.751998762513929\n",
      "[EPOCH #0, step #1330] loss: 4.751979830030388\n",
      "[EPOCH #0, step #1332] loss: 4.75173785430725\n",
      "[EPOCH #0, step #1334] loss: 4.7515980674086435\n",
      "[EPOCH #0, step #1336] loss: 4.751908073339697\n",
      "[EPOCH #0, step #1338] loss: 4.751900305046441\n",
      "[EPOCH #0, step #1340] loss: 4.7517648088140865\n",
      "[EPOCH #0, step #1342] loss: 4.751673182887278\n",
      "[EPOCH #0, step #1344] loss: 4.751803222287543\n",
      "[EPOCH #0, step #1346] loss: 4.751689083592132\n",
      "[EPOCH #0, step #1348] loss: 4.751593512725265\n",
      "[EPOCH #0, step #1350] loss: 4.751600895520936\n",
      "[EPOCH #0, step #1352] loss: 4.75153168976439\n",
      "[EPOCH #0, step #1354] loss: 4.751652635947365\n",
      "[EPOCH #0, step #1356] loss: 4.751774110070203\n",
      "[EPOCH #0, step #1358] loss: 4.751653714422096\n",
      "[EPOCH #0, step #1360] loss: 4.751643871052313\n",
      "[EPOCH #0, step #1362] loss: 4.751825223840166\n",
      "[EPOCH #0, step #1364] loss: 4.751924684196164\n",
      "[EPOCH #0, step #1366] loss: 4.751814901523548\n",
      "[EPOCH #0, step #1368] loss: 4.752048148997427\n",
      "[EPOCH #0, step #1370] loss: 4.75217808536332\n",
      "[EPOCH #0, step #1372] loss: 4.752220808461211\n",
      "[EPOCH #0, step #1374] loss: 4.752395021611994\n",
      "[EPOCH #0, step #1376] loss: 4.752579530772733\n",
      "[EPOCH #0, step #1378] loss: 4.752640192019412\n",
      "[EPOCH #0, step #1380] loss: 4.75257091087159\n",
      "[EPOCH #0, step #1382] loss: 4.752647557468925\n",
      "[EPOCH #0, step #1384] loss: 4.7524835954934685\n",
      "[EPOCH #0, step #1386] loss: 4.752463110763561\n",
      "[EPOCH #0, step #1388] loss: 4.752544722340792\n",
      "[EPOCH #0, step #1390] loss: 4.752469904383338\n",
      "[EPOCH #0, step #1392] loss: 4.752375893685258\n",
      "[EPOCH #0, step #1394] loss: 4.75257959024026\n",
      "[EPOCH #0, step #1396] loss: 4.75241912085411\n",
      "[EPOCH #0, step #1398] loss: 4.752257174300329\n",
      "[EPOCH #0, step #1400] loss: 4.752424739412883\n",
      "[EPOCH #0, step #1402] loss: 4.752561647044024\n",
      "[EPOCH #0, step #1404] loss: 4.752478287313332\n",
      "[EPOCH #0, step #1406] loss: 4.752471923489222\n",
      "[EPOCH #0, step #1408] loss: 4.7525452701143545\n",
      "[EPOCH #0, step #1410] loss: 4.752515758714466\n",
      "[EPOCH #0, step #1412] loss: 4.752411401617721\n",
      "[EPOCH #0, step #1414] loss: 4.752373681894039\n",
      "[EPOCH #0, step #1416] loss: 4.752572354415646\n",
      "[EPOCH #0, step #1418] loss: 4.752375886337786\n",
      "[EPOCH #0, step #1420] loss: 4.752274824648487\n",
      "[EPOCH #0, step #1422] loss: 4.752040853989602\n",
      "[EPOCH #0, step #1424] loss: 4.752130278202525\n",
      "[EPOCH #0, step #1426] loss: 4.75205226736463\n",
      "[EPOCH #0, step #1428] loss: 4.75196207045174\n",
      "[EPOCH #0, step #1430] loss: 4.751800779693198\n",
      "[EPOCH #0, step #1432] loss: 4.751695392353088\n",
      "[EPOCH #0, step #1434] loss: 4.751575275248351\n",
      "[EPOCH #0, step #1436] loss: 4.7515525744868885\n",
      "[EPOCH #0, step #1438] loss: 4.751504035522906\n",
      "[EPOCH #0, step #1440] loss: 4.751536864029873\n",
      "[EPOCH #0, step #1442] loss: 4.751572408365526\n",
      "[EPOCH #0, step #1444] loss: 4.75152345050165\n",
      "[EPOCH #0, step #1446] loss: 4.751512071059833\n",
      "[EPOCH #0, step #1448] loss: 4.7514478707823615\n",
      "[EPOCH #0, step #1450] loss: 4.751228305900779\n",
      "[EPOCH #0, step #1452] loss: 4.751063731154818\n",
      "[EPOCH #0, step #1454] loss: 4.750935876000788\n",
      "[EPOCH #0, step #1456] loss: 4.750700679967562\n",
      "[EPOCH #0, step #1458] loss: 4.750626525787395\n",
      "[EPOCH #0, step #1460] loss: 4.7506289929253525\n",
      "[EPOCH #0, step #1462] loss: 4.750829447969865\n",
      "[EPOCH #0, step #1464] loss: 4.750763446560492\n",
      "[EPOCH #0, step #1466] loss: 4.750830172192262\n",
      "[EPOCH #0, step #1468] loss: 4.750708472574544\n",
      "[EPOCH #0, step #1470] loss: 4.750574599472865\n",
      "[EPOCH #0, step #1472] loss: 4.750656007830172\n",
      "[EPOCH #0, step #1474] loss: 4.750684419405663\n",
      "[EPOCH #0, step #1476] loss: 4.750527524915836\n",
      "[EPOCH #0, step #1478] loss: 4.750269666082397\n",
      "[EPOCH #0, step #1480] loss: 4.750401106981874\n",
      "[EPOCH #0, step #1482] loss: 4.750222576846164\n",
      "[EPOCH #0, step #1484] loss: 4.750277130370991\n",
      "[EPOCH #0, step #1486] loss: 4.750128637646058\n",
      "[EPOCH #0, step #1488] loss: 4.749872926259857\n",
      "[EPOCH #0, step #1490] loss: 4.749802071004007\n",
      "[EPOCH #0, step #1492] loss: 4.749819258595982\n",
      "[EPOCH #0, step #1494] loss: 4.7498681476682325\n",
      "[EPOCH #0, step #1496] loss: 4.750118597714839\n",
      "[EPOCH #0, step #1498] loss: 4.750004720019849\n",
      "[EPOCH #0, step #1500] loss: 4.750047769807006\n",
      "[EPOCH #0, step #1502] loss: 4.74996438632389\n",
      "[EPOCH #0, step #1504] loss: 4.750069621710286\n",
      "[EPOCH #0, step #1506] loss: 4.749957470998277\n",
      "[EPOCH #0, step #1508] loss: 4.749799824140181\n",
      "[EPOCH #0, step #1510] loss: 4.749741280654816\n",
      "[EPOCH #0, step #1512] loss: 4.749741616031624\n",
      "[EPOCH #0, step #1514] loss: 4.749411159301355\n",
      "[EPOCH #0, step #1516] loss: 4.7495473998113145\n",
      "[EPOCH #0, step #1518] loss: 4.749600254759496\n",
      "[EPOCH #0, step #1520] loss: 4.749574016380436\n",
      "[EPOCH #0, step #1522] loss: 4.749723930697025\n",
      "[EPOCH #0, step #1524] loss: 4.749709017081339\n",
      "[EPOCH #0, step #1526] loss: 4.749926019042789\n",
      "[EPOCH #0, step #1528] loss: 4.75014193374554\n",
      "[EPOCH #0, step #1530] loss: 4.750281870559802\n",
      "[EPOCH #0, step #1532] loss: 4.750238552640951\n",
      "[EPOCH #0, step #1534] loss: 4.750532700728128\n",
      "[EPOCH #0, step #1536] loss: 4.75066930966945\n",
      "[EPOCH #0, step #1538] loss: 4.750531783113238\n",
      "[EPOCH #0, step #1540] loss: 4.750538394328915\n",
      "[EPOCH #0, step #1542] loss: 4.750742377666244\n",
      "[EPOCH #0, step #1544] loss: 4.750524452666249\n",
      "[EPOCH #0, step #1546] loss: 4.750591871425577\n",
      "[EPOCH #0, step #1548] loss: 4.750465838504345\n",
      "[EPOCH #0, step #1550] loss: 4.750489983537134\n",
      "[EPOCH #0, step #1552] loss: 4.750569011960886\n",
      "[EPOCH #0, step #1554] loss: 4.750547515157718\n",
      "[EPOCH #0, step #1556] loss: 4.75058550978595\n",
      "[EPOCH #0, step #1558] loss: 4.750450833781244\n",
      "[EPOCH #0, step #1560] loss: 4.7505453379775835\n",
      "[EPOCH #0, step #1562] loss: 4.750493309243093\n",
      "[EPOCH #0, step #1564] loss: 4.7508239078826415\n",
      "[EPOCH #0, step #1566] loss: 4.75081346623823\n",
      "[EPOCH #0, step #1568] loss: 4.750689987318769\n",
      "[EPOCH #0, step #1570] loss: 4.7507335695015405\n",
      "[EPOCH #0, step #1572] loss: 4.750792854391536\n",
      "[EPOCH #0, step #1574] loss: 4.750827353038485\n",
      "[EPOCH #0, step #1576] loss: 4.750831194292841\n",
      "[EPOCH #0, step #1578] loss: 4.750835374913086\n",
      "[EPOCH #0, step #1580] loss: 4.750920327685439\n",
      "[EPOCH #0, step #1582] loss: 4.750758421097236\n",
      "[EPOCH #0, step #1584] loss: 4.7506593057409825\n",
      "[EPOCH #0, step #1586] loss: 4.750531972093108\n",
      "[EPOCH #0, step #1588] loss: 4.750474596713758\n",
      "[EPOCH #0, step #1590] loss: 4.75041993499027\n",
      "[EPOCH #0, step #1592] loss: 4.750307207053664\n",
      "[EPOCH #0, step #1594] loss: 4.750408263490492\n",
      "[EPOCH #0, step #1596] loss: 4.750655159326218\n",
      "[EPOCH #0, step #1598] loss: 4.750503456838583\n",
      "[EPOCH #0, step #1600] loss: 4.750519767096458\n",
      "[EPOCH #0, step #1602] loss: 4.75052824216713\n",
      "[EPOCH #0, step #1604] loss: 4.750406683271176\n",
      "[EPOCH #0, step #1606] loss: 4.750225471871643\n",
      "[EPOCH #0, step #1608] loss: 4.750098136287805\n",
      "[EPOCH #0, step #1610] loss: 4.750099590000789\n",
      "[EPOCH #0, step #1612] loss: 4.75001597626196\n",
      "[EPOCH #0, step #1614] loss: 4.749999745194757\n",
      "[EPOCH #0, step #1616] loss: 4.7499078126623075\n",
      "[EPOCH #0, step #1618] loss: 4.749698578832766\n",
      "[EPOCH #0, step #1620] loss: 4.749691890831865\n",
      "[EPOCH #0, step #1622] loss: 4.749583173224167\n",
      "[EPOCH #0, step #1624] loss: 4.749655285468468\n",
      "[EPOCH #0, step #1626] loss: 4.74965149446287\n",
      "[EPOCH #0, step #1628] loss: 4.749529913497747\n",
      "[EPOCH #0, step #1630] loss: 4.749407383797726\n",
      "[EPOCH #0, step #1632] loss: 4.749540055958829\n",
      "[EPOCH #0, step #1634] loss: 4.749499506469166\n",
      "[EPOCH #0, step #1636] loss: 4.749250715799862\n",
      "[EPOCH #0, step #1638] loss: 4.749142741633305\n",
      "[EPOCH #0, step #1640] loss: 4.749034933605112\n",
      "[EPOCH #0, step #1642] loss: 4.748796611316887\n",
      "[EPOCH #0, step #1644] loss: 4.7487209007008095\n",
      "[EPOCH #0, step #1646] loss: 4.748679779773057\n",
      "[EPOCH #0, step #1648] loss: 4.748853024459016\n",
      "[EPOCH #0, step #1650] loss: 4.748880048436443\n",
      "[EPOCH #0, step #1652] loss: 4.74898270845269\n",
      "[EPOCH #0, step #1654] loss: 4.748723732812887\n",
      "[EPOCH #0, step #1656] loss: 4.748773172812499\n",
      "[EPOCH #0, step #1658] loss: 4.7487540377271396\n",
      "[EPOCH #0, step #1660] loss: 4.748816899547083\n",
      "[EPOCH #0, step #1662] loss: 4.748837576239351\n",
      "[EPOCH #0, step #1664] loss: 4.749059333457603\n",
      "[EPOCH #0, step #1666] loss: 4.748952316680066\n",
      "[EPOCH #0, step #1668] loss: 4.7489387707627415\n",
      "[EPOCH #0, step #1670] loss: 4.748887668748875\n",
      "[EPOCH #0, step #1672] loss: 4.74890150285665\n",
      "[EPOCH #0, step #1674] loss: 4.748834006750761\n",
      "[EPOCH #0, step #1676] loss: 4.748825093998963\n",
      "[EPOCH #0, step #1678] loss: 4.748903564784838\n",
      "[EPOCH #0, step #1680] loss: 4.748852540594279\n",
      "[EPOCH #0, step #1682] loss: 4.748944478309799\n",
      "[EPOCH #0, step #1684] loss: 4.749026308427578\n",
      "[EPOCH #0, step #1686] loss: 4.749057423806996\n",
      "[EPOCH #0, step #1688] loss: 4.749208174491508\n",
      "[EPOCH #0, step #1690] loss: 4.749217117417853\n",
      "[EPOCH #0, step #1692] loss: 4.74922668391688\n",
      "[EPOCH #0, step #1694] loss: 4.74938028161153\n",
      "[EPOCH #0, step #1696] loss: 4.749376991319179\n",
      "[EPOCH #0, step #1698] loss: 4.749300792541414\n",
      "[EPOCH #0, step #1700] loss: 4.749422306596777\n",
      "[EPOCH #0, step #1702] loss: 4.749420391134283\n",
      "[EPOCH #0, step #1704] loss: 4.749433820058873\n",
      "[EPOCH #0, step #1706] loss: 4.749458644567489\n",
      "[EPOCH #0, step #1708] loss: 4.749576613803995\n",
      "[EPOCH #0, step #1710] loss: 4.749643098673996\n",
      "[EPOCH #0, step #1712] loss: 4.749661062032384\n",
      "[EPOCH #0, step #1714] loss: 4.749757171719832\n",
      "[EPOCH #0, step #1716] loss: 4.74978845231951\n",
      "[EPOCH #0, step #1718] loss: 4.749799065703636\n",
      "[EPOCH #0, step #1720] loss: 4.749861488087381\n",
      "[EPOCH #0, step #1722] loss: 4.749684055669323\n",
      "[EPOCH #0, step #1724] loss: 4.749565031355706\n",
      "[EPOCH #0, step #1726] loss: 4.749562160224672\n",
      "[EPOCH #0, step #1728] loss: 4.749605091684053\n",
      "[EPOCH #0, step #1730] loss: 4.749507264964227\n",
      "[EPOCH #0, step #1732] loss: 4.749421832882044\n",
      "[EPOCH #0, step #1734] loss: 4.7492820679282595\n",
      "[EPOCH #0, step #1736] loss: 4.749259028547305\n",
      "[EPOCH #0, step #1738] loss: 4.749512396851376\n",
      "[EPOCH #0, step #1740] loss: 4.749534887941592\n",
      "[EPOCH #0, step #1742] loss: 4.749511692213451\n",
      "[EPOCH #0, step #1744] loss: 4.749563476485988\n",
      "[EPOCH #0, step #1746] loss: 4.749555164428868\n",
      "[EPOCH #0, step #1748] loss: 4.74963719971593\n",
      "[EPOCH #0, step #1750] loss: 4.749643237028443\n",
      "[EPOCH #0, step #1752] loss: 4.7495515211881125\n",
      "[EPOCH #0, step #1754] loss: 4.7494185724829\n",
      "[EPOCH #0, step #1756] loss: 4.7494078171002165\n",
      "[EPOCH #0, step #1758] loss: 4.749282450347953\n",
      "[EPOCH #0, step #1760] loss: 4.749314917413299\n",
      "[EPOCH #0, step #1762] loss: 4.7494637177738355\n",
      "[EPOCH #0, step #1764] loss: 4.74965711085722\n",
      "[EPOCH #0, step #1766] loss: 4.749527665134466\n",
      "[EPOCH #0, step #1768] loss: 4.749593233644389\n",
      "[EPOCH #0, step #1770] loss: 4.7495787939333365\n",
      "[EPOCH #0, step #1772] loss: 4.749617735006953\n",
      "[EPOCH #0, step #1774] loss: 4.749433364062242\n",
      "[EPOCH #0, step #1776] loss: 4.749431952507092\n",
      "[EPOCH #0, step #1778] loss: 4.749630669116169\n",
      "[EPOCH #0, step #1780] loss: 4.749564230810184\n",
      "[EPOCH #0, step #1782] loss: 4.74955802303449\n",
      "[EPOCH #0, step #1784] loss: 4.749509952983269\n",
      "[EPOCH #0, step #1786] loss: 4.749737684060958\n",
      "[EPOCH #0, step #1788] loss: 4.749680414887498\n",
      "[EPOCH #0, step #1790] loss: 4.749698014315249\n",
      "[EPOCH #0, step #1792] loss: 4.749703049327299\n",
      "[EPOCH #0, step #1794] loss: 4.749597204628097\n",
      "[EPOCH #0, step #1796] loss: 4.749599625816197\n",
      "[EPOCH #0, step #1798] loss: 4.749521281733786\n",
      "[EPOCH #0, step #1800] loss: 4.74947767056471\n",
      "[EPOCH #0, step #1802] loss: 4.74946855824323\n",
      "[EPOCH #0, step #1804] loss: 4.749514874328867\n",
      "[EPOCH #0, step #1806] loss: 4.749522536382797\n",
      "[EPOCH #0, step #1808] loss: 4.749581338556647\n",
      "[EPOCH #0, step #1810] loss: 4.749639562451991\n",
      "[EPOCH #0, step #1812] loss: 4.749669211514405\n",
      "[EPOCH #0, step #1814] loss: 4.749553910562815\n",
      "[EPOCH #0, step #1816] loss: 4.749643267021116\n",
      "[EPOCH #0, step #1818] loss: 4.749635315780262\n",
      "[EPOCH #0, step #1820] loss: 4.749589696420149\n",
      "[EPOCH #0, step #1822] loss: 4.749674710740901\n",
      "[EPOCH #0, step #1824] loss: 4.749733130311313\n",
      "[EPOCH #0, step #1826] loss: 4.749684738081926\n",
      "[EPOCH #0, step #1828] loss: 4.749579973205183\n",
      "[EPOCH #0, step #1830] loss: 4.749470428714721\n",
      "[EPOCH #0, step #1832] loss: 4.749544532066216\n",
      "[EPOCH #0, step #1834] loss: 4.749596297448598\n",
      "[EPOCH #0, step #1836] loss: 4.749487386037944\n",
      "[EPOCH #0, step #1838] loss: 4.749445218505777\n",
      "[EPOCH #0, step #1840] loss: 4.749358973381378\n",
      "[EPOCH #0, step #1842] loss: 4.749285096392577\n",
      "[EPOCH #0, step #1844] loss: 4.749264975550375\n",
      "[EPOCH #0, step #1846] loss: 4.7492192081973945\n",
      "[EPOCH #0, step #1848] loss: 4.749095555187368\n",
      "[EPOCH #0, step #1850] loss: 4.749045189363386\n",
      "[EPOCH #0, step #1852] loss: 4.748953130138416\n",
      "[EPOCH #0, step #1854] loss: 4.74894158628109\n",
      "[EPOCH #0, step #1856] loss: 4.748865045651224\n",
      "[EPOCH #0, step #1858] loss: 4.748947458282602\n",
      "[EPOCH #0, step #1860] loss: 4.7490489486723035\n",
      "[EPOCH #0, step #1862] loss: 4.748913412252806\n",
      "[EPOCH #0, step #1864] loss: 4.749090081437344\n",
      "[EPOCH #0, step #1866] loss: 4.749281519801462\n",
      "[EPOCH #0, step #1868] loss: 4.749137429900192\n",
      "[EPOCH #0, step #1870] loss: 4.749080287651988\n",
      "[EPOCH #0, step #1872] loss: 4.7489362320541\n",
      "[EPOCH #0, step #1874] loss: 4.748781374867757\n",
      "[EPOCH #0, step #1876] loss: 4.748943932207582\n",
      "[EPOCH #0, step #1878] loss: 4.748945868351537\n",
      "[EPOCH #0, step #1880] loss: 4.748949751329194\n",
      "[EPOCH #0, step #1882] loss: 4.7489500291412075\n",
      "[EPOCH #0, step #1884] loss: 4.7490215799852775\n",
      "[EPOCH #0, step #1886] loss: 4.748953078623227\n",
      "[EPOCH #0, step #1888] loss: 4.748884168991404\n",
      "[EPOCH #0, step #1890] loss: 4.7489833438415365\n",
      "[EPOCH #0, step #1892] loss: 4.749021299667479\n",
      "[EPOCH #0, step #1894] loss: 4.748961961992812\n",
      "[EPOCH #0, step #1896] loss: 4.749134744413916\n",
      "[EPOCH #0, step #1898] loss: 4.74902835703323\n",
      "[EPOCH #0, step #1900] loss: 4.748792207598498\n",
      "[EPOCH #0, step #1902] loss: 4.748806612152084\n",
      "[EPOCH #0, step #1904] loss: 4.748819157457727\n",
      "[EPOCH #0, step #1906] loss: 4.748844037458543\n",
      "[EPOCH #0, step #1908] loss: 4.7487893007137565\n",
      "[EPOCH #0, step #1910] loss: 4.748775083441063\n",
      "[EPOCH #0, step #1912] loss: 4.748782982953356\n",
      "[EPOCH #0, step #1914] loss: 4.7485511505883915\n",
      "[EPOCH #0, step #1916] loss: 4.748538034684843\n",
      "[EPOCH #0, step #1918] loss: 4.748388838059832\n",
      "[EPOCH #0, step #1920] loss: 4.74843462974314\n",
      "[EPOCH #0, step #1922] loss: 4.748346500575264\n",
      "[EPOCH #0, step #1924] loss: 4.748433839376871\n",
      "[EPOCH #0, step #1926] loss: 4.748253871321864\n",
      "[EPOCH #0, step #1928] loss: 4.74819340564971\n",
      "[EPOCH #0, step #1930] loss: 4.748295765723174\n",
      "[EPOCH #0, step #1932] loss: 4.748320979647728\n",
      "[EPOCH #0, step #1934] loss: 4.7481695564526305\n",
      "[EPOCH #0, step #1936] loss: 4.748148205727108\n",
      "[EPOCH #0, step #1938] loss: 4.748173276193982\n",
      "[EPOCH #0, step #1940] loss: 4.748184915795147\n",
      "[EPOCH #0, step #1942] loss: 4.748289917647931\n",
      "[EPOCH #0, step #1944] loss: 4.74821971412796\n",
      "[EPOCH #0, step #1946] loss: 4.748142364396153\n",
      "[EPOCH #0, step #1948] loss: 4.748091027331756\n",
      "[EPOCH #0, step #1950] loss: 4.748140786746292\n",
      "[EPOCH #0, step #1952] loss: 4.748066368679236\n",
      "[EPOCH #0, step #1954] loss: 4.748134497913253\n",
      "[EPOCH #0, step #1956] loss: 4.748154008394611\n",
      "[EPOCH #0, step #1958] loss: 4.748094054860324\n",
      "[EPOCH #0, step #1960] loss: 4.748074549062711\n",
      "[EPOCH #0, step #1962] loss: 4.747963839505196\n",
      "[EPOCH #0, step #1964] loss: 4.748037547919586\n",
      "[EPOCH #0, step #1966] loss: 4.7479038507609905\n",
      "[EPOCH #0, step #1968] loss: 4.747952393676618\n",
      "[EPOCH #0, step #1970] loss: 4.747913154474314\n",
      "[EPOCH #0, step #1972] loss: 4.747814200943155\n",
      "[EPOCH #0, step #1974] loss: 4.747747847158697\n",
      "[EPOCH #0, step #1976] loss: 4.747738528746092\n",
      "[EPOCH #0, step #1978] loss: 4.747737596587981\n",
      "[EPOCH #0, step #1980] loss: 4.747856816759259\n",
      "[EPOCH #0, step #1982] loss: 4.747801420006918\n",
      "[EPOCH #0, step #1984] loss: 4.747712818020838\n",
      "[EPOCH #0, step #1986] loss: 4.747703438736292\n",
      "[EPOCH #0, step #1988] loss: 4.747668027757939\n",
      "[EPOCH #0, step #1990] loss: 4.747730392597837\n",
      "[EPOCH #0, step #1992] loss: 4.747679577601118\n",
      "[EPOCH #0, step #1994] loss: 4.747814626622021\n",
      "[EPOCH #0, step #1996] loss: 4.747875719352191\n",
      "[EPOCH #0, step #1998] loss: 4.747854599897834\n",
      "[EPOCH #0, step #2000] loss: 4.747881301696869\n",
      "[EPOCH #0, step #2002] loss: 4.747844851975196\n",
      "[EPOCH #0, step #2004] loss: 4.747950986971581\n",
      "[EPOCH #0, step #2006] loss: 4.74798472033369\n",
      "[EPOCH #0, step #2008] loss: 4.74781225996269\n",
      "[EPOCH #0, step #2010] loss: 4.747725956004867\n",
      "[EPOCH #0, step #2012] loss: 4.747716605456031\n",
      "[EPOCH #0, step #2014] loss: 4.747721319222273\n",
      "[EPOCH #0, step #2016] loss: 4.7478219898963365\n",
      "[EPOCH #0, step #2018] loss: 4.747793275569559\n",
      "[EPOCH #0, step #2020] loss: 4.747664653092903\n",
      "[EPOCH #0, step #2022] loss: 4.747651040053167\n",
      "[EPOCH #0, step #2024] loss: 4.747860595326364\n",
      "[EPOCH #0, step #2026] loss: 4.747827558136363\n",
      "[EPOCH #0, step #2028] loss: 4.747881605215058\n",
      "[EPOCH #0, step #2030] loss: 4.74773125059202\n",
      "[EPOCH #0, step #2032] loss: 4.747763799553837\n",
      "[EPOCH #0, step #2034] loss: 4.74776830884104\n",
      "[EPOCH #0, step #2036] loss: 4.747832376230097\n",
      "[EPOCH #0, step #2038] loss: 4.747939985601735\n",
      "[EPOCH #0, step #2040] loss: 4.747977397653298\n",
      "[EPOCH #0, step #2042] loss: 4.748013093082915\n",
      "[EPOCH #0, step #2044] loss: 4.748062786438063\n",
      "[EPOCH #0, step #2046] loss: 4.747963318081675\n",
      "[EPOCH #0, step #2048] loss: 4.747919728198593\n",
      "[EPOCH #0, step #2050] loss: 4.74778929737822\n",
      "[EPOCH #0, step #2052] loss: 4.747794294845053\n",
      "[EPOCH #0, step #2054] loss: 4.7478780289346\n",
      "[EPOCH #0, step #2056] loss: 4.747971282517614\n",
      "[EPOCH #0, step #2058] loss: 4.747858893703407\n",
      "[EPOCH #0, step #2060] loss: 4.74790612174029\n",
      "[EPOCH #0, step #2062] loss: 4.74791637833004\n",
      "[EPOCH #0, step #2064] loss: 4.74781103295795\n",
      "[EPOCH #0, step #2066] loss: 4.747713319679707\n",
      "[EPOCH #0, step #2068] loss: 4.747750585813232\n",
      "[EPOCH #0, step #2070] loss: 4.747626227724673\n",
      "[EPOCH #0, step #2072] loss: 4.747638776459903\n",
      "[EPOCH #0, step #2074] loss: 4.7474328298453825\n",
      "[EPOCH #0, step #2076] loss: 4.747402412936156\n",
      "[EPOCH #0, step #2078] loss: 4.747380319267813\n",
      "[EPOCH #0, step #2080] loss: 4.747297345417615\n",
      "[EPOCH #0, step #2082] loss: 4.747295540110515\n",
      "[EPOCH #0, step #2084] loss: 4.747377798139906\n",
      "[EPOCH #0, step #2086] loss: 4.747390563372624\n",
      "[EPOCH #0, step #2088] loss: 4.747313005265692\n",
      "[EPOCH #0, step #2090] loss: 4.747352006502667\n",
      "[EPOCH #0, step #2092] loss: 4.747459078313515\n",
      "[EPOCH #0, step #2094] loss: 4.747416861131254\n",
      "[EPOCH #0, step #2096] loss: 4.7475313678717805\n",
      "[EPOCH #0, step #2098] loss: 4.747484270761898\n",
      "[EPOCH #0, step #2100] loss: 4.747562985372566\n",
      "[EPOCH #0, step #2102] loss: 4.747536425923825\n",
      "[EPOCH #0, step #2104] loss: 4.747635721310867\n",
      "[EPOCH #0, step #2106] loss: 4.74769479514413\n",
      "[EPOCH #0, step #2108] loss: 4.747695184194855\n",
      "[EPOCH #0, step #2110] loss: 4.747794695004418\n",
      "[EPOCH #0, step #2112] loss: 4.747818188592827\n",
      "[EPOCH #0, step #2114] loss: 4.747846354716777\n",
      "[EPOCH #0, step #2116] loss: 4.747894401982758\n",
      "[EPOCH #0, step #2118] loss: 4.74793174886771\n",
      "[EPOCH #0, step #2120] loss: 4.7478324630697735\n",
      "[EPOCH #0, step #2122] loss: 4.747908094498666\n",
      "[EPOCH #0, step #2124] loss: 4.747885782354018\n",
      "[EPOCH #0, step #2126] loss: 4.74795203910616\n",
      "[EPOCH #0, step #2128] loss: 4.748049280015221\n",
      "[EPOCH #0, step #2130] loss: 4.748113908548413\n",
      "[EPOCH #0, step #2132] loss: 4.748224612753323\n",
      "[EPOCH #0, step #2134] loss: 4.7481867676317275\n",
      "[EPOCH #0, step #2136] loss: 4.748191573098825\n",
      "[EPOCH #0, step #2138] loss: 4.748176356017339\n",
      "[EPOCH #0, step #2140] loss: 4.748294271525241\n",
      "[EPOCH #0, step #2142] loss: 4.748360315216962\n",
      "[EPOCH #0, step #2144] loss: 4.748358606013941\n",
      "[EPOCH #0, step #2146] loss: 4.7482928512370695\n",
      "[EPOCH #0, step #2148] loss: 4.74840376731017\n",
      "[EPOCH #0, step #2150] loss: 4.748406884837627\n",
      "[EPOCH #0, step #2152] loss: 4.74832190284605\n",
      "[EPOCH #0, step #2154] loss: 4.748283829213295\n",
      "[EPOCH #0, step #2156] loss: 4.748398993266198\n",
      "[EPOCH #0, step #2158] loss: 4.748305779245508\n",
      "[EPOCH #0, step #2160] loss: 4.748217900667627\n",
      "[EPOCH #0, step #2162] loss: 4.748160367534495\n",
      "[EPOCH #0, step #2164] loss: 4.748145115788484\n",
      "[EPOCH #0, step #2166] loss: 4.748187261404505\n",
      "[EPOCH #0, step #2168] loss: 4.748211033831219\n",
      "[EPOCH #0, step #2170] loss: 4.748207224947359\n",
      "[EPOCH #0, step #2172] loss: 4.748187249012933\n",
      "[EPOCH #0, step #2174] loss: 4.748295206968812\n",
      "[EPOCH #0, step #2176] loss: 4.748326625526088\n",
      "[EPOCH #0, step #2178] loss: 4.748087142683487\n",
      "[EPOCH #0, step #2180] loss: 4.747963274361507\n",
      "[EPOCH #0, step #2182] loss: 4.747850716687428\n",
      "[EPOCH #0, step #2184] loss: 4.747935345953062\n",
      "[EPOCH #0, step #2186] loss: 4.748010114496246\n",
      "[EPOCH #0, step #2188] loss: 4.748016898737699\n",
      "[EPOCH #0, step #2190] loss: 4.7479565007563655\n",
      "[EPOCH #0, step #2192] loss: 4.747911131224347\n",
      "[EPOCH #0, step #2194] loss: 4.747916389543538\n",
      "[EPOCH #0, step #2196] loss: 4.747885962969399\n",
      "[EPOCH #0, step #2198] loss: 4.747903163783276\n",
      "[EPOCH #0, step #2200] loss: 4.747963005388287\n",
      "[EPOCH #0, step #2202] loss: 4.747886759229427\n",
      "[EPOCH #0, step #2204] loss: 4.74793124274602\n",
      "[EPOCH #0, step #2206] loss: 4.747840622943832\n",
      "[EPOCH #0, step #2208] loss: 4.747921920135691\n",
      "[EPOCH #0, step #2210] loss: 4.747970348351253\n",
      "[EPOCH #0, step #2212] loss: 4.747957370747714\n",
      "[EPOCH #0, step #2214] loss: 4.747971532360964\n",
      "[EPOCH #0, step #2216] loss: 4.747970939381573\n",
      "[EPOCH #0, step #2218] loss: 4.747942955013651\n",
      "[EPOCH #0, step #2220] loss: 4.7479537285861\n",
      "[EPOCH #0, step #2222] loss: 4.747973406899367\n",
      "[EPOCH #0, step #2224] loss: 4.747920000312034\n",
      "[EPOCH #0, step #2226] loss: 4.7478678353870185\n",
      "[EPOCH #0, step #2228] loss: 4.747757537956461\n",
      "[EPOCH #0, step #2230] loss: 4.747878727384983\n",
      "[EPOCH #0, step #2232] loss: 4.747721805948342\n",
      "[EPOCH #0, step #2234] loss: 4.747689298335338\n",
      "[EPOCH #0, step #2236] loss: 4.7475445192735215\n",
      "[EPOCH #0, step #2238] loss: 4.7475963435358315\n",
      "[EPOCH #0, step #2240] loss: 4.747608037784343\n",
      "[EPOCH #0, step #2242] loss: 4.747645853461828\n",
      "[EPOCH #0, step #2244] loss: 4.747634225220882\n",
      "[EPOCH #0, step #2246] loss: 4.747591097619091\n",
      "[EPOCH #0, step #2248] loss: 4.747627453784935\n",
      "[EPOCH #0, step #2250] loss: 4.747640696169269\n",
      "[EPOCH #0, step #2252] loss: 4.747576192279313\n",
      "[EPOCH #0, step #2254] loss: 4.747591060756844\n",
      "[EPOCH #0, step #2256] loss: 4.747520917910203\n",
      "[EPOCH #0, step #2258] loss: 4.747555093989007\n",
      "[EPOCH #0, step #2260] loss: 4.747521898167579\n",
      "[EPOCH #0, step #2262] loss: 4.747576929961376\n",
      "[EPOCH #0, step #2264] loss: 4.7475106605630835\n",
      "[EPOCH #0, step #2266] loss: 4.747490070546625\n",
      "[EPOCH #0, step #2268] loss: 4.747538818983109\n",
      "[EPOCH #0, step #2270] loss: 4.747565314193383\n",
      "[EPOCH #0, step #2272] loss: 4.747477887854721\n",
      "[EPOCH #0, step #2274] loss: 4.747677515679664\n",
      "[EPOCH #0, step #2276] loss: 4.747735574259065\n",
      "[EPOCH #0, step #2278] loss: 4.747633360937637\n",
      "[EPOCH #0, step #2280] loss: 4.74767668379968\n",
      "[EPOCH #0, step #2282] loss: 4.7475259087872095\n",
      "[EPOCH #0, step #2284] loss: 4.747522789018159\n",
      "[EPOCH #0, step #2286] loss: 4.747506280731415\n",
      "[EPOCH #0, step #2288] loss: 4.74762418771113\n",
      "[EPOCH #0, step #2290] loss: 4.747599011058632\n",
      "[EPOCH #0, step #2292] loss: 4.747621039454198\n",
      "[EPOCH #0, step #2294] loss: 4.747679494305114\n",
      "[EPOCH #0, step #2296] loss: 4.747777268322956\n",
      "[EPOCH #0, step #2298] loss: 4.747774157331217\n",
      "[EPOCH #0, step #2300] loss: 4.747800713256876\n",
      "[EPOCH #0, step #2302] loss: 4.747706177024494\n",
      "[EPOCH #0, step #2304] loss: 4.7476836190047855\n",
      "[EPOCH #0, step #2306] loss: 4.747655851168192\n",
      "[EPOCH #0, step #2308] loss: 4.7476645180961174\n",
      "[EPOCH #0, step #2310] loss: 4.747851961054156\n",
      "[EPOCH #0, step #2312] loss: 4.747937651872119\n",
      "[EPOCH #0, step #2314] loss: 4.747924606949413\n",
      "[EPOCH #0, step #2316] loss: 4.747925151459452\n",
      "[EPOCH #0, step #2318] loss: 4.748086122461\n",
      "[EPOCH #0, step #2320] loss: 4.748145592412985\n",
      "[EPOCH #0, step #2322] loss: 4.748109008954919\n",
      "[EPOCH #0, step #2324] loss: 4.748182120169362\n",
      "[EPOCH #0, step #2326] loss: 4.7481902894617205\n",
      "[EPOCH #0, step #2328] loss: 4.748227666713179\n",
      "[EPOCH #0, step #2330] loss: 4.74818280672935\n",
      "[EPOCH #0, step #2332] loss: 4.748119714584056\n",
      "[EPOCH #0, step #2334] loss: 4.748062965344055\n",
      "[EPOCH #0, step #2336] loss: 4.748088906859046\n",
      "[EPOCH #0, step #2338] loss: 4.748049033508692\n",
      "[EPOCH #0, step #2340] loss: 4.748034678479129\n",
      "[EPOCH #0, step #2342] loss: 4.747989835262909\n",
      "[EPOCH #0, step #2344] loss: 4.7479963568990415\n",
      "[EPOCH #0, step #2346] loss: 4.748116031625599\n",
      "[EPOCH #0, step #2348] loss: 4.748053325800857\n",
      "[EPOCH #0, step #2350] loss: 4.747992189830032\n",
      "[EPOCH #0, step #2352] loss: 4.7480420924428675\n",
      "[EPOCH #0, step #2354] loss: 4.748006971727764\n",
      "[EPOCH #0, step #2356] loss: 4.747923052710499\n",
      "[EPOCH #0, step #2358] loss: 4.7479112494946945\n",
      "[EPOCH #0, step #2360] loss: 4.747908903046818\n",
      "[EPOCH #0, step #2362] loss: 4.7477867697135805\n",
      "[EPOCH #0, step #2364] loss: 4.747895796455238\n",
      "[EPOCH #0, step #2366] loss: 4.747914288282697\n",
      "[EPOCH #0, step #2368] loss: 4.747931997702564\n",
      "[EPOCH #0, step #2370] loss: 4.747923363114851\n",
      "[EPOCH #0, step #2372] loss: 4.747877283735312\n",
      "[EPOCH #0, step #2374] loss: 4.747805472323769\n",
      "[EPOCH #0, step #2376] loss: 4.747718469004296\n",
      "[EPOCH #0, step #2378] loss: 4.747793728185832\n",
      "[EPOCH #0, step #2380] loss: 4.747817639491479\n",
      "[EPOCH #0, step #2382] loss: 4.747723130783612\n",
      "[EPOCH #0, step #2384] loss: 4.747693040635851\n",
      "[EPOCH #0, step #2386] loss: 4.7477307866995035\n",
      "[EPOCH #0, step #2388] loss: 4.747653812681496\n",
      "[EPOCH #0, step #2390] loss: 4.747659253655294\n",
      "[EPOCH #0, step #2392] loss: 4.7475605903686064\n",
      "[EPOCH #0, step #2394] loss: 4.74759204143771\n",
      "[EPOCH #0, step #2396] loss: 4.747558804815593\n",
      "[EPOCH #0, step #2398] loss: 4.747547980694534\n",
      "[EPOCH #0, step #2400] loss: 4.747389293322708\n",
      "[EPOCH #0, step #2402] loss: 4.7473473806853494\n",
      "[EPOCH #0, step #2404] loss: 4.747441732957804\n",
      "[EPOCH #0, step #2406] loss: 4.747319117491801\n",
      "[EPOCH #0, step #2408] loss: 4.747234366651687\n",
      "[EPOCH #0, step #2410] loss: 4.747217644304222\n",
      "[EPOCH #0, step #2412] loss: 4.7471415940232164\n",
      "[EPOCH #0, step #2414] loss: 4.747190107618059\n",
      "[EPOCH #0, step #2416] loss: 4.747164181751509\n",
      "[EPOCH #0, step #2418] loss: 4.747136122927088\n",
      "[EPOCH #0, step #2420] loss: 4.747137039567458\n",
      "[EPOCH #0, step #2422] loss: 4.7471263029828235\n",
      "[EPOCH #0, step #2424] loss: 4.747069500205443\n",
      "[EPOCH #0, step #2426] loss: 4.747012833484073\n",
      "[EPOCH #0, step #2428] loss: 4.746922887171576\n",
      "[EPOCH #0, step #2430] loss: 4.746853828430176\n",
      "[EPOCH #0, step #2432] loss: 4.746817569599394\n",
      "[EPOCH #0, step #2434] loss: 4.7466993175248104\n",
      "[EPOCH #0, step #2436] loss: 4.746623202822079\n",
      "[EPOCH #0, step #2438] loss: 4.7465604895106495\n",
      "[EPOCH #0, step #2440] loss: 4.746579467737494\n",
      "[EPOCH #0, step #2442] loss: 4.746532312924873\n",
      "[EPOCH #0, step #2444] loss: 4.746459245974301\n",
      "[EPOCH #0, step #2446] loss: 4.746493472242336\n",
      "[EPOCH #0, step #2448] loss: 4.7464295340927825\n",
      "[EPOCH #0, step #2450] loss: 4.74641527656826\n",
      "[EPOCH #0, step #2452] loss: 4.746350148430757\n",
      "[EPOCH #0, step #2454] loss: 4.7464494073949375\n",
      "[EPOCH #0, step #2456] loss: 4.746528020562997\n",
      "[EPOCH #0, step #2458] loss: 4.746517944646202\n",
      "[EPOCH #0, step #2460] loss: 4.746667862519749\n",
      "[EPOCH #0, step #2462] loss: 4.74669109473033\n",
      "[EPOCH #0, step #2464] loss: 4.746782165705311\n",
      "[EPOCH #0, step #2466] loss: 4.7466050379180595\n",
      "[EPOCH #0, step #2468] loss: 4.746689366541106\n",
      "[EPOCH #0, step #2470] loss: 4.746818272703719\n",
      "[EPOCH #0, step #2472] loss: 4.746847919544323\n",
      "[EPOCH #0, step #2474] loss: 4.746784408068416\n",
      "[EPOCH #0, step #2476] loss: 4.746805080734148\n",
      "[EPOCH #0, step #2478] loss: 4.746871097475447\n",
      "[EPOCH #0, step #2480] loss: 4.746821243020134\n",
      "[EPOCH #0, step #2482] loss: 4.746788445663606\n",
      "[EPOCH #0, step #2484] loss: 4.746776164033763\n",
      "[EPOCH #0, step #2486] loss: 4.7467291641772205\n",
      "[EPOCH #0, step #2488] loss: 4.746710242397482\n",
      "[EPOCH #0, step #2490] loss: 4.746669999187321\n",
      "[EPOCH #0, step #2492] loss: 4.746628889251415\n",
      "[EPOCH #0, step #2494] loss: 4.7466807097853545\n",
      "[EPOCH #0, step #2496] loss: 4.746582575497457\n",
      "[EPOCH #0, step #2498] loss: 4.746594876277538\n",
      "[EPOCH #0, step #2500] loss: 4.7467008264290715\n",
      "[EPOCH #0, step #2502] loss: 4.746783766516009\n",
      "[EPOCH #0, step #2504] loss: 4.74692659892008\n",
      "[EPOCH #0, step #2506] loss: 4.746972267210222\n",
      "[EPOCH #0, step #2508] loss: 4.746849588209436\n",
      "[EPOCH #0, step #2510] loss: 4.746782630812548\n",
      "[EPOCH #0, step #2512] loss: 4.746844096436142\n",
      "[EPOCH #0, step #2514] loss: 4.746747077009076\n",
      "[EPOCH #0, step #2516] loss: 4.74675982861373\n",
      "[EPOCH #0, step #2518] loss: 4.74666961785014\n",
      "[EPOCH #0, step #2520] loss: 4.746660361541542\n",
      "[EPOCH #0, step #2522] loss: 4.74669964314452\n",
      "[EPOCH #0, step #2524] loss: 4.746653874274528\n",
      "[EPOCH #0, step #2526] loss: 4.746681186395245\n",
      "[EPOCH #0, step #2528] loss: 4.746661788062552\n",
      "[EPOCH #0, step #2530] loss: 4.746610581568699\n",
      "[EPOCH #0, step #2532] loss: 4.746571514854025\n",
      "[EPOCH #0, step #2534] loss: 4.746574110674435\n",
      "[EPOCH #0, step #2536] loss: 4.746526732987948\n",
      "[EPOCH #0, step #2538] loss: 4.7463723137980605\n",
      "[EPOCH #0, step #2540] loss: 4.746485092188608\n",
      "[EPOCH #0, step #2542] loss: 4.746518268390195\n",
      "[EPOCH #0, step #2544] loss: 4.746437245689347\n",
      "[EPOCH #0, step #2546] loss: 4.746445480027665\n",
      "[EPOCH #0, step #2548] loss: 4.746478497537738\n",
      "[EPOCH #0, step #2550] loss: 4.746404082202201\n",
      "[EPOCH #0, step #2552] loss: 4.7463534861698\n",
      "[EPOCH #0, step #2554] loss: 4.746355230887575\n",
      "[EPOCH #0, step #2556] loss: 4.746538566340214\n",
      "[EPOCH #0, step #2558] loss: 4.746545366031577\n",
      "[EPOCH #0, step #2560] loss: 4.746483864611083\n",
      "[EPOCH #0, step #2562] loss: 4.7464706578665865\n",
      "[EPOCH #0, step #2564] loss: 4.74658026890448\n",
      "[EPOCH #0, step #2566] loss: 4.7465842731462455\n",
      "[EPOCH #0, step #2568] loss: 4.74668169448514\n",
      "[EPOCH #0, step #2570] loss: 4.746575926955411\n",
      "[EPOCH #0, step #2572] loss: 4.746528688127597\n",
      "[EPOCH #0, step #2574] loss: 4.746405946676013\n",
      "[EPOCH #0, step #2576] loss: 4.746371867816764\n",
      "[EPOCH #0, step #2578] loss: 4.74635149240956\n",
      "[EPOCH #0, step #2580] loss: 4.746301672062362\n",
      "[EPOCH #0, step #2582] loss: 4.7463869727308605\n",
      "[EPOCH #0, step #2584] loss: 4.746488111484904\n",
      "[EPOCH #0, step #2586] loss: 4.746442392981094\n",
      "[EPOCH #0, step #2588] loss: 4.746367538882389\n",
      "[EPOCH #0, step #2590] loss: 4.746393371576142\n",
      "[EPOCH #0, step #2592] loss: 4.7464598763462575\n",
      "[EPOCH #0, step #2594] loss: 4.746439178187494\n",
      "[EPOCH #0, step #2596] loss: 4.746518182993211\n",
      "[EPOCH #0, step #2598] loss: 4.746474298159404\n",
      "[EPOCH #0, step #2600] loss: 4.746485145675911\n",
      "[EPOCH #0, step #2602] loss: 4.746552373554173\n",
      "[EPOCH #0, step #2604] loss: 4.746551331540216\n",
      "[EPOCH #0, step #2606] loss: 4.746533282286919\n",
      "[EPOCH #0, step #2608] loss: 4.746602881955388\n",
      "[EPOCH #0, step #2610] loss: 4.746644591152873\n",
      "[EPOCH #0, step #2612] loss: 4.746524160226733\n",
      "[EPOCH #0, step #2614] loss: 4.746535827461666\n",
      "[EPOCH #0, step #2616] loss: 4.746526097176318\n",
      "[EPOCH #0, step #2618] loss: 4.746420002384193\n",
      "[EPOCH #0, step #2620] loss: 4.746365575925033\n",
      "[EPOCH #0, step #2622] loss: 4.746421597633842\n",
      "[EPOCH #0, step #2624] loss: 4.746527543749128\n",
      "[EPOCH #0, step #2626] loss: 4.746651193512045\n",
      "[EPOCH #0, step #2628] loss: 4.746567463865748\n",
      "[EPOCH #0, step #2630] loss: 4.746492021757131\n",
      "[EPOCH #0, step #2632] loss: 4.74644932075005\n",
      "[EPOCH #0, step #2634] loss: 4.746472565725135\n",
      "[EPOCH #0, step #2636] loss: 4.746481724385742\n",
      "[EPOCH #0, step #2638] loss: 4.746470604398207\n",
      "[EPOCH #0, step #2640] loss: 4.746570489299158\n",
      "[EPOCH #0] loss: 4.746570489299158\n",
      "[EPOCH #1, step #0] loss: 4.928469181060791\n",
      "[EPOCH #1, step #2] loss: 4.858626365661621\n",
      "[EPOCH #1, step #4] loss: 4.844528484344482\n",
      "[EPOCH #1, step #6] loss: 4.7413128444126675\n",
      "[EPOCH #1, step #8] loss: 4.708477020263672\n",
      "[EPOCH #1, step #10] loss: 4.650796630165794\n",
      "[EPOCH #1, step #12] loss: 4.588686833014855\n",
      "[EPOCH #1, step #14] loss: 4.547485478719076\n",
      "[EPOCH #1, step #16] loss: 4.497687704422894\n",
      "[EPOCH #1, step #18] loss: 4.431939225447805\n",
      "[EPOCH #1, step #20] loss: 4.41138490041097\n",
      "[EPOCH #1, step #22] loss: 4.372549471647843\n",
      "[EPOCH #1, step #24] loss: 4.365690841674804\n",
      "[EPOCH #1, step #26] loss: 4.354999453933151\n",
      "[EPOCH #1, step #28] loss: 4.3373688993782835\n",
      "[EPOCH #1, step #30] loss: 4.321197555911157\n",
      "[EPOCH #1, step #32] loss: 4.2923798488848135\n",
      "[EPOCH #1, step #34] loss: 4.25656681060791\n",
      "[EPOCH #1, step #36] loss: 4.2415191160665975\n",
      "[EPOCH #1, step #38] loss: 4.21776933547778\n",
      "[EPOCH #1, step #40] loss: 4.198344149240634\n",
      "[EPOCH #1, step #42] loss: 4.176001504410145\n",
      "[EPOCH #1, step #44] loss: 4.164073170555962\n",
      "[EPOCH #1, step #46] loss: 4.152596478766584\n",
      "[EPOCH #1, step #48] loss: 4.1353790127501195\n",
      "[EPOCH #1, step #50] loss: 4.13386477208605\n",
      "[EPOCH #1, step #52] loss: 4.119402939418577\n",
      "[EPOCH #1, step #54] loss: 4.096813084862449\n",
      "[EPOCH #1, step #56] loss: 4.084827699159321\n",
      "[EPOCH #1, step #58] loss: 4.072158065892882\n",
      "[EPOCH #1, step #60] loss: 4.0683650227843735\n",
      "[EPOCH #1, step #62] loss: 4.051782713996039\n",
      "[EPOCH #1, step #64] loss: 4.0460654222048245\n",
      "[EPOCH #1, step #66] loss: 4.034560395710504\n",
      "[EPOCH #1, step #68] loss: 4.026899731677512\n",
      "[EPOCH #1, step #70] loss: 4.027871256143275\n",
      "[EPOCH #1, step #72] loss: 4.012113238034183\n",
      "[EPOCH #1, step #74] loss: 4.005981794993082\n",
      "[EPOCH #1, step #76] loss: 3.997664367997801\n",
      "[EPOCH #1, step #78] loss: 3.9930974048904226\n",
      "[EPOCH #1, step #80] loss: 3.990133306126536\n",
      "[EPOCH #1, step #82] loss: 3.97044209112604\n",
      "[EPOCH #1, step #84] loss: 3.966615643220789\n",
      "[EPOCH #1, step #86] loss: 3.956501944311734\n",
      "[EPOCH #1, step #88] loss: 3.948336812887299\n",
      "[EPOCH #1, step #90] loss: 3.9425062399644117\n",
      "[EPOCH #1, step #92] loss: 3.944103928022487\n",
      "[EPOCH #1, step #94] loss: 3.934981531845896\n",
      "[EPOCH #1, step #96] loss: 3.9316592536021755\n",
      "[EPOCH #1, step #98] loss: 3.917537212371826\n",
      "[EPOCH #1, step #100] loss: 3.915001111455483\n",
      "[EPOCH #1, step #102] loss: 3.9084018480430527\n",
      "[EPOCH #1, step #104] loss: 3.9086017903827486\n",
      "[EPOCH #1, step #106] loss: 3.9065570563913505\n",
      "[EPOCH #1, step #108] loss: 3.8946737236932876\n",
      "[EPOCH #1, step #110] loss: 3.8942354047620618\n",
      "[EPOCH #1, step #112] loss: 3.888546361332446\n",
      "[EPOCH #1, step #114] loss: 3.8820509371550185\n",
      "[EPOCH #1, step #116] loss: 3.8743602332905827\n",
      "[EPOCH #1, step #118] loss: 3.8690929913721166\n",
      "[EPOCH #1, step #120] loss: 3.868703601774105\n",
      "[EPOCH #1, step #122] loss: 3.8633298815750496\n",
      "[EPOCH #1, step #124] loss: 3.8615576801300047\n",
      "[EPOCH #1, step #126] loss: 3.8603371841700995\n",
      "[EPOCH #1, step #128] loss: 3.8598862507546596\n",
      "[EPOCH #1, step #130] loss: 3.8663310531441493\n",
      "[EPOCH #1, step #132] loss: 3.8641729749234996\n",
      "[EPOCH #1, step #134] loss: 3.8608835573549625\n",
      "[EPOCH #1, step #136] loss: 3.858855007338698\n",
      "[EPOCH #1, step #138] loss: 3.856235531594256\n",
      "[EPOCH #1, step #140] loss: 3.8559638236431364\n",
      "[EPOCH #1, step #142] loss: 3.851039834789463\n",
      "[EPOCH #1, step #144] loss: 3.852856660711354\n",
      "[EPOCH #1, step #146] loss: 3.8519013463234413\n",
      "[EPOCH #1, step #148] loss: 3.8435244080204294\n",
      "[EPOCH #1, step #150] loss: 3.8401785417897814\n",
      "[EPOCH #1, step #152] loss: 3.8366782026353223\n",
      "[EPOCH #1, step #154] loss: 3.8312723575099823\n",
      "[EPOCH #1, step #156] loss: 3.8242779264024866\n",
      "[EPOCH #1, step #158] loss: 3.823267177965656\n",
      "[EPOCH #1, step #160] loss: 3.817256520253531\n",
      "[EPOCH #1, step #162] loss: 3.817708195352847\n",
      "[EPOCH #1, step #164] loss: 3.8082509402072793\n",
      "[EPOCH #1, step #166] loss: 3.806037814317349\n",
      "[EPOCH #1, step #168] loss: 3.806626534320899\n",
      "[EPOCH #1, step #170] loss: 3.803816070333559\n",
      "[EPOCH #1, step #172] loss: 3.7990470828348504\n",
      "[EPOCH #1, step #174] loss: 3.798581529344831\n",
      "[EPOCH #1, step #176] loss: 3.7984853962720453\n",
      "[EPOCH #1, step #178] loss: 3.795046272224554\n",
      "[EPOCH #1, step #180] loss: 3.7948622795758324\n",
      "[EPOCH #1, step #182] loss: 3.7902313542496313\n",
      "[EPOCH #1, step #184] loss: 3.7872652401795257\n",
      "[EPOCH #1, step #186] loss: 3.784766378249714\n",
      "[EPOCH #1, step #188] loss: 3.7848162487070396\n",
      "[EPOCH #1, step #190] loss: 3.7812852959358256\n",
      "[EPOCH #1, step #192] loss: 3.7806006849120934\n",
      "[EPOCH #1, step #194] loss: 3.7805822629194994\n",
      "[EPOCH #1, step #196] loss: 3.780331088806772\n",
      "[EPOCH #1, step #198] loss: 3.7806396484375\n",
      "[EPOCH #1, step #200] loss: 3.777799000194417\n",
      "[EPOCH #1, step #202] loss: 3.7773371917273613\n",
      "[EPOCH #1, step #204] loss: 3.776975307231996\n",
      "[EPOCH #1, step #206] loss: 3.7736171789215383\n",
      "[EPOCH #1, step #208] loss: 3.7711150440873142\n",
      "[EPOCH #1, step #210] loss: 3.7709434733006626\n",
      "[EPOCH #1, step #212] loss: 3.7684475077150013\n",
      "[EPOCH #1, step #214] loss: 3.766830894558929\n",
      "[EPOCH #1, step #216] loss: 3.767512421454153\n",
      "[EPOCH #1, step #218] loss: 3.766091121386175\n",
      "[EPOCH #1, step #220] loss: 3.76454031737142\n",
      "[EPOCH #1, step #222] loss: 3.7600528915901354\n",
      "[EPOCH #1, step #224] loss: 3.7572938007778593\n",
      "[EPOCH #1, step #226] loss: 3.755229285109936\n",
      "[EPOCH #1, step #228] loss: 3.754301763517888\n",
      "[EPOCH #1, step #230] loss: 3.7533971675030595\n",
      "[EPOCH #1, step #232] loss: 3.7512786286071647\n",
      "[EPOCH #1, step #234] loss: 3.7457052240980433\n",
      "[EPOCH #1, step #236] loss: 3.741661068759387\n",
      "[EPOCH #1, step #238] loss: 3.7426813077727124\n",
      "[EPOCH #1, step #240] loss: 3.7394681402261822\n",
      "[EPOCH #1, step #242] loss: 3.7363307740953235\n",
      "[EPOCH #1, step #244] loss: 3.7357528092909833\n",
      "[EPOCH #1, step #246] loss: 3.733015382820778\n",
      "[EPOCH #1, step #248] loss: 3.7331926822662354\n",
      "[EPOCH #1, step #250] loss: 3.730441042151584\n",
      "[EPOCH #1, step #252] loss: 3.730505008471342\n",
      "[EPOCH #1, step #254] loss: 3.7322500443926043\n",
      "[EPOCH #1, step #256] loss: 3.734412010541686\n",
      "[EPOCH #1, step #258] loss: 3.7308023896456683\n",
      "[EPOCH #1, step #260] loss: 3.7294383597099916\n",
      "[EPOCH #1, step #262] loss: 3.727903308070658\n",
      "[EPOCH #1, step #264] loss: 3.7251506094662648\n",
      "[EPOCH #1, step #266] loss: 3.725640160314153\n",
      "[EPOCH #1, step #268] loss: 3.724996159954142\n",
      "[EPOCH #1, step #270] loss: 3.724755867820824\n",
      "[EPOCH #1, step #272] loss: 3.7248606646890607\n",
      "[EPOCH #1, step #274] loss: 3.724718330556696\n",
      "[EPOCH #1, step #276] loss: 3.723342866243438\n",
      "[EPOCH #1, step #278] loss: 3.7240550808581827\n",
      "[EPOCH #1, step #280] loss: 3.7226883066930805\n",
      "[EPOCH #1, step #282] loss: 3.72108111027694\n",
      "[EPOCH #1, step #284] loss: 3.7179288487685356\n",
      "[EPOCH #1, step #286] loss: 3.714976162029891\n",
      "[EPOCH #1, step #288] loss: 3.714204200823827\n",
      "[EPOCH #1, step #290] loss: 3.7126749332008493\n",
      "[EPOCH #1, step #292] loss: 3.7135819745958867\n",
      "[EPOCH #1, step #294] loss: 3.7117046647152656\n",
      "[EPOCH #1, step #296] loss: 3.7121214633838897\n",
      "[EPOCH #1, step #298] loss: 3.708906967105674\n",
      "[EPOCH #1, step #300] loss: 3.7076828844127463\n",
      "[EPOCH #1, step #302] loss: 3.7059433483841397\n",
      "[EPOCH #1, step #304] loss: 3.7035654521379313\n",
      "[EPOCH #1, step #306] loss: 3.7031845976553055\n",
      "[EPOCH #1, step #308] loss: 3.700737521100584\n",
      "[EPOCH #1, step #310] loss: 3.702458860789848\n",
      "[EPOCH #1, step #312] loss: 3.7027009622738385\n",
      "[EPOCH #1, step #314] loss: 3.698067381268456\n",
      "[EPOCH #1, step #316] loss: 3.6947923391023267\n",
      "[EPOCH #1, step #318] loss: 3.695451353037245\n",
      "[EPOCH #1, step #320] loss: 3.694825162025998\n",
      "[EPOCH #1, step #322] loss: 3.6931484786349555\n",
      "[EPOCH #1, step #324] loss: 3.69314885359544\n",
      "[EPOCH #1, step #326] loss: 3.6921555026226454\n",
      "[EPOCH #1, step #328] loss: 3.692123334820872\n",
      "[EPOCH #1, step #330] loss: 3.691135539749238\n",
      "[EPOCH #1, step #332] loss: 3.6899215272954993\n",
      "[EPOCH #1, step #334] loss: 3.688907769188952\n",
      "[EPOCH #1, step #336] loss: 3.688149522249352\n",
      "[EPOCH #1, step #338] loss: 3.6860534680628145\n",
      "[EPOCH #1, step #340] loss: 3.6890925144519975\n",
      "[EPOCH #1, step #342] loss: 3.6891259820398714\n",
      "[EPOCH #1, step #344] loss: 3.6874703255252563\n",
      "[EPOCH #1, step #346] loss: 3.687170197022408\n",
      "[EPOCH #1, step #348] loss: 3.6864871514219266\n",
      "[EPOCH #1, step #350] loss: 3.687038619973381\n",
      "[EPOCH #1, step #352] loss: 3.6867395527977442\n",
      "[EPOCH #1, step #354] loss: 3.683638416209691\n",
      "[EPOCH #1, step #356] loss: 3.680965231246307\n",
      "[EPOCH #1, step #358] loss: 3.679389285509965\n",
      "[EPOCH #1, step #360] loss: 3.678724166098724\n",
      "[EPOCH #1, step #362] loss: 3.6773225683154482\n",
      "[EPOCH #1, step #364] loss: 3.676248349882152\n",
      "[EPOCH #1, step #366] loss: 3.6742774894516863\n",
      "[EPOCH #1, step #368] loss: 3.673664264239593\n",
      "[EPOCH #1, step #370] loss: 3.670281576660444\n",
      "[EPOCH #1, step #372] loss: 3.66691135209623\n",
      "[EPOCH #1, step #374] loss: 3.663755781809489\n",
      "[EPOCH #1, step #376] loss: 3.664798190170005\n",
      "[EPOCH #1, step #378] loss: 3.663273634570884\n",
      "[EPOCH #1, step #380] loss: 3.661417872886958\n",
      "[EPOCH #1, step #382] loss: 3.6630054379256527\n",
      "[EPOCH #1, step #384] loss: 3.662310335852883\n",
      "[EPOCH #1, step #386] loss: 3.66062159932553\n",
      "[EPOCH #1, step #388] loss: 3.6591797261127774\n",
      "[EPOCH #1, step #390] loss: 3.6576862219349504\n",
      "[EPOCH #1, step #392] loss: 3.657967700909719\n",
      "[EPOCH #1, step #394] loss: 3.656171836128718\n",
      "[EPOCH #1, step #396] loss: 3.6567180565082156\n",
      "[EPOCH #1, step #398] loss: 3.657378865961443\n",
      "[EPOCH #1, step #400] loss: 3.657492183984961\n",
      "[EPOCH #1, step #402] loss: 3.6576109914566683\n",
      "[EPOCH #1, step #404] loss: 3.6580930480250604\n",
      "[EPOCH #1, step #406] loss: 3.655364889477629\n",
      "[EPOCH #1, step #408] loss: 3.6536689294288096\n",
      "[EPOCH #1, step #410] loss: 3.6550340588655494\n",
      "[EPOCH #1, step #412] loss: 3.652284587555301\n",
      "[EPOCH #1, step #414] loss: 3.6490397436073025\n",
      "[EPOCH #1, step #416] loss: 3.6474032287689138\n",
      "[EPOCH #1, step #418] loss: 3.6467002143723297\n",
      "[EPOCH #1, step #420] loss: 3.6455714260880567\n",
      "[EPOCH #1, step #422] loss: 3.64449359282816\n",
      "[EPOCH #1, step #424] loss: 3.645175521513995\n",
      "[EPOCH #1, step #426] loss: 3.6470763102627473\n",
      "[EPOCH #1, step #428] loss: 3.6466756545024595\n",
      "[EPOCH #1, step #430] loss: 3.6459648548312087\n",
      "[EPOCH #1, step #432] loss: 3.6430098035869642\n",
      "[EPOCH #1, step #434] loss: 3.640597848782594\n",
      "[EPOCH #1, step #436] loss: 3.639418180801776\n",
      "[EPOCH #1, step #438] loss: 3.6373872311620343\n",
      "[EPOCH #1, step #440] loss: 3.6377237443210317\n",
      "[EPOCH #1, step #442] loss: 3.6371787181018975\n",
      "[EPOCH #1, step #444] loss: 3.63608082546277\n",
      "[EPOCH #1, step #446] loss: 3.6365415793937323\n",
      "[EPOCH #1, step #448] loss: 3.634347642715896\n",
      "[EPOCH #1, step #450] loss: 3.6358662317703145\n",
      "[EPOCH #1, step #452] loss: 3.6343384468002826\n",
      "[EPOCH #1, step #454] loss: 3.634562081033057\n",
      "[EPOCH #1, step #456] loss: 3.633048592824122\n",
      "[EPOCH #1, step #458] loss: 3.633231588957876\n",
      "[EPOCH #1, step #460] loss: 3.6327614091223595\n",
      "[EPOCH #1, step #462] loss: 3.632324889720645\n",
      "[EPOCH #1, step #464] loss: 3.63220471361632\n",
      "[EPOCH #1, step #466] loss: 3.6289257339542886\n",
      "[EPOCH #1, step #468] loss: 3.6295574418008965\n",
      "[EPOCH #1, step #470] loss: 3.628343100760393\n",
      "[EPOCH #1, step #472] loss: 3.625782447435891\n",
      "[EPOCH #1, step #474] loss: 3.6236573128951224\n",
      "[EPOCH #1, step #476] loss: 3.6245363478390678\n",
      "[EPOCH #1, step #478] loss: 3.62398327218217\n",
      "[EPOCH #1, step #480] loss: 3.622827112055122\n",
      "[EPOCH #1, step #482] loss: 3.62383504683927\n",
      "[EPOCH #1, step #484] loss: 3.6237210735832295\n",
      "[EPOCH #1, step #486] loss: 3.623287509354233\n",
      "[EPOCH #1, step #488] loss: 3.6231938476211454\n",
      "[EPOCH #1, step #490] loss: 3.621966575168058\n",
      "[EPOCH #1, step #492] loss: 3.621227397396405\n",
      "[EPOCH #1, step #494] loss: 3.6191337291640466\n",
      "[EPOCH #1, step #496] loss: 3.618383411430497\n",
      "[EPOCH #1, step #498] loss: 3.6195054517719214\n",
      "[EPOCH #1, step #500] loss: 3.6196175530523123\n",
      "[EPOCH #1, step #502] loss: 3.6183546068180155\n",
      "[EPOCH #1, step #504] loss: 3.6168156288637974\n",
      "[EPOCH #1, step #506] loss: 3.615619376331156\n",
      "[EPOCH #1, step #508] loss: 3.6136190244865793\n",
      "[EPOCH #1, step #510] loss: 3.613157364253662\n",
      "[EPOCH #1, step #512] loss: 3.6122478899667603\n",
      "[EPOCH #1, step #514] loss: 3.611774933916851\n",
      "[EPOCH #1, step #516] loss: 3.6124615567795773\n",
      "[EPOCH #1, step #518] loss: 3.610323091003477\n",
      "[EPOCH #1, step #520] loss: 3.6086148186059464\n",
      "[EPOCH #1, step #522] loss: 3.6072231893557665\n",
      "[EPOCH #1, step #524] loss: 3.6062087136223204\n",
      "[EPOCH #1, step #526] loss: 3.6047612752136287\n",
      "[EPOCH #1, step #528] loss: 3.6045032160493964\n",
      "[EPOCH #1, step #530] loss: 3.6032923493681652\n",
      "[EPOCH #1, step #532] loss: 3.603660782551005\n",
      "[EPOCH #1, step #534] loss: 3.6035643858330273\n",
      "[EPOCH #1, step #536] loss: 3.60332459400042\n",
      "[EPOCH #1, step #538] loss: 3.601794033192084\n",
      "[EPOCH #1, step #540] loss: 3.602049622209588\n",
      "[EPOCH #1, step #542] loss: 3.600982505313599\n",
      "[EPOCH #1, step #544] loss: 3.5987245428452797\n",
      "[EPOCH #1, step #546] loss: 3.5988094893844296\n",
      "[EPOCH #1, step #548] loss: 3.5980833310681395\n",
      "[EPOCH #1, step #550] loss: 3.5969293606475996\n",
      "[EPOCH #1, step #552] loss: 3.596340405358975\n",
      "[EPOCH #1, step #554] loss: 3.5962223864890435\n",
      "[EPOCH #1, step #556] loss: 3.5955976472508757\n",
      "[EPOCH #1, step #558] loss: 3.5949166349946706\n",
      "[EPOCH #1, step #560] loss: 3.5936724068646764\n",
      "[EPOCH #1, step #562] loss: 3.593107110018417\n",
      "[EPOCH #1, step #564] loss: 3.593183394659937\n",
      "[EPOCH #1, step #566] loss: 3.5936461853182085\n",
      "[EPOCH #1, step #568] loss: 3.5931731326299308\n",
      "[EPOCH #1, step #570] loss: 3.592643549897416\n",
      "[EPOCH #1, step #572] loss: 3.5929956556822824\n",
      "[EPOCH #1, step #574] loss: 3.5917705751501994\n",
      "[EPOCH #1, step #576] loss: 3.5928737770126844\n",
      "[EPOCH #1, step #578] loss: 3.5923538166830165\n",
      "[EPOCH #1, step #580] loss: 3.591348617294364\n",
      "[EPOCH #1, step #582] loss: 3.5916393809212215\n",
      "[EPOCH #1, step #584] loss: 3.592044265453632\n",
      "[EPOCH #1, step #586] loss: 3.5918599527576713\n",
      "[EPOCH #1, step #588] loss: 3.5909154135828714\n",
      "[EPOCH #1, step #590] loss: 3.590126629408241\n",
      "[EPOCH #1, step #592] loss: 3.5900527952493344\n",
      "[EPOCH #1, step #594] loss: 3.5892915649574344\n",
      "[EPOCH #1, step #596] loss: 3.587907513581728\n",
      "[EPOCH #1, step #598] loss: 3.5876184465093086\n",
      "[EPOCH #1, step #600] loss: 3.5868891420062887\n",
      "[EPOCH #1, step #602] loss: 3.5858535248644117\n",
      "[EPOCH #1, step #604] loss: 3.5861486537397402\n",
      "[EPOCH #1, step #606] loss: 3.585568646033474\n",
      "[EPOCH #1, step #608] loss: 3.5846330966855504\n",
      "[EPOCH #1, step #610] loss: 3.583967133161479\n",
      "[EPOCH #1, step #612] loss: 3.583808002036314\n",
      "[EPOCH #1, step #614] loss: 3.5842266672025853\n",
      "[EPOCH #1, step #616] loss: 3.583696144712995\n",
      "[EPOCH #1, step #618] loss: 3.5828789728716233\n",
      "[EPOCH #1, step #620] loss: 3.5819089447242627\n",
      "[EPOCH #1, step #622] loss: 3.5813549360915133\n",
      "[EPOCH #1, step #624] loss: 3.58131346282959\n",
      "[EPOCH #1, step #626] loss: 3.581067825998796\n",
      "[EPOCH #1, step #628] loss: 3.5814850595683474\n",
      "[EPOCH #1, step #630] loss: 3.579558391011838\n",
      "[EPOCH #1, step #632] loss: 3.578999082812389\n",
      "[EPOCH #1, step #634] loss: 3.578883615628941\n",
      "[EPOCH #1, step #636] loss: 3.577363663977319\n",
      "[EPOCH #1, step #638] loss: 3.5776593517249737\n",
      "[EPOCH #1, step #640] loss: 3.5771787557140713\n",
      "[EPOCH #1, step #642] loss: 3.576711287580048\n",
      "[EPOCH #1, step #644] loss: 3.5759249476499337\n",
      "[EPOCH #1, step #646] loss: 3.5752685858624798\n",
      "[EPOCH #1, step #648] loss: 3.5746811237100093\n",
      "[EPOCH #1, step #650] loss: 3.574609414406819\n",
      "[EPOCH #1, step #652] loss: 3.573230508640751\n",
      "[EPOCH #1, step #654] loss: 3.573775076684151\n",
      "[EPOCH #1, step #656] loss: 3.573779821395874\n",
      "[EPOCH #1, step #658] loss: 3.573258059520461\n",
      "[EPOCH #1, step #660] loss: 3.5729881959675662\n",
      "[EPOCH #1, step #662] loss: 3.5721495572258446\n",
      "[EPOCH #1, step #664] loss: 3.570761736174275\n",
      "[EPOCH #1, step #666] loss: 3.5701319563692655\n",
      "[EPOCH #1, step #668] loss: 3.569588623773891\n",
      "[EPOCH #1, step #670] loss: 3.569138919187907\n",
      "[EPOCH #1, step #672] loss: 3.569525488772683\n",
      "[EPOCH #1, step #674] loss: 3.5690941146568016\n",
      "[EPOCH #1, step #676] loss: 3.5688212245394597\n",
      "[EPOCH #1, step #678] loss: 3.5680108066862215\n",
      "[EPOCH #1, step #680] loss: 3.5672788409934695\n",
      "[EPOCH #1, step #682] loss: 3.5657170010869708\n",
      "[EPOCH #1, step #684] loss: 3.5652240422520323\n",
      "[EPOCH #1, step #686] loss: 3.56403275625848\n",
      "[EPOCH #1, step #688] loss: 3.5637260171948393\n",
      "[EPOCH #1, step #690] loss: 3.563466112106478\n",
      "[EPOCH #1, step #692] loss: 3.562550854373288\n",
      "[EPOCH #1, step #694] loss: 3.5619924380624894\n",
      "[EPOCH #1, step #696] loss: 3.5608607576772506\n",
      "[EPOCH #1, step #698] loss: 3.5613470797204494\n",
      "[EPOCH #1, step #700] loss: 3.5619725507608324\n",
      "[EPOCH #1, step #702] loss: 3.561448012103736\n",
      "[EPOCH #1, step #704] loss: 3.5614626509078007\n",
      "[EPOCH #1, step #706] loss: 3.5594238307553607\n",
      "[EPOCH #1, step #708] loss: 3.5579624142397948\n",
      "[EPOCH #1, step #710] loss: 3.5577659543221176\n",
      "[EPOCH #1, step #712] loss: 3.5568883679859433\n",
      "[EPOCH #1, step #714] loss: 3.556780392973573\n",
      "[EPOCH #1, step #716] loss: 3.556439565315406\n",
      "[EPOCH #1, step #718] loss: 3.5570201419490766\n",
      "[EPOCH #1, step #720] loss: 3.556611091849212\n",
      "[EPOCH #1, step #722] loss: 3.555895862407869\n",
      "[EPOCH #1, step #724] loss: 3.5566501952861915\n",
      "[EPOCH #1, step #726] loss: 3.555967295186228\n",
      "[EPOCH #1, step #728] loss: 3.5563416350198542\n",
      "[EPOCH #1, step #730] loss: 3.5560944129275884\n",
      "[EPOCH #1, step #732] loss: 3.5549802773646086\n",
      "[EPOCH #1, step #734] loss: 3.5542087623051235\n",
      "[EPOCH #1, step #736] loss: 3.5536398463850913\n",
      "[EPOCH #1, step #738] loss: 3.5536484440866762\n",
      "[EPOCH #1, step #740] loss: 3.55309722426771\n",
      "[EPOCH #1, step #742] loss: 3.5527288445232696\n",
      "[EPOCH #1, step #744] loss: 3.551380708873672\n",
      "[EPOCH #1, step #746] loss: 3.551503084111246\n",
      "[EPOCH #1, step #748] loss: 3.5499932084764754\n",
      "[EPOCH #1, step #750] loss: 3.548864996703106\n",
      "[EPOCH #1, step #752] loss: 3.546437044700937\n",
      "[EPOCH #1, step #754] loss: 3.5466231642969395\n",
      "[EPOCH #1, step #756] loss: 3.5468067880350818\n",
      "[EPOCH #1, step #758] loss: 3.5460453661691216\n",
      "[EPOCH #1, step #760] loss: 3.5451400608960024\n",
      "[EPOCH #1, step #762] loss: 3.54447936605501\n",
      "[EPOCH #1, step #764] loss: 3.5435043250813205\n",
      "[EPOCH #1, step #766] loss: 3.543446503531047\n",
      "[EPOCH #1, step #768] loss: 3.5441560038354525\n",
      "[EPOCH #1, step #770] loss: 3.543200089619163\n",
      "[EPOCH #1, step #772] loss: 3.5437396352997417\n",
      "[EPOCH #1, step #774] loss: 3.5432110463419266\n",
      "[EPOCH #1, step #776] loss: 3.5428126898511496\n",
      "[EPOCH #1, step #778] loss: 3.5415835579491395\n",
      "[EPOCH #1, step #780] loss: 3.541130202680483\n",
      "[EPOCH #1, step #782] loss: 3.5417755968573754\n",
      "[EPOCH #1, step #784] loss: 3.5406829691236945\n",
      "[EPOCH #1, step #786] loss: 3.5407639994857574\n",
      "[EPOCH #1, step #788] loss: 3.5405647887627585\n",
      "[EPOCH #1, step #790] loss: 3.540858572563239\n",
      "[EPOCH #1, step #792] loss: 3.5405565808429693\n",
      "[EPOCH #1, step #794] loss: 3.5397517045338946\n",
      "[EPOCH #1, step #796] loss: 3.5395882910437684\n",
      "[EPOCH #1, step #798] loss: 3.539285945653617\n",
      "[EPOCH #1, step #800] loss: 3.5389832944310413\n",
      "[EPOCH #1, step #802] loss: 3.537937862044699\n",
      "[EPOCH #1, step #804] loss: 3.5381500383341535\n",
      "[EPOCH #1, step #806] loss: 3.5376168487978985\n",
      "[EPOCH #1, step #808] loss: 3.537634824792888\n",
      "[EPOCH #1, step #810] loss: 3.5372003374205563\n",
      "[EPOCH #1, step #812] loss: 3.536878569392994\n",
      "[EPOCH #1, step #814] loss: 3.5360440573077994\n",
      "[EPOCH #1, step #816] loss: 3.536339092721554\n",
      "[EPOCH #1, step #818] loss: 3.5376581997225136\n",
      "[EPOCH #1, step #820] loss: 3.536845210699158\n",
      "[EPOCH #1, step #822] loss: 3.536073764358372\n",
      "[EPOCH #1, step #824] loss: 3.53679193872394\n",
      "[EPOCH #1, step #826] loss: 3.5366868557635933\n",
      "[EPOCH #1, step #828] loss: 3.5355584897523333\n",
      "[EPOCH #1, step #830] loss: 3.5351448899715527\n",
      "[EPOCH #1, step #832] loss: 3.53463963758187\n",
      "[EPOCH #1, step #834] loss: 3.5338593354482137\n",
      "[EPOCH #1, step #836] loss: 3.53348685706673\n",
      "[EPOCH #1, step #838] loss: 3.5331158186170284\n",
      "[EPOCH #1, step #840] loss: 3.5317950920599395\n",
      "[EPOCH #1, step #842] loss: 3.531345833238758\n",
      "[EPOCH #1, step #844] loss: 3.5308479173648992\n",
      "[EPOCH #1, step #846] loss: 3.5302899930947222\n",
      "[EPOCH #1, step #848] loss: 3.529630337222024\n",
      "[EPOCH #1, step #850] loss: 3.5296640533397956\n",
      "[EPOCH #1, step #852] loss: 3.529988255897692\n",
      "[EPOCH #1, step #854] loss: 3.530078152606362\n",
      "[EPOCH #1, step #856] loss: 3.5303310986299636\n",
      "[EPOCH #1, step #858] loss: 3.5304071806084987\n",
      "[EPOCH #1, step #860] loss: 3.52932911744378\n",
      "[EPOCH #1, step #862] loss: 3.528875018878994\n",
      "[EPOCH #1, step #864] loss: 3.5283445427183473\n",
      "[EPOCH #1, step #866] loss: 3.5277720622400373\n",
      "[EPOCH #1, step #868] loss: 3.5277240429978103\n",
      "[EPOCH #1, step #870] loss: 3.5267504378383387\n",
      "[EPOCH #1, step #872] loss: 3.526401478263794\n",
      "[EPOCH #1, step #874] loss: 3.5260490899767194\n",
      "[EPOCH #1, step #876] loss: 3.5252739159791746\n",
      "[EPOCH #1, step #878] loss: 3.5244750569704855\n",
      "[EPOCH #1, step #880] loss: 3.5237328294457426\n",
      "[EPOCH #1, step #882] loss: 3.5230103994414868\n",
      "[EPOCH #1, step #884] loss: 3.5231927025789593\n",
      "[EPOCH #1, step #886] loss: 3.522843100172557\n",
      "[EPOCH #1, step #888] loss: 3.522146758400281\n",
      "[EPOCH #1, step #890] loss: 3.522764169005834\n",
      "[EPOCH #1, step #892] loss: 3.522241735778757\n",
      "[EPOCH #1, step #894] loss: 3.522566513509058\n",
      "[EPOCH #1, step #896] loss: 3.5218330939345\n",
      "[EPOCH #1, step #898] loss: 3.5208166621550836\n",
      "[EPOCH #1, step #900] loss: 3.5204574320874653\n",
      "[EPOCH #1, step #902] loss: 3.5204700921989094\n",
      "[EPOCH #1, step #904] loss: 3.520367666634407\n",
      "[EPOCH #1, step #906] loss: 3.5198773649062187\n",
      "[EPOCH #1, step #908] loss: 3.5194949836227365\n",
      "[EPOCH #1, step #910] loss: 3.518994805579913\n",
      "[EPOCH #1, step #912] loss: 3.519206290438298\n",
      "[EPOCH #1, step #914] loss: 3.519673612208966\n",
      "[EPOCH #1, step #916] loss: 3.5201964149558296\n",
      "[EPOCH #1, step #918] loss: 3.5196370328730935\n",
      "[EPOCH #1, step #920] loss: 3.518472120376156\n",
      "[EPOCH #1, step #922] loss: 3.518435646885782\n",
      "[EPOCH #1, step #924] loss: 3.519427626841777\n",
      "[EPOCH #1, step #926] loss: 3.5198841833116683\n",
      "[EPOCH #1, step #928] loss: 3.519679072598723\n",
      "[EPOCH #1, step #930] loss: 3.519154107148102\n",
      "[EPOCH #1, step #932] loss: 3.518628286778735\n",
      "[EPOCH #1, step #934] loss: 3.5173225930667815\n",
      "[EPOCH #1, step #936] loss: 3.516902553869795\n",
      "[EPOCH #1, step #938] loss: 3.516087493449608\n",
      "[EPOCH #1, step #940] loss: 3.5159653406214133\n",
      "[EPOCH #1, step #942] loss: 3.516487614600302\n",
      "[EPOCH #1, step #944] loss: 3.5161280008850904\n",
      "[EPOCH #1, step #946] loss: 3.5162000072040174\n",
      "[EPOCH #1, step #948] loss: 3.5160335160407175\n",
      "[EPOCH #1, step #950] loss: 3.515683414808207\n",
      "[EPOCH #1, step #952] loss: 3.5151417763260704\n",
      "[EPOCH #1, step #954] loss: 3.5150362449166663\n",
      "[EPOCH #1, step #956] loss: 3.516045056415825\n",
      "[EPOCH #1, step #958] loss: 3.5162799850619\n",
      "[EPOCH #1, step #960] loss: 3.5164192539099974\n",
      "[EPOCH #1, step #962] loss: 3.517023045202158\n",
      "[EPOCH #1, step #964] loss: 3.516905405854932\n",
      "[EPOCH #1, step #966] loss: 3.5151085663729393\n",
      "[EPOCH #1, step #968] loss: 3.514294516560463\n",
      "[EPOCH #1, step #970] loss: 3.515176009689865\n",
      "[EPOCH #1, step #972] loss: 3.5145992904380807\n",
      "[EPOCH #1, step #974] loss: 3.5138789423918113\n",
      "[EPOCH #1, step #976] loss: 3.5133052872781745\n",
      "[EPOCH #1, step #978] loss: 3.51305348490792\n",
      "[EPOCH #1, step #980] loss: 3.513459258852384\n",
      "[EPOCH #1, step #982] loss: 3.513301987128971\n",
      "[EPOCH #1, step #984] loss: 3.514028073325375\n",
      "[EPOCH #1, step #986] loss: 3.5134174628098322\n",
      "[EPOCH #1, step #988] loss: 3.513510155364404\n",
      "[EPOCH #1, step #990] loss: 3.5141224767317323\n",
      "[EPOCH #1, step #992] loss: 3.514022558717449\n",
      "[EPOCH #1, step #994] loss: 3.513252738971806\n",
      "[EPOCH #1, step #996] loss: 3.5129520737658533\n",
      "[EPOCH #1, step #998] loss: 3.5123685866862804\n",
      "[EPOCH #1, step #1000] loss: 3.5119300858005063\n",
      "[EPOCH #1, step #1002] loss: 3.5111226179782795\n",
      "[EPOCH #1, step #1004] loss: 3.5106517784631075\n",
      "[EPOCH #1, step #1006] loss: 3.5110189561929106\n",
      "[EPOCH #1, step #1008] loss: 3.510697719953932\n",
      "[EPOCH #1, step #1010] loss: 3.5105995822966864\n",
      "[EPOCH #1, step #1012] loss: 3.5102088733816195\n",
      "[EPOCH #1, step #1014] loss: 3.5099369046723314\n",
      "[EPOCH #1, step #1016] loss: 3.509308331132287\n",
      "[EPOCH #1, step #1018] loss: 3.5093627132775156\n",
      "[EPOCH #1, step #1020] loss: 3.5090722762172297\n",
      "[EPOCH #1, step #1022] loss: 3.508802693144201\n",
      "[EPOCH #1, step #1024] loss: 3.5091395954969453\n",
      "[EPOCH #1, step #1026] loss: 3.5096833982161213\n",
      "[EPOCH #1, step #1028] loss: 3.509360743217728\n",
      "[EPOCH #1, step #1030] loss: 3.509193422260155\n",
      "[EPOCH #1, step #1032] loss: 3.5089693531062225\n",
      "[EPOCH #1, step #1034] loss: 3.508392522415677\n",
      "[EPOCH #1, step #1036] loss: 3.5090878212118564\n",
      "[EPOCH #1, step #1038] loss: 3.5088211613509146\n",
      "[EPOCH #1, step #1040] loss: 3.508290717970512\n",
      "[EPOCH #1, step #1042] loss: 3.508201762790991\n",
      "[EPOCH #1, step #1044] loss: 3.5082965643211987\n",
      "[EPOCH #1, step #1046] loss: 3.50911524097694\n",
      "[EPOCH #1, step #1048] loss: 3.5090361914257415\n",
      "[EPOCH #1, step #1050] loss: 3.508715878431282\n",
      "[EPOCH #1, step #1052] loss: 3.508507095737222\n",
      "[EPOCH #1, step #1054] loss: 3.5078280295240933\n",
      "[EPOCH #1, step #1056] loss: 3.507109327370419\n",
      "[EPOCH #1, step #1058] loss: 3.506953218503309\n",
      "[EPOCH #1, step #1060] loss: 3.506945444434235\n",
      "[EPOCH #1, step #1062] loss: 3.505840932885544\n",
      "[EPOCH #1, step #1064] loss: 3.5062552091661194\n",
      "[EPOCH #1, step #1066] loss: 3.5060692432335636\n",
      "[EPOCH #1, step #1068] loss: 3.506329160633212\n",
      "[EPOCH #1, step #1070] loss: 3.5055510657174245\n",
      "[EPOCH #1, step #1072] loss: 3.5055491155480407\n",
      "[EPOCH #1, step #1074] loss: 3.5059649294476176\n",
      "[EPOCH #1, step #1076] loss: 3.505752092395099\n",
      "[EPOCH #1, step #1078] loss: 3.5065937504048033\n",
      "[EPOCH #1, step #1080] loss: 3.5061877587677484\n",
      "[EPOCH #1, step #1082] loss: 3.5057257806825506\n",
      "[EPOCH #1, step #1084] loss: 3.5051002621101346\n",
      "[EPOCH #1, step #1086] loss: 3.505001721298793\n",
      "[EPOCH #1, step #1088] loss: 3.5044831616600463\n",
      "[EPOCH #1, step #1090] loss: 3.503898058977835\n",
      "[EPOCH #1, step #1092] loss: 3.5034983795057824\n",
      "[EPOCH #1, step #1094] loss: 3.5031495817175737\n",
      "[EPOCH #1, step #1096] loss: 3.502826328590553\n",
      "[EPOCH #1, step #1098] loss: 3.502147388631805\n",
      "[EPOCH #1, step #1100] loss: 3.5017708354814827\n",
      "[EPOCH #1, step #1102] loss: 3.50127918960177\n",
      "[EPOCH #1, step #1104] loss: 3.5003475042489858\n",
      "[EPOCH #1, step #1106] loss: 3.5001500165451724\n",
      "[EPOCH #1, step #1108] loss: 3.499480004349305\n",
      "[EPOCH #1, step #1110] loss: 3.4989541813020337\n",
      "[EPOCH #1, step #1112] loss: 3.4988565888366288\n",
      "[EPOCH #1, step #1114] loss: 3.4989554565583645\n",
      "[EPOCH #1, step #1116] loss: 3.498478852966158\n",
      "[EPOCH #1, step #1118] loss: 3.497442131281112\n",
      "[EPOCH #1, step #1120] loss: 3.497157025613708\n",
      "[EPOCH #1, step #1122] loss: 3.497097664822029\n",
      "[EPOCH #1, step #1124] loss: 3.4964231380886504\n",
      "[EPOCH #1, step #1126] loss: 3.4959505264380386\n",
      "[EPOCH #1, step #1128] loss: 3.495963295969528\n",
      "[EPOCH #1, step #1130] loss: 3.4963811046775732\n",
      "[EPOCH #1, step #1132] loss: 3.496782334210698\n",
      "[EPOCH #1, step #1134] loss: 3.4964550228370967\n",
      "[EPOCH #1, step #1136] loss: 3.4962745048335067\n",
      "[EPOCH #1, step #1138] loss: 3.496189266469418\n",
      "[EPOCH #1, step #1140] loss: 3.4968046281130873\n",
      "[EPOCH #1, step #1142] loss: 3.496011033771545\n",
      "[EPOCH #1, step #1144] loss: 3.4958670945146717\n",
      "[EPOCH #1, step #1146] loss: 3.4956739377019295\n",
      "[EPOCH #1, step #1148] loss: 3.4954751886628417\n",
      "[EPOCH #1, step #1150] loss: 3.495968035259628\n",
      "[EPOCH #1, step #1152] loss: 3.4959545788926243\n",
      "[EPOCH #1, step #1154] loss: 3.4956809320491113\n",
      "[EPOCH #1, step #1156] loss: 3.495781686456428\n",
      "[EPOCH #1, step #1158] loss: 3.4953911526641317\n",
      "[EPOCH #1, step #1160] loss: 3.4956504926919734\n",
      "[EPOCH #1, step #1162] loss: 3.49557419143047\n",
      "[EPOCH #1, step #1164] loss: 3.495761204993776\n",
      "[EPOCH #1, step #1166] loss: 3.495817839843959\n",
      "[EPOCH #1, step #1168] loss: 3.4956471869025707\n",
      "[EPOCH #1, step #1170] loss: 3.495636374394574\n",
      "[EPOCH #1, step #1172] loss: 3.4959068473139485\n",
      "[EPOCH #1, step #1174] loss: 3.4953907992991997\n",
      "[EPOCH #1, step #1176] loss: 3.495095227062449\n",
      "[EPOCH #1, step #1178] loss: 3.4952237231939702\n",
      "[EPOCH #1, step #1180] loss: 3.4944137884942683\n",
      "[EPOCH #1, step #1182] loss: 3.4944728442600796\n",
      "[EPOCH #1, step #1184] loss: 3.4938294535447776\n",
      "[EPOCH #1, step #1186] loss: 3.493425780820967\n",
      "[EPOCH #1, step #1188] loss: 3.493528592275711\n",
      "[EPOCH #1, step #1190] loss: 3.4931762460697406\n",
      "[EPOCH #1, step #1192] loss: 3.492803307410957\n",
      "[EPOCH #1, step #1194] loss: 3.492543744442353\n",
      "[EPOCH #1, step #1196] loss: 3.4926317036898813\n",
      "[EPOCH #1, step #1198] loss: 3.4923256594902083\n",
      "[EPOCH #1, step #1200] loss: 3.4921239827892165\n",
      "[EPOCH #1, step #1202] loss: 3.49163514281548\n",
      "[EPOCH #1, step #1204] loss: 3.491502909442696\n",
      "[EPOCH #1, step #1206] loss: 3.4909467932204126\n",
      "[EPOCH #1, step #1208] loss: 3.4907994195387517\n",
      "[EPOCH #1, step #1210] loss: 3.4904647434574825\n",
      "[EPOCH #1, step #1212] loss: 3.4898511803258283\n",
      "[EPOCH #1, step #1214] loss: 3.4891592127796063\n",
      "[EPOCH #1, step #1216] loss: 3.4894136512681118\n",
      "[EPOCH #1, step #1218] loss: 3.490099667329295\n",
      "[EPOCH #1, step #1220] loss: 3.4897418207657522\n",
      "[EPOCH #1, step #1222] loss: 3.48967657241291\n",
      "[EPOCH #1, step #1224] loss: 3.4891578707403066\n",
      "[EPOCH #1, step #1226] loss: 3.4889740483393585\n",
      "[EPOCH #1, step #1228] loss: 3.488626951895854\n",
      "[EPOCH #1, step #1230] loss: 3.4886991601954818\n",
      "[EPOCH #1, step #1232] loss: 3.4880404882284073\n",
      "[EPOCH #1, step #1234] loss: 3.4874517182106914\n",
      "[EPOCH #1, step #1236] loss: 3.4874240617744365\n",
      "[EPOCH #1, step #1238] loss: 3.487453877300481\n",
      "[EPOCH #1, step #1240] loss: 3.4877307607126657\n",
      "[EPOCH #1, step #1242] loss: 3.487379396527598\n",
      "[EPOCH #1, step #1244] loss: 3.487696556872632\n",
      "[EPOCH #1, step #1246] loss: 3.4870873009957783\n",
      "[EPOCH #1, step #1248] loss: 3.4868533363143763\n",
      "[EPOCH #1, step #1250] loss: 3.486431070559507\n",
      "[EPOCH #1, step #1252] loss: 3.4869286218644713\n",
      "[EPOCH #1, step #1254] loss: 3.4863991099049847\n",
      "[EPOCH #1, step #1256] loss: 3.4862323006468343\n",
      "[EPOCH #1, step #1258] loss: 3.486382790838943\n",
      "[EPOCH #1, step #1260] loss: 3.485954930534635\n",
      "[EPOCH #1, step #1262] loss: 3.486160360435975\n",
      "[EPOCH #1, step #1264] loss: 3.4860867490881517\n",
      "[EPOCH #1, step #1266] loss: 3.4859295081414854\n",
      "[EPOCH #1, step #1268] loss: 3.485837059381724\n",
      "[EPOCH #1, step #1270] loss: 3.4858165213106935\n",
      "[EPOCH #1, step #1272] loss: 3.4859317551423317\n",
      "[EPOCH #1, step #1274] loss: 3.485765463698144\n",
      "[EPOCH #1, step #1276] loss: 3.4855579693240863\n",
      "[EPOCH #1, step #1278] loss: 3.485454497997233\n",
      "[EPOCH #1, step #1280] loss: 3.4850929436694824\n",
      "[EPOCH #1, step #1282] loss: 3.4850066170874556\n",
      "[EPOCH #1, step #1284] loss: 3.4844696326942297\n",
      "[EPOCH #1, step #1286] loss: 3.4840984388942764\n",
      "[EPOCH #1, step #1288] loss: 3.484664557792125\n",
      "[EPOCH #1, step #1290] loss: 3.484496606772118\n",
      "[EPOCH #1, step #1292] loss: 3.4841954874051972\n",
      "[EPOCH #1, step #1294] loss: 3.4838729532529027\n",
      "[EPOCH #1, step #1296] loss: 3.4831357844168163\n",
      "[EPOCH #1, step #1298] loss: 3.482889603797613\n",
      "[EPOCH #1, step #1300] loss: 3.4826951120378054\n",
      "[EPOCH #1, step #1302] loss: 3.4824241622081646\n",
      "[EPOCH #1, step #1304] loss: 3.4814296917897076\n",
      "[EPOCH #1, step #1306] loss: 3.4810321323727504\n",
      "[EPOCH #1, step #1308] loss: 3.4810761469206435\n",
      "[EPOCH #1, step #1310] loss: 3.4807128551622823\n",
      "[EPOCH #1, step #1312] loss: 3.480408577726528\n",
      "[EPOCH #1, step #1314] loss: 3.4807250617574828\n",
      "[EPOCH #1, step #1316] loss: 3.4804649262511558\n",
      "[EPOCH #1, step #1318] loss: 3.48046927365324\n",
      "[EPOCH #1, step #1320] loss: 3.480771359307219\n",
      "[EPOCH #1, step #1322] loss: 3.480718573745416\n",
      "[EPOCH #1, step #1324] loss: 3.4810289481900774\n",
      "[EPOCH #1, step #1326] loss: 3.481065046724378\n",
      "[EPOCH #1, step #1328] loss: 3.4809252146583827\n",
      "[EPOCH #1, step #1330] loss: 3.480922913748371\n",
      "[EPOCH #1, step #1332] loss: 3.480565355491209\n",
      "[EPOCH #1, step #1334] loss: 3.480471948380774\n",
      "[EPOCH #1, step #1336] loss: 3.4801988528571917\n",
      "[EPOCH #1, step #1338] loss: 3.4800433092281122\n",
      "[EPOCH #1, step #1340] loss: 3.47943528119883\n",
      "[EPOCH #1, step #1342] loss: 3.479560707011333\n",
      "[EPOCH #1, step #1344] loss: 3.4791864801073604\n",
      "[EPOCH #1, step #1346] loss: 3.4789040209013353\n",
      "[EPOCH #1, step #1348] loss: 3.478812199685201\n",
      "[EPOCH #1, step #1350] loss: 3.47892776372078\n",
      "[EPOCH #1, step #1352] loss: 3.478067314369275\n",
      "[EPOCH #1, step #1354] loss: 3.4777556704419124\n",
      "[EPOCH #1, step #1356] loss: 3.4774444359756167\n",
      "[EPOCH #1, step #1358] loss: 3.478374899442447\n",
      "[EPOCH #1, step #1360] loss: 3.478475791053856\n",
      "[EPOCH #1, step #1362] loss: 3.4783934434371577\n",
      "[EPOCH #1, step #1364] loss: 3.478634017029088\n",
      "[EPOCH #1, step #1366] loss: 3.478367728914932\n",
      "[EPOCH #1, step #1368] loss: 3.4782585582217656\n",
      "[EPOCH #1, step #1370] loss: 3.4785219301367047\n",
      "[EPOCH #1, step #1372] loss: 3.478851413692077\n",
      "[EPOCH #1, step #1374] loss: 3.47879004426436\n",
      "[EPOCH #1, step #1376] loss: 3.478947611935518\n",
      "[EPOCH #1, step #1378] loss: 3.4787527510357386\n",
      "[EPOCH #1, step #1380] loss: 3.4789152221348214\n",
      "[EPOCH #1, step #1382] loss: 3.479683819148132\n",
      "[EPOCH #1, step #1384] loss: 3.4796990472056804\n",
      "[EPOCH #1, step #1386] loss: 3.4795604640834177\n",
      "[EPOCH #1, step #1388] loss: 3.479128943663694\n",
      "[EPOCH #1, step #1390] loss: 3.4784557142333106\n",
      "[EPOCH #1, step #1392] loss: 3.4784788020805584\n",
      "[EPOCH #1, step #1394] loss: 3.478563806233013\n",
      "[EPOCH #1, step #1396] loss: 3.4776586138015997\n",
      "[EPOCH #1, step #1398] loss: 3.4769778795290027\n",
      "[EPOCH #1, step #1400] loss: 3.4771029741571766\n",
      "[EPOCH #1, step #1402] loss: 3.476264718593399\n",
      "[EPOCH #1, step #1404] loss: 3.4760824693903802\n",
      "[EPOCH #1, step #1406] loss: 3.475976203465682\n",
      "[EPOCH #1, step #1408] loss: 3.4758021657224916\n",
      "[EPOCH #1, step #1410] loss: 3.4749259703722006\n",
      "[EPOCH #1, step #1412] loss: 3.4746991523343116\n",
      "[EPOCH #1, step #1414] loss: 3.4746800737751666\n",
      "[EPOCH #1, step #1416] loss: 3.474369566627346\n",
      "[EPOCH #1, step #1418] loss: 3.4740916887919107\n",
      "[EPOCH #1, step #1420] loss: 3.4739871785139047\n",
      "[EPOCH #1, step #1422] loss: 3.4735983714869723\n",
      "[EPOCH #1, step #1424] loss: 3.473045876820882\n",
      "[EPOCH #1, step #1426] loss: 3.4729751384416994\n",
      "[EPOCH #1, step #1428] loss: 3.472676524588743\n",
      "[EPOCH #1, step #1430] loss: 3.4725841724481055\n",
      "[EPOCH #1, step #1432] loss: 3.4724750430618894\n",
      "[EPOCH #1, step #1434] loss: 3.4720500143562876\n",
      "[EPOCH #1, step #1436] loss: 3.4717641268327326\n",
      "[EPOCH #1, step #1438] loss: 3.4716474782931797\n",
      "[EPOCH #1, step #1440] loss: 3.471687951243478\n",
      "[EPOCH #1, step #1442] loss: 3.4720799866766874\n",
      "[EPOCH #1, step #1444] loss: 3.471471007719997\n",
      "[EPOCH #1, step #1446] loss: 3.4710802842438757\n",
      "[EPOCH #1, step #1448] loss: 3.4710625830479866\n",
      "[EPOCH #1, step #1450] loss: 3.470946073203478\n",
      "[EPOCH #1, step #1452] loss: 3.470536047888885\n",
      "[EPOCH #1, step #1454] loss: 3.470242002657599\n",
      "[EPOCH #1, step #1456] loss: 3.4697658649109453\n",
      "[EPOCH #1, step #1458] loss: 3.469611110713402\n",
      "[EPOCH #1, step #1460] loss: 3.4695618800477246\n",
      "[EPOCH #1, step #1462] loss: 3.4694184256349616\n",
      "[EPOCH #1, step #1464] loss: 3.469141750530985\n",
      "[EPOCH #1, step #1466] loss: 3.4684619897063604\n",
      "[EPOCH #1, step #1468] loss: 3.4675044731025877\n",
      "[EPOCH #1, step #1470] loss: 3.4675010393299592\n",
      "[EPOCH #1, step #1472] loss: 3.46738170509183\n",
      "[EPOCH #1, step #1474] loss: 3.467390479394945\n",
      "[EPOCH #1, step #1476] loss: 3.4675557266318\n",
      "[EPOCH #1, step #1478] loss: 3.467318362041935\n",
      "[EPOCH #1, step #1480] loss: 3.466981489218867\n",
      "[EPOCH #1, step #1482] loss: 3.4667107727747872\n",
      "[EPOCH #1, step #1484] loss: 3.46633881479\n",
      "[EPOCH #1, step #1486] loss: 3.466310426542722\n",
      "[EPOCH #1, step #1488] loss: 3.465649398242651\n",
      "[EPOCH #1, step #1490] loss: 3.4659704431281804\n",
      "[EPOCH #1, step #1492] loss: 3.4656028680488395\n",
      "[EPOCH #1, step #1494] loss: 3.465625711587759\n",
      "[EPOCH #1, step #1496] loss: 3.465694057678651\n",
      "[EPOCH #1, step #1498] loss: 3.465060810155277\n",
      "[EPOCH #1, step #1500] loss: 3.4645417411672046\n",
      "[EPOCH #1, step #1502] loss: 3.4646486679237047\n",
      "[EPOCH #1, step #1504] loss: 3.4648862986073543\n",
      "[EPOCH #1, step #1506] loss: 3.46491930511667\n",
      "[EPOCH #1, step #1508] loss: 3.4648071939065175\n",
      "[EPOCH #1, step #1510] loss: 3.4644405740842688\n",
      "[EPOCH #1, step #1512] loss: 3.4643084197665237\n",
      "[EPOCH #1, step #1514] loss: 3.46410708175634\n",
      "[EPOCH #1, step #1516] loss: 3.4635493945509648\n",
      "[EPOCH #1, step #1518] loss: 3.4634652429697463\n",
      "[EPOCH #1, step #1520] loss: 3.4639595548703745\n",
      "[EPOCH #1, step #1522] loss: 3.4634722620738\n",
      "[EPOCH #1, step #1524] loss: 3.463069808834889\n",
      "[EPOCH #1, step #1526] loss: 3.463087111731019\n",
      "[EPOCH #1, step #1528] loss: 3.463113683590786\n",
      "[EPOCH #1, step #1530] loss: 3.4632518420727094\n",
      "[EPOCH #1, step #1532] loss: 3.462702628917657\n",
      "[EPOCH #1, step #1534] loss: 3.4628389068069207\n",
      "[EPOCH #1, step #1536] loss: 3.4631548078322303\n",
      "[EPOCH #1, step #1538] loss: 3.462636010843566\n",
      "[EPOCH #1, step #1540] loss: 3.4623150242218177\n",
      "[EPOCH #1, step #1542] loss: 3.462259082769467\n",
      "[EPOCH #1, step #1544] loss: 3.4622571914327183\n",
      "[EPOCH #1, step #1546] loss: 3.4618712149671222\n",
      "[EPOCH #1, step #1548] loss: 3.4621114416842773\n",
      "[EPOCH #1, step #1550] loss: 3.4622223391062517\n",
      "[EPOCH #1, step #1552] loss: 3.4620138534943825\n",
      "[EPOCH #1, step #1554] loss: 3.461341063232667\n",
      "[EPOCH #1, step #1556] loss: 3.461427315275257\n",
      "[EPOCH #1, step #1558] loss: 3.4606047430582882\n",
      "[EPOCH #1, step #1560] loss: 3.4605936316784036\n",
      "[EPOCH #1, step #1562] loss: 3.4600732312779052\n",
      "[EPOCH #1, step #1564] loss: 3.4595872732777946\n",
      "[EPOCH #1, step #1566] loss: 3.4598070960480007\n",
      "[EPOCH #1, step #1568] loss: 3.4597771997433546\n",
      "[EPOCH #1, step #1570] loss: 3.459651095634961\n",
      "[EPOCH #1, step #1572] loss: 3.459459595368004\n",
      "[EPOCH #1, step #1574] loss: 3.459337642306373\n",
      "[EPOCH #1, step #1576] loss: 3.4594084891474695\n",
      "[EPOCH #1, step #1578] loss: 3.458570996140135\n",
      "[EPOCH #1, step #1580] loss: 3.4583292367864003\n",
      "[EPOCH #1, step #1582] loss: 3.458602147394543\n",
      "[EPOCH #1, step #1584] loss: 3.458568259067716\n",
      "[EPOCH #1, step #1586] loss: 3.4584968147145627\n",
      "[EPOCH #1, step #1588] loss: 3.458646539624492\n",
      "[EPOCH #1, step #1590] loss: 3.4585650977213227\n",
      "[EPOCH #1, step #1592] loss: 3.4584646154632375\n",
      "[EPOCH #1, step #1594] loss: 3.4582009340902107\n",
      "[EPOCH #1, step #1596] loss: 3.4585929966450633\n",
      "[EPOCH #1, step #1598] loss: 3.4586650309822127\n",
      "[EPOCH #1, step #1600] loss: 3.4583916805596147\n",
      "[EPOCH #1, step #1602] loss: 3.457938467496942\n",
      "[EPOCH #1, step #1604] loss: 3.4576784530532696\n",
      "[EPOCH #1, step #1606] loss: 3.4575893864100515\n",
      "[EPOCH #1, step #1608] loss: 3.457000998846117\n",
      "[EPOCH #1, step #1610] loss: 3.4573330189558678\n",
      "[EPOCH #1, step #1612] loss: 3.45745306686075\n",
      "[EPOCH #1, step #1614] loss: 3.4577960849915494\n",
      "[EPOCH #1, step #1616] loss: 3.457361984193952\n",
      "[EPOCH #1, step #1618] loss: 3.457287764534529\n",
      "[EPOCH #1, step #1620] loss: 3.4567331886820996\n",
      "[EPOCH #1, step #1622] loss: 3.4564523974540275\n",
      "[EPOCH #1, step #1624] loss: 3.4563072719573973\n",
      "[EPOCH #1, step #1626] loss: 3.455623441923595\n",
      "[EPOCH #1, step #1628] loss: 3.455639279349733\n",
      "[EPOCH #1, step #1630] loss: 3.454783556872098\n",
      "[EPOCH #1, step #1632] loss: 3.454986443493312\n",
      "[EPOCH #1, step #1634] loss: 3.455259983109407\n",
      "[EPOCH #1, step #1636] loss: 3.455257533888447\n",
      "[EPOCH #1, step #1638] loss: 3.4553992116938574\n",
      "[EPOCH #1, step #1640] loss: 3.455482447023874\n",
      "[EPOCH #1, step #1642] loss: 3.455195431729605\n",
      "[EPOCH #1, step #1644] loss: 3.454820165518207\n",
      "[EPOCH #1, step #1646] loss: 3.455072931325573\n",
      "[EPOCH #1, step #1648] loss: 3.4550473921366933\n",
      "[EPOCH #1, step #1650] loss: 3.4555552339351663\n",
      "[EPOCH #1, step #1652] loss: 3.4554467384409917\n",
      "[EPOCH #1, step #1654] loss: 3.4560140879132595\n",
      "[EPOCH #1, step #1656] loss: 3.4562426750100226\n",
      "[EPOCH #1, step #1658] loss: 3.455869843975617\n",
      "[EPOCH #1, step #1660] loss: 3.455746994041521\n",
      "[EPOCH #1, step #1662] loss: 3.4555986730434487\n",
      "[EPOCH #1, step #1664] loss: 3.4556755383809405\n",
      "[EPOCH #1, step #1666] loss: 3.455123803253723\n",
      "[EPOCH #1, step #1668] loss: 3.4549331880744307\n",
      "[EPOCH #1, step #1670] loss: 3.4549919757066836\n",
      "[EPOCH #1, step #1672] loss: 3.4553709352223088\n",
      "[EPOCH #1, step #1674] loss: 3.455527631418029\n",
      "[EPOCH #1, step #1676] loss: 3.455630018800657\n",
      "[EPOCH #1, step #1678] loss: 3.4555516891922533\n",
      "[EPOCH #1, step #1680] loss: 3.455298742604071\n",
      "[EPOCH #1, step #1682] loss: 3.454829628242316\n",
      "[EPOCH #1, step #1684] loss: 3.4545169223309977\n",
      "[EPOCH #1, step #1686] loss: 3.454143233460368\n",
      "[EPOCH #1, step #1688] loss: 3.453850168388507\n",
      "[EPOCH #1, step #1690] loss: 3.453554920583417\n",
      "[EPOCH #1, step #1692] loss: 3.4534500859154096\n",
      "[EPOCH #1, step #1694] loss: 3.453097906084539\n",
      "[EPOCH #1, step #1696] loss: 3.452319433918123\n",
      "[EPOCH #1, step #1698] loss: 3.452244512749953\n",
      "[EPOCH #1, step #1700] loss: 3.451901456175798\n",
      "[EPOCH #1, step #1702] loss: 3.451417698974968\n",
      "[EPOCH #1, step #1704] loss: 3.451262237459334\n",
      "[EPOCH #1, step #1706] loss: 3.451779719745477\n",
      "[EPOCH #1, step #1708] loss: 3.4521613309368044\n",
      "[EPOCH #1, step #1710] loss: 3.451849617921839\n",
      "[EPOCH #1, step #1712] loss: 3.4512624588892327\n",
      "[EPOCH #1, step #1714] loss: 3.4510951881853553\n",
      "[EPOCH #1, step #1716] loss: 3.451193563036955\n",
      "[EPOCH #1, step #1718] loss: 3.45090720083493\n",
      "[EPOCH #1, step #1720] loss: 3.450697698543267\n",
      "[EPOCH #1, step #1722] loss: 3.4510222503907864\n",
      "[EPOCH #1, step #1724] loss: 3.450549107150755\n",
      "[EPOCH #1, step #1726] loss: 3.4506273727991297\n",
      "[EPOCH #1, step #1728] loss: 3.4502385061141587\n",
      "[EPOCH #1, step #1730] loss: 3.4509738410297395\n",
      "[EPOCH #1, step #1732] loss: 3.4510261009555294\n",
      "[EPOCH #1, step #1734] loss: 3.4505625257574515\n",
      "[EPOCH #1, step #1736] loss: 3.450457933515945\n",
      "[EPOCH #1, step #1738] loss: 3.4500298342394924\n",
      "[EPOCH #1, step #1740] loss: 3.450210001193402\n",
      "[EPOCH #1, step #1742] loss: 3.450173895863781\n",
      "[EPOCH #1, step #1744] loss: 3.450148319309967\n",
      "[EPOCH #1, step #1746] loss: 3.4504254770469993\n",
      "[EPOCH #1, step #1748] loss: 3.4503285020606866\n",
      "[EPOCH #1, step #1750] loss: 3.4497674768274678\n",
      "[EPOCH #1, step #1752] loss: 3.449624805265471\n",
      "[EPOCH #1, step #1754] loss: 3.44965179394453\n",
      "[EPOCH #1, step #1756] loss: 3.4499929564611165\n",
      "[EPOCH #1, step #1758] loss: 3.4496972812990903\n",
      "[EPOCH #1, step #1760] loss: 3.4495841383189387\n",
      "[EPOCH #1, step #1762] loss: 3.44969518415913\n",
      "[EPOCH #1, step #1764] loss: 3.449867687954781\n",
      "[EPOCH #1, step #1766] loss: 3.4499863941384765\n",
      "[EPOCH #1, step #1768] loss: 3.4497514071607402\n",
      "[EPOCH #1, step #1770] loss: 3.4493765742142006\n",
      "[EPOCH #1, step #1772] loss: 3.449008021862979\n",
      "[EPOCH #1, step #1774] loss: 3.448751480478636\n",
      "[EPOCH #1, step #1776] loss: 3.4486550020916646\n",
      "[EPOCH #1, step #1778] loss: 3.4481088761334475\n",
      "[EPOCH #1, step #1780] loss: 3.4479775055280766\n",
      "[EPOCH #1, step #1782] loss: 3.4477674194887924\n",
      "[EPOCH #1, step #1784] loss: 3.4476879447114235\n",
      "[EPOCH #1, step #1786] loss: 3.447729369015686\n",
      "[EPOCH #1, step #1788] loss: 3.447515666651419\n",
      "[EPOCH #1, step #1790] loss: 3.4476823379446313\n",
      "[EPOCH #1, step #1792] loss: 3.447406380919857\n",
      "[EPOCH #1, step #1794] loss: 3.447325648305144\n",
      "[EPOCH #1, step #1796] loss: 3.4470022717116606\n",
      "[EPOCH #1, step #1798] loss: 3.4469258435372847\n",
      "[EPOCH #1, step #1800] loss: 3.446777429930175\n",
      "[EPOCH #1, step #1802] loss: 3.4465287712899038\n",
      "[EPOCH #1, step #1804] loss: 3.446531858074368\n",
      "[EPOCH #1, step #1806] loss: 3.4463667357634233\n",
      "[EPOCH #1, step #1808] loss: 3.446077761826692\n",
      "[EPOCH #1, step #1810] loss: 3.4459311365752114\n",
      "[EPOCH #1, step #1812] loss: 3.4460780354934837\n",
      "[EPOCH #1, step #1814] loss: 3.4463180940013287\n",
      "[EPOCH #1, step #1816] loss: 3.446120695550821\n",
      "[EPOCH #1, step #1818] loss: 3.446150771990918\n",
      "[EPOCH #1, step #1820] loss: 3.446064641225606\n",
      "[EPOCH #1, step #1822] loss: 3.4458298883265477\n",
      "[EPOCH #1, step #1824] loss: 3.4457966193107707\n",
      "[EPOCH #1, step #1826] loss: 3.445736929048393\n",
      "[EPOCH #1, step #1828] loss: 3.445581715116089\n",
      "[EPOCH #1, step #1830] loss: 3.44559680106925\n",
      "[EPOCH #1, step #1832] loss: 3.4453445141652344\n",
      "[EPOCH #1, step #1834] loss: 3.4454632382301935\n",
      "[EPOCH #1, step #1836] loss: 3.4454842086193933\n",
      "[EPOCH #1, step #1838] loss: 3.4454580394388086\n",
      "[EPOCH #1, step #1840] loss: 3.4451381655117532\n",
      "[EPOCH #1, step #1842] loss: 3.4450011364610313\n",
      "[EPOCH #1, step #1844] loss: 3.4440784900169064\n",
      "[EPOCH #1, step #1846] loss: 3.444034967541243\n",
      "[EPOCH #1, step #1848] loss: 3.444049066695218\n",
      "[EPOCH #1, step #1850] loss: 3.4439978424373154\n",
      "[EPOCH #1, step #1852] loss: 3.4437437570873493\n",
      "[EPOCH #1, step #1854] loss: 3.443798735842229\n",
      "[EPOCH #1, step #1856] loss: 3.4435505520353793\n",
      "[EPOCH #1, step #1858] loss: 3.4437122043580892\n",
      "[EPOCH #1, step #1860] loss: 3.443646839650199\n",
      "[EPOCH #1, step #1862] loss: 3.443496033253365\n",
      "[EPOCH #1, step #1864] loss: 3.4437095400475304\n",
      "[EPOCH #1, step #1866] loss: 3.443919071617392\n",
      "[EPOCH #1, step #1868] loss: 3.443742911531174\n",
      "[EPOCH #1, step #1870] loss: 3.443655073802016\n",
      "[EPOCH #1, step #1872] loss: 3.4435080076626003\n",
      "[EPOCH #1, step #1874] loss: 3.443265479660034\n",
      "[EPOCH #1, step #1876] loss: 3.4427545752751425\n",
      "[EPOCH #1, step #1878] loss: 3.4429940342459036\n",
      "[EPOCH #1, step #1880] loss: 3.4428796775793535\n",
      "[EPOCH #1, step #1882] loss: 3.4424996869841347\n",
      "[EPOCH #1, step #1884] loss: 3.442365119311791\n",
      "[EPOCH #1, step #1886] loss: 3.442369189158785\n",
      "[EPOCH #1, step #1888] loss: 3.4423726020255625\n",
      "[EPOCH #1, step #1890] loss: 3.4420151387743694\n",
      "[EPOCH #1, step #1892] loss: 3.4419574082621933\n",
      "[EPOCH #1, step #1894] loss: 3.441313366902535\n",
      "[EPOCH #1, step #1896] loss: 3.4410591336633884\n",
      "[EPOCH #1, step #1898] loss: 3.4408983248921055\n",
      "[EPOCH #1, step #1900] loss: 3.44103833200554\n",
      "[EPOCH #1, step #1902] loss: 3.441276106030078\n",
      "[EPOCH #1, step #1904] loss: 3.4409069951125018\n",
      "[EPOCH #1, step #1906] loss: 3.4403403605474625\n",
      "[EPOCH #1, step #1908] loss: 3.439949289674594\n",
      "[EPOCH #1, step #1910] loss: 3.4396189741287353\n",
      "[EPOCH #1, step #1912] loss: 3.439385296222868\n",
      "[EPOCH #1, step #1914] loss: 3.4394260487419506\n",
      "[EPOCH #1, step #1916] loss: 3.439284701240392\n",
      "[EPOCH #1, step #1918] loss: 3.439047315084667\n",
      "[EPOCH #1, step #1920] loss: 3.4392225805637056\n",
      "[EPOCH #1, step #1922] loss: 3.439077923462779\n",
      "[EPOCH #1, step #1924] loss: 3.4387238611493793\n",
      "[EPOCH #1, step #1926] loss: 3.438911859767698\n",
      "[EPOCH #1, step #1928] loss: 3.438547164804272\n",
      "[EPOCH #1, step #1930] loss: 3.438181197686494\n",
      "[EPOCH #1, step #1932] loss: 3.4382485602103054\n",
      "[EPOCH #1, step #1934] loss: 3.4382106352222057\n",
      "[EPOCH #1, step #1936] loss: 3.438191852805906\n",
      "[EPOCH #1, step #1938] loss: 3.4380878466438425\n",
      "[EPOCH #1, step #1940] loss: 3.4373203097760587\n",
      "[EPOCH #1, step #1942] loss: 3.437524368145757\n",
      "[EPOCH #1, step #1944] loss: 3.437236738695277\n",
      "[EPOCH #1, step #1946] loss: 3.436964157610963\n",
      "[EPOCH #1, step #1948] loss: 3.436755507098766\n",
      "[EPOCH #1, step #1950] loss: 3.4366331299655197\n",
      "[EPOCH #1, step #1952] loss: 3.4366582394622864\n",
      "[EPOCH #1, step #1954] loss: 3.4365904789751447\n",
      "[EPOCH #1, step #1956] loss: 3.436292199011536\n",
      "[EPOCH #1, step #1958] loss: 3.4359562040409792\n",
      "[EPOCH #1, step #1960] loss: 3.4360370888872938\n",
      "[EPOCH #1, step #1962] loss: 3.4358867503402797\n",
      "[EPOCH #1, step #1964] loss: 3.4353820627275615\n",
      "[EPOCH #1, step #1966] loss: 3.435241898365535\n",
      "[EPOCH #1, step #1968] loss: 3.4352146235137133\n",
      "[EPOCH #1, step #1970] loss: 3.435257660683371\n",
      "[EPOCH #1, step #1972] loss: 3.4351098194257657\n",
      "[EPOCH #1, step #1974] loss: 3.435583986451354\n",
      "[EPOCH #1, step #1976] loss: 3.435261860482308\n",
      "[EPOCH #1, step #1978] loss: 3.434603741820742\n",
      "[EPOCH #1, step #1980] loss: 3.434680856279145\n",
      "[EPOCH #1, step #1982] loss: 3.4347494954240485\n",
      "[EPOCH #1, step #1984] loss: 3.4347438202096474\n",
      "[EPOCH #1, step #1986] loss: 3.4344571597862914\n",
      "[EPOCH #1, step #1988] loss: 3.434403753688193\n",
      "[EPOCH #1, step #1990] loss: 3.4342175885578428\n",
      "[EPOCH #1, step #1992] loss: 3.43396352393748\n",
      "[EPOCH #1, step #1994] loss: 3.433283723625623\n",
      "[EPOCH #1, step #1996] loss: 3.4333933862257315\n",
      "[EPOCH #1, step #1998] loss: 3.4331709733899083\n",
      "[EPOCH #1, step #2000] loss: 3.4325884808783886\n",
      "[EPOCH #1, step #2002] loss: 3.432241622055404\n",
      "[EPOCH #1, step #2004] loss: 3.4324690091045125\n",
      "[EPOCH #1, step #2006] loss: 3.432536198122322\n",
      "[EPOCH #1, step #2008] loss: 3.432498324742182\n",
      "[EPOCH #1, step #2010] loss: 3.4326720480774244\n",
      "[EPOCH #1, step #2012] loss: 3.4324473451158624\n",
      "[EPOCH #1, step #2014] loss: 3.4323160726438386\n",
      "[EPOCH #1, step #2016] loss: 3.432522199977786\n",
      "[EPOCH #1, step #2018] loss: 3.43283905175253\n",
      "[EPOCH #1, step #2020] loss: 3.4324372983816414\n",
      "[EPOCH #1, step #2022] loss: 3.4325242300641694\n",
      "[EPOCH #1, step #2024] loss: 3.43208574389234\n",
      "[EPOCH #1, step #2026] loss: 3.4320939882200343\n",
      "[EPOCH #1, step #2028] loss: 3.431946421327586\n",
      "[EPOCH #1, step #2030] loss: 3.4318489049350847\n",
      "[EPOCH #1, step #2032] loss: 3.4316875771589603\n",
      "[EPOCH #1, step #2034] loss: 3.4317890095769332\n",
      "[EPOCH #1, step #2036] loss: 3.4317808134858963\n",
      "[EPOCH #1, step #2038] loss: 3.431604678634337\n",
      "[EPOCH #1, step #2040] loss: 3.4318501067360145\n",
      "[EPOCH #1, step #2042] loss: 3.4315215999801194\n",
      "[EPOCH #1, step #2044] loss: 3.431017802805073\n",
      "[EPOCH #1, step #2046] loss: 3.4308554124529325\n",
      "[EPOCH #1, step #2048] loss: 3.430630106178943\n",
      "[EPOCH #1, step #2050] loss: 3.430550861800374\n",
      "[EPOCH #1, step #2052] loss: 3.4301015693155428\n",
      "[EPOCH #1, step #2054] loss: 3.430064614033757\n",
      "[EPOCH #1, step #2056] loss: 3.4300407525342127\n",
      "[EPOCH #1, step #2058] loss: 3.4301019054179402\n",
      "[EPOCH #1, step #2060] loss: 3.4302609545232485\n",
      "[EPOCH #1, step #2062] loss: 3.430171759453319\n",
      "[EPOCH #1, step #2064] loss: 3.430081534212496\n",
      "[EPOCH #1, step #2066] loss: 3.43002528422226\n",
      "[EPOCH #1, step #2068] loss: 3.430021368863791\n",
      "[EPOCH #1, step #2070] loss: 3.4298964673801646\n",
      "[EPOCH #1, step #2072] loss: 3.4296962506404203\n",
      "[EPOCH #1, step #2074] loss: 3.4296499674004246\n",
      "[EPOCH #1, step #2076] loss: 3.4298076901690684\n",
      "[EPOCH #1, step #2078] loss: 3.429765038102679\n",
      "[EPOCH #1, step #2080] loss: 3.4299548229078214\n",
      "[EPOCH #1, step #2082] loss: 3.4298668670013326\n",
      "[EPOCH #1, step #2084] loss: 3.429805303020157\n",
      "[EPOCH #1, step #2086] loss: 3.429584332271004\n",
      "[EPOCH #1, step #2088] loss: 3.4300029689812557\n",
      "[EPOCH #1, step #2090] loss: 3.4299625166861536\n",
      "[EPOCH #1, step #2092] loss: 3.4297999359464395\n",
      "[EPOCH #1, step #2094] loss: 3.4299280122242566\n",
      "[EPOCH #1, step #2096] loss: 3.4301691900052282\n",
      "[EPOCH #1, step #2098] loss: 3.4302321251600683\n",
      "[EPOCH #1, step #2100] loss: 3.4300989760835074\n",
      "[EPOCH #1, step #2102] loss: 3.4298254042311163\n",
      "[EPOCH #1, step #2104] loss: 3.4299131400228395\n",
      "[EPOCH #1, step #2106] loss: 3.429754303595892\n",
      "[EPOCH #1, step #2108] loss: 3.4296087289433053\n",
      "[EPOCH #1, step #2110] loss: 3.4295897166461304\n",
      "[EPOCH #1, step #2112] loss: 3.429055871187202\n",
      "[EPOCH #1, step #2114] loss: 3.4288577327773364\n",
      "[EPOCH #1, step #2116] loss: 3.428606707474281\n",
      "[EPOCH #1, step #2118] loss: 3.4281741518521995\n",
      "[EPOCH #1, step #2120] loss: 3.428244077501742\n",
      "[EPOCH #1, step #2122] loss: 3.4282623125706086\n",
      "[EPOCH #1, step #2124] loss: 3.4280436019897462\n",
      "[EPOCH #1, step #2126] loss: 3.4279500228673796\n",
      "[EPOCH #1, step #2128] loss: 3.427457045478516\n",
      "[EPOCH #1, step #2130] loss: 3.4277630906214687\n",
      "[EPOCH #1, step #2132] loss: 3.4275828885517936\n",
      "[EPOCH #1, step #2134] loss: 3.4277495388683166\n",
      "[EPOCH #1, step #2136] loss: 3.4276109071998437\n",
      "[EPOCH #1, step #2138] loss: 3.427695064245958\n",
      "[EPOCH #1, step #2140] loss: 3.427191541552377\n",
      "[EPOCH #1, step #2142] loss: 3.4273850886664876\n",
      "[EPOCH #1, step #2144] loss: 3.4271364027525717\n",
      "[EPOCH #1, step #2146] loss: 3.4274197472046852\n",
      "[EPOCH #1, step #2148] loss: 3.4271790456527773\n",
      "[EPOCH #1, step #2150] loss: 3.4269051501940484\n",
      "[EPOCH #1, step #2152] loss: 3.426882255592293\n",
      "[EPOCH #1, step #2154] loss: 3.4267187613069043\n",
      "[EPOCH #1, step #2156] loss: 3.42665594631066\n",
      "[EPOCH #1, step #2158] loss: 3.426072932990296\n",
      "[EPOCH #1, step #2160] loss: 3.4258885230250185\n",
      "[EPOCH #1, step #2162] loss: 3.425817174929135\n",
      "[EPOCH #1, step #2164] loss: 3.4256935554618924\n",
      "[EPOCH #1, step #2166] loss: 3.4258122112985356\n",
      "[EPOCH #1, step #2168] loss: 3.4258800760948915\n",
      "[EPOCH #1, step #2170] loss: 3.4257304487224003\n",
      "[EPOCH #1, step #2172] loss: 3.425590877098473\n",
      "[EPOCH #1, step #2174] loss: 3.4257279678870893\n",
      "[EPOCH #1, step #2176] loss: 3.4259693738677166\n",
      "[EPOCH #1, step #2178] loss: 3.4258947883411635\n",
      "[EPOCH #1, step #2180] loss: 3.4258035131137015\n",
      "[EPOCH #1, step #2182] loss: 3.4256451982890472\n",
      "[EPOCH #1, step #2184] loss: 3.4254397249330917\n",
      "[EPOCH #1, step #2186] loss: 3.425367486166202\n",
      "[EPOCH #1, step #2188] loss: 3.425227715578075\n",
      "[EPOCH #1, step #2190] loss: 3.4253743562128163\n",
      "[EPOCH #1, step #2192] loss: 3.4252638127561363\n",
      "[EPOCH #1, step #2194] loss: 3.425474960548731\n",
      "[EPOCH #1, step #2196] loss: 3.4253035795162305\n",
      "[EPOCH #1, step #2198] loss: 3.425515376204195\n",
      "[EPOCH #1, step #2200] loss: 3.425176131297436\n",
      "[EPOCH #1, step #2202] loss: 3.4249893718560176\n",
      "[EPOCH #1, step #2204] loss: 3.4247018648653613\n",
      "[EPOCH #1, step #2206] loss: 3.4244835317323905\n",
      "[EPOCH #1, step #2208] loss: 3.4241936934269517\n",
      "[EPOCH #1, step #2210] loss: 3.424125153788718\n",
      "[EPOCH #1, step #2212] loss: 3.424112121389831\n",
      "[EPOCH #1, step #2214] loss: 3.424024363511303\n",
      "[EPOCH #1, step #2216] loss: 3.4239209504830823\n",
      "[EPOCH #1, step #2218] loss: 3.4238002186371648\n",
      "[EPOCH #1, step #2220] loss: 3.42387256145692\n",
      "[EPOCH #1, step #2222] loss: 3.4238917282772023\n",
      "[EPOCH #1, step #2224] loss: 3.4237475196966964\n",
      "[EPOCH #1, step #2226] loss: 3.424020949103771\n",
      "[EPOCH #1, step #2228] loss: 3.424097585164682\n",
      "[EPOCH #1, step #2230] loss: 3.4237489628717004\n",
      "[EPOCH #1, step #2232] loss: 3.423178218375179\n",
      "[EPOCH #1, step #2234] loss: 3.4230391415973638\n",
      "[EPOCH #1, step #2236] loss: 3.422851942243137\n",
      "[EPOCH #1, step #2238] loss: 3.422936953940313\n",
      "[EPOCH #1, step #2240] loss: 3.4227743078586728\n",
      "[EPOCH #1, step #2242] loss: 3.4229000992292384\n",
      "[EPOCH #1, step #2244] loss: 3.4225373621772817\n",
      "[EPOCH #1, step #2246] loss: 3.4222050178936976\n",
      "[EPOCH #1, step #2248] loss: 3.42222336410363\n",
      "[EPOCH #1, step #2250] loss: 3.4221874797995597\n",
      "[EPOCH #1, step #2252] loss: 3.422272767377228\n",
      "[EPOCH #1, step #2254] loss: 3.4223167908958216\n",
      "[EPOCH #1, step #2256] loss: 3.422020057370613\n",
      "[EPOCH #1, step #2258] loss: 3.4220701119919803\n",
      "[EPOCH #1, step #2260] loss: 3.421630594278004\n",
      "[EPOCH #1, step #2262] loss: 3.4216601227007235\n",
      "[EPOCH #1, step #2264] loss: 3.4213481110452815\n",
      "[EPOCH #1, step #2266] loss: 3.4212221387308706\n",
      "[EPOCH #1, step #2268] loss: 3.4212535663342467\n",
      "[EPOCH #1, step #2270] loss: 3.421249029028526\n",
      "[EPOCH #1, step #2272] loss: 3.4205469594251414\n",
      "[EPOCH #1, step #2274] loss: 3.420305131765512\n",
      "[EPOCH #1, step #2276] loss: 3.4204587248474243\n",
      "[EPOCH #1, step #2278] loss: 3.419885466631278\n",
      "[EPOCH #1, step #2280] loss: 3.4195376370675414\n",
      "[EPOCH #1, step #2282] loss: 3.419690187281284\n",
      "[EPOCH #1, step #2284] loss: 3.419488679293209\n",
      "[EPOCH #1, step #2286] loss: 3.419524209385727\n",
      "[EPOCH #1, step #2288] loss: 3.4193878283194445\n",
      "[EPOCH #1, step #2290] loss: 3.419480887629069\n",
      "[EPOCH #1, step #2292] loss: 3.4196025440384092\n",
      "[EPOCH #1, step #2294] loss: 3.4194877333630664\n",
      "[EPOCH #1, step #2296] loss: 3.41941970485576\n",
      "[EPOCH #1, step #2298] loss: 3.4191855524560064\n",
      "[EPOCH #1, step #2300] loss: 3.419504204194269\n",
      "[EPOCH #1, step #2302] loss: 3.4192594283050526\n",
      "[EPOCH #1, step #2304] loss: 3.4188219072503276\n",
      "[EPOCH #1, step #2306] loss: 3.418952815192677\n",
      "[EPOCH #1, step #2308] loss: 3.41877037067339\n",
      "[EPOCH #1, step #2310] loss: 3.4188599095412635\n",
      "[EPOCH #1, step #2312] loss: 3.4188697242242045\n",
      "[EPOCH #1, step #2314] loss: 3.418756543276892\n",
      "[EPOCH #1, step #2316] loss: 3.419009593626935\n",
      "[EPOCH #1, step #2318] loss: 3.4190086149459975\n",
      "[EPOCH #1, step #2320] loss: 3.418697017064027\n",
      "[EPOCH #1, step #2322] loss: 3.4183116111065672\n",
      "[EPOCH #1, step #2324] loss: 3.418381595406481\n",
      "[EPOCH #1, step #2326] loss: 3.418252094856495\n",
      "[EPOCH #1, step #2328] loss: 3.418524231405\n",
      "[EPOCH #1, step #2330] loss: 3.4186004710678173\n",
      "[EPOCH #1, step #2332] loss: 3.418136694888112\n",
      "[EPOCH #1, step #2334] loss: 3.4179415428153455\n",
      "[EPOCH #1, step #2336] loss: 3.418070260204206\n",
      "[EPOCH #1, step #2338] loss: 3.417990236111103\n",
      "[EPOCH #1, step #2340] loss: 3.417851810830111\n",
      "[EPOCH #1, step #2342] loss: 3.417905194302476\n",
      "[EPOCH #1, step #2344] loss: 3.4181220555864673\n",
      "[EPOCH #1, step #2346] loss: 3.418052474742348\n",
      "[EPOCH #1, step #2348] loss: 3.417904885691143\n",
      "[EPOCH #1, step #2350] loss: 3.4180002995421663\n",
      "[EPOCH #1, step #2352] loss: 3.417871895719983\n",
      "[EPOCH #1, step #2354] loss: 3.417731259684148\n",
      "[EPOCH #1, step #2356] loss: 3.4175695450466774\n",
      "[EPOCH #1, step #2358] loss: 3.4175882673000775\n",
      "[EPOCH #1, step #2360] loss: 3.4172916064975323\n",
      "[EPOCH #1, step #2362] loss: 3.4173207420239344\n",
      "[EPOCH #1, step #2364] loss: 3.417034590622335\n",
      "[EPOCH #1, step #2366] loss: 3.417110517048161\n",
      "[EPOCH #1, step #2368] loss: 3.417166812654385\n",
      "[EPOCH #1, step #2370] loss: 3.416936184272943\n",
      "[EPOCH #1, step #2372] loss: 3.4169897696562974\n",
      "[EPOCH #1, step #2374] loss: 3.4169519163432875\n",
      "[EPOCH #1, step #2376] loss: 3.416882130795785\n",
      "[EPOCH #1, step #2378] loss: 3.4165957733681847\n",
      "[EPOCH #1, step #2380] loss: 3.4165935247998838\n",
      "[EPOCH #1, step #2382] loss: 3.416505684134243\n",
      "[EPOCH #1, step #2384] loss: 3.4163057342265386\n",
      "[EPOCH #1, step #2386] loss: 3.416290162345781\n",
      "[EPOCH #1, step #2388] loss: 3.415844306231244\n",
      "[EPOCH #1, step #2390] loss: 3.4153507533605905\n",
      "[EPOCH #1, step #2392] loss: 3.415086115348105\n",
      "[EPOCH #1, step #2394] loss: 3.415317371244968\n",
      "[EPOCH #1, step #2396] loss: 3.4151515247526394\n",
      "[EPOCH #1, step #2398] loss: 3.414625746352119\n",
      "[EPOCH #1, step #2400] loss: 3.4145137228999523\n",
      "[EPOCH #1, step #2402] loss: 3.4145851187838945\n",
      "[EPOCH #1, step #2404] loss: 3.414273938121518\n",
      "[EPOCH #1, step #2406] loss: 3.414149033958901\n",
      "[EPOCH #1, step #2408] loss: 3.414019937879468\n",
      "[EPOCH #1, step #2410] loss: 3.4140902111412625\n",
      "[EPOCH #1, step #2412] loss: 3.4140261189847676\n",
      "[EPOCH #1, step #2414] loss: 3.413859514370715\n",
      "[EPOCH #1, step #2416] loss: 3.4135795445495467\n",
      "[EPOCH #1, step #2418] loss: 3.4137919770138674\n",
      "[EPOCH #1, step #2420] loss: 3.4138175526319188\n",
      "[EPOCH #1, step #2422] loss: 3.413628096232176\n",
      "[EPOCH #1, step #2424] loss: 3.414075929209129\n",
      "[EPOCH #1, step #2426] loss: 3.414240121252015\n",
      "[EPOCH #1, step #2428] loss: 3.414250770288517\n",
      "[EPOCH #1, step #2430] loss: 3.4142686881336446\n",
      "[EPOCH #1, step #2432] loss: 3.414200760433324\n",
      "[EPOCH #1, step #2434] loss: 3.413687829070512\n",
      "[EPOCH #1, step #2436] loss: 3.413526645389599\n",
      "[EPOCH #1, step #2438] loss: 3.413080261495574\n",
      "[EPOCH #1, step #2440] loss: 3.4127712370869765\n",
      "[EPOCH #1, step #2442] loss: 3.4125015479933762\n",
      "[EPOCH #1, step #2444] loss: 3.412487841237542\n",
      "[EPOCH #1, step #2446] loss: 3.412311367858026\n",
      "[EPOCH #1, step #2448] loss: 3.4121815175024914\n",
      "[EPOCH #1, step #2450] loss: 3.4120566840076485\n",
      "[EPOCH #1, step #2452] loss: 3.4118634906050334\n",
      "[EPOCH #1, step #2454] loss: 3.4119140771644423\n",
      "[EPOCH #1, step #2456] loss: 3.411627722989751\n",
      "[EPOCH #1, step #2458] loss: 3.4118323251453537\n",
      "[EPOCH #1, step #2460] loss: 3.4116396898178976\n",
      "[EPOCH #1, step #2462] loss: 3.4117372063084246\n",
      "[EPOCH #1, step #2464] loss: 3.411588214135315\n",
      "[EPOCH #1, step #2466] loss: 3.4115528967586375\n",
      "[EPOCH #1, step #2468] loss: 3.411413688802584\n",
      "[EPOCH #1, step #2470] loss: 3.4114037577993037\n",
      "[EPOCH #1, step #2472] loss: 3.4113741638300827\n",
      "[EPOCH #1, step #2474] loss: 3.4112041778757116\n",
      "[EPOCH #1, step #2476] loss: 3.41111452520784\n",
      "[EPOCH #1, step #2478] loss: 3.4109088643609926\n",
      "[EPOCH #1, step #2480] loss: 3.4107026309075255\n",
      "[EPOCH #1, step #2482] loss: 3.4105234070262074\n",
      "[EPOCH #1, step #2484] loss: 3.410495084823977\n",
      "[EPOCH #1, step #2486] loss: 3.4106155042912905\n",
      "[EPOCH #1, step #2488] loss: 3.4105819560955597\n",
      "[EPOCH #1, step #2490] loss: 3.4105801785245853\n",
      "[EPOCH #1, step #2492] loss: 3.410413476964627\n",
      "[EPOCH #1, step #2494] loss: 3.4103487277556517\n",
      "[EPOCH #1, step #2496] loss: 3.4099771570672406\n",
      "[EPOCH #1, step #2498] loss: 3.409765775034837\n",
      "[EPOCH #1, step #2500] loss: 3.4096505928878447\n",
      "[EPOCH #1, step #2502] loss: 3.4094549566566874\n",
      "[EPOCH #1, step #2504] loss: 3.409572590658527\n",
      "[EPOCH #1, step #2506] loss: 3.4096307210538033\n",
      "[EPOCH #1, step #2508] loss: 3.409614070186866\n",
      "[EPOCH #1, step #2510] loss: 3.409373919643856\n",
      "[EPOCH #1, step #2512] loss: 3.409100046586668\n",
      "[EPOCH #1, step #2514] loss: 3.4091594536783205\n",
      "[EPOCH #1, step #2516] loss: 3.409025741561235\n",
      "[EPOCH #1, step #2518] loss: 3.408857761654129\n",
      "[EPOCH #1, step #2520] loss: 3.408772523040953\n",
      "[EPOCH #1, step #2522] loss: 3.4089929428168624\n",
      "[EPOCH #1, step #2524] loss: 3.4091330586801663\n",
      "[EPOCH #1, step #2526] loss: 3.408735824470988\n",
      "[EPOCH #1, step #2528] loss: 3.4082730561200223\n",
      "[EPOCH #1, step #2530] loss: 3.4082490616944976\n",
      "[EPOCH #1, step #2532] loss: 3.408044902505585\n",
      "[EPOCH #1, step #2534] loss: 3.4079689969207645\n",
      "[EPOCH #1, step #2536] loss: 3.407746169150792\n",
      "[EPOCH #1, step #2538] loss: 3.407507108487027\n",
      "[EPOCH #1, step #2540] loss: 3.407351931733721\n",
      "[EPOCH #1, step #2542] loss: 3.407150995998555\n",
      "[EPOCH #1, step #2544] loss: 3.406870763100435\n",
      "[EPOCH #1, step #2546] loss: 3.407113417860195\n",
      "[EPOCH #1, step #2548] loss: 3.406872160250461\n",
      "[EPOCH #1, step #2550] loss: 3.4067367745679484\n",
      "[EPOCH #1, step #2552] loss: 3.4064329644534523\n",
      "[EPOCH #1, step #2554] loss: 3.4064933335477825\n",
      "[EPOCH #1, step #2556] loss: 3.406349509872866\n",
      "[EPOCH #1, step #2558] loss: 3.405993570057197\n",
      "[EPOCH #1, step #2560] loss: 3.40587294376348\n",
      "[EPOCH #1, step #2562] loss: 3.4059308777086166\n",
      "[EPOCH #1, step #2564] loss: 3.405991544500429\n",
      "[EPOCH #1, step #2566] loss: 3.405929594779228\n",
      "[EPOCH #1, step #2568] loss: 3.4060132247365\n",
      "[EPOCH #1, step #2570] loss: 3.4061124234846805\n",
      "[EPOCH #1, step #2572] loss: 3.406101769618936\n",
      "[EPOCH #1, step #2574] loss: 3.4059225302761043\n",
      "[EPOCH #1, step #2576] loss: 3.4056949569219577\n",
      "[EPOCH #1, step #2578] loss: 3.4055367699423793\n",
      "[EPOCH #1, step #2580] loss: 3.4053012556181725\n",
      "[EPOCH #1, step #2582] loss: 3.4054530790227817\n",
      "[EPOCH #1, step #2584] loss: 3.405614159047258\n",
      "[EPOCH #1, step #2586] loss: 3.4056426409731326\n",
      "[EPOCH #1, step #2588] loss: 3.405445513646084\n",
      "[EPOCH #1, step #2590] loss: 3.4053746367549858\n",
      "[EPOCH #1, step #2592] loss: 3.405556285974375\n",
      "[EPOCH #1, step #2594] loss: 3.4051605616920484\n",
      "[EPOCH #1, step #2596] loss: 3.4051731717921605\n",
      "[EPOCH #1, step #2598] loss: 3.4051901320853384\n",
      "[EPOCH #1, step #2600] loss: 3.404895909883938\n",
      "[EPOCH #1, step #2602] loss: 3.4045689245210076\n",
      "[EPOCH #1, step #2604] loss: 3.4045302106414326\n",
      "[EPOCH #1, step #2606] loss: 3.404220338977979\n",
      "[EPOCH #1, step #2608] loss: 3.403939859855216\n",
      "[EPOCH #1, step #2610] loss: 3.4039487589460276\n",
      "[EPOCH #1, step #2612] loss: 3.4040509247478745\n",
      "[EPOCH #1, step #2614] loss: 3.4042443290951145\n",
      "[EPOCH #1, step #2616] loss: 3.403892434094305\n",
      "[EPOCH #1, step #2618] loss: 3.4037287504721254\n",
      "[EPOCH #1, step #2620] loss: 3.4037514800836184\n",
      "[EPOCH #1, step #2622] loss: 3.4037297900014827\n",
      "[EPOCH #1, step #2624] loss: 3.403625883102417\n",
      "[EPOCH #1, step #2626] loss: 3.40368318385301\n",
      "[EPOCH #1, step #2628] loss: 3.4035716286264206\n",
      "[EPOCH #1, step #2630] loss: 3.4032260009609034\n",
      "[EPOCH #1, step #2632] loss: 3.4029619489112735\n",
      "[EPOCH #1, step #2634] loss: 3.402780469318947\n",
      "[EPOCH #1, step #2636] loss: 3.4025767267225002\n",
      "[EPOCH #1, step #2638] loss: 3.4025932076574503\n",
      "[EPOCH #1, step #2640] loss: 3.4025453248289397\n",
      "[EPOCH #1, elapsed time: 558.135[sec]] loss: 3.4025453248289397\n",
      "[EPOCH #2, step #0] loss: 3.7743959426879883\n",
      "[EPOCH #2, step #2] loss: 3.4841674168904624\n",
      "[EPOCH #2, step #4] loss: 3.3480897426605223\n",
      "[EPOCH #2, step #6] loss: 3.3235838072640553\n",
      "[EPOCH #2, step #8] loss: 3.3407381905449762\n",
      "[EPOCH #2, step #10] loss: 3.341308658773249\n",
      "[EPOCH #2, step #12] loss: 3.319900054198045\n",
      "[EPOCH #2, step #14] loss: 3.257436736424764\n",
      "[EPOCH #2, step #16] loss: 3.2119987571940705\n",
      "[EPOCH #2, step #18] loss: 3.2311109116202905\n",
      "[EPOCH #2, step #20] loss: 3.2345218431381952\n",
      "[EPOCH #2, step #22] loss: 3.2660114039545474\n",
      "[EPOCH #2, step #24] loss: 3.3048997211456297\n",
      "[EPOCH #2, step #26] loss: 3.3017632519757307\n",
      "[EPOCH #2, step #28] loss: 3.3085757699506035\n",
      "[EPOCH #2, step #30] loss: 3.2891489075076197\n",
      "[EPOCH #2, step #32] loss: 3.3061216961253774\n",
      "[EPOCH #2, step #34] loss: 3.3178107738494873\n",
      "[EPOCH #2, step #36] loss: 3.3424175236676192\n",
      "[EPOCH #2, step #38] loss: 3.354089547426273\n",
      "[EPOCH #2, step #40] loss: 3.3430488691097353\n",
      "[EPOCH #2, step #42] loss: 3.331298828125\n",
      "[EPOCH #2, step #44] loss: 3.3286181926727294\n",
      "[EPOCH #2, step #46] loss: 3.3173946522651834\n",
      "[EPOCH #2, step #48] loss: 3.3195217774838817\n",
      "[EPOCH #2, step #50] loss: 3.3218948887843713\n",
      "[EPOCH #2, step #52] loss: 3.325616665606229\n",
      "[EPOCH #2, step #54] loss: 3.325830364227295\n",
      "[EPOCH #2, step #56] loss: 3.326703117604841\n",
      "[EPOCH #2, step #58] loss: 3.3257148144608837\n",
      "[EPOCH #2, step #60] loss: 3.318926224943067\n",
      "[EPOCH #2, step #62] loss: 3.316077641078404\n",
      "[EPOCH #2, step #64] loss: 3.3146308312049277\n",
      "[EPOCH #2, step #66] loss: 3.3232275550045185\n",
      "[EPOCH #2, step #68] loss: 3.319573354030001\n",
      "[EPOCH #2, step #70] loss: 3.318799139748157\n",
      "[EPOCH #2, step #72] loss: 3.3228393384855086\n",
      "[EPOCH #2, step #74] loss: 3.331723610560099\n",
      "[EPOCH #2, step #76] loss: 3.335655840960416\n",
      "[EPOCH #2, step #78] loss: 3.3381658566148977\n",
      "[EPOCH #2, step #80] loss: 3.3340769638249905\n",
      "[EPOCH #2, step #82] loss: 3.327169257474233\n",
      "[EPOCH #2, step #84] loss: 3.3275034792283003\n",
      "[EPOCH #2, step #86] loss: 3.331165343865581\n",
      "[EPOCH #2, step #88] loss: 3.324934442391556\n",
      "[EPOCH #2, step #90] loss: 3.3185869494637292\n",
      "[EPOCH #2, step #92] loss: 3.314514667757096\n",
      "[EPOCH #2, step #94] loss: 3.3133475002489594\n",
      "[EPOCH #2, step #96] loss: 3.313991989057089\n",
      "[EPOCH #2, step #98] loss: 3.3085454738501348\n",
      "[EPOCH #2, step #100] loss: 3.307364791926771\n",
      "[EPOCH #2, step #102] loss: 3.297352425103049\n",
      "[EPOCH #2, step #104] loss: 3.292375326156616\n",
      "[EPOCH #2, step #106] loss: 3.2946512476306093\n",
      "[EPOCH #2, step #108] loss: 3.288969372390607\n",
      "[EPOCH #2, step #110] loss: 3.286970705599398\n",
      "[EPOCH #2, step #112] loss: 3.286908156048935\n",
      "[EPOCH #2, step #114] loss: 3.287185005519701\n",
      "[EPOCH #2, step #116] loss: 3.2830507490370007\n",
      "[EPOCH #2, step #118] loss: 3.2820608956473216\n",
      "[EPOCH #2, step #120] loss: 3.27693171934648\n",
      "[EPOCH #2, step #122] loss: 3.274764190844404\n",
      "[EPOCH #2, step #124] loss: 3.275410835266113\n",
      "[EPOCH #2, step #126] loss: 3.2816768612448626\n",
      "[EPOCH #2, step #128] loss: 3.281943304594173\n",
      "[EPOCH #2, step #130] loss: 3.290051503945853\n",
      "[EPOCH #2, step #132] loss: 3.289638832995766\n",
      "[EPOCH #2, step #134] loss: 3.2905217824158846\n",
      "[EPOCH #2, step #136] loss: 3.288476185206949\n",
      "[EPOCH #2, step #138] loss: 3.294434180362619\n",
      "[EPOCH #2, step #140] loss: 3.291908465378673\n",
      "[EPOCH #2, step #142] loss: 3.2946454795090467\n",
      "[EPOCH #2, step #144] loss: 3.291455104433257\n",
      "[EPOCH #2, step #146] loss: 3.296115437332465\n",
      "[EPOCH #2, step #148] loss: 3.2935839563408154\n",
      "[EPOCH #2, step #150] loss: 3.297198796114385\n",
      "[EPOCH #2, step #152] loss: 3.299200400807499\n",
      "[EPOCH #2, step #154] loss: 3.2958945059007214\n",
      "[EPOCH #2, step #156] loss: 3.28859066052042\n",
      "[EPOCH #2, step #158] loss: 3.2893997408309072\n",
      "[EPOCH #2, step #160] loss: 3.288744775404841\n",
      "[EPOCH #2, step #162] loss: 3.2834310209824262\n",
      "[EPOCH #2, step #164] loss: 3.2822454510313093\n",
      "[EPOCH #2, step #166] loss: 3.2828374894079335\n",
      "[EPOCH #2, step #168] loss: 3.279741790873059\n",
      "[EPOCH #2, step #170] loss: 3.2833942586218403\n",
      "[EPOCH #2, step #172] loss: 3.2856989292740133\n",
      "[EPOCH #2, step #174] loss: 3.2863798591068814\n",
      "[EPOCH #2, step #176] loss: 3.2806073083715925\n",
      "[EPOCH #2, step #178] loss: 3.2775212639536937\n",
      "[EPOCH #2, step #180] loss: 3.2834176925005836\n",
      "[EPOCH #2, step #182] loss: 3.285440143991689\n",
      "[EPOCH #2, step #184] loss: 3.281941555641793\n",
      "[EPOCH #2, step #186] loss: 3.2837512760876333\n",
      "[EPOCH #2, step #188] loss: 3.2814021577280035\n",
      "[EPOCH #2, step #190] loss: 3.2815543619125926\n",
      "[EPOCH #2, step #192] loss: 3.2834831951813377\n",
      "[EPOCH #2, step #194] loss: 3.285546656143971\n",
      "[EPOCH #2, step #196] loss: 3.2830752309808875\n",
      "[EPOCH #2, step #198] loss: 3.2869163170531768\n",
      "[EPOCH #2, step #200] loss: 3.2894150392333072\n",
      "[EPOCH #2, step #202] loss: 3.289407509301096\n",
      "[EPOCH #2, step #204] loss: 3.2905458706181223\n",
      "[EPOCH #2, step #206] loss: 3.292014770461741\n",
      "[EPOCH #2, step #208] loss: 3.2875205443806625\n",
      "[EPOCH #2, step #210] loss: 3.29076028209162\n",
      "[EPOCH #2, step #212] loss: 3.2893470251504247\n",
      "[EPOCH #2, step #214] loss: 3.285850046956262\n",
      "[EPOCH #2, step #216] loss: 3.283403583385977\n",
      "[EPOCH #2, step #218] loss: 3.282260210002394\n",
      "[EPOCH #2, step #220] loss: 3.2811631530658154\n",
      "[EPOCH #2, step #222] loss: 3.2808821447227032\n",
      "[EPOCH #2, step #224] loss: 3.280711145401001\n",
      "[EPOCH #2, step #226] loss: 3.2761238598088336\n",
      "[EPOCH #2, step #228] loss: 3.275961716622765\n",
      "[EPOCH #2, step #230] loss: 3.2800864327005494\n",
      "[EPOCH #2, step #232] loss: 3.2767272711823425\n",
      "[EPOCH #2, step #234] loss: 3.2753330829295706\n",
      "[EPOCH #2, step #236] loss: 3.2722961912678263\n",
      "[EPOCH #2, step #238] loss: 3.2728371161297276\n",
      "[EPOCH #2, step #240] loss: 3.272992954214579\n",
      "[EPOCH #2, step #242] loss: 3.2694400593086526\n",
      "[EPOCH #2, step #244] loss: 3.265941740542042\n",
      "[EPOCH #2, step #246] loss: 3.2669030384496156\n",
      "[EPOCH #2, step #248] loss: 3.266298911657678\n",
      "[EPOCH #2, step #250] loss: 3.2672017750987017\n",
      "[EPOCH #2, step #252] loss: 3.268706609137916\n",
      "[EPOCH #2, step #254] loss: 3.2704699254503438\n",
      "[EPOCH #2, step #256] loss: 3.266971382185643\n",
      "[EPOCH #2, step #258] loss: 3.2679935035558283\n",
      "[EPOCH #2, step #260] loss: 3.2673686537249336\n",
      "[EPOCH #2, step #262] loss: 3.266001637897564\n",
      "[EPOCH #2, step #264] loss: 3.26733076347495\n",
      "[EPOCH #2, step #266] loss: 3.2671450568495617\n",
      "[EPOCH #2, step #268] loss: 3.2672019332758113\n",
      "[EPOCH #2, step #270] loss: 3.266472890368247\n",
      "[EPOCH #2, step #272] loss: 3.2675912118220065\n",
      "[EPOCH #2, step #274] loss: 3.266377492384477\n",
      "[EPOCH #2, step #276] loss: 3.2673791771761347\n",
      "[EPOCH #2, step #278] loss: 3.26899598576262\n",
      "[EPOCH #2, step #280] loss: 3.2678711287067452\n",
      "[EPOCH #2, step #282] loss: 3.2664205471955423\n",
      "[EPOCH #2, step #284] loss: 3.270333020728931\n",
      "[EPOCH #2, step #286] loss: 3.268486831246353\n",
      "[EPOCH #2, step #288] loss: 3.26874044527232\n",
      "[EPOCH #2, step #290] loss: 3.2666107838096488\n",
      "[EPOCH #2, step #292] loss: 3.269337907992939\n",
      "[EPOCH #2, step #294] loss: 3.267244459410845\n",
      "[EPOCH #2, step #296] loss: 3.2652334974269674\n",
      "[EPOCH #2, step #298] loss: 3.2637964762174168\n",
      "[EPOCH #2, step #300] loss: 3.263521966744103\n",
      "[EPOCH #2, step #302] loss: 3.261635724467413\n",
      "[EPOCH #2, step #304] loss: 3.2626143502407388\n",
      "[EPOCH #2, step #306] loss: 3.258801820612109\n",
      "[EPOCH #2, step #308] loss: 3.25706193979504\n",
      "[EPOCH #2, step #310] loss: 3.257419481921426\n",
      "[EPOCH #2, step #312] loss: 3.2578059865262943\n",
      "[EPOCH #2, step #314] loss: 3.2561945551917666\n",
      "[EPOCH #2, step #316] loss: 3.257685599642973\n",
      "[EPOCH #2, step #318] loss: 3.2568903671910396\n",
      "[EPOCH #2, step #320] loss: 3.2560110144154675\n",
      "[EPOCH #2, step #322] loss: 3.25532604740131\n",
      "[EPOCH #2, step #324] loss: 3.2563939842811\n",
      "[EPOCH #2, step #326] loss: 3.258576242931028\n",
      "[EPOCH #2, step #328] loss: 3.2564403561473254\n",
      "[EPOCH #2, step #330] loss: 3.256901181356424\n",
      "[EPOCH #2, step #332] loss: 3.255882941208802\n",
      "[EPOCH #2, step #334] loss: 3.257743155778344\n",
      "[EPOCH #2, step #336] loss: 3.255303809480073\n",
      "[EPOCH #2, step #338] loss: 3.2561179227181944\n",
      "[EPOCH #2, step #340] loss: 3.255131495663148\n",
      "[EPOCH #2, step #342] loss: 3.2549346487306643\n",
      "[EPOCH #2, step #344] loss: 3.255553612501725\n",
      "[EPOCH #2, step #346] loss: 3.25701732869107\n",
      "[EPOCH #2, step #348] loss: 3.2568357591984265\n",
      "[EPOCH #2, step #350] loss: 3.2545069860257314\n",
      "[EPOCH #2, step #352] loss: 3.255788801074366\n",
      "[EPOCH #2, step #354] loss: 3.2558302355484225\n",
      "[EPOCH #2, step #356] loss: 3.2564489654466215\n",
      "[EPOCH #2, step #358] loss: 3.255779266357422\n",
      "[EPOCH #2, step #360] loss: 3.2550460948838422\n",
      "[EPOCH #2, step #362] loss: 3.255582749022597\n",
      "[EPOCH #2, step #364] loss: 3.255109372204297\n",
      "[EPOCH #2, step #366] loss: 3.253861031675209\n",
      "[EPOCH #2, step #368] loss: 3.2551407012836076\n",
      "[EPOCH #2, step #370] loss: 3.25516103348642\n",
      "[EPOCH #2, step #372] loss: 3.2526039949371093\n",
      "[EPOCH #2, step #374] loss: 3.2521760781606037\n",
      "[EPOCH #2, step #376] loss: 3.252862183421613\n",
      "[EPOCH #2, step #378] loss: 3.2507243886157515\n",
      "[EPOCH #2, step #380] loss: 3.2508002372551466\n",
      "[EPOCH #2, step #382] loss: 3.251370837732021\n",
      "[EPOCH #2, step #384] loss: 3.251277220713628\n",
      "[EPOCH #2, step #386] loss: 3.251363298381638\n",
      "[EPOCH #2, step #388] loss: 3.2504743371340794\n",
      "[EPOCH #2, step #390] loss: 3.2502995878839127\n",
      "[EPOCH #2, step #392] loss: 3.250975935210405\n",
      "[EPOCH #2, step #394] loss: 3.2518007815638676\n",
      "[EPOCH #2, step #396] loss: 3.2517052891872993\n",
      "[EPOCH #2, step #398] loss: 3.2533345939521503\n",
      "[EPOCH #2, step #400] loss: 3.2546043199791277\n",
      "[EPOCH #2, step #402] loss: 3.254724037558506\n",
      "[EPOCH #2, step #404] loss: 3.255811438737092\n",
      "[EPOCH #2, step #406] loss: 3.255029613907273\n",
      "[EPOCH #2, step #408] loss: 3.2550917534490087\n",
      "[EPOCH #2, step #410] loss: 3.2557282645046857\n",
      "[EPOCH #2, step #412] loss: 3.2563418002740523\n",
      "[EPOCH #2, step #414] loss: 3.2587899926197097\n",
      "[EPOCH #2, step #416] loss: 3.2569773002780025\n",
      "[EPOCH #2, step #418] loss: 3.2568335766439507\n",
      "[EPOCH #2, step #420] loss: 3.2554743335252705\n",
      "[EPOCH #2, step #422] loss: 3.2566366573315704\n",
      "[EPOCH #2, step #424] loss: 3.2579183348487404\n",
      "[EPOCH #2, step #426] loss: 3.2598541687467337\n",
      "[EPOCH #2, step #428] loss: 3.2589791404617414\n",
      "[EPOCH #2, step #430] loss: 3.259122676473206\n",
      "[EPOCH #2, step #432] loss: 3.2577188284788066\n",
      "[EPOCH #2, step #434] loss: 3.259746405722081\n",
      "[EPOCH #2, step #436] loss: 3.258745759248188\n",
      "[EPOCH #2, step #438] loss: 3.258949539927524\n",
      "[EPOCH #2, step #440] loss: 3.258868233687213\n",
      "[EPOCH #2, step #442] loss: 3.258787871483486\n",
      "[EPOCH #2, step #444] loss: 3.2583549537015766\n",
      "[EPOCH #2, step #446] loss: 3.2601872909255745\n",
      "[EPOCH #2, step #448] loss: 3.258224346589936\n",
      "[EPOCH #2, step #450] loss: 3.2567444032681756\n",
      "[EPOCH #2, step #452] loss: 3.257211474130222\n",
      "[EPOCH #2, step #454] loss: 3.2580353684477754\n",
      "[EPOCH #2, step #456] loss: 3.2585567598530707\n",
      "[EPOCH #2, step #458] loss: 3.259254107548001\n",
      "[EPOCH #2, step #460] loss: 3.2603240080355564\n",
      "[EPOCH #2, step #462] loss: 3.2613324017040672\n",
      "[EPOCH #2, step #464] loss: 3.260703925676243\n",
      "[EPOCH #2, step #466] loss: 3.259269916015723\n",
      "[EPOCH #2, step #468] loss: 3.260561902385785\n",
      "[EPOCH #2, step #470] loss: 3.2602205792809746\n",
      "[EPOCH #2, step #472] loss: 3.258190385382977\n",
      "[EPOCH #2, step #474] loss: 3.2570120936945863\n",
      "[EPOCH #2, step #476] loss: 3.257170782149213\n",
      "[EPOCH #2, step #478] loss: 3.2583206327076\n",
      "[EPOCH #2, step #480] loss: 3.2587978007144094\n",
      "[EPOCH #2, step #482] loss: 3.25768412319523\n",
      "[EPOCH #2, step #484] loss: 3.25725992291244\n",
      "[EPOCH #2, step #486] loss: 3.2575471685162807\n",
      "[EPOCH #2, step #488] loss: 3.257040230530659\n",
      "[EPOCH #2, step #490] loss: 3.257920033091683\n",
      "[EPOCH #2, step #492] loss: 3.2589491830636232\n",
      "[EPOCH #2, step #494] loss: 3.2589680907702205\n",
      "[EPOCH #2, step #496] loss: 3.2602967908924496\n",
      "[EPOCH #2, step #498] loss: 3.261843273300446\n",
      "[EPOCH #2, step #500] loss: 3.261907503276528\n",
      "[EPOCH #2, step #502] loss: 3.2622369018274084\n",
      "[EPOCH #2, step #504] loss: 3.2631916782643535\n",
      "[EPOCH #2, step #506] loss: 3.26258977019105\n",
      "[EPOCH #2, step #508] loss: 3.2624606945659886\n",
      "[EPOCH #2, step #510] loss: 3.2611175936495487\n",
      "[EPOCH #2, step #512] loss: 3.2625851682287443\n",
      "[EPOCH #2, step #514] loss: 3.2612429104962395\n",
      "[EPOCH #2, step #516] loss: 3.2605591774909133\n",
      "[EPOCH #2, step #518] loss: 3.259859639555503\n",
      "[EPOCH #2, step #520] loss: 3.2596224379402203\n",
      "[EPOCH #2, step #522] loss: 3.260070177154833\n",
      "[EPOCH #2, step #524] loss: 3.260481649580456\n",
      "[EPOCH #2, step #526] loss: 3.2599740390307086\n",
      "[EPOCH #2, step #528] loss: 3.2632109262542146\n",
      "[EPOCH #2, step #530] loss: 3.2646557732490495\n",
      "[EPOCH #2, step #532] loss: 3.2651550577460715\n",
      "[EPOCH #2, step #534] loss: 3.2657098600797565\n",
      "[EPOCH #2, step #536] loss: 3.265558472780764\n",
      "[EPOCH #2, step #538] loss: 3.2663466629601587\n",
      "[EPOCH #2, step #540] loss: 3.266089209787506\n",
      "[EPOCH #2, step #542] loss: 3.2667274097491803\n",
      "[EPOCH #2, step #544] loss: 3.26559600655092\n",
      "[EPOCH #2, step #546] loss: 3.26437777357084\n",
      "[EPOCH #2, step #548] loss: 3.2651062380854983\n",
      "[EPOCH #2, step #550] loss: 3.264366305674918\n",
      "[EPOCH #2, step #552] loss: 3.263539166821278\n",
      "[EPOCH #2, step #554] loss: 3.262725912128483\n",
      "[EPOCH #2, step #556] loss: 3.262720083633916\n",
      "[EPOCH #2, step #558] loss: 3.26403983590428\n",
      "[EPOCH #2, step #560] loss: 3.2628133543459916\n",
      "[EPOCH #2, step #562] loss: 3.2640972539756055\n",
      "[EPOCH #2, step #564] loss: 3.263883013429895\n",
      "[EPOCH #2, step #566] loss: 3.2664589902921537\n",
      "[EPOCH #2, step #568] loss: 3.268124516693276\n",
      "[EPOCH #2, step #570] loss: 3.2675074424509827\n",
      "[EPOCH #2, step #572] loss: 3.2667434860482474\n",
      "[EPOCH #2, step #574] loss: 3.2662672926032026\n",
      "[EPOCH #2, step #576] loss: 3.265036354031885\n",
      "[EPOCH #2, step #578] loss: 3.2645691647636665\n",
      "[EPOCH #2, step #580] loss: 3.263545641595443\n",
      "[EPOCH #2, step #582] loss: 3.2638235885528513\n",
      "[EPOCH #2, step #584] loss: 3.2637281756115777\n",
      "[EPOCH #2, step #586] loss: 3.264075624069526\n",
      "[EPOCH #2, step #588] loss: 3.2648459662808222\n",
      "[EPOCH #2, step #590] loss: 3.2652509377894265\n",
      "[EPOCH #2, step #592] loss: 3.265336622878231\n",
      "[EPOCH #2, step #594] loss: 3.265063730207812\n",
      "[EPOCH #2, step #596] loss: 3.2642697361446107\n",
      "[EPOCH #2, step #598] loss: 3.2646211861370005\n",
      "[EPOCH #2, step #600] loss: 3.264418610320512\n",
      "[EPOCH #2, step #602] loss: 3.262453634742876\n",
      "[EPOCH #2, step #604] loss: 3.2624014022921726\n",
      "[EPOCH #2, step #606] loss: 3.262622447147401\n",
      "[EPOCH #2, step #608] loss: 3.2631866982809234\n",
      "[EPOCH #2, step #610] loss: 3.263663526447627\n",
      "[EPOCH #2, step #612] loss: 3.2631597059974857\n",
      "[EPOCH #2, step #614] loss: 3.2631402038946384\n",
      "[EPOCH #2, step #616] loss: 3.263776554667196\n",
      "[EPOCH #2, step #618] loss: 3.2638011383124432\n",
      "[EPOCH #2, step #620] loss: 3.263372628969079\n",
      "[EPOCH #2, step #622] loss: 3.2627814294438493\n",
      "[EPOCH #2, step #624] loss: 3.2630063037872317\n",
      "[EPOCH #2, step #626] loss: 3.2632125237721956\n",
      "[EPOCH #2, step #628] loss: 3.2629415416565912\n",
      "[EPOCH #2, step #630] loss: 3.263129534698704\n",
      "[EPOCH #2, step #632] loss: 3.2632525287344936\n",
      "[EPOCH #2, step #634] loss: 3.262316355367345\n",
      "[EPOCH #2, step #636] loss: 3.262056284834115\n",
      "[EPOCH #2, step #638] loss: 3.262112939488347\n",
      "[EPOCH #2, step #640] loss: 3.2615845221997053\n",
      "[EPOCH #2, step #642] loss: 3.261121854440977\n",
      "[EPOCH #2, step #644] loss: 3.260441257417664\n",
      "[EPOCH #2, step #646] loss: 3.2607697615114946\n",
      "[EPOCH #2, step #648] loss: 3.26026482942108\n",
      "[EPOCH #2, step #650] loss: 3.259159158451766\n",
      "[EPOCH #2, step #652] loss: 3.258515346652698\n",
      "[EPOCH #2, step #654] loss: 3.2596951695798917\n",
      "[EPOCH #2, step #656] loss: 3.259786311894247\n",
      "[EPOCH #2, step #658] loss: 3.260175532382971\n",
      "[EPOCH #2, step #660] loss: 3.2604267550308297\n",
      "[EPOCH #2, step #662] loss: 3.2607004951747474\n",
      "[EPOCH #2, step #664] loss: 3.2617349915038374\n",
      "[EPOCH #2, step #666] loss: 3.2618366685406914\n",
      "[EPOCH #2, step #668] loss: 3.2613038093698044\n",
      "[EPOCH #2, step #670] loss: 3.2614253253055576\n",
      "[EPOCH #2, step #672] loss: 3.262206090329129\n",
      "[EPOCH #2, step #674] loss: 3.2628022045559355\n",
      "[EPOCH #2, step #676] loss: 3.261923596284942\n",
      "[EPOCH #2, step #678] loss: 3.26283670981608\n",
      "[EPOCH #2, step #680] loss: 3.2626270817239904\n",
      "[EPOCH #2, step #682] loss: 3.2620610279708795\n",
      "[EPOCH #2, step #684] loss: 3.2621246731194264\n",
      "[EPOCH #2, step #686] loss: 3.2622085554630997\n",
      "[EPOCH #2, step #688] loss: 3.2625719673920783\n",
      "[EPOCH #2, step #690] loss: 3.262633170819317\n",
      "[EPOCH #2, step #692] loss: 3.263487682026014\n",
      "[EPOCH #2, step #694] loss: 3.262535488348213\n",
      "[EPOCH #2, step #696] loss: 3.2624982166153456\n",
      "[EPOCH #2, step #698] loss: 3.262642901001741\n",
      "[EPOCH #2, step #700] loss: 3.2623888565368215\n",
      "[EPOCH #2, step #702] loss: 3.2620756493862797\n",
      "[EPOCH #2, step #704] loss: 3.263005806875567\n",
      "[EPOCH #2, step #706] loss: 3.261438045339841\n",
      "[EPOCH #2, step #708] loss: 3.2611636133557145\n",
      "[EPOCH #2, step #710] loss: 3.2605789240905505\n",
      "[EPOCH #2, step #712] loss: 3.2600537493386073\n",
      "[EPOCH #2, step #714] loss: 3.260886975935289\n",
      "[EPOCH #2, step #716] loss: 3.2613995065250156\n",
      "[EPOCH #2, step #718] loss: 3.262104803066758\n",
      "[EPOCH #2, step #720] loss: 3.2627906875372266\n",
      "[EPOCH #2, step #722] loss: 3.262940902762723\n",
      "[EPOCH #2, step #724] loss: 3.2633054700391044\n",
      "[EPOCH #2, step #726] loss: 3.263550846907919\n",
      "[EPOCH #2, step #728] loss: 3.2630470027975913\n",
      "[EPOCH #2, step #730] loss: 3.2621114772582676\n",
      "[EPOCH #2, step #732] loss: 3.2614678842146434\n",
      "[EPOCH #2, step #734] loss: 3.261705781975571\n",
      "[EPOCH #2, step #736] loss: 3.2624860890361314\n",
      "[EPOCH #2, step #738] loss: 3.26233840537168\n",
      "[EPOCH #2, step #740] loss: 3.260937937840759\n",
      "[EPOCH #2, step #742] loss: 3.260709059190301\n",
      "[EPOCH #2, step #744] loss: 3.260353334158059\n",
      "[EPOCH #2, step #746] loss: 3.2610410360926125\n",
      "[EPOCH #2, step #748] loss: 3.2608568655632846\n",
      "[EPOCH #2, step #750] loss: 3.2599235171484406\n",
      "[EPOCH #2, step #752] loss: 3.2604261581324643\n",
      "[EPOCH #2, step #754] loss: 3.261009604094044\n",
      "[EPOCH #2, step #756] loss: 3.2610350899211324\n",
      "[EPOCH #2, step #758] loss: 3.2608295966514014\n",
      "[EPOCH #2, step #760] loss: 3.2608327918860978\n",
      "[EPOCH #2, step #762] loss: 3.26044336277723\n",
      "[EPOCH #2, step #764] loss: 3.2601255448035946\n",
      "[EPOCH #2, step #766] loss: 3.260037135269682\n",
      "[EPOCH #2, step #768] loss: 3.2597788576961957\n",
      "[EPOCH #2, step #770] loss: 3.2597429087499084\n",
      "[EPOCH #2, step #772] loss: 3.2600404383448396\n",
      "[EPOCH #2, step #774] loss: 3.2600678880753056\n",
      "[EPOCH #2, step #776] loss: 3.2604156648176823\n",
      "[EPOCH #2, step #778] loss: 3.260677558619802\n",
      "[EPOCH #2, step #780] loss: 3.260608148941157\n",
      "[EPOCH #2, step #782] loss: 3.2604053574655856\n",
      "[EPOCH #2, step #784] loss: 3.2590781187555593\n",
      "[EPOCH #2, step #786] loss: 3.2602701150780256\n",
      "[EPOCH #2, step #788] loss: 3.2612771507451743\n",
      "[EPOCH #2, step #790] loss: 3.2609002867480448\n",
      "[EPOCH #2, step #792] loss: 3.2606591504879887\n",
      "[EPOCH #2, step #794] loss: 3.2604534946897497\n",
      "[EPOCH #2, step #796] loss: 3.261242169508222\n",
      "[EPOCH #2, step #798] loss: 3.2608829565131767\n",
      "[EPOCH #2, step #800] loss: 3.260416046063998\n",
      "[EPOCH #2, step #802] loss: 3.259396245737895\n",
      "[EPOCH #2, step #804] loss: 3.2586934910057495\n",
      "[EPOCH #2, step #806] loss: 3.258228667754371\n",
      "[EPOCH #2, step #808] loss: 3.2575759993790108\n",
      "[EPOCH #2, step #810] loss: 3.258331223275012\n",
      "[EPOCH #2, step #812] loss: 3.2572507494636715\n",
      "[EPOCH #2, step #814] loss: 3.257124074397643\n",
      "[EPOCH #2, step #816] loss: 3.2573306029400304\n",
      "[EPOCH #2, step #818] loss: 3.257046551931472\n",
      "[EPOCH #2, step #820] loss: 3.2581458178855907\n",
      "[EPOCH #2, step #822] loss: 3.258030481790424\n",
      "[EPOCH #2, step #824] loss: 3.258215009515936\n",
      "[EPOCH #2, step #826] loss: 3.2586957013016025\n",
      "[EPOCH #2, step #828] loss: 3.258628577461289\n",
      "[EPOCH #2, step #830] loss: 3.259460434586563\n",
      "[EPOCH #2, step #832] loss: 3.259597291751784\n",
      "[EPOCH #2, step #834] loss: 3.25978287793919\n",
      "[EPOCH #2, step #836] loss: 3.2603205996459814\n",
      "[EPOCH #2, step #838] loss: 3.2609217908584176\n",
      "[EPOCH #2, step #840] loss: 3.26176306353738\n",
      "[EPOCH #2, step #842] loss: 3.2622832180059396\n",
      "[EPOCH #2, step #844] loss: 3.2634013133641533\n",
      "[EPOCH #2, step #846] loss: 3.262690407044772\n",
      "[EPOCH #2, step #848] loss: 3.2627749611547894\n",
      "[EPOCH #2, step #850] loss: 3.2612759083614504\n",
      "[EPOCH #2, step #852] loss: 3.2604078958629863\n",
      "[EPOCH #2, step #854] loss: 3.2603599910847625\n",
      "[EPOCH #2, step #856] loss: 3.2595283434299214\n",
      "[EPOCH #2, step #858] loss: 3.2600059376051593\n",
      "[EPOCH #2, step #860] loss: 3.2606102099402023\n",
      "[EPOCH #2, step #862] loss: 3.260290468277826\n",
      "[EPOCH #2, step #864] loss: 3.259282197842019\n",
      "[EPOCH #2, step #866] loss: 3.2591019405488186\n",
      "[EPOCH #2, step #868] loss: 3.259115681137945\n",
      "[EPOCH #2, step #870] loss: 3.2594122240655596\n",
      "[EPOCH #2, step #872] loss: 3.259649228667339\n",
      "[EPOCH #2, step #874] loss: 3.2591323580060685\n",
      "[EPOCH #2, step #876] loss: 3.258740627099606\n",
      "[EPOCH #2, step #878] loss: 3.2580779195509946\n",
      "[EPOCH #2, step #880] loss: 3.258304205979987\n",
      "[EPOCH #2, step #882] loss: 3.257976993699036\n",
      "[EPOCH #2, step #884] loss: 3.2579515834312653\n",
      "[EPOCH #2, step #886] loss: 3.2588337946099384\n",
      "[EPOCH #2, step #888] loss: 3.2593144820699274\n",
      "[EPOCH #2, step #890] loss: 3.2589147077532594\n",
      "[EPOCH #2, step #892] loss: 3.2590398945973944\n",
      "[EPOCH #2, step #894] loss: 3.25911731053997\n",
      "[EPOCH #2, step #896] loss: 3.2583490898510816\n",
      "[EPOCH #2, step #898] loss: 3.257928653076308\n",
      "[EPOCH #2, step #900] loss: 3.2574754348737947\n",
      "[EPOCH #2, step #902] loss: 3.256600184826096\n",
      "[EPOCH #2, step #904] loss: 3.256595652933279\n",
      "[EPOCH #2, step #906] loss: 3.2563098725568103\n",
      "[EPOCH #2, step #908] loss: 3.256793481157427\n",
      "[EPOCH #2, step #910] loss: 3.2565245471330426\n",
      "[EPOCH #2, step #912] loss: 3.256348146927448\n",
      "[EPOCH #2, step #914] loss: 3.256532036671873\n",
      "[EPOCH #2, step #916] loss: 3.256088468994673\n",
      "[EPOCH #2, step #918] loss: 3.255627223275302\n",
      "[EPOCH #2, step #920] loss: 3.255837898948164\n",
      "[EPOCH #2, step #922] loss: 3.2555513622184744\n",
      "[EPOCH #2, step #924] loss: 3.256125568183693\n",
      "[EPOCH #2, step #926] loss: 3.2573328020760415\n",
      "[EPOCH #2, step #928] loss: 3.257928175869748\n",
      "[EPOCH #2, step #930] loss: 3.2583673279472376\n",
      "[EPOCH #2, step #932] loss: 3.2576365705982795\n",
      "[EPOCH #2, step #934] loss: 3.2574252095451968\n",
      "[EPOCH #2, step #936] loss: 3.25783426250185\n",
      "[EPOCH #2, step #938] loss: 3.2575617836941038\n",
      "[EPOCH #2, step #940] loss: 3.2571967466478013\n",
      "[EPOCH #2, step #942] loss: 3.2566307918390818\n",
      "[EPOCH #2, step #944] loss: 3.2571952600327747\n",
      "[EPOCH #2, step #946] loss: 3.2563090598822395\n",
      "[EPOCH #2, step #948] loss: 3.2568833127036863\n",
      "[EPOCH #2, step #950] loss: 3.2564626712779017\n",
      "[EPOCH #2, step #952] loss: 3.2563481916285513\n",
      "[EPOCH #2, step #954] loss: 3.256484012953274\n",
      "[EPOCH #2, step #956] loss: 3.25594710705796\n",
      "[EPOCH #2, step #958] loss: 3.2551099590265715\n",
      "[EPOCH #2, step #960] loss: 3.2559419494513793\n",
      "[EPOCH #2, step #962] loss: 3.2550283146795826\n",
      "[EPOCH #2, step #964] loss: 3.2543878288466694\n",
      "[EPOCH #2, step #966] loss: 3.2541696120442625\n",
      "[EPOCH #2, step #968] loss: 3.2537693084208956\n",
      "[EPOCH #2, step #970] loss: 3.253490933674617\n",
      "[EPOCH #2, step #972] loss: 3.252913356439918\n",
      "[EPOCH #2, step #974] loss: 3.253301463738466\n",
      "[EPOCH #2, step #976] loss: 3.2537284121908696\n",
      "[EPOCH #2, step #978] loss: 3.2535942148748287\n",
      "[EPOCH #2, step #980] loss: 3.2535634760219874\n",
      "[EPOCH #2, step #982] loss: 3.253789868990257\n",
      "[EPOCH #2, step #984] loss: 3.2532048537646454\n",
      "[EPOCH #2, step #986] loss: 3.253033194015333\n",
      "[EPOCH #2, step #988] loss: 3.2529907940130744\n",
      "[EPOCH #2, step #990] loss: 3.2523677262480395\n",
      "[EPOCH #2, step #992] loss: 3.251942307567308\n",
      "[EPOCH #2, step #994] loss: 3.250619036468429\n",
      "[EPOCH #2, step #996] loss: 3.2509961271716454\n",
      "[EPOCH #2, step #998] loss: 3.2510870889619783\n",
      "[EPOCH #2, step #1000] loss: 3.2502379295947432\n",
      "[EPOCH #2, step #1002] loss: 3.250069039412296\n",
      "[EPOCH #2, step #1004] loss: 3.249462882321865\n",
      "[EPOCH #2, step #1006] loss: 3.249693880014884\n",
      "[EPOCH #2, step #1008] loss: 3.2498250886635454\n",
      "[EPOCH #2, step #1010] loss: 3.249401334249419\n",
      "[EPOCH #2, step #1012] loss: 3.2490901996446855\n",
      "[EPOCH #2, step #1014] loss: 3.2480479893426004\n",
      "[EPOCH #2, step #1016] loss: 3.2485257238296166\n",
      "[EPOCH #2, step #1018] loss: 3.2482044139951904\n",
      "[EPOCH #2, step #1020] loss: 3.248095216760439\n",
      "[EPOCH #2, step #1022] loss: 3.2480667700166226\n",
      "[EPOCH #2, step #1024] loss: 3.247711723606761\n",
      "[EPOCH #2, step #1026] loss: 3.247375676787468\n",
      "[EPOCH #2, step #1028] loss: 3.2474834110345276\n",
      "[EPOCH #2, step #1030] loss: 3.248487716502755\n",
      "[EPOCH #2, step #1032] loss: 3.248312161021865\n",
      "[EPOCH #2, step #1034] loss: 3.248592350218031\n",
      "[EPOCH #2, step #1036] loss: 3.248408594839598\n",
      "[EPOCH #2, step #1038] loss: 3.247437713465172\n",
      "[EPOCH #2, step #1040] loss: 3.2472689076184538\n",
      "[EPOCH #2, step #1042] loss: 3.246877963339494\n",
      "[EPOCH #2, step #1044] loss: 3.2468089594224994\n",
      "[EPOCH #2, step #1046] loss: 3.2464468540639797\n",
      "[EPOCH #2, step #1048] loss: 3.2462600827558026\n",
      "[EPOCH #2, step #1050] loss: 3.247030430130455\n",
      "[EPOCH #2, step #1052] loss: 3.2476862615782762\n",
      "[EPOCH #2, step #1054] loss: 3.2464326508237287\n",
      "[EPOCH #2, step #1056] loss: 3.246291140057393\n",
      "[EPOCH #2, step #1058] loss: 3.246211157979776\n",
      "[EPOCH #2, step #1060] loss: 3.246456572067951\n",
      "[EPOCH #2, step #1062] loss: 3.2467661047475245\n",
      "[EPOCH #2, step #1064] loss: 3.2464881874585934\n",
      "[EPOCH #2, step #1066] loss: 3.2460729756306126\n",
      "[EPOCH #2, step #1068] loss: 3.246696598848748\n",
      "[EPOCH #2, step #1070] loss: 3.246891537077732\n",
      "[EPOCH #2, step #1072] loss: 3.2469709581675534\n",
      "[EPOCH #2, step #1074] loss: 3.2468114109926445\n",
      "[EPOCH #2, step #1076] loss: 3.2467652757409113\n",
      "[EPOCH #2, step #1078] loss: 3.2469993477291923\n",
      "[EPOCH #2, step #1080] loss: 3.247088596845092\n",
      "[EPOCH #2, step #1082] loss: 3.2474032194660643\n",
      "[EPOCH #2, step #1084] loss: 3.2474064721489833\n",
      "[EPOCH #2, step #1086] loss: 3.246944200477144\n",
      "[EPOCH #2, step #1088] loss: 3.2468247696059467\n",
      "[EPOCH #2, step #1090] loss: 3.2462336562512446\n",
      "[EPOCH #2, step #1092] loss: 3.245124538339613\n",
      "[EPOCH #2, step #1094] loss: 3.2455962030854946\n",
      "[EPOCH #2, step #1096] loss: 3.245917831427853\n",
      "[EPOCH #2, step #1098] loss: 3.246778577972912\n",
      "[EPOCH #2, step #1100] loss: 3.24627347927977\n",
      "[EPOCH #2, step #1102] loss: 3.246153975224776\n",
      "[EPOCH #2, step #1104] loss: 3.246343292477983\n",
      "[EPOCH #2, step #1106] loss: 3.246714169632344\n",
      "[EPOCH #2, step #1108] loss: 3.2461990538322785\n",
      "[EPOCH #2, step #1110] loss: 3.2460700871169728\n",
      "[EPOCH #2, step #1112] loss: 3.2468557085738134\n",
      "[EPOCH #2, step #1114] loss: 3.2467018557236336\n",
      "[EPOCH #2, step #1116] loss: 3.2463111685340493\n",
      "[EPOCH #2, step #1118] loss: 3.245525559748359\n",
      "[EPOCH #2, step #1120] loss: 3.2455269869260763\n",
      "[EPOCH #2, step #1122] loss: 3.245423407822131\n",
      "[EPOCH #2, step #1124] loss: 3.2454492365519205\n",
      "[EPOCH #2, step #1126] loss: 3.245775031324182\n",
      "[EPOCH #2, step #1128] loss: 3.2463933819685495\n",
      "[EPOCH #2, step #1130] loss: 3.246284183414503\n",
      "[EPOCH #2, step #1132] loss: 3.2463964285938904\n",
      "[EPOCH #2, step #1134] loss: 3.2458831335479466\n",
      "[EPOCH #2, step #1136] loss: 3.2457886652246106\n",
      "[EPOCH #2, step #1138] loss: 3.246346742673544\n",
      "[EPOCH #2, step #1140] loss: 3.2467121559731487\n",
      "[EPOCH #2, step #1142] loss: 3.2460434357533825\n",
      "[EPOCH #2, step #1144] loss: 3.2460417662124965\n",
      "[EPOCH #2, step #1146] loss: 3.2463177992347645\n",
      "[EPOCH #2, step #1148] loss: 3.2464087580057512\n",
      "[EPOCH #2, step #1150] loss: 3.2467578497272696\n",
      "[EPOCH #2, step #1152] loss: 3.2469974893750257\n",
      "[EPOCH #2, step #1154] loss: 3.247211975452704\n",
      "[EPOCH #2, step #1156] loss: 3.247470858185685\n",
      "[EPOCH #2, step #1158] loss: 3.2476261139327436\n",
      "[EPOCH #2, step #1160] loss: 3.247458181126584\n",
      "[EPOCH #2, step #1162] loss: 3.247307782119856\n",
      "[EPOCH #2, step #1164] loss: 3.2475415772122886\n",
      "[EPOCH #2, step #1166] loss: 3.247747584296376\n",
      "[EPOCH #2, step #1168] loss: 3.2481450037226094\n",
      "[EPOCH #2, step #1170] loss: 3.2485440033913475\n",
      "[EPOCH #2, step #1172] loss: 3.2491615225083788\n",
      "[EPOCH #2, step #1174] loss: 3.2488135670601053\n",
      "[EPOCH #2, step #1176] loss: 3.248754322883327\n",
      "[EPOCH #2, step #1178] loss: 3.248681608552747\n",
      "[EPOCH #2, step #1180] loss: 3.2485038915193254\n",
      "[EPOCH #2, step #1182] loss: 3.2486989193817166\n",
      "[EPOCH #2, step #1184] loss: 3.249088525973292\n",
      "[EPOCH #2, step #1186] loss: 3.2489936620925994\n",
      "[EPOCH #2, step #1188] loss: 3.2486153846031685\n",
      "[EPOCH #2, step #1190] loss: 3.249001689561769\n",
      "[EPOCH #2, step #1192] loss: 3.2485907261649403\n",
      "[EPOCH #2, step #1194] loss: 3.2488032759981675\n",
      "[EPOCH #2, step #1196] loss: 3.2491462836189875\n",
      "[EPOCH #2, step #1198] loss: 3.2487109192616748\n",
      "[EPOCH #2, step #1200] loss: 3.2482885830011297\n",
      "[EPOCH #2, step #1202] loss: 3.2487775647233947\n",
      "[EPOCH #2, step #1204] loss: 3.248772967010118\n",
      "[EPOCH #2, step #1206] loss: 3.2483578113454774\n",
      "[EPOCH #2, step #1208] loss: 3.248037712548369\n",
      "[EPOCH #2, step #1210] loss: 3.248616897990938\n",
      "[EPOCH #2, step #1212] loss: 3.249205928151507\n",
      "[EPOCH #2, step #1214] loss: 3.2486520278601\n",
      "[EPOCH #2, step #1216] loss: 3.2482841185300977\n",
      "[EPOCH #2, step #1218] loss: 3.2481323743668415\n",
      "[EPOCH #2, step #1220] loss: 3.2482490443870926\n",
      "[EPOCH #2, step #1222] loss: 3.2481095155340087\n",
      "[EPOCH #2, step #1224] loss: 3.2483118440666976\n",
      "[EPOCH #2, step #1226] loss: 3.248698527189222\n",
      "[EPOCH #2, step #1228] loss: 3.24832104918819\n",
      "[EPOCH #2, step #1230] loss: 3.2485737384188766\n",
      "[EPOCH #2, step #1232] loss: 3.248455273263936\n",
      "[EPOCH #2, step #1234] loss: 3.248911901620718\n",
      "[EPOCH #2, step #1236] loss: 3.2489087402386922\n",
      "[EPOCH #2, step #1238] loss: 3.2490695767868325\n",
      "[EPOCH #2, step #1240] loss: 3.24887031436831\n",
      "[EPOCH #2, step #1242] loss: 3.2487751856410303\n",
      "[EPOCH #2, step #1244] loss: 3.2492766889702365\n",
      "[EPOCH #2, step #1246] loss: 3.2493328333092193\n",
      "[EPOCH #2, step #1248] loss: 3.2502519708332582\n",
      "[EPOCH #2, step #1250] loss: 3.2502689767512773\n",
      "[EPOCH #2, step #1252] loss: 3.2501949006238178\n",
      "[EPOCH #2, step #1254] loss: 3.2506380852475107\n",
      "[EPOCH #2, step #1256] loss: 3.251003388478439\n",
      "[EPOCH #2, step #1258] loss: 3.25094305910318\n",
      "[EPOCH #2, step #1260] loss: 3.251194502038676\n",
      "[EPOCH #2, step #1262] loss: 3.2507497903758913\n",
      "[EPOCH #2, step #1264] loss: 3.2510716751158943\n",
      "[EPOCH #2, step #1266] loss: 3.250818826311107\n",
      "[EPOCH #2, step #1268] loss: 3.250948222252776\n",
      "[EPOCH #2, step #1270] loss: 3.2514159628765307\n",
      "[EPOCH #2, step #1272] loss: 3.2519386502016463\n",
      "[EPOCH #2, step #1274] loss: 3.25161200429879\n",
      "[EPOCH #2, step #1276] loss: 3.2516756871760406\n",
      "[EPOCH #2, step #1278] loss: 3.2516858898353727\n",
      "[EPOCH #2, step #1280] loss: 3.251925407025518\n",
      "[EPOCH #2, step #1282] loss: 3.252834136655158\n",
      "[EPOCH #2, step #1284] loss: 3.252743445667311\n",
      "[EPOCH #2, step #1286] loss: 3.2524667843979462\n",
      "[EPOCH #2, step #1288] loss: 3.252204010151818\n",
      "[EPOCH #2, step #1290] loss: 3.2517553072027416\n",
      "[EPOCH #2, step #1292] loss: 3.2517267086666313\n",
      "[EPOCH #2, step #1294] loss: 3.251947051677925\n",
      "[EPOCH #2, step #1296] loss: 3.2525650421839636\n",
      "[EPOCH #2, step #1298] loss: 3.2520250213247155\n",
      "[EPOCH #2, step #1300] loss: 3.2513547084773164\n",
      "[EPOCH #2, step #1302] loss: 3.251019467414569\n",
      "[EPOCH #2, step #1304] loss: 3.2507216482783643\n",
      "[EPOCH #2, step #1306] loss: 3.2499685513672247\n",
      "[EPOCH #2, step #1308] loss: 3.249920103001722\n",
      "[EPOCH #2, step #1310] loss: 3.250485863420092\n",
      "[EPOCH #2, step #1312] loss: 3.2506345444204965\n",
      "[EPOCH #2, step #1314] loss: 3.2507139963795475\n",
      "[EPOCH #2, step #1316] loss: 3.251712742047321\n",
      "[EPOCH #2, step #1318] loss: 3.251675254201419\n",
      "[EPOCH #2, step #1320] loss: 3.252390242814115\n",
      "[EPOCH #2, step #1322] loss: 3.2526273846355966\n",
      "[EPOCH #2, step #1324] loss: 3.252446274667416\n",
      "[EPOCH #2, step #1326] loss: 3.252359262335633\n",
      "[EPOCH #2, step #1328] loss: 3.252490672247135\n",
      "[EPOCH #2, step #1330] loss: 3.2526408038938253\n",
      "[EPOCH #2, step #1332] loss: 3.253130539502761\n",
      "[EPOCH #2, step #1334] loss: 3.2538365390863313\n",
      "[EPOCH #2, step #1336] loss: 3.253841650156094\n",
      "[EPOCH #2, step #1338] loss: 3.2540517036731424\n",
      "[EPOCH #2, step #1340] loss: 3.2533322850952033\n",
      "[EPOCH #2, step #1342] loss: 3.2529180207895148\n",
      "[EPOCH #2, step #1344] loss: 3.2529188992808744\n",
      "[EPOCH #2, step #1346] loss: 3.253203565842502\n",
      "[EPOCH #2, step #1348] loss: 3.253051830098044\n",
      "[EPOCH #2, step #1350] loss: 3.2531388083005464\n",
      "[EPOCH #2, step #1352] loss: 3.2532098064225248\n",
      "[EPOCH #2, step #1354] loss: 3.253457124470785\n",
      "[EPOCH #2, step #1356] loss: 3.253034108336919\n",
      "[EPOCH #2, step #1358] loss: 3.253266813476091\n",
      "[EPOCH #2, step #1360] loss: 3.2535994098433494\n",
      "[EPOCH #2, step #1362] loss: 3.253331311555444\n",
      "[EPOCH #2, step #1364] loss: 3.2534445345183434\n",
      "[EPOCH #2, step #1366] loss: 3.2532607216685028\n",
      "[EPOCH #2, step #1368] loss: 3.2528257465780523\n",
      "[EPOCH #2, step #1370] loss: 3.252469562771197\n",
      "[EPOCH #2, step #1372] loss: 3.252152544516156\n",
      "[EPOCH #2, step #1374] loss: 3.252459572358565\n",
      "[EPOCH #2, step #1376] loss: 3.2533062525910577\n",
      "[EPOCH #2, step #1378] loss: 3.2525487839959513\n",
      "[EPOCH #2, step #1380] loss: 3.2521770271505677\n",
      "[EPOCH #2, step #1382] loss: 3.2522556187015295\n",
      "[EPOCH #2, step #1384] loss: 3.251878503599752\n",
      "[EPOCH #2, step #1386] loss: 3.251794332553778\n",
      "[EPOCH #2, step #1388] loss: 3.251818789776664\n",
      "[EPOCH #2, step #1390] loss: 3.251923864304291\n",
      "[EPOCH #2, step #1392] loss: 3.251554409035315\n",
      "[EPOCH #2, step #1394] loss: 3.2511826462215847\n",
      "[EPOCH #2, step #1396] loss: 3.2509391058660357\n",
      "[EPOCH #2, step #1398] loss: 3.2506351912336915\n",
      "[EPOCH #2, step #1400] loss: 3.2511162888910157\n",
      "[EPOCH #2, step #1402] loss: 3.2512354363056053\n",
      "[EPOCH #2, step #1404] loss: 3.2510802436977944\n",
      "[EPOCH #2, step #1406] loss: 3.2505552816153758\n",
      "[EPOCH #2, step #1408] loss: 3.250545016242055\n",
      "[EPOCH #2, step #1410] loss: 3.2505242572787125\n",
      "[EPOCH #2, step #1412] loss: 3.250844603975192\n",
      "[EPOCH #2, step #1414] loss: 3.2507778292409943\n",
      "[EPOCH #2, step #1416] loss: 3.2509186294773724\n",
      "[EPOCH #2, step #1418] loss: 3.2505739000668568\n",
      "[EPOCH #2, step #1420] loss: 3.250632090941019\n",
      "[EPOCH #2, step #1422] loss: 3.2507143612068803\n",
      "[EPOCH #2, step #1424] loss: 3.2507612338819003\n",
      "[EPOCH #2, step #1426] loss: 3.2506944707163417\n",
      "[EPOCH #2, step #1428] loss: 3.250841226838034\n",
      "[EPOCH #2, step #1430] loss: 3.250266532144873\n",
      "[EPOCH #2, step #1432] loss: 3.2504770060310975\n",
      "[EPOCH #2, step #1434] loss: 3.2508606618289746\n",
      "[EPOCH #2, step #1436] loss: 3.251050434099276\n",
      "[EPOCH #2, step #1438] loss: 3.2505381332659904\n",
      "[EPOCH #2, step #1440] loss: 3.2503693021063502\n",
      "[EPOCH #2, step #1442] loss: 3.250324870675947\n",
      "[EPOCH #2, step #1444] loss: 3.2496545669529264\n",
      "[EPOCH #2, step #1446] loss: 3.2504187742429678\n",
      "[EPOCH #2, step #1448] loss: 3.2508256126058934\n",
      "[EPOCH #2, step #1450] loss: 3.2507486282587874\n",
      "[EPOCH #2, step #1452] loss: 3.250223298607739\n",
      "[EPOCH #2, step #1454] loss: 3.2499714215596516\n",
      "[EPOCH #2, step #1456] loss: 3.2496130491006676\n",
      "[EPOCH #2, step #1458] loss: 3.24915256869556\n",
      "[EPOCH #2, step #1460] loss: 3.24902319206431\n",
      "[EPOCH #2, step #1462] loss: 3.249109244232543\n",
      "[EPOCH #2, step #1464] loss: 3.2491455297828127\n",
      "[EPOCH #2, step #1466] loss: 3.2495124983641266\n",
      "[EPOCH #2, step #1468] loss: 3.2492774459599962\n",
      "[EPOCH #2, step #1470] loss: 3.2490243149970683\n",
      "[EPOCH #2, step #1472] loss: 3.2491161054441577\n",
      "[EPOCH #2, step #1474] loss: 3.2497540798833815\n",
      "[EPOCH #2, step #1476] loss: 3.250032360681264\n",
      "[EPOCH #2, step #1478] loss: 3.250203770346703\n",
      "[EPOCH #2, step #1480] loss: 3.249835492146329\n",
      "[EPOCH #2, step #1482] loss: 3.2496509270658382\n",
      "[EPOCH #2, step #1484] loss: 3.249569290334528\n",
      "[EPOCH #2, step #1486] loss: 3.2497532697997\n",
      "[EPOCH #2, step #1488] loss: 3.249832577317893\n",
      "[EPOCH #2, step #1490] loss: 3.2494429848003836\n",
      "[EPOCH #2, step #1492] loss: 3.249801379278532\n",
      "[EPOCH #2, step #1494] loss: 3.249475723445216\n",
      "[EPOCH #2, step #1496] loss: 3.2496228957064726\n",
      "[EPOCH #2, step #1498] loss: 3.2492940786920284\n",
      "[EPOCH #2, step #1500] loss: 3.249360537386036\n",
      "[EPOCH #2, step #1502] loss: 3.2489653180935187\n",
      "[EPOCH #2, step #1504] loss: 3.24894695582976\n",
      "[EPOCH #2, step #1506] loss: 3.248693411446757\n",
      "[EPOCH #2, step #1508] loss: 3.2488923184993332\n",
      "[EPOCH #2, step #1510] loss: 3.2487572814201693\n",
      "[EPOCH #2, step #1512] loss: 3.2478362913566636\n",
      "[EPOCH #2, step #1514] loss: 3.247404900793195\n",
      "[EPOCH #2, step #1516] loss: 3.2469365764937277\n",
      "[EPOCH #2, step #1518] loss: 3.2470849227403007\n",
      "[EPOCH #2, step #1520] loss: 3.2473737682503043\n",
      "[EPOCH #2, step #1522] loss: 3.2471964608190564\n",
      "[EPOCH #2, step #1524] loss: 3.2477110826773723\n",
      "[EPOCH #2, step #1526] loss: 3.2478549241551247\n",
      "[EPOCH #2, step #1528] loss: 3.2480736503887986\n",
      "[EPOCH #2, step #1530] loss: 3.247555344965939\n",
      "[EPOCH #2, step #1532] loss: 3.2476958258044695\n",
      "[EPOCH #2, step #1534] loss: 3.247653697135006\n",
      "[EPOCH #2, step #1536] loss: 3.247947612984582\n",
      "[EPOCH #2, step #1538] loss: 3.247789384328831\n",
      "[EPOCH #2, step #1540] loss: 3.2477026022849804\n",
      "[EPOCH #2, step #1542] loss: 3.247199306685818\n",
      "[EPOCH #2, step #1544] loss: 3.247521379773285\n",
      "[EPOCH #2, step #1546] loss: 3.2472631100615765\n",
      "[EPOCH #2, step #1548] loss: 3.247208632523972\n",
      "[EPOCH #2, step #1550] loss: 3.2474965774036852\n",
      "[EPOCH #2, step #1552] loss: 3.247384668547955\n",
      "[EPOCH #2, step #1554] loss: 3.24802387213017\n",
      "[EPOCH #2, step #1556] loss: 3.2483533056561735\n",
      "[EPOCH #2, step #1558] loss: 3.2481057890108413\n",
      "[EPOCH #2, step #1560] loss: 3.2477909620567287\n",
      "[EPOCH #2, step #1562] loss: 3.2477461236147107\n",
      "[EPOCH #2, step #1564] loss: 3.247241520348448\n",
      "[EPOCH #2, step #1566] loss: 3.2474360549685755\n",
      "[EPOCH #2, step #1568] loss: 3.247526508444354\n",
      "[EPOCH #2, step #1570] loss: 3.247495270473497\n",
      "[EPOCH #2, step #1572] loss: 3.2474923203571997\n",
      "[EPOCH #2, step #1574] loss: 3.2469518201313323\n",
      "[EPOCH #2, step #1576] loss: 3.2468194573002673\n",
      "[EPOCH #2, step #1578] loss: 3.2469886123869824\n",
      "[EPOCH #2, step #1580] loss: 3.2463663007397203\n",
      "[EPOCH #2, step #1582] loss: 3.246116981879865\n",
      "[EPOCH #2, step #1584] loss: 3.245898133497509\n",
      "[EPOCH #2, step #1586] loss: 3.245738593001146\n",
      "[EPOCH #2, step #1588] loss: 3.245442509126033\n",
      "[EPOCH #2, step #1590] loss: 3.245547076278629\n",
      "[EPOCH #2, step #1592] loss: 3.24549677606955\n",
      "[EPOCH #2, step #1594] loss: 3.246007912136544\n",
      "[EPOCH #2, step #1596] loss: 3.2461801475484293\n",
      "[EPOCH #2, step #1598] loss: 3.2458884687704024\n",
      "[EPOCH #2, step #1600] loss: 3.2455237416607523\n",
      "[EPOCH #2, step #1602] loss: 3.2455275832156576\n",
      "[EPOCH #2, step #1604] loss: 3.245328139367505\n",
      "[EPOCH #2, step #1606] loss: 3.245068573106055\n",
      "[EPOCH #2, step #1608] loss: 3.245094270569853\n",
      "[EPOCH #2, step #1610] loss: 3.245326954411247\n",
      "[EPOCH #2, step #1612] loss: 3.2451176777879276\n",
      "[EPOCH #2, step #1614] loss: 3.244987055834602\n",
      "[EPOCH #2, step #1616] loss: 3.2449193625960295\n",
      "[EPOCH #2, step #1618] loss: 3.244654525128907\n",
      "[EPOCH #2, step #1620] loss: 3.24466323970204\n",
      "[EPOCH #2, step #1622] loss: 3.2446758685402863\n",
      "[EPOCH #2, step #1624] loss: 3.2446291172321025\n",
      "[EPOCH #2, step #1626] loss: 3.2443283300276686\n",
      "[EPOCH #2, step #1628] loss: 3.2443084595319025\n",
      "[EPOCH #2, step #1630] loss: 3.2442552137053284\n",
      "[EPOCH #2, step #1632] loss: 3.24439913819191\n",
      "[EPOCH #2, step #1634] loss: 3.24452966170938\n",
      "[EPOCH #2, step #1636] loss: 3.244570181729521\n",
      "[EPOCH #2, step #1638] loss: 3.2446902365565227\n",
      "[EPOCH #2, step #1640] loss: 3.2449259832092787\n",
      "[EPOCH #2, step #1642] loss: 3.2448922573468053\n",
      "[EPOCH #2, step #1644] loss: 3.2449286649292364\n",
      "[EPOCH #2, step #1646] loss: 3.2447544523782272\n",
      "[EPOCH #2, step #1648] loss: 3.244996444899939\n",
      "[EPOCH #2, step #1650] loss: 3.2449687747504767\n",
      "[EPOCH #2, step #1652] loss: 3.2447665590123558\n",
      "[EPOCH #2, step #1654] loss: 3.244922677123655\n",
      "[EPOCH #2, step #1656] loss: 3.2450479143127735\n",
      "[EPOCH #2, step #1658] loss: 3.2452275056016955\n",
      "[EPOCH #2, step #1660] loss: 3.2454836529324673\n",
      "[EPOCH #2, step #1662] loss: 3.2457740448787136\n",
      "[EPOCH #2, step #1664] loss: 3.246036934781003\n",
      "[EPOCH #2, step #1666] loss: 3.245726529609392\n",
      "[EPOCH #2, step #1668] loss: 3.2454740885419713\n",
      "[EPOCH #2, step #1670] loss: 3.245041840808396\n",
      "[EPOCH #2, step #1672] loss: 3.2448226707251\n",
      "[EPOCH #2, step #1674] loss: 3.2441964575070052\n",
      "[EPOCH #2, step #1676] loss: 3.243566427475367\n",
      "[EPOCH #2, step #1678] loss: 3.243672888501902\n",
      "[EPOCH #2, step #1680] loss: 3.2434332058728983\n",
      "[EPOCH #2, step #1682] loss: 3.2438261476359194\n",
      "[EPOCH #2, step #1684] loss: 3.2434344761449436\n",
      "[EPOCH #2, step #1686] loss: 3.2437645173595726\n",
      "[EPOCH #2, step #1688] loss: 3.2434021540122884\n",
      "[EPOCH #2, step #1690] loss: 3.2434031021517455\n",
      "[EPOCH #2, step #1692] loss: 3.2433068365017057\n",
      "[EPOCH #2, step #1694] loss: 3.2435325521283445\n",
      "[EPOCH #2, step #1696] loss: 3.243372874324576\n",
      "[EPOCH #2, step #1698] loss: 3.2431009537055253\n",
      "[EPOCH #2, step #1700] loss: 3.2426373489039007\n",
      "[EPOCH #2, step #1702] loss: 3.242525800563837\n",
      "[EPOCH #2, step #1704] loss: 3.243043219798471\n",
      "[EPOCH #2, step #1706] loss: 3.24305711186414\n",
      "[EPOCH #2, step #1708] loss: 3.2430308999340043\n",
      "[EPOCH #2, step #1710] loss: 3.243064335812469\n",
      "[EPOCH #2, step #1712] loss: 3.2426957247623283\n",
      "[EPOCH #2, step #1714] loss: 3.2427925849447443\n",
      "[EPOCH #2, step #1716] loss: 3.2426656632487276\n",
      "[EPOCH #2, step #1718] loss: 3.2426417901669913\n",
      "[EPOCH #2, step #1720] loss: 3.2419903515245525\n",
      "[EPOCH #2, step #1722] loss: 3.2415592646917077\n",
      "[EPOCH #2, step #1724] loss: 3.241085881357608\n",
      "[EPOCH #2, step #1726] loss: 3.2409609717030574\n",
      "[EPOCH #2, step #1728] loss: 3.2407999209684744\n",
      "[EPOCH #2, step #1730] loss: 3.2407594050788107\n",
      "[EPOCH #2, step #1732] loss: 3.2403133742326404\n",
      "[EPOCH #2, step #1734] loss: 3.239953987055622\n",
      "[EPOCH #2, step #1736] loss: 3.240438570561736\n",
      "[EPOCH #2, step #1738] loss: 3.2403298733630628\n",
      "[EPOCH #2, step #1740] loss: 3.2400241885494734\n",
      "[EPOCH #2, step #1742] loss: 3.2398725982650696\n",
      "[EPOCH #2, step #1744] loss: 3.23993747528098\n",
      "[EPOCH #2, step #1746] loss: 3.240441492508668\n",
      "[EPOCH #2, step #1748] loss: 3.240568567371968\n",
      "[EPOCH #2, step #1750] loss: 3.240275197543804\n",
      "[EPOCH #2, step #1752] loss: 3.2400282650624965\n",
      "[EPOCH #2, step #1754] loss: 3.2401692489613154\n",
      "[EPOCH #2, step #1756] loss: 3.2399420569956674\n",
      "[EPOCH #2, step #1758] loss: 3.2398283390784686\n",
      "[EPOCH #2, step #1760] loss: 3.239892155253569\n",
      "[EPOCH #2, step #1762] loss: 3.2399296269657527\n",
      "[EPOCH #2, step #1764] loss: 3.2399225360611004\n",
      "[EPOCH #2, step #1766] loss: 3.24005401788894\n",
      "[EPOCH #2, step #1768] loss: 3.2397694111948567\n",
      "[EPOCH #2, step #1770] loss: 3.240113773162859\n",
      "[EPOCH #2, step #1772] loss: 3.240011341613494\n",
      "[EPOCH #2, step #1774] loss: 3.239974404724551\n",
      "[EPOCH #2, step #1776] loss: 3.2398801606628553\n",
      "[EPOCH #2, step #1778] loss: 3.240015548022025\n",
      "[EPOCH #2, step #1780] loss: 3.2401713834994967\n",
      "[EPOCH #2, step #1782] loss: 3.240233713097607\n",
      "[EPOCH #2, step #1784] loss: 3.2395916854634006\n",
      "[EPOCH #2, step #1786] loss: 3.2390498929538736\n",
      "[EPOCH #2, step #1788] loss: 3.238883078198249\n",
      "[EPOCH #2, step #1790] loss: 3.2387087506198937\n",
      "[EPOCH #2, step #1792] loss: 3.2384521160721444\n",
      "[EPOCH #2, step #1794] loss: 3.238413361121685\n",
      "[EPOCH #2, step #1796] loss: 3.2380998677257438\n",
      "[EPOCH #2, step #1798] loss: 3.2380515757768005\n",
      "[EPOCH #2, step #1800] loss: 3.2380424966287906\n",
      "[EPOCH #2, step #1802] loss: 3.23813059339243\n",
      "[EPOCH #2, step #1804] loss: 3.2381781608444173\n",
      "[EPOCH #2, step #1806] loss: 3.2384679741007094\n",
      "[EPOCH #2, step #1808] loss: 3.2378952204027907\n",
      "[EPOCH #2, step #1810] loss: 3.2377022456754183\n",
      "[EPOCH #2, step #1812] loss: 3.23789094668755\n",
      "[EPOCH #2, step #1814] loss: 3.2381997907128874\n",
      "[EPOCH #2, step #1816] loss: 3.238001937714001\n",
      "[EPOCH #2, step #1818] loss: 3.2377207135026964\n",
      "[EPOCH #2, step #1820] loss: 3.237577860464047\n",
      "[EPOCH #2, step #1822] loss: 3.2376115918486446\n",
      "[EPOCH #2, step #1824] loss: 3.237544208030178\n",
      "[EPOCH #2, step #1826] loss: 3.237948703322178\n",
      "[EPOCH #2, step #1828] loss: 3.2379915611283776\n",
      "[EPOCH #2, step #1830] loss: 3.2378028798533034\n",
      "[EPOCH #2, step #1832] loss: 3.2377659271664783\n",
      "[EPOCH #2, step #1834] loss: 3.237675460482813\n",
      "[EPOCH #2, step #1836] loss: 3.2378082449305103\n",
      "[EPOCH #2, step #1838] loss: 3.237765963924132\n",
      "[EPOCH #2, step #1840] loss: 3.2375320975123896\n",
      "[EPOCH #2, step #1842] loss: 3.2374368764367105\n",
      "[EPOCH #2, step #1844] loss: 3.2374138098099046\n",
      "[EPOCH #2, step #1846] loss: 3.237078096651037\n",
      "[EPOCH #2, step #1848] loss: 3.2370005185182444\n",
      "[EPOCH #2, step #1850] loss: 3.236832904918718\n",
      "[EPOCH #2, step #1852] loss: 3.236732533861995\n",
      "[EPOCH #2, step #1854] loss: 3.236721892241198\n",
      "[EPOCH #2, step #1856] loss: 3.237140023714763\n",
      "[EPOCH #2, step #1858] loss: 3.2374420464840417\n",
      "[EPOCH #2, step #1860] loss: 3.237859592970695\n",
      "[EPOCH #2, step #1862] loss: 3.2382695636503556\n",
      "[EPOCH #2, step #1864] loss: 3.2379348715252916\n",
      "[EPOCH #2, step #1866] loss: 3.2374436790188432\n",
      "[EPOCH #2, step #1868] loss: 3.237097516169581\n",
      "[EPOCH #2, step #1870] loss: 3.236809614442492\n",
      "[EPOCH #2, step #1872] loss: 3.2366970042398253\n",
      "[EPOCH #2, step #1874] loss: 3.236414750289917\n",
      "[EPOCH #2, step #1876] loss: 3.2365979166442433\n",
      "[EPOCH #2, step #1878] loss: 3.2364348884336867\n",
      "[EPOCH #2, step #1880] loss: 3.2363873523071307\n",
      "[EPOCH #2, step #1882] loss: 3.236215975584657\n",
      "[EPOCH #2, step #1884] loss: 3.236480966613527\n",
      "[EPOCH #2, step #1886] loss: 3.236735116759494\n",
      "[EPOCH #2, step #1888] loss: 3.236715982299813\n",
      "[EPOCH #2, step #1890] loss: 3.236584995739804\n",
      "[EPOCH #2, step #1892] loss: 3.2364667351133787\n",
      "[EPOCH #2, step #1894] loss: 3.2356993457572756\n",
      "[EPOCH #2, step #1896] loss: 3.2356446364960294\n",
      "[EPOCH #2, step #1898] loss: 3.2358303906228305\n",
      "[EPOCH #2, step #1900] loss: 3.235536424691021\n",
      "[EPOCH #2, step #1902] loss: 3.235775873276414\n",
      "[EPOCH #2, step #1904] loss: 3.2358494947588663\n",
      "[EPOCH #2, step #1906] loss: 3.2361541085675802\n",
      "[EPOCH #2, step #1908] loss: 3.236231967877068\n",
      "[EPOCH #2, step #1910] loss: 3.2361675103505454\n",
      "[EPOCH #2, step #1912] loss: 3.2362141703911247\n",
      "[EPOCH #2, step #1914] loss: 3.2358314936217067\n",
      "[EPOCH #2, step #1916] loss: 3.2359101231295924\n",
      "[EPOCH #2, step #1918] loss: 3.2359528798743917\n",
      "[EPOCH #2, step #1920] loss: 3.235767102874486\n",
      "[EPOCH #2, step #1922] loss: 3.235163295014848\n",
      "[EPOCH #2, step #1924] loss: 3.2355414154003195\n",
      "[EPOCH #2, step #1926] loss: 3.234917786681844\n",
      "[EPOCH #2, step #1928] loss: 3.234540795093376\n",
      "[EPOCH #2, step #1930] loss: 3.2342687735960065\n",
      "[EPOCH #2, step #1932] loss: 3.2341154991994543\n",
      "[EPOCH #2, step #1934] loss: 3.2345119950691243\n",
      "[EPOCH #2, step #1936] loss: 3.2340779133414137\n",
      "[EPOCH #2, step #1938] loss: 3.2338956568030857\n",
      "[EPOCH #2, step #1940] loss: 3.233986929849765\n",
      "[EPOCH #2, step #1942] loss: 3.2335694720925465\n",
      "[EPOCH #2, step #1944] loss: 3.233425676792017\n",
      "[EPOCH #2, step #1946] loss: 3.2335714708552583\n",
      "[EPOCH #2, step #1948] loss: 3.2335924582337037\n",
      "[EPOCH #2, step #1950] loss: 3.2335154779259208\n",
      "[EPOCH #2, step #1952] loss: 3.2331691490218266\n",
      "[EPOCH #2, step #1954] loss: 3.2327622957546693\n",
      "[EPOCH #2, step #1956] loss: 3.232664815125931\n",
      "[EPOCH #2, step #1958] loss: 3.232229401713552\n",
      "[EPOCH #2, step #1960] loss: 3.2320526470522806\n",
      "[EPOCH #2, step #1962] loss: 3.2318819872408695\n",
      "[EPOCH #2, step #1964] loss: 3.2317834429461842\n",
      "[EPOCH #2, step #1966] loss: 3.2318438846081965\n",
      "[EPOCH #2, step #1968] loss: 3.2317041096316794\n",
      "[EPOCH #2, step #1970] loss: 3.231676879841201\n",
      "[EPOCH #2, step #1972] loss: 3.2317233747839023\n",
      "[EPOCH #2, step #1974] loss: 3.231667708867713\n",
      "[EPOCH #2, step #1976] loss: 3.2314363452118338\n",
      "[EPOCH #2, step #1978] loss: 3.2315386471692933\n",
      "[EPOCH #2, step #1980] loss: 3.23131418757701\n",
      "[EPOCH #2, step #1982] loss: 3.231172700272145\n",
      "[EPOCH #2, step #1984] loss: 3.230926809503089\n",
      "[EPOCH #2, step #1986] loss: 3.231037771755808\n",
      "[EPOCH #2, step #1988] loss: 3.230749466959827\n",
      "[EPOCH #2, step #1990] loss: 3.2302766890456365\n",
      "[EPOCH #2, step #1992] loss: 3.230022428984386\n",
      "[EPOCH #2, step #1994] loss: 3.2299539573210523\n",
      "[EPOCH #2, step #1996] loss: 3.2297903547062536\n",
      "[EPOCH #2, step #1998] loss: 3.2299152796956645\n",
      "[EPOCH #2, step #2000] loss: 3.2300142731921544\n",
      "[EPOCH #2, step #2002] loss: 3.230231587433303\n",
      "[EPOCH #2, step #2004] loss: 3.230401915505045\n",
      "[EPOCH #2, step #2006] loss: 3.2307305771970727\n",
      "[EPOCH #2, step #2008] loss: 3.2307997820328693\n",
      "[EPOCH #2, step #2010] loss: 3.2310936358483615\n",
      "[EPOCH #2, step #2012] loss: 3.2310515444964483\n",
      "[EPOCH #2, step #2014] loss: 3.230942480617362\n",
      "[EPOCH #2, step #2016] loss: 3.231185536840852\n",
      "[EPOCH #2, step #2018] loss: 3.2310361380385784\n",
      "[EPOCH #2, step #2020] loss: 3.231071286836159\n",
      "[EPOCH #2, step #2022] loss: 3.231000331505301\n",
      "[EPOCH #2, step #2024] loss: 3.2313745690569466\n",
      "[EPOCH #2, step #2026] loss: 3.231030074845527\n",
      "[EPOCH #2, step #2028] loss: 3.2310482701272902\n",
      "[EPOCH #2, step #2030] loss: 3.2306951627773586\n",
      "[EPOCH #2, step #2032] loss: 3.2305759926236326\n",
      "[EPOCH #2, step #2034] loss: 3.2306356821364792\n",
      "[EPOCH #2, step #2036] loss: 3.230984327660862\n",
      "[EPOCH #2, step #2038] loss: 3.2307179196317035\n",
      "[EPOCH #2, step #2040] loss: 3.2304417416489866\n",
      "[EPOCH #2, step #2042] loss: 3.2305752401309915\n",
      "[EPOCH #2, step #2044] loss: 3.2302406755228206\n",
      "[EPOCH #2, step #2046] loss: 3.230369548187293\n",
      "[EPOCH #2, step #2048] loss: 3.23022040243321\n",
      "[EPOCH #2, step #2050] loss: 3.2301765079907474\n",
      "[EPOCH #2, step #2052] loss: 3.230165614306027\n",
      "[EPOCH #2, step #2054] loss: 3.230127747273503\n",
      "[EPOCH #2, step #2056] loss: 3.2303276684426776\n",
      "[EPOCH #2, step #2058] loss: 3.2303513832147868\n",
      "[EPOCH #2, step #2060] loss: 3.2305459036866417\n",
      "[EPOCH #2, step #2062] loss: 3.2306557563803926\n",
      "[EPOCH #2, step #2064] loss: 3.2305980781666013\n",
      "[EPOCH #2, step #2066] loss: 3.230471174671965\n",
      "[EPOCH #2, step #2068] loss: 3.2303591535078726\n",
      "[EPOCH #2, step #2070] loss: 3.2300399428459254\n",
      "[EPOCH #2, step #2072] loss: 3.230299208857734\n",
      "[EPOCH #2, step #2074] loss: 3.230121124451419\n",
      "[EPOCH #2, step #2076] loss: 3.230250759427801\n",
      "[EPOCH #2, step #2078] loss: 3.2302163373168242\n",
      "[EPOCH #2, step #2080] loss: 3.2303308328145515\n",
      "[EPOCH #2, step #2082] loss: 3.2301313592102225\n",
      "[EPOCH #2, step #2084] loss: 3.2303103808304674\n",
      "[EPOCH #2, step #2086] loss: 3.2301105849755456\n",
      "[EPOCH #2, step #2088] loss: 3.2300227633385408\n",
      "[EPOCH #2, step #2090] loss: 3.229770613081543\n",
      "[EPOCH #2, step #2092] loss: 3.229840154688836\n",
      "[EPOCH #2, step #2094] loss: 3.229813300567481\n",
      "[EPOCH #2, step #2096] loss: 3.2299851363877425\n",
      "[EPOCH #2, step #2098] loss: 3.2300419253130763\n",
      "[EPOCH #2, step #2100] loss: 3.2297056548542096\n",
      "[EPOCH #2, step #2102] loss: 3.2294223080688127\n",
      "[EPOCH #2, step #2104] loss: 3.229233760788435\n",
      "[EPOCH #2, step #2106] loss: 3.229190306353467\n",
      "[EPOCH #2, step #2108] loss: 3.229150616427404\n",
      "[EPOCH #2, step #2110] loss: 3.2289534000241322\n",
      "[EPOCH #2, step #2112] loss: 3.2288510257803993\n",
      "[EPOCH #2, step #2114] loss: 3.2287828198561432\n",
      "[EPOCH #2, step #2116] loss: 3.2286789599712806\n",
      "[EPOCH #2, step #2118] loss: 3.228693737067834\n",
      "[EPOCH #2, step #2120] loss: 3.2288077878030275\n",
      "[EPOCH #2, step #2122] loss: 3.2286931207255267\n",
      "[EPOCH #2, step #2124] loss: 3.2287747178919175\n",
      "[EPOCH #2, step #2126] loss: 3.2285860929906116\n",
      "[EPOCH #2, step #2128] loss: 3.2286193142367283\n",
      "[EPOCH #2, step #2130] loss: 3.228322761711074\n",
      "[EPOCH #2, step #2132] loss: 3.2277667674446016\n",
      "[EPOCH #2, step #2134] loss: 3.2278376025394198\n",
      "[EPOCH #2, step #2136] loss: 3.2277309727211336\n",
      "[EPOCH #2, step #2138] loss: 3.2275340981771032\n",
      "[EPOCH #2, step #2140] loss: 3.227731403932567\n",
      "[EPOCH #2, step #2142] loss: 3.2275552360242736\n",
      "[EPOCH #2, step #2144] loss: 3.2273975758007913\n",
      "[EPOCH #2, step #2146] loss: 3.2277574174949386\n",
      "[EPOCH #2, step #2148] loss: 3.227562926768591\n",
      "[EPOCH #2, step #2150] loss: 3.227202535063541\n",
      "[EPOCH #2, step #2152] loss: 3.2268589429948364\n",
      "[EPOCH #2, step #2154] loss: 3.226695390975669\n",
      "[EPOCH #2, step #2156] loss: 3.226927741346505\n",
      "[EPOCH #2, step #2158] loss: 3.2271128709032446\n",
      "[EPOCH #2, step #2160] loss: 3.2268055345878617\n",
      "[EPOCH #2, step #2162] loss: 3.227011017594357\n",
      "[EPOCH #2, step #2164] loss: 3.2271679196566962\n",
      "[EPOCH #2, step #2166] loss: 3.227163028871072\n",
      "[EPOCH #2, step #2168] loss: 3.22682796320425\n",
      "[EPOCH #2, step #2170] loss: 3.2265890669240647\n",
      "[EPOCH #2, step #2172] loss: 3.226469882556057\n",
      "[EPOCH #2, step #2174] loss: 3.226722889823475\n",
      "[EPOCH #2, step #2176] loss: 3.2268607379867773\n",
      "[EPOCH #2, step #2178] loss: 3.2265450146943397\n",
      "[EPOCH #2, step #2180] loss: 3.226314025972481\n",
      "[EPOCH #2, step #2182] loss: 3.2263145593126814\n",
      "[EPOCH #2, step #2184] loss: 3.226103547726943\n",
      "[EPOCH #2, step #2186] loss: 3.2258751860044295\n",
      "[EPOCH #2, step #2188] loss: 3.2259036895617768\n",
      "[EPOCH #2, step #2190] loss: 3.2259888024375845\n",
      "[EPOCH #2, step #2192] loss: 3.225829461911363\n",
      "[EPOCH #2, step #2194] loss: 3.225865316716848\n",
      "[EPOCH #2, step #2196] loss: 3.2259445600635526\n",
      "[EPOCH #2, step #2198] loss: 3.225790610254868\n",
      "[EPOCH #2, step #2200] loss: 3.2258624935193474\n",
      "[EPOCH #2, step #2202] loss: 3.2254072010544177\n",
      "[EPOCH #2, step #2204] loss: 3.2257867887717526\n",
      "[EPOCH #2, step #2206] loss: 3.2256896524986747\n",
      "[EPOCH #2, step #2208] loss: 3.225664571919232\n",
      "[EPOCH #2, step #2210] loss: 3.2257834081464307\n",
      "[EPOCH #2, step #2212] loss: 3.225697490245683\n",
      "[EPOCH #2, step #2214] loss: 3.2253918858736954\n",
      "[EPOCH #2, step #2216] loss: 3.225183920329038\n",
      "[EPOCH #2, step #2218] loss: 3.2250455815064685\n",
      "[EPOCH #2, step #2220] loss: 3.2253017318189707\n",
      "[EPOCH #2, step #2222] loss: 3.2257781351572534\n",
      "[EPOCH #2, step #2224] loss: 3.2255944173791433\n",
      "[EPOCH #2, step #2226] loss: 3.225543535178587\n",
      "[EPOCH #2, step #2228] loss: 3.2254139231480305\n",
      "[EPOCH #2, step #2230] loss: 3.225772399423583\n",
      "[EPOCH #2, step #2232] loss: 3.2254714055067746\n",
      "[EPOCH #2, step #2234] loss: 3.2254622075381696\n",
      "[EPOCH #2, step #2236] loss: 3.2254626915078637\n",
      "[EPOCH #2, step #2238] loss: 3.225482105090715\n",
      "[EPOCH #2, step #2240] loss: 3.2251755393333386\n",
      "[EPOCH #2, step #2242] loss: 3.225186786638856\n",
      "[EPOCH #2, step #2244] loss: 3.2254615004184783\n",
      "[EPOCH #2, step #2246] loss: 3.225393434816538\n",
      "[EPOCH #2, step #2248] loss: 3.225128586846704\n",
      "[EPOCH #2, step #2250] loss: 3.2251117244290013\n",
      "[EPOCH #2, step #2252] loss: 3.225095841496244\n",
      "[EPOCH #2, step #2254] loss: 3.2249941368060737\n",
      "[EPOCH #2, step #2256] loss: 3.224742693556691\n",
      "[EPOCH #2, step #2258] loss: 3.224620874501161\n",
      "[EPOCH #2, step #2260] loss: 3.224389168183817\n",
      "[EPOCH #2, step #2262] loss: 3.224259086272593\n",
      "[EPOCH #2, step #2264] loss: 3.2240983593542842\n",
      "[EPOCH #2, step #2266] loss: 3.2240097187245844\n",
      "[EPOCH #2, step #2268] loss: 3.2244131504149625\n",
      "[EPOCH #2, step #2270] loss: 3.2242643621092797\n",
      "[EPOCH #2, step #2272] loss: 3.2243884566383008\n",
      "[EPOCH #2, step #2274] loss: 3.224294382766053\n",
      "[EPOCH #2, step #2276] loss: 3.2241106664710366\n",
      "[EPOCH #2, step #2278] loss: 3.2239862800219856\n",
      "[EPOCH #2, step #2280] loss: 3.223985041405537\n",
      "[EPOCH #2, step #2282] loss: 3.2239729195794045\n",
      "[EPOCH #2, step #2284] loss: 3.2243069754126816\n",
      "[EPOCH #2, step #2286] loss: 3.2244846664457065\n",
      "[EPOCH #2, step #2288] loss: 3.224445768296328\n",
      "[EPOCH #2, step #2290] loss: 3.2243136701912714\n",
      "[EPOCH #2, step #2292] loss: 3.223970006971031\n",
      "[EPOCH #2, step #2294] loss: 3.2242269519107793\n",
      "[EPOCH #2, step #2296] loss: 3.2244846532279428\n",
      "[EPOCH #2, step #2298] loss: 3.224796638015873\n",
      "[EPOCH #2, step #2300] loss: 3.2248345884433367\n",
      "[EPOCH #2, step #2302] loss: 3.224799566769983\n",
      "[EPOCH #2, step #2304] loss: 3.224565619553506\n",
      "[EPOCH #2, step #2306] loss: 3.2249475480366123\n",
      "[EPOCH #2, step #2308] loss: 3.224820480454066\n",
      "[EPOCH #2, step #2310] loss: 3.2248177151822364\n",
      "[EPOCH #2, step #2312] loss: 3.2249333751289244\n",
      "[EPOCH #2, step #2314] loss: 3.224669743717103\n",
      "[EPOCH #2, step #2316] loss: 3.224382433646395\n",
      "[EPOCH #2, step #2318] loss: 3.2242355552301576\n",
      "[EPOCH #2, step #2320] loss: 3.224275966904791\n",
      "[EPOCH #2, step #2322] loss: 3.2241217713390915\n",
      "[EPOCH #2, step #2324] loss: 3.2240915468687654\n",
      "[EPOCH #2, step #2326] loss: 3.224371686225599\n",
      "[EPOCH #2, step #2328] loss: 3.2244748267660635\n",
      "[EPOCH #2, step #2330] loss: 3.2245962437720115\n",
      "[EPOCH #2, step #2332] loss: 3.224306771339289\n",
      "[EPOCH #2, step #2334] loss: 3.224133914236849\n",
      "[EPOCH #2, step #2336] loss: 3.2242756290827543\n",
      "[EPOCH #2, step #2338] loss: 3.2244131007526815\n",
      "[EPOCH #2, step #2340] loss: 3.224464203444672\n",
      "[EPOCH #2, step #2342] loss: 3.224370234754418\n",
      "[EPOCH #2, step #2344] loss: 3.224114811598365\n",
      "[EPOCH #2, step #2346] loss: 3.223993132253581\n",
      "[EPOCH #2, step #2348] loss: 3.223743554732809\n",
      "[EPOCH #2, step #2350] loss: 3.2235458008738997\n",
      "[EPOCH #2, step #2352] loss: 3.2235952254512994\n",
      "[EPOCH #2, step #2354] loss: 3.2235438064405115\n",
      "[EPOCH #2, step #2356] loss: 3.2233542891240594\n",
      "[EPOCH #2, step #2358] loss: 3.2232551233098383\n",
      "[EPOCH #2, step #2360] loss: 3.2229861229569767\n",
      "[EPOCH #2, step #2362] loss: 3.2227553800662796\n",
      "[EPOCH #2, step #2364] loss: 3.2228290796783963\n",
      "[EPOCH #2, step #2366] loss: 3.2227288190956904\n",
      "[EPOCH #2, step #2368] loss: 3.2228098550694257\n",
      "[EPOCH #2, step #2370] loss: 3.22292083288134\n",
      "[EPOCH #2, step #2372] loss: 3.222802277612023\n",
      "[EPOCH #2, step #2374] loss: 3.2228023309205707\n",
      "[EPOCH #2, step #2376] loss: 3.222716150135366\n",
      "[EPOCH #2, step #2378] loss: 3.2228175894259\n",
      "[EPOCH #2, step #2380] loss: 3.222800653654705\n",
      "[EPOCH #2, step #2382] loss: 3.222791804719061\n",
      "[EPOCH #2, step #2384] loss: 3.2228519035835186\n",
      "[EPOCH #2, step #2386] loss: 3.2231450356546185\n",
      "[EPOCH #2, step #2388] loss: 3.2228553966779296\n",
      "[EPOCH #2, step #2390] loss: 3.222749711279588\n",
      "[EPOCH #2, step #2392] loss: 3.2225020915157767\n",
      "[EPOCH #2, step #2394] loss: 3.222357745957026\n",
      "[EPOCH #2, step #2396] loss: 3.2221066052584435\n",
      "[EPOCH #2, step #2398] loss: 3.2221547518734535\n",
      "[EPOCH #2, step #2400] loss: 3.221917495088049\n",
      "[EPOCH #2, step #2402] loss: 3.222060906901937\n",
      "[EPOCH #2, step #2404] loss: 3.222192020574875\n",
      "[EPOCH #2, step #2406] loss: 3.222507712154602\n",
      "[EPOCH #2, step #2408] loss: 3.222727500239365\n",
      "[EPOCH #2, step #2410] loss: 3.2228787204429157\n",
      "[EPOCH #2, step #2412] loss: 3.2231005969207613\n",
      "[EPOCH #2, step #2414] loss: 3.222992780736761\n",
      "[EPOCH #2, step #2416] loss: 3.222930373561545\n",
      "[EPOCH #2, step #2418] loss: 3.2232113535022777\n",
      "[EPOCH #2, step #2420] loss: 3.223359754882239\n",
      "[EPOCH #2, step #2422] loss: 3.223185874586306\n",
      "[EPOCH #2, step #2424] loss: 3.2233104151303005\n",
      "[EPOCH #2, step #2426] loss: 3.223294485533832\n",
      "[EPOCH #2, step #2428] loss: 3.223163116905806\n",
      "[EPOCH #2, step #2430] loss: 3.2231744570479086\n",
      "[EPOCH #2, step #2432] loss: 3.223417879277867\n",
      "[EPOCH #2, step #2434] loss: 3.2232505418436728\n",
      "[EPOCH #2, step #2436] loss: 3.223159292950511\n",
      "[EPOCH #2, step #2438] loss: 3.223348326935246\n",
      "[EPOCH #2, step #2440] loss: 3.223474735414723\n",
      "[EPOCH #2, step #2442] loss: 3.223356412270008\n",
      "[EPOCH #2, step #2444] loss: 3.2233355143806444\n",
      "[EPOCH #2, step #2446] loss: 3.2234095198113333\n",
      "[EPOCH #2, step #2448] loss: 3.2235198627447\n",
      "[EPOCH #2, step #2450] loss: 3.223611050360254\n",
      "[EPOCH #2, step #2452] loss: 3.223093061314959\n",
      "[EPOCH #2, step #2454] loss: 3.2227752845787467\n",
      "[EPOCH #2, step #2456] loss: 3.2226933657662094\n",
      "[EPOCH #2, step #2458] loss: 3.2229165028342877\n",
      "[EPOCH #2, step #2460] loss: 3.22299200526489\n",
      "[EPOCH #2, step #2462] loss: 3.2230794354274996\n",
      "[EPOCH #2, step #2464] loss: 3.223144662017513\n",
      "[EPOCH #2, step #2466] loss: 3.2229635046714225\n",
      "[EPOCH #2, step #2468] loss: 3.222846512274976\n",
      "[EPOCH #2, step #2470] loss: 3.2227010221338523\n",
      "[EPOCH #2, step #2472] loss: 3.2228276329487864\n",
      "[EPOCH #2, step #2474] loss: 3.2228231764321373\n",
      "[EPOCH #2, step #2476] loss: 3.222833087283563\n",
      "[EPOCH #2, step #2478] loss: 3.222863805923985\n",
      "[EPOCH #2, step #2480] loss: 3.222691488189113\n",
      "[EPOCH #2, step #2482] loss: 3.222427673316799\n",
      "[EPOCH #2, step #2484] loss: 3.2221513490082034\n",
      "[EPOCH #2, step #2486] loss: 3.2222179310073513\n",
      "[EPOCH #2, step #2488] loss: 3.221988369155093\n",
      "[EPOCH #2, step #2490] loss: 3.2217933346783956\n",
      "[EPOCH #2, step #2492] loss: 3.2215041902904\n",
      "[EPOCH #2, step #2494] loss: 3.2213324989250047\n",
      "[EPOCH #2, step #2496] loss: 3.221237222242222\n",
      "[EPOCH #2, step #2498] loss: 3.221167211963826\n",
      "[EPOCH #2, step #2500] loss: 3.221236330087258\n",
      "[EPOCH #2, step #2502] loss: 3.2210153273187347\n",
      "[EPOCH #2, step #2504] loss: 3.2210247154959184\n",
      "[EPOCH #2, step #2506] loss: 3.221001594153162\n",
      "[EPOCH #2, step #2508] loss: 3.220700572524331\n",
      "[EPOCH #2, step #2510] loss: 3.22105200952479\n",
      "[EPOCH #2, step #2512] loss: 3.2210252072485708\n",
      "[EPOCH #2, step #2514] loss: 3.22118465004574\n",
      "[EPOCH #2, step #2516] loss: 3.221223920216672\n",
      "[EPOCH #2, step #2518] loss: 3.220973746650699\n",
      "[EPOCH #2, step #2520] loss: 3.2211381718545904\n",
      "[EPOCH #2, step #2522] loss: 3.2209450904688945\n",
      "[EPOCH #2, step #2524] loss: 3.220949124440108\n",
      "[EPOCH #2, step #2526] loss: 3.221160926883005\n",
      "[EPOCH #2, step #2528] loss: 3.221221180478779\n",
      "[EPOCH #2, step #2530] loss: 3.221398406234102\n",
      "[EPOCH #2, step #2532] loss: 3.2210929147160856\n",
      "[EPOCH #2, step #2534] loss: 3.2210896062192598\n",
      "[EPOCH #2, step #2536] loss: 3.2210983467891454\n",
      "[EPOCH #2, step #2538] loss: 3.2212503045597805\n",
      "[EPOCH #2, step #2540] loss: 3.221328021972398\n",
      "[EPOCH #2, step #2542] loss: 3.2213881160160054\n",
      "[EPOCH #2, step #2544] loss: 3.22134909086471\n",
      "[EPOCH #2, step #2546] loss: 3.2214902888853314\n",
      "[EPOCH #2, step #2548] loss: 3.2212674516750437\n",
      "[EPOCH #2, step #2550] loss: 3.221098004206168\n",
      "[EPOCH #2, step #2552] loss: 3.221310135994152\n",
      "[EPOCH #2, step #2554] loss: 3.2215941139863196\n",
      "[EPOCH #2, step #2556] loss: 3.2213304723479292\n",
      "[EPOCH #2, step #2558] loss: 3.2214933900732508\n",
      "[EPOCH #2, step #2560] loss: 3.221587589596082\n",
      "[EPOCH #2, step #2562] loss: 3.221534518864394\n",
      "[EPOCH #2, step #2564] loss: 3.2215905307561568\n",
      "[EPOCH #2, step #2566] loss: 3.2213187727614234\n",
      "[EPOCH #2, step #2568] loss: 3.2213358557980643\n",
      "[EPOCH #2, step #2570] loss: 3.221195704924926\n",
      "[EPOCH #2, step #2572] loss: 3.2212429231027664\n",
      "[EPOCH #2, step #2574] loss: 3.2209083096032005\n",
      "[EPOCH #2, step #2576] loss: 3.221009198019659\n",
      "[EPOCH #2, step #2578] loss: 3.220998507869849\n",
      "[EPOCH #2, step #2580] loss: 3.2210933409287734\n",
      "[EPOCH #2, step #2582] loss: 3.220788734715159\n",
      "[EPOCH #2, step #2584] loss: 3.220376037675131\n",
      "[EPOCH #2, step #2586] loss: 3.2202734349084534\n",
      "[EPOCH #2, step #2588] loss: 3.2204025734709516\n",
      "[EPOCH #2, step #2590] loss: 3.2204271607857238\n",
      "[EPOCH #2, step #2592] loss: 3.220386995388374\n",
      "[EPOCH #2, step #2594] loss: 3.2202758652167054\n",
      "[EPOCH #2, step #2596] loss: 3.2199252524833106\n",
      "[EPOCH #2, step #2598] loss: 3.2199935336991796\n",
      "[EPOCH #2, step #2600] loss: 3.2196368137170426\n",
      "[EPOCH #2, step #2602] loss: 3.219785726303968\n",
      "[EPOCH #2, step #2604] loss: 3.2199108307741424\n",
      "[EPOCH #2, step #2606] loss: 3.219758913806905\n",
      "[EPOCH #2, step #2608] loss: 3.2201339202440757\n",
      "[EPOCH #2, step #2610] loss: 3.220024524726707\n",
      "[EPOCH #2, step #2612] loss: 3.2202941802010607\n",
      "[EPOCH #2, step #2614] loss: 3.2200614466949813\n",
      "[EPOCH #2, step #2616] loss: 3.2200023824584636\n",
      "[EPOCH #2, step #2618] loss: 3.22040180624692\n",
      "[EPOCH #2, step #2620] loss: 3.220324165445391\n",
      "[EPOCH #2, step #2622] loss: 3.2203518416288013\n",
      "[EPOCH #2, step #2624] loss: 3.2202232257298062\n",
      "[EPOCH #2, step #2626] loss: 3.2202715232066432\n",
      "[EPOCH #2, step #2628] loss: 3.2202797253442927\n",
      "[EPOCH #2, step #2630] loss: 3.2199510219476832\n",
      "[EPOCH #2, step #2632] loss: 3.2201078704134214\n",
      "[EPOCH #2, step #2634] loss: 3.2201179176851737\n",
      "[EPOCH #2, step #2636] loss: 3.2201440779151094\n",
      "[EPOCH #2, step #2638] loss: 3.2200299740379714\n",
      "[EPOCH #2, step #2640] loss: 3.220185718767483\n",
      "[EPOCH #2, elapsed time: 1114.199[sec]] loss: 3.220185718767483\n",
      "[EPOCH #3, step #0] loss: 2.9488525390625\n",
      "[EPOCH #3, step #2] loss: 3.0564191341400146\n",
      "[EPOCH #3, step #4] loss: 3.1345115184783934\n",
      "[EPOCH #3, step #6] loss: 3.213117701666696\n",
      "[EPOCH #3, step #8] loss: 3.215198967191908\n",
      "[EPOCH #3, step #10] loss: 3.154370437968861\n",
      "[EPOCH #3, step #12] loss: 3.1934009148524356\n",
      "[EPOCH #3, step #14] loss: 3.2354774792989094\n",
      "[EPOCH #3, step #16] loss: 3.2296734978170956\n",
      "[EPOCH #3, step #18] loss: 3.2272714941125167\n",
      "[EPOCH #3, step #20] loss: 3.1946287155151367\n",
      "[EPOCH #3, step #22] loss: 3.177754889363828\n",
      "[EPOCH #3, step #24] loss: 3.158016815185547\n",
      "[EPOCH #3, step #26] loss: 3.1735167415053755\n",
      "[EPOCH #3, step #28] loss: 3.1690225436769683\n",
      "[EPOCH #3, step #30] loss: 3.158237918730705\n",
      "[EPOCH #3, step #32] loss: 3.1353991392887\n",
      "[EPOCH #3, step #34] loss: 3.1610240868159702\n",
      "[EPOCH #3, step #36] loss: 3.167143886153762\n",
      "[EPOCH #3, step #38] loss: 3.1523298361362557\n",
      "[EPOCH #3, step #40] loss: 3.1465866158648237\n",
      "[EPOCH #3, step #42] loss: 3.1388296637424205\n",
      "[EPOCH #3, step #44] loss: 3.1304288758171928\n",
      "[EPOCH #3, step #46] loss: 3.113850238475394\n",
      "[EPOCH #3, step #48] loss: 3.1124487166502037\n",
      "[EPOCH #3, step #50] loss: 3.121780923768586\n",
      "[EPOCH #3, step #52] loss: 3.115776741279746\n",
      "[EPOCH #3, step #54] loss: 3.1105716878717597\n",
      "[EPOCH #3, step #56] loss: 3.11895397671482\n",
      "[EPOCH #3, step #58] loss: 3.1150223562272927\n",
      "[EPOCH #3, step #60] loss: 3.1078857398423994\n",
      "[EPOCH #3, step #62] loss: 3.120701857975551\n",
      "[EPOCH #3, step #64] loss: 3.116422873276931\n",
      "[EPOCH #3, step #66] loss: 3.1206133507970555\n",
      "[EPOCH #3, step #68] loss: 3.118708347928697\n",
      "[EPOCH #3, step #70] loss: 3.1076864558206476\n",
      "[EPOCH #3, step #72] loss: 3.104145112102979\n",
      "[EPOCH #3, step #74] loss: 3.1099498589833576\n",
      "[EPOCH #3, step #76] loss: 3.1145831114285953\n",
      "[EPOCH #3, step #78] loss: 3.10723225376274\n",
      "[EPOCH #3, step #80] loss: 3.109599081086524\n",
      "[EPOCH #3, step #82] loss: 3.115797419145883\n",
      "[EPOCH #3, step #84] loss: 3.1192268652074477\n",
      "[EPOCH #3, step #86] loss: 3.128765648808973\n",
      "[EPOCH #3, step #88] loss: 3.138220800442642\n",
      "[EPOCH #3, step #90] loss: 3.1383982647906294\n",
      "[EPOCH #3, step #92] loss: 3.1406130508709977\n",
      "[EPOCH #3, step #94] loss: 3.1441743223290692\n",
      "[EPOCH #3, step #96] loss: 3.1441698393870876\n",
      "[EPOCH #3, step #98] loss: 3.1395836430366595\n",
      "[EPOCH #3, step #100] loss: 3.1467701019627032\n",
      "[EPOCH #3, step #102] loss: 3.1493329909241314\n",
      "[EPOCH #3, step #104] loss: 3.153500273114159\n",
      "[EPOCH #3, step #106] loss: 3.1559173414640336\n",
      "[EPOCH #3, step #108] loss: 3.154706983391298\n",
      "[EPOCH #3, step #110] loss: 3.15972613429164\n",
      "[EPOCH #3, step #112] loss: 3.157989320501817\n",
      "[EPOCH #3, step #114] loss: 3.160516181199447\n",
      "[EPOCH #3, step #116] loss: 3.1592165486425414\n",
      "[EPOCH #3, step #118] loss: 3.1632546757449624\n",
      "[EPOCH #3, step #120] loss: 3.165507637764797\n",
      "[EPOCH #3, step #122] loss: 3.1673307069918004\n",
      "[EPOCH #3, step #124] loss: 3.1703494701385497\n",
      "[EPOCH #3, step #126] loss: 3.166942382422019\n",
      "[EPOCH #3, step #128] loss: 3.1681888713393103\n",
      "[EPOCH #3, step #130] loss: 3.168720292681046\n",
      "[EPOCH #3, step #132] loss: 3.1749414339997712\n",
      "[EPOCH #3, step #134] loss: 3.1748164176940916\n",
      "[EPOCH #3, step #136] loss: 3.179252554900455\n",
      "[EPOCH #3, step #138] loss: 3.1727395383574124\n",
      "[EPOCH #3, step #140] loss: 3.171548829856494\n",
      "[EPOCH #3, step #142] loss: 3.172500240219223\n",
      "[EPOCH #3, step #144] loss: 3.171163537584502\n",
      "[EPOCH #3, step #146] loss: 3.1642428709536183\n",
      "[EPOCH #3, step #148] loss: 3.1585353956926587\n",
      "[EPOCH #3, step #150] loss: 3.1592281101555226\n",
      "[EPOCH #3, step #152] loss: 3.164120568169488\n",
      "[EPOCH #3, step #154] loss: 3.161681688985517\n",
      "[EPOCH #3, step #156] loss: 3.1652456696625726\n",
      "[EPOCH #3, step #158] loss: 3.1628621464255473\n",
      "[EPOCH #3, step #160] loss: 3.1627877395345556\n",
      "[EPOCH #3, step #162] loss: 3.1681158645021403\n",
      "[EPOCH #3, step #164] loss: 3.1705848462653883\n",
      "[EPOCH #3, step #166] loss: 3.171107372124038\n",
      "[EPOCH #3, step #168] loss: 3.1753538712947327\n",
      "[EPOCH #3, step #170] loss: 3.1763985909913717\n",
      "[EPOCH #3, step #172] loss: 3.177903012733239\n",
      "[EPOCH #3, step #174] loss: 3.1748956980024063\n",
      "[EPOCH #3, step #176] loss: 3.1668794101240945\n",
      "[EPOCH #3, step #178] loss: 3.1707236833412553\n",
      "[EPOCH #3, step #180] loss: 3.1725288596601118\n",
      "[EPOCH #3, step #182] loss: 3.1728359363118157\n",
      "[EPOCH #3, step #184] loss: 3.1746655992559485\n",
      "[EPOCH #3, step #186] loss: 3.1757812856990384\n",
      "[EPOCH #3, step #188] loss: 3.17336100245279\n",
      "[EPOCH #3, step #190] loss: 3.173898922835345\n",
      "[EPOCH #3, step #192] loss: 3.172727215475369\n",
      "[EPOCH #3, step #194] loss: 3.1734227559505364\n",
      "[EPOCH #3, step #196] loss: 3.1752520152154915\n",
      "[EPOCH #3, step #198] loss: 3.170970153568977\n",
      "[EPOCH #3, step #200] loss: 3.1731720528199303\n",
      "[EPOCH #3, step #202] loss: 3.179579592690679\n",
      "[EPOCH #3, step #204] loss: 3.1831578929249833\n",
      "[EPOCH #3, step #206] loss: 3.183213830570092\n",
      "[EPOCH #3, step #208] loss: 3.1839599255739786\n",
      "[EPOCH #3, step #210] loss: 3.183483635644777\n",
      "[EPOCH #3, step #212] loss: 3.1853233178456626\n",
      "[EPOCH #3, step #214] loss: 3.186448556323384\n",
      "[EPOCH #3, step #216] loss: 3.1867888940644153\n",
      "[EPOCH #3, step #218] loss: 3.1838066425497673\n",
      "[EPOCH #3, step #220] loss: 3.185533671357513\n",
      "[EPOCH #3, step #222] loss: 3.1866081894245917\n",
      "[EPOCH #3, step #224] loss: 3.1846584479014077\n",
      "[EPOCH #3, step #226] loss: 3.1815962728424743\n",
      "[EPOCH #3, step #228] loss: 3.1823273975255706\n",
      "[EPOCH #3, step #230] loss: 3.1811725928153827\n",
      "[EPOCH #3, step #232] loss: 3.1841602325439453\n",
      "[EPOCH #3, step #234] loss: 3.179925409276435\n",
      "[EPOCH #3, step #236] loss: 3.1845066165119285\n",
      "[EPOCH #3, step #238] loss: 3.182248949505794\n",
      "[EPOCH #3, step #240] loss: 3.1817009933756597\n",
      "[EPOCH #3, step #242] loss: 3.1807858904693354\n",
      "[EPOCH #3, step #244] loss: 3.181871843338013\n",
      "[EPOCH #3, step #246] loss: 3.18521622414531\n",
      "[EPOCH #3, step #248] loss: 3.1855565161111365\n",
      "[EPOCH #3, step #250] loss: 3.184769479402033\n",
      "[EPOCH #3, step #252] loss: 3.182656171293598\n",
      "[EPOCH #3, step #254] loss: 3.1796542167663575\n",
      "[EPOCH #3, step #256] loss: 3.1760706901550293\n",
      "[EPOCH #3, step #258] loss: 3.1747627009756316\n",
      "[EPOCH #3, step #260] loss: 3.176022111227686\n",
      "[EPOCH #3, step #262] loss: 3.1752703924142818\n",
      "[EPOCH #3, step #264] loss: 3.1730685342032956\n",
      "[EPOCH #3, step #266] loss: 3.172370519530907\n",
      "[EPOCH #3, step #268] loss: 3.1753916563154596\n",
      "[EPOCH #3, step #270] loss: 3.1741698210529736\n",
      "[EPOCH #3, step #272] loss: 3.172087264148307\n",
      "[EPOCH #3, step #274] loss: 3.176647845181552\n",
      "[EPOCH #3, step #276] loss: 3.1779874249055498\n",
      "[EPOCH #3, step #278] loss: 3.1774493895978484\n",
      "[EPOCH #3, step #280] loss: 3.1801958194406854\n",
      "[EPOCH #3, step #282] loss: 3.177683123430178\n",
      "[EPOCH #3, step #284] loss: 3.1775120751899584\n",
      "[EPOCH #3, step #286] loss: 3.1786090513555014\n",
      "[EPOCH #3, step #288] loss: 3.1801299850833455\n",
      "[EPOCH #3, step #290] loss: 3.176657016334665\n",
      "[EPOCH #3, step #292] loss: 3.177618008017947\n",
      "[EPOCH #3, step #294] loss: 3.1793353210061284\n",
      "[EPOCH #3, step #296] loss: 3.1822237727617977\n",
      "[EPOCH #3, step #298] loss: 3.183236383674137\n",
      "[EPOCH #3, step #300] loss: 3.183416338854058\n",
      "[EPOCH #3, step #302] loss: 3.184246816257439\n",
      "[EPOCH #3, step #304] loss: 3.181836558170006\n",
      "[EPOCH #3, step #306] loss: 3.1820768090723393\n",
      "[EPOCH #3, step #308] loss: 3.182010623629425\n",
      "[EPOCH #3, step #310] loss: 3.1805902301690203\n",
      "[EPOCH #3, step #312] loss: 3.1775796969477743\n",
      "[EPOCH #3, step #314] loss: 3.1771611281803676\n",
      "[EPOCH #3, step #316] loss: 3.176897821366223\n",
      "[EPOCH #3, step #318] loss: 3.179425679030463\n",
      "[EPOCH #3, step #320] loss: 3.182495994359905\n",
      "[EPOCH #3, step #322] loss: 3.180946857198473\n",
      "[EPOCH #3, step #324] loss: 3.1783223042121302\n",
      "[EPOCH #3, step #326] loss: 3.179040593838473\n",
      "[EPOCH #3, step #328] loss: 3.1770902064071236\n",
      "[EPOCH #3, step #330] loss: 3.175240322182186\n",
      "[EPOCH #3, step #332] loss: 3.17727810054928\n",
      "[EPOCH #3, step #334] loss: 3.176044191531281\n",
      "[EPOCH #3, step #336] loss: 3.176455335730261\n",
      "[EPOCH #3, step #338] loss: 3.177653821872047\n",
      "[EPOCH #3, step #340] loss: 3.177084259273719\n",
      "[EPOCH #3, step #342] loss: 3.175920461426671\n",
      "[EPOCH #3, step #344] loss: 3.1750641470370087\n",
      "[EPOCH #3, step #346] loss: 3.1748732592942734\n",
      "[EPOCH #3, step #348] loss: 3.175618473643218\n",
      "[EPOCH #3, step #350] loss: 3.1754845282291075\n",
      "[EPOCH #3, step #352] loss: 3.1750240244878927\n",
      "[EPOCH #3, step #354] loss: 3.173854389996596\n",
      "[EPOCH #3, step #356] loss: 3.1722933965570785\n",
      "[EPOCH #3, step #358] loss: 3.1716500955703864\n",
      "[EPOCH #3, step #360] loss: 3.17239481788593\n",
      "[EPOCH #3, step #362] loss: 3.17203117533492\n",
      "[EPOCH #3, step #364] loss: 3.1716914862802583\n",
      "[EPOCH #3, step #366] loss: 3.174044759137104\n",
      "[EPOCH #3, step #368] loss: 3.1722280029358902\n",
      "[EPOCH #3, step #370] loss: 3.171112644061888\n",
      "[EPOCH #3, step #372] loss: 3.1706003778421845\n",
      "[EPOCH #3, step #374] loss: 3.169994946161906\n",
      "[EPOCH #3, step #376] loss: 3.169199217535773\n",
      "[EPOCH #3, step #378] loss: 3.168925818793063\n",
      "[EPOCH #3, step #380] loss: 3.1676511695691607\n",
      "[EPOCH #3, step #382] loss: 3.168338477455916\n",
      "[EPOCH #3, step #384] loss: 3.166474998152101\n",
      "[EPOCH #3, step #386] loss: 3.1676417610750027\n",
      "[EPOCH #3, step #388] loss: 3.168253913330235\n",
      "[EPOCH #3, step #390] loss: 3.166730105114715\n",
      "[EPOCH #3, step #392] loss: 3.1654263561918534\n",
      "[EPOCH #3, step #394] loss: 3.165367217607136\n",
      "[EPOCH #3, step #396] loss: 3.165465422781649\n",
      "[EPOCH #3, step #398] loss: 3.1647117215589176\n",
      "[EPOCH #3, step #400] loss: 3.1673735811228765\n",
      "[EPOCH #3, step #402] loss: 3.1655698846055023\n",
      "[EPOCH #3, step #404] loss: 3.165075774840367\n",
      "[EPOCH #3, step #406] loss: 3.1633991205135787\n",
      "[EPOCH #3, step #408] loss: 3.1636905804240034\n",
      "[EPOCH #3, step #410] loss: 3.162002699798621\n",
      "[EPOCH #3, step #412] loss: 3.1627791395487566\n",
      "[EPOCH #3, step #414] loss: 3.1604630625391583\n",
      "[EPOCH #3, step #416] loss: 3.158567418869165\n",
      "[EPOCH #3, step #418] loss: 3.1589740472078893\n",
      "[EPOCH #3, step #420] loss: 3.15690494716309\n",
      "[EPOCH #3, step #422] loss: 3.1559856095776206\n",
      "[EPOCH #3, step #424] loss: 3.1549436860926012\n",
      "[EPOCH #3, step #426] loss: 3.1544515092702325\n",
      "[EPOCH #3, step #428] loss: 3.1541108834993588\n",
      "[EPOCH #3, step #430] loss: 3.154553428326848\n",
      "[EPOCH #3, step #432] loss: 3.15327559277312\n",
      "[EPOCH #3, step #434] loss: 3.152716922211921\n",
      "[EPOCH #3, step #436] loss: 3.1531994686519527\n",
      "[EPOCH #3, step #438] loss: 3.1533594908095166\n",
      "[EPOCH #3, step #440] loss: 3.153107857217594\n",
      "[EPOCH #3, step #442] loss: 3.1524074260591117\n",
      "[EPOCH #3, step #444] loss: 3.151444924279545\n",
      "[EPOCH #3, step #446] loss: 3.151997829176969\n",
      "[EPOCH #3, step #448] loss: 3.150448408317991\n",
      "[EPOCH #3, step #450] loss: 3.150900987722393\n",
      "[EPOCH #3, step #452] loss: 3.151106550204043\n",
      "[EPOCH #3, step #454] loss: 3.1497496688758932\n",
      "[EPOCH #3, step #456] loss: 3.148532886212973\n",
      "[EPOCH #3, step #458] loss: 3.148518062105366\n",
      "[EPOCH #3, step #460] loss: 3.149477574416717\n",
      "[EPOCH #3, step #462] loss: 3.1505555941785643\n",
      "[EPOCH #3, step #464] loss: 3.1495927897832727\n",
      "[EPOCH #3, step #466] loss: 3.14910215637158\n",
      "[EPOCH #3, step #468] loss: 3.1498077896866463\n",
      "[EPOCH #3, step #470] loss: 3.1501738144333955\n",
      "[EPOCH #3, step #472] loss: 3.14945740125144\n",
      "[EPOCH #3, step #474] loss: 3.1489826764558493\n",
      "[EPOCH #3, step #476] loss: 3.148732283830143\n",
      "[EPOCH #3, step #478] loss: 3.1480385774361563\n",
      "[EPOCH #3, step #480] loss: 3.1468910244051482\n",
      "[EPOCH #3, step #482] loss: 3.146215565703177\n",
      "[EPOCH #3, step #484] loss: 3.145879160989191\n",
      "[EPOCH #3, step #486] loss: 3.1455057749268454\n",
      "[EPOCH #3, step #488] loss: 3.1456793978170383\n",
      "[EPOCH #3, step #490] loss: 3.1458456705641114\n",
      "[EPOCH #3, step #492] loss: 3.145867405750205\n",
      "[EPOCH #3, step #494] loss: 3.145575120232322\n",
      "[EPOCH #3, step #496] loss: 3.1446719409474424\n",
      "[EPOCH #3, step #498] loss: 3.143339869971266\n",
      "[EPOCH #3, step #500] loss: 3.1414541079850493\n",
      "[EPOCH #3, step #502] loss: 3.1409461066926685\n",
      "[EPOCH #3, step #504] loss: 3.1413792964255456\n",
      "[EPOCH #3, step #506] loss: 3.1405258216331227\n",
      "[EPOCH #3, step #508] loss: 3.1406187336665004\n",
      "[EPOCH #3, step #510] loss: 3.1416078262366427\n",
      "[EPOCH #3, step #512] loss: 3.140647023741962\n",
      "[EPOCH #3, step #514] loss: 3.1411249401499926\n",
      "[EPOCH #3, step #516] loss: 3.142201402892919\n",
      "[EPOCH #3, step #518] loss: 3.141621787653723\n",
      "[EPOCH #3, step #520] loss: 3.1419125518505955\n",
      "[EPOCH #3, step #522] loss: 3.1428614159615043\n",
      "[EPOCH #3, step #524] loss: 3.142626309621902\n",
      "[EPOCH #3, step #526] loss: 3.143413991584271\n",
      "[EPOCH #3, step #528] loss: 3.144251037410166\n",
      "[EPOCH #3, step #530] loss: 3.1437143794560836\n",
      "[EPOCH #3, step #532] loss: 3.1433430988390496\n",
      "[EPOCH #3, step #534] loss: 3.1420922787390024\n",
      "[EPOCH #3, step #536] loss: 3.1411716698268273\n",
      "[EPOCH #3, step #538] loss: 3.1402893137179855\n",
      "[EPOCH #3, step #540] loss: 3.1395788629041803\n",
      "[EPOCH #3, step #542] loss: 3.1383491177146166\n",
      "[EPOCH #3, step #544] loss: 3.136974357027526\n",
      "[EPOCH #3, step #546] loss: 3.1392108871034554\n",
      "[EPOCH #3, step #548] loss: 3.1403104937575987\n",
      "[EPOCH #3, step #550] loss: 3.1393475160408366\n",
      "[EPOCH #3, step #552] loss: 3.1374276361241265\n",
      "[EPOCH #3, step #554] loss: 3.137908839320277\n",
      "[EPOCH #3, step #556] loss: 3.1386529312099545\n",
      "[EPOCH #3, step #558] loss: 3.138638566772925\n",
      "[EPOCH #3, step #560] loss: 3.1379114757034654\n",
      "[EPOCH #3, step #562] loss: 3.137382194796845\n",
      "[EPOCH #3, step #564] loss: 3.1369570424071456\n",
      "[EPOCH #3, step #566] loss: 3.1361050668847623\n",
      "[EPOCH #3, step #568] loss: 3.1350116562131\n",
      "[EPOCH #3, step #570] loss: 3.1353079759093383\n",
      "[EPOCH #3, step #572] loss: 3.135574449419351\n",
      "[EPOCH #3, step #574] loss: 3.1364755651225216\n",
      "[EPOCH #3, step #576] loss: 3.1362434065734495\n",
      "[EPOCH #3, step #578] loss: 3.136121129742558\n",
      "[EPOCH #3, step #580] loss: 3.136247019349195\n",
      "[EPOCH #3, step #582] loss: 3.13607454381717\n",
      "[EPOCH #3, step #584] loss: 3.134922295757848\n",
      "[EPOCH #3, step #586] loss: 3.135594394787816\n",
      "[EPOCH #3, step #588] loss: 3.134921935296828\n",
      "[EPOCH #3, step #590] loss: 3.135917168179946\n",
      "[EPOCH #3, step #592] loss: 3.1361018658488438\n",
      "[EPOCH #3, step #594] loss: 3.135499254996035\n",
      "[EPOCH #3, step #596] loss: 3.1369763766501215\n",
      "[EPOCH #3, step #598] loss: 3.136931260957543\n",
      "[EPOCH #3, step #600] loss: 3.136101865133708\n",
      "[EPOCH #3, step #602] loss: 3.137212365816284\n",
      "[EPOCH #3, step #604] loss: 3.1362344938861435\n",
      "[EPOCH #3, step #606] loss: 3.134933549843274\n",
      "[EPOCH #3, step #608] loss: 3.133065134042198\n",
      "[EPOCH #3, step #610] loss: 3.133874659452423\n",
      "[EPOCH #3, step #612] loss: 3.1349759490805\n",
      "[EPOCH #3, step #614] loss: 3.133326057108437\n",
      "[EPOCH #3, step #616] loss: 3.1347963102826037\n",
      "[EPOCH #3, step #618] loss: 3.134884945988077\n",
      "[EPOCH #3, step #620] loss: 3.136161326978134\n",
      "[EPOCH #3, step #622] loss: 3.1352618624655046\n",
      "[EPOCH #3, step #624] loss: 3.1354021774291994\n",
      "[EPOCH #3, step #626] loss: 3.135536315148337\n",
      "[EPOCH #3, step #628] loss: 3.1347741072431847\n",
      "[EPOCH #3, step #630] loss: 3.135202981779579\n",
      "[EPOCH #3, step #632] loss: 3.1341624824921666\n",
      "[EPOCH #3, step #634] loss: 3.1344613856217993\n",
      "[EPOCH #3, step #636] loss: 3.134806342551622\n",
      "[EPOCH #3, step #638] loss: 3.1350649548621616\n",
      "[EPOCH #3, step #640] loss: 3.1352413124673637\n",
      "[EPOCH #3, step #642] loss: 3.1356917120991374\n",
      "[EPOCH #3, step #644] loss: 3.134864346555961\n",
      "[EPOCH #3, step #646] loss: 3.1342405347956754\n",
      "[EPOCH #3, step #648] loss: 3.134301848698103\n",
      "[EPOCH #3, step #650] loss: 3.1341014534647016\n",
      "[EPOCH #3, step #652] loss: 3.132599594764651\n",
      "[EPOCH #3, step #654] loss: 3.1335906010547667\n",
      "[EPOCH #3, step #656] loss: 3.133315179627417\n",
      "[EPOCH #3, step #658] loss: 3.132817227851277\n",
      "[EPOCH #3, step #660] loss: 3.133573675299917\n",
      "[EPOCH #3, step #662] loss: 3.133528154540026\n",
      "[EPOCH #3, step #664] loss: 3.133908504471743\n",
      "[EPOCH #3, step #666] loss: 3.134012913954133\n",
      "[EPOCH #3, step #668] loss: 3.134164567129138\n",
      "[EPOCH #3, step #670] loss: 3.1340236276521414\n",
      "[EPOCH #3, step #672] loss: 3.1331964435407347\n",
      "[EPOCH #3, step #674] loss: 3.1331278486605045\n",
      "[EPOCH #3, step #676] loss: 3.1323192425985633\n",
      "[EPOCH #3, step #678] loss: 3.1327047088184834\n",
      "[EPOCH #3, step #680] loss: 3.132958709055807\n",
      "[EPOCH #3, step #682] loss: 3.133018847963995\n",
      "[EPOCH #3, step #684] loss: 3.1328897723316276\n",
      "[EPOCH #3, step #686] loss: 3.1328748188685123\n",
      "[EPOCH #3, step #688] loss: 3.1318165570800294\n",
      "[EPOCH #3, step #690] loss: 3.1327858185112563\n",
      "[EPOCH #3, step #692] loss: 3.1314025947025845\n",
      "[EPOCH #3, step #694] loss: 3.1314017371308034\n",
      "[EPOCH #3, step #696] loss: 3.132072205522994\n",
      "[EPOCH #3, step #698] loss: 3.1322020512282083\n",
      "[EPOCH #3, step #700] loss: 3.1320809528933102\n",
      "[EPOCH #3, step #702] loss: 3.1324182180727527\n",
      "[EPOCH #3, step #704] loss: 3.132146210366107\n",
      "[EPOCH #3, step #706] loss: 3.131294839621602\n",
      "[EPOCH #3, step #708] loss: 3.1308967680453583\n",
      "[EPOCH #3, step #710] loss: 3.131033396754419\n",
      "[EPOCH #3, step #712] loss: 3.1308484395122127\n",
      "[EPOCH #3, step #714] loss: 3.1314105584071235\n",
      "[EPOCH #3, step #716] loss: 3.1310893648006592\n",
      "[EPOCH #3, step #718] loss: 3.1314496088757466\n",
      "[EPOCH #3, step #720] loss: 3.1314890139972618\n",
      "[EPOCH #3, step #722] loss: 3.1313024787322448\n",
      "[EPOCH #3, step #724] loss: 3.1315436655899576\n",
      "[EPOCH #3, step #726] loss: 3.1316843393580296\n",
      "[EPOCH #3, step #728] loss: 3.132055552408038\n",
      "[EPOCH #3, step #730] loss: 3.131251671734978\n",
      "[EPOCH #3, step #732] loss: 3.1309004467473676\n",
      "[EPOCH #3, step #734] loss: 3.1306816380040177\n",
      "[EPOCH #3, step #736] loss: 3.1300844162742996\n",
      "[EPOCH #3, step #738] loss: 3.1299634266932053\n",
      "[EPOCH #3, step #740] loss: 3.1305517432821586\n",
      "[EPOCH #3, step #742] loss: 3.130250456195179\n",
      "[EPOCH #3, step #744] loss: 3.1296968748105454\n",
      "[EPOCH #3, step #746] loss: 3.129080547705552\n",
      "[EPOCH #3, step #748] loss: 3.1287141524265225\n",
      "[EPOCH #3, step #750] loss: 3.129508447075652\n",
      "[EPOCH #3, step #752] loss: 3.1295144231829193\n",
      "[EPOCH #3, step #754] loss: 3.127932858624995\n",
      "[EPOCH #3, step #756] loss: 3.128445189726715\n",
      "[EPOCH #3, step #758] loss: 3.127881370514278\n",
      "[EPOCH #3, step #760] loss: 3.1279806404640107\n",
      "[EPOCH #3, step #762] loss: 3.1275030966667483\n",
      "[EPOCH #3, step #764] loss: 3.126958360858992\n",
      "[EPOCH #3, step #766] loss: 3.127274244052335\n",
      "[EPOCH #3, step #768] loss: 3.1270066150918274\n",
      "[EPOCH #3, step #770] loss: 3.1277703172038036\n",
      "[EPOCH #3, step #772] loss: 3.12838378791513\n",
      "[EPOCH #3, step #774] loss: 3.128633597589308\n",
      "[EPOCH #3, step #776] loss: 3.12867200297892\n",
      "[EPOCH #3, step #778] loss: 3.127960205078125\n",
      "[EPOCH #3, step #780] loss: 3.1282807425255963\n",
      "[EPOCH #3, step #782] loss: 3.128526568260534\n",
      "[EPOCH #3, step #784] loss: 3.128704664509767\n",
      "[EPOCH #3, step #786] loss: 3.1292410887484325\n",
      "[EPOCH #3, step #788] loss: 3.12916008691824\n",
      "[EPOCH #3, step #790] loss: 3.1301178422800056\n",
      "[EPOCH #3, step #792] loss: 3.128567238623678\n",
      "[EPOCH #3, step #794] loss: 3.1288876473528786\n",
      "[EPOCH #3, step #796] loss: 3.1290187075865012\n",
      "[EPOCH #3, step #798] loss: 3.129856628530166\n",
      "[EPOCH #3, step #800] loss: 3.130519628227129\n",
      "[EPOCH #3, step #802] loss: 3.130319922828437\n",
      "[EPOCH #3, step #804] loss: 3.1310114099372246\n",
      "[EPOCH #3, step #806] loss: 3.1305868563598858\n",
      "[EPOCH #3, step #808] loss: 3.1302224048430602\n",
      "[EPOCH #3, step #810] loss: 3.129275484237953\n",
      "[EPOCH #3, step #812] loss: 3.1287579120011935\n",
      "[EPOCH #3, step #814] loss: 3.128601310297024\n",
      "[EPOCH #3, step #816] loss: 3.128920983509451\n",
      "[EPOCH #3, step #818] loss: 3.129931330826402\n",
      "[EPOCH #3, step #820] loss: 3.130640397844303\n",
      "[EPOCH #3, step #822] loss: 3.1311717047870955\n",
      "[EPOCH #3, step #824] loss: 3.130565237854466\n",
      "[EPOCH #3, step #826] loss: 3.1304707117784125\n",
      "[EPOCH #3, step #828] loss: 3.130527118146779\n",
      "[EPOCH #3, step #830] loss: 3.1309936175707875\n",
      "[EPOCH #3, step #832] loss: 3.130639347566419\n",
      "[EPOCH #3, step #834] loss: 3.130309682549117\n",
      "[EPOCH #3, step #836] loss: 3.1301219067408335\n",
      "[EPOCH #3, step #838] loss: 3.1305452671892167\n",
      "[EPOCH #3, step #840] loss: 3.130737299017617\n",
      "[EPOCH #3, step #842] loss: 3.1310401667055285\n",
      "[EPOCH #3, step #844] loss: 3.131225980668378\n",
      "[EPOCH #3, step #846] loss: 3.130985391182207\n",
      "[EPOCH #3, step #848] loss: 3.131233575346894\n",
      "[EPOCH #3, step #850] loss: 3.131730862986187\n",
      "[EPOCH #3, step #852] loss: 3.131560058414866\n",
      "[EPOCH #3, step #854] loss: 3.1319978892454627\n",
      "[EPOCH #3, step #856] loss: 3.1324151010190593\n",
      "[EPOCH #3, step #858] loss: 3.131744520211803\n",
      "[EPOCH #3, step #860] loss: 3.132126122263112\n",
      "[EPOCH #3, step #862] loss: 3.1316501978236584\n",
      "[EPOCH #3, step #864] loss: 3.1306252967415515\n",
      "[EPOCH #3, step #866] loss: 3.1301991136566305\n",
      "[EPOCH #3, step #868] loss: 3.1290643113666357\n",
      "[EPOCH #3, step #870] loss: 3.130139490766996\n",
      "[EPOCH #3, step #872] loss: 3.130658551999384\n",
      "[EPOCH #3, step #874] loss: 3.130220677784511\n",
      "[EPOCH #3, step #876] loss: 3.129352670710878\n",
      "[EPOCH #3, step #878] loss: 3.128357816203598\n",
      "[EPOCH #3, step #880] loss: 3.128816623828468\n",
      "[EPOCH #3, step #882] loss: 3.1290271843816253\n",
      "[EPOCH #3, step #884] loss: 3.1288274681500794\n",
      "[EPOCH #3, step #886] loss: 3.1287249196341773\n",
      "[EPOCH #3, step #888] loss: 3.1283765740281955\n",
      "[EPOCH #3, step #890] loss: 3.1289688944147627\n",
      "[EPOCH #3, step #892] loss: 3.1287281996458938\n",
      "[EPOCH #3, step #894] loss: 3.128693508968673\n",
      "[EPOCH #3, step #896] loss: 3.1287154367270413\n",
      "[EPOCH #3, step #898] loss: 3.1293364186440744\n",
      "[EPOCH #3, step #900] loss: 3.129441977341088\n",
      "[EPOCH #3, step #902] loss: 3.1296425515235065\n",
      "[EPOCH #3, step #904] loss: 3.1297676031102135\n",
      "[EPOCH #3, step #906] loss: 3.1288547163630005\n",
      "[EPOCH #3, step #908] loss: 3.128709441888975\n",
      "[EPOCH #3, step #910] loss: 3.1294302594910337\n",
      "[EPOCH #3, step #912] loss: 3.1298676508541043\n",
      "[EPOCH #3, step #914] loss: 3.130255754658433\n",
      "[EPOCH #3, step #916] loss: 3.1303024112593256\n",
      "[EPOCH #3, step #918] loss: 3.129700456013228\n",
      "[EPOCH #3, step #920] loss: 3.130223192428274\n",
      "[EPOCH #3, step #922] loss: 3.1300946658765767\n",
      "[EPOCH #3, step #924] loss: 3.1297853157971356\n",
      "[EPOCH #3, step #926] loss: 3.130098131739463\n",
      "[EPOCH #3, step #928] loss: 3.129419346286868\n",
      "[EPOCH #3, step #930] loss: 3.1296635388559735\n",
      "[EPOCH #3, step #932] loss: 3.129811676113296\n",
      "[EPOCH #3, step #934] loss: 3.1304332039572977\n",
      "[EPOCH #3, step #936] loss: 3.130274676462375\n",
      "[EPOCH #3, step #938] loss: 3.1293934790598064\n",
      "[EPOCH #3, step #940] loss: 3.128755587193715\n",
      "[EPOCH #3, step #942] loss: 3.1287345332533025\n",
      "[EPOCH #3, step #944] loss: 3.129043477426761\n",
      "[EPOCH #3, step #946] loss: 3.129675584458751\n",
      "[EPOCH #3, step #948] loss: 3.1296154575930757\n",
      "[EPOCH #3, step #950] loss: 3.129960628463392\n",
      "[EPOCH #3, step #952] loss: 3.129602850046388\n",
      "[EPOCH #3, step #954] loss: 3.1296248054005087\n",
      "[EPOCH #3, step #956] loss: 3.129733964318143\n",
      "[EPOCH #3, step #958] loss: 3.129179492359738\n",
      "[EPOCH #3, step #960] loss: 3.129566719579151\n",
      "[EPOCH #3, step #962] loss: 3.1289448718291824\n",
      "[EPOCH #3, step #964] loss: 3.129096318526589\n",
      "[EPOCH #3, step #966] loss: 3.129638437401299\n",
      "[EPOCH #3, step #968] loss: 3.1304693158073937\n",
      "[EPOCH #3, step #970] loss: 3.13057920370485\n",
      "[EPOCH #3, step #972] loss: 3.1307764619374447\n",
      "[EPOCH #3, step #974] loss: 3.131058828891852\n",
      "[EPOCH #3, step #976] loss: 3.130590056445728\n",
      "[EPOCH #3, step #978] loss: 3.130121700366744\n",
      "[EPOCH #3, step #980] loss: 3.1296249002248624\n",
      "[EPOCH #3, step #982] loss: 3.1290255519405146\n",
      "[EPOCH #3, step #984] loss: 3.1292815312516264\n",
      "[EPOCH #3, step #986] loss: 3.1298394700797254\n",
      "[EPOCH #3, step #988] loss: 3.1298022945199624\n",
      "[EPOCH #3, step #990] loss: 3.129493201897676\n",
      "[EPOCH #3, step #992] loss: 3.129726296585252\n",
      "[EPOCH #3, step #994] loss: 3.1302363398087083\n",
      "[EPOCH #3, step #996] loss: 3.1292654344047923\n",
      "[EPOCH #3, step #998] loss: 3.1293887330724433\n",
      "[EPOCH #3, step #1000] loss: 3.128751991035698\n",
      "[EPOCH #3, step #1002] loss: 3.1295951677343306\n",
      "[EPOCH #3, step #1004] loss: 3.1293847195544644\n",
      "[EPOCH #3, step #1006] loss: 3.1296103211358384\n",
      "[EPOCH #3, step #1008] loss: 3.1298306836599874\n",
      "[EPOCH #3, step #1010] loss: 3.130260078654681\n",
      "[EPOCH #3, step #1012] loss: 3.1299691308521447\n",
      "[EPOCH #3, step #1014] loss: 3.129885523190052\n",
      "[EPOCH #3, step #1016] loss: 3.1291551463371885\n",
      "[EPOCH #3, step #1018] loss: 3.128557173379855\n",
      "[EPOCH #3, step #1020] loss: 3.1287840225320607\n",
      "[EPOCH #3, step #1022] loss: 3.128265373168453\n",
      "[EPOCH #3, step #1024] loss: 3.127967353681239\n",
      "[EPOCH #3, step #1026] loss: 3.1283087307893775\n",
      "[EPOCH #3, step #1028] loss: 3.1279421978719726\n",
      "[EPOCH #3, step #1030] loss: 3.1281979911889297\n",
      "[EPOCH #3, step #1032] loss: 3.128743339023424\n",
      "[EPOCH #3, step #1034] loss: 3.128743092679747\n",
      "[EPOCH #3, step #1036] loss: 3.1284647381064286\n",
      "[EPOCH #3, step #1038] loss: 3.127963434169795\n",
      "[EPOCH #3, step #1040] loss: 3.1272906702831316\n",
      "[EPOCH #3, step #1042] loss: 3.1273843181441867\n",
      "[EPOCH #3, step #1044] loss: 3.127286193017184\n",
      "[EPOCH #3, step #1046] loss: 3.1281403033302984\n",
      "[EPOCH #3, step #1048] loss: 3.1285086187211757\n",
      "[EPOCH #3, step #1050] loss: 3.1277547980579845\n",
      "[EPOCH #3, step #1052] loss: 3.1280498909927617\n",
      "[EPOCH #3, step #1054] loss: 3.1269936986444122\n",
      "[EPOCH #3, step #1056] loss: 3.127359842960859\n",
      "[EPOCH #3, step #1058] loss: 3.1270054600169006\n",
      "[EPOCH #3, step #1060] loss: 3.126878555038984\n",
      "[EPOCH #3, step #1062] loss: 3.1263967659376974\n",
      "[EPOCH #3, step #1064] loss: 3.1259833593323756\n",
      "[EPOCH #3, step #1066] loss: 3.125265049956732\n",
      "[EPOCH #3, step #1068] loss: 3.1252037488600157\n",
      "[EPOCH #3, step #1070] loss: 3.125545713859223\n",
      "[EPOCH #3, step #1072] loss: 3.125424155317948\n",
      "[EPOCH #3, step #1074] loss: 3.1252050954242083\n",
      "[EPOCH #3, step #1076] loss: 3.1250375047699657\n",
      "[EPOCH #3, step #1078] loss: 3.124906329781618\n",
      "[EPOCH #3, step #1080] loss: 3.1245914123104637\n",
      "[EPOCH #3, step #1082] loss: 3.1244612880470792\n",
      "[EPOCH #3, step #1084] loss: 3.124456075369488\n",
      "[EPOCH #3, step #1086] loss: 3.1243888967719338\n",
      "[EPOCH #3, step #1088] loss: 3.1236235596260093\n",
      "[EPOCH #3, step #1090] loss: 3.1234599866526156\n",
      "[EPOCH #3, step #1092] loss: 3.123491083059634\n",
      "[EPOCH #3, step #1094] loss: 3.1237073016493286\n",
      "[EPOCH #3, step #1096] loss: 3.124216357251135\n",
      "[EPOCH #3, step #1098] loss: 3.124259684495865\n",
      "[EPOCH #3, step #1100] loss: 3.12491822870724\n",
      "[EPOCH #3, step #1102] loss: 3.1247009947855475\n",
      "[EPOCH #3, step #1104] loss: 3.1243090217469502\n",
      "[EPOCH #3, step #1106] loss: 3.1244733208670956\n",
      "[EPOCH #3, step #1108] loss: 3.124496496293435\n",
      "[EPOCH #3, step #1110] loss: 3.1242355424793424\n",
      "[EPOCH #3, step #1112] loss: 3.1242515922985095\n",
      "[EPOCH #3, step #1114] loss: 3.124464986142556\n",
      "[EPOCH #3, step #1116] loss: 3.1246730918423236\n",
      "[EPOCH #3, step #1118] loss: 3.12506659399515\n",
      "[EPOCH #3, step #1120] loss: 3.124985959038577\n",
      "[EPOCH #3, step #1122] loss: 3.1250097760109634\n",
      "[EPOCH #3, step #1124] loss: 3.124914239671495\n",
      "[EPOCH #3, step #1126] loss: 3.1244512207230954\n",
      "[EPOCH #3, step #1128] loss: 3.1251685912889964\n",
      "[EPOCH #3, step #1130] loss: 3.1254330823950593\n",
      "[EPOCH #3, step #1132] loss: 3.12573591461249\n",
      "[EPOCH #3, step #1134] loss: 3.125507110436057\n",
      "[EPOCH #3, step #1136] loss: 3.1258742838652385\n",
      "[EPOCH #3, step #1138] loss: 3.1260784166334386\n",
      "[EPOCH #3, step #1140] loss: 3.1262916141596935\n",
      "[EPOCH #3, step #1142] loss: 3.1264128384627696\n",
      "[EPOCH #3, step #1144] loss: 3.126588128019108\n",
      "[EPOCH #3, step #1146] loss: 3.1272019387954404\n",
      "[EPOCH #3, step #1148] loss: 3.1268916001415334\n",
      "[EPOCH #3, step #1150] loss: 3.1266438254680353\n",
      "[EPOCH #3, step #1152] loss: 3.1268237350510186\n",
      "[EPOCH #3, step #1154] loss: 3.1271875329863974\n",
      "[EPOCH #3, step #1156] loss: 3.127611468553337\n",
      "[EPOCH #3, step #1158] loss: 3.1278914691457675\n",
      "[EPOCH #3, step #1160] loss: 3.127369163572326\n",
      "[EPOCH #3, step #1162] loss: 3.12761305676179\n",
      "[EPOCH #3, step #1164] loss: 3.127797729262978\n",
      "[EPOCH #3, step #1166] loss: 3.128007005807571\n",
      "[EPOCH #3, step #1168] loss: 3.1286610284353342\n",
      "[EPOCH #3, step #1170] loss: 3.128962967357098\n",
      "[EPOCH #3, step #1172] loss: 3.1291701360736663\n",
      "[EPOCH #3, step #1174] loss: 3.128842869007841\n",
      "[EPOCH #3, step #1176] loss: 3.1286068290801197\n",
      "[EPOCH #3, step #1178] loss: 3.128659834590779\n",
      "[EPOCH #3, step #1180] loss: 3.1290857925140485\n",
      "[EPOCH #3, step #1182] loss: 3.128247633891295\n",
      "[EPOCH #3, step #1184] loss: 3.1274823027824046\n",
      "[EPOCH #3, step #1186] loss: 3.127113922363384\n",
      "[EPOCH #3, step #1188] loss: 3.12718722998745\n",
      "[EPOCH #3, step #1190] loss: 3.1271286389089052\n",
      "[EPOCH #3, step #1192] loss: 3.1262386039036816\n",
      "[EPOCH #3, step #1194] loss: 3.126084432043291\n",
      "[EPOCH #3, step #1196] loss: 3.1263617620731057\n",
      "[EPOCH #3, step #1198] loss: 3.126313783806299\n",
      "[EPOCH #3, step #1200] loss: 3.1264441053039524\n",
      "[EPOCH #3, step #1202] loss: 3.1265681679805715\n",
      "[EPOCH #3, step #1204] loss: 3.1264967489044695\n",
      "[EPOCH #3, step #1206] loss: 3.1263296339466438\n",
      "[EPOCH #3, step #1208] loss: 3.126025380922606\n",
      "[EPOCH #3, step #1210] loss: 3.1261234129882864\n",
      "[EPOCH #3, step #1212] loss: 3.126021074835957\n",
      "[EPOCH #3, step #1214] loss: 3.1261839652748264\n",
      "[EPOCH #3, step #1216] loss: 3.1263416222891265\n",
      "[EPOCH #3, step #1218] loss: 3.1268712693888014\n",
      "[EPOCH #3, step #1220] loss: 3.1262182656318607\n",
      "[EPOCH #3, step #1222] loss: 3.1264780400136605\n",
      "[EPOCH #3, step #1224] loss: 3.1260892679253405\n",
      "[EPOCH #3, step #1226] loss: 3.1262884155559463\n",
      "[EPOCH #3, step #1228] loss: 3.1264060829789586\n",
      "[EPOCH #3, step #1230] loss: 3.127030806460098\n",
      "[EPOCH #3, step #1232] loss: 3.1277887925614407\n",
      "[EPOCH #3, step #1234] loss: 3.1270546463337023\n",
      "[EPOCH #3, step #1236] loss: 3.127201036377693\n",
      "[EPOCH #3, step #1238] loss: 3.127371114333663\n",
      "[EPOCH #3, step #1240] loss: 3.126930795687999\n",
      "[EPOCH #3, step #1242] loss: 3.1265659251715614\n",
      "[EPOCH #3, step #1244] loss: 3.1258785678679684\n",
      "[EPOCH #3, step #1246] loss: 3.1256268414479784\n",
      "[EPOCH #3, step #1248] loss: 3.125613625666158\n",
      "[EPOCH #3, step #1250] loss: 3.125289028497051\n",
      "[EPOCH #3, step #1252] loss: 3.124523230961391\n",
      "[EPOCH #3, step #1254] loss: 3.1250665233429684\n",
      "[EPOCH #3, step #1256] loss: 3.12491136006922\n",
      "[EPOCH #3, step #1258] loss: 3.125155546629211\n",
      "[EPOCH #3, step #1260] loss: 3.1250182822240333\n",
      "[EPOCH #3, step #1262] loss: 3.1253940530551962\n",
      "[EPOCH #3, step #1264] loss: 3.125122364896088\n",
      "[EPOCH #3, step #1266] loss: 3.125267835517709\n",
      "[EPOCH #3, step #1268] loss: 3.1247249801296055\n",
      "[EPOCH #3, step #1270] loss: 3.124557551204642\n",
      "[EPOCH #3, step #1272] loss: 3.12486108343899\n",
      "[EPOCH #3, step #1274] loss: 3.1246596601897596\n",
      "[EPOCH #3, step #1276] loss: 3.1244669602826507\n",
      "[EPOCH #3, step #1278] loss: 3.1243201122328674\n",
      "[EPOCH #3, step #1280] loss: 3.1245989055023076\n",
      "[EPOCH #3, step #1282] loss: 3.123928508996406\n",
      "[EPOCH #3, step #1284] loss: 3.123547637509001\n",
      "[EPOCH #3, step #1286] loss: 3.123739493198884\n",
      "[EPOCH #3, step #1288] loss: 3.1236476252669045\n",
      "[EPOCH #3, step #1290] loss: 3.1237988732380058\n",
      "[EPOCH #3, step #1292] loss: 3.123649608415868\n",
      "[EPOCH #3, step #1294] loss: 3.1237275705374348\n",
      "[EPOCH #3, step #1296] loss: 3.12355299499647\n",
      "[EPOCH #3, step #1298] loss: 3.1237291327983807\n",
      "[EPOCH #3, step #1300] loss: 3.12413970055166\n",
      "[EPOCH #3, step #1302] loss: 3.123406109586644\n",
      "[EPOCH #3, step #1304] loss: 3.123240234660006\n",
      "[EPOCH #3, step #1306] loss: 3.123178467279914\n",
      "[EPOCH #3, step #1308] loss: 3.1227709975654245\n",
      "[EPOCH #3, step #1310] loss: 3.1226061816255526\n",
      "[EPOCH #3, step #1312] loss: 3.122170227135036\n",
      "[EPOCH #3, step #1314] loss: 3.1215930160914085\n",
      "[EPOCH #3, step #1316] loss: 3.1215489147826405\n",
      "[EPOCH #3, step #1318] loss: 3.121366297502785\n",
      "[EPOCH #3, step #1320] loss: 3.1212064866131675\n",
      "[EPOCH #3, step #1322] loss: 3.1208452978372034\n",
      "[EPOCH #3, step #1324] loss: 3.1211318838371422\n",
      "[EPOCH #3, step #1326] loss: 3.121058145140739\n",
      "[EPOCH #3, step #1328] loss: 3.120625534480994\n",
      "[EPOCH #3, step #1330] loss: 3.1202419429323354\n",
      "[EPOCH #3, step #1332] loss: 3.120269473834943\n",
      "[EPOCH #3, step #1334] loss: 3.1207100721780727\n",
      "[EPOCH #3, step #1336] loss: 3.1214333284944407\n",
      "[EPOCH #3, step #1338] loss: 3.1215011877653933\n",
      "[EPOCH #3, step #1340] loss: 3.121723763894715\n",
      "[EPOCH #3, step #1342] loss: 3.120960479695142\n",
      "[EPOCH #3, step #1344] loss: 3.120729208701605\n",
      "[EPOCH #3, step #1346] loss: 3.120384952981293\n",
      "[EPOCH #3, step #1348] loss: 3.12066949377067\n",
      "[EPOCH #3, step #1350] loss: 3.1206362362176203\n",
      "[EPOCH #3, step #1352] loss: 3.121030716920727\n",
      "[EPOCH #3, step #1354] loss: 3.1203858637721775\n",
      "[EPOCH #3, step #1356] loss: 3.1202221825092966\n",
      "[EPOCH #3, step #1358] loss: 3.1207785197616587\n",
      "[EPOCH #3, step #1360] loss: 3.120846532531559\n",
      "[EPOCH #3, step #1362] loss: 3.120395826576216\n",
      "[EPOCH #3, step #1364] loss: 3.120076945063832\n",
      "[EPOCH #3, step #1366] loss: 3.1203720170211513\n",
      "[EPOCH #3, step #1368] loss: 3.1201929098676815\n",
      "[EPOCH #3, step #1370] loss: 3.1201626920595733\n",
      "[EPOCH #3, step #1372] loss: 3.120119488161147\n",
      "[EPOCH #3, step #1374] loss: 3.1201181326779452\n",
      "[EPOCH #3, step #1376] loss: 3.120029098154759\n",
      "[EPOCH #3, step #1378] loss: 3.119423604340031\n",
      "[EPOCH #3, step #1380] loss: 3.1187665973514167\n",
      "[EPOCH #3, step #1382] loss: 3.1188139882711767\n",
      "[EPOCH #3, step #1384] loss: 3.11943083153735\n",
      "[EPOCH #3, step #1386] loss: 3.119897211380115\n",
      "[EPOCH #3, step #1388] loss: 3.1201478533816904\n",
      "[EPOCH #3, step #1390] loss: 3.120328706944272\n",
      "[EPOCH #3, step #1392] loss: 3.1201705764885523\n",
      "[EPOCH #3, step #1394] loss: 3.1203475882075593\n",
      "[EPOCH #3, step #1396] loss: 3.1208241774341254\n",
      "[EPOCH #3, step #1398] loss: 3.1208423187768486\n",
      "[EPOCH #3, step #1400] loss: 3.1207749297327863\n",
      "[EPOCH #3, step #1402] loss: 3.1206220533706763\n",
      "[EPOCH #3, step #1404] loss: 3.1206747234928227\n",
      "[EPOCH #3, step #1406] loss: 3.1209671161830554\n",
      "[EPOCH #3, step #1408] loss: 3.1209746755717744\n",
      "[EPOCH #3, step #1410] loss: 3.1208630562842443\n",
      "[EPOCH #3, step #1412] loss: 3.1210006172574336\n",
      "[EPOCH #3, step #1414] loss: 3.121209614858189\n",
      "[EPOCH #3, step #1416] loss: 3.1204598686942435\n",
      "[EPOCH #3, step #1418] loss: 3.1204539897829986\n",
      "[EPOCH #3, step #1420] loss: 3.120409830785989\n",
      "[EPOCH #3, step #1422] loss: 3.120287827011237\n",
      "[EPOCH #3, step #1424] loss: 3.120314096651579\n",
      "[EPOCH #3, step #1426] loss: 3.120492676879947\n",
      "[EPOCH #3, step #1428] loss: 3.120399483272961\n",
      "[EPOCH #3, step #1430] loss: 3.1208662198691166\n",
      "[EPOCH #3, step #1432] loss: 3.121459020074231\n",
      "[EPOCH #3, step #1434] loss: 3.1215060561376164\n",
      "[EPOCH #3, step #1436] loss: 3.1217776131281525\n",
      "[EPOCH #3, step #1438] loss: 3.121753241139373\n",
      "[EPOCH #3, step #1440] loss: 3.121569946852928\n",
      "[EPOCH #3, step #1442] loss: 3.121504665495039\n",
      "[EPOCH #3, step #1444] loss: 3.1217666804171764\n",
      "[EPOCH #3, step #1446] loss: 3.121963187588769\n",
      "[EPOCH #3, step #1448] loss: 3.1217432152081392\n",
      "[EPOCH #3, step #1450] loss: 3.121408286544884\n",
      "[EPOCH #3, step #1452] loss: 3.121125480017823\n",
      "[EPOCH #3, step #1454] loss: 3.12112252736829\n",
      "[EPOCH #3, step #1456] loss: 3.12166132203667\n",
      "[EPOCH #3, step #1458] loss: 3.121331252326207\n",
      "[EPOCH #3, step #1460] loss: 3.121518264642568\n",
      "[EPOCH #3, step #1462] loss: 3.1213880817285564\n",
      "[EPOCH #3, step #1464] loss: 3.1211654479186284\n",
      "[EPOCH #3, step #1466] loss: 3.120773621782122\n",
      "[EPOCH #3, step #1468] loss: 3.1205623896451447\n",
      "[EPOCH #3, step #1470] loss: 3.12029717440187\n",
      "[EPOCH #3, step #1472] loss: 3.119926366851351\n",
      "[EPOCH #3, step #1474] loss: 3.119772907838983\n",
      "[EPOCH #3, step #1476] loss: 3.1198663437132423\n",
      "[EPOCH #3, step #1478] loss: 3.11987369603769\n",
      "[EPOCH #3, step #1480] loss: 3.119350447677262\n",
      "[EPOCH #3, step #1482] loss: 3.1195051510914387\n",
      "[EPOCH #3, step #1484] loss: 3.1202663683329366\n",
      "[EPOCH #3, step #1486] loss: 3.119958117894863\n",
      "[EPOCH #3, step #1488] loss: 3.1195620776343618\n",
      "[EPOCH #3, step #1490] loss: 3.119160694375604\n",
      "[EPOCH #3, step #1492] loss: 3.119194103421098\n",
      "[EPOCH #3, step #1494] loss: 3.119678856935788\n",
      "[EPOCH #3, step #1496] loss: 3.119739329408787\n",
      "[EPOCH #3, step #1498] loss: 3.119866392785824\n",
      "[EPOCH #3, step #1500] loss: 3.119770948446885\n",
      "[EPOCH #3, step #1502] loss: 3.1197778382304504\n",
      "[EPOCH #3, step #1504] loss: 3.119103084608566\n",
      "[EPOCH #3, step #1506] loss: 3.1189294173057776\n",
      "[EPOCH #3, step #1508] loss: 3.119503310218088\n",
      "[EPOCH #3, step #1510] loss: 3.1197674673649747\n",
      "[EPOCH #3, step #1512] loss: 3.1197483109387827\n",
      "[EPOCH #3, step #1514] loss: 3.1198606621707627\n",
      "[EPOCH #3, step #1516] loss: 3.1198402027904635\n",
      "[EPOCH #3, step #1518] loss: 3.1194059781293637\n",
      "[EPOCH #3, step #1520] loss: 3.119289688183084\n",
      "[EPOCH #3, step #1522] loss: 3.119360695367414\n",
      "[EPOCH #3, step #1524] loss: 3.1193583275841883\n",
      "[EPOCH #3, step #1526] loss: 3.119373111843516\n",
      "[EPOCH #3, step #1528] loss: 3.1195129994242077\n",
      "[EPOCH #3, step #1530] loss: 3.1193243027667763\n",
      "[EPOCH #3, step #1532] loss: 3.119318121022363\n",
      "[EPOCH #3, step #1534] loss: 3.1184300661863644\n",
      "[EPOCH #3, step #1536] loss: 3.11822630789812\n",
      "[EPOCH #3, step #1538] loss: 3.1181386208053996\n",
      "[EPOCH #3, step #1540] loss: 3.118485515880399\n",
      "[EPOCH #3, step #1542] loss: 3.118923967871972\n",
      "[EPOCH #3, step #1544] loss: 3.118835795813008\n",
      "[EPOCH #3, step #1546] loss: 3.118954220046208\n",
      "[EPOCH #3, step #1548] loss: 3.118901428520641\n",
      "[EPOCH #3, step #1550] loss: 3.1189626997167412\n",
      "[EPOCH #3, step #1552] loss: 3.1190641059924613\n",
      "[EPOCH #3, step #1554] loss: 3.119202464799789\n",
      "[EPOCH #3, step #1556] loss: 3.1188061999907153\n",
      "[EPOCH #3, step #1558] loss: 3.1185555577354602\n",
      "[EPOCH #3, step #1560] loss: 3.1186440462946967\n",
      "[EPOCH #3, step #1562] loss: 3.11900403342488\n",
      "[EPOCH #3, step #1564] loss: 3.119032342136858\n",
      "[EPOCH #3, step #1566] loss: 3.1191842692934486\n",
      "[EPOCH #3, step #1568] loss: 3.118842688542859\n",
      "[EPOCH #3, step #1570] loss: 3.118711501454791\n",
      "[EPOCH #3, step #1572] loss: 3.118762260929309\n",
      "[EPOCH #3, step #1574] loss: 3.118940062144446\n",
      "[EPOCH #3, step #1576] loss: 3.1186041532624125\n",
      "[EPOCH #3, step #1578] loss: 3.118641530206643\n",
      "[EPOCH #3, step #1580] loss: 3.1192243554635866\n",
      "[EPOCH #3, step #1582] loss: 3.119482715748867\n",
      "[EPOCH #3, step #1584] loss: 3.119735029593627\n",
      "[EPOCH #3, step #1586] loss: 3.119849031498842\n",
      "[EPOCH #3, step #1588] loss: 3.119585728330234\n",
      "[EPOCH #3, step #1590] loss: 3.1194823678374517\n",
      "[EPOCH #3, step #1592] loss: 3.1194489368910467\n",
      "[EPOCH #3, step #1594] loss: 3.1191535590958073\n",
      "[EPOCH #3, step #1596] loss: 3.119478924989551\n",
      "[EPOCH #3, step #1598] loss: 3.119695213752064\n",
      "[EPOCH #3, step #1600] loss: 3.119872823199356\n",
      "[EPOCH #3, step #1602] loss: 3.1205734532844702\n",
      "[EPOCH #3, step #1604] loss: 3.120666844822536\n",
      "[EPOCH #3, step #1606] loss: 3.1207459234048955\n",
      "[EPOCH #3, step #1608] loss: 3.1207039913381385\n",
      "[EPOCH #3, step #1610] loss: 3.120710516183144\n",
      "[EPOCH #3, step #1612] loss: 3.12038706757604\n",
      "[EPOCH #3, step #1614] loss: 3.120119117583284\n",
      "[EPOCH #3, step #1616] loss: 3.120145506083265\n",
      "[EPOCH #3, step #1618] loss: 3.1199825119280096\n",
      "[EPOCH #3, step #1620] loss: 3.1196630467315427\n",
      "[EPOCH #3, step #1622] loss: 3.1195368965099566\n",
      "[EPOCH #3, step #1624] loss: 3.1198295684227575\n",
      "[EPOCH #3, step #1626] loss: 3.1200586374947537\n",
      "[EPOCH #3, step #1628] loss: 3.120043348431807\n",
      "[EPOCH #3, step #1630] loss: 3.1205970182658556\n",
      "[EPOCH #3, step #1632] loss: 3.1205957570984113\n",
      "[EPOCH #3, step #1634] loss: 3.120517759148134\n",
      "[EPOCH #3, step #1636] loss: 3.1204537143777182\n",
      "[EPOCH #3, step #1638] loss: 3.1203631784510075\n",
      "[EPOCH #3, step #1640] loss: 3.1200260887924745\n",
      "[EPOCH #3, step #1642] loss: 3.119738328188422\n",
      "[EPOCH #3, step #1644] loss: 3.1194431151300215\n",
      "[EPOCH #3, step #1646] loss: 3.1199887859944364\n",
      "[EPOCH #3, step #1648] loss: 3.1200714472787463\n",
      "[EPOCH #3, step #1650] loss: 3.119551978784211\n",
      "[EPOCH #3, step #1652] loss: 3.1195360610359595\n",
      "[EPOCH #3, step #1654] loss: 3.1199642842629887\n",
      "[EPOCH #3, step #1656] loss: 3.119722127482772\n",
      "[EPOCH #3, step #1658] loss: 3.119570936762906\n",
      "[EPOCH #3, step #1660] loss: 3.1196498184732326\n",
      "[EPOCH #3, step #1662] loss: 3.119574938204657\n",
      "[EPOCH #3, step #1664] loss: 3.1195575149925623\n",
      "[EPOCH #3, step #1666] loss: 3.119472080458405\n",
      "[EPOCH #3, step #1668] loss: 3.1194054767569788\n",
      "[EPOCH #3, step #1670] loss: 3.1194203270901326\n",
      "[EPOCH #3, step #1672] loss: 3.1195290407875094\n",
      "[EPOCH #3, step #1674] loss: 3.119775185086834\n",
      "[EPOCH #3, step #1676] loss: 3.1194971935622524\n",
      "[EPOCH #3, step #1678] loss: 3.119647981440332\n",
      "[EPOCH #3, step #1680] loss: 3.119614018775536\n",
      "[EPOCH #3, step #1682] loss: 3.119785149042194\n",
      "[EPOCH #3, step #1684] loss: 3.1195106026680604\n",
      "[EPOCH #3, step #1686] loss: 3.1199011445116134\n",
      "[EPOCH #3, step #1688] loss: 3.120056799278237\n",
      "[EPOCH #3, step #1690] loss: 3.1198567550327563\n",
      "[EPOCH #3, step #1692] loss: 3.120348927964843\n",
      "[EPOCH #3, step #1694] loss: 3.1200199222846017\n",
      "[EPOCH #3, step #1696] loss: 3.120463143395338\n",
      "[EPOCH #3, step #1698] loss: 3.120659943255626\n",
      "[EPOCH #3, step #1700] loss: 3.120803103735698\n",
      "[EPOCH #3, step #1702] loss: 3.1205240567151895\n",
      "[EPOCH #3, step #1704] loss: 3.12066792127324\n",
      "[EPOCH #3, step #1706] loss: 3.120685372774407\n",
      "[EPOCH #3, step #1708] loss: 3.1208160926613493\n",
      "[EPOCH #3, step #1710] loss: 3.120458959627402\n",
      "[EPOCH #3, step #1712] loss: 3.12023743606073\n",
      "[EPOCH #3, step #1714] loss: 3.1205568059192803\n",
      "[EPOCH #3, step #1716] loss: 3.1209519713440117\n",
      "[EPOCH #3, step #1718] loss: 3.1206867080431056\n",
      "[EPOCH #3, step #1720] loss: 3.1207010462005633\n",
      "[EPOCH #3, step #1722] loss: 3.120819844275246\n",
      "[EPOCH #3, step #1724] loss: 3.1210016158007194\n",
      "[EPOCH #3, step #1726] loss: 3.1207441759026664\n",
      "[EPOCH #3, step #1728] loss: 3.1205695262873636\n",
      "[EPOCH #3, step #1730] loss: 3.1202490660305453\n",
      "[EPOCH #3, step #1732] loss: 3.119748441868962\n",
      "[EPOCH #3, step #1734] loss: 3.1193421858531942\n",
      "[EPOCH #3, step #1736] loss: 3.1197983171312575\n",
      "[EPOCH #3, step #1738] loss: 3.119468031565714\n",
      "[EPOCH #3, step #1740] loss: 3.119590042926059\n",
      "[EPOCH #3, step #1742] loss: 3.1193402192405224\n",
      "[EPOCH #3, step #1744] loss: 3.1194340050049703\n",
      "[EPOCH #3, step #1746] loss: 3.119760265333965\n",
      "[EPOCH #3, step #1748] loss: 3.1197778805110574\n",
      "[EPOCH #3, step #1750] loss: 3.119671866527494\n",
      "[EPOCH #3, step #1752] loss: 3.120054814588391\n",
      "[EPOCH #3, step #1754] loss: 3.1199353852502982\n",
      "[EPOCH #3, step #1756] loss: 3.1197174402822063\n",
      "[EPOCH #3, step #1758] loss: 3.119647575278659\n",
      "[EPOCH #3, step #1760] loss: 3.1194509189958803\n",
      "[EPOCH #3, step #1762] loss: 3.1193418803028403\n",
      "[EPOCH #3, step #1764] loss: 3.1192999629055813\n",
      "[EPOCH #3, step #1766] loss: 3.1194923091353033\n",
      "[EPOCH #3, step #1768] loss: 3.119456005608584\n",
      "[EPOCH #3, step #1770] loss: 3.1196767598609343\n",
      "[EPOCH #3, step #1772] loss: 3.1194683112334345\n",
      "[EPOCH #3, step #1774] loss: 3.1198097492271746\n",
      "[EPOCH #3, step #1776] loss: 3.1197001971618876\n",
      "[EPOCH #3, step #1778] loss: 3.119888349488051\n",
      "[EPOCH #3, step #1780] loss: 3.119781136713558\n",
      "[EPOCH #3, step #1782] loss: 3.1201796249532996\n",
      "[EPOCH #3, step #1784] loss: 3.119793235287279\n",
      "[EPOCH #3, step #1786] loss: 3.1198648819392396\n",
      "[EPOCH #3, step #1788] loss: 3.11982092636134\n",
      "[EPOCH #3, step #1790] loss: 3.120255626426071\n",
      "[EPOCH #3, step #1792] loss: 3.120275584400132\n",
      "[EPOCH #3, step #1794] loss: 3.120419598821146\n",
      "[EPOCH #3, step #1796] loss: 3.120512993918171\n",
      "[EPOCH #3, step #1798] loss: 3.120630687975499\n",
      "[EPOCH #3, step #1800] loss: 3.120717436521997\n",
      "[EPOCH #3, step #1802] loss: 3.1204500741847543\n",
      "[EPOCH #3, step #1804] loss: 3.1204074451467667\n",
      "[EPOCH #3, step #1806] loss: 3.1205326761405114\n",
      "[EPOCH #3, step #1808] loss: 3.120472856751458\n",
      "[EPOCH #3, step #1810] loss: 3.1202942198116697\n",
      "[EPOCH #3, step #1812] loss: 3.1199768997987727\n",
      "[EPOCH #3, step #1814] loss: 3.120038279488724\n",
      "[EPOCH #3, step #1816] loss: 3.120032988082726\n",
      "[EPOCH #3, step #1818] loss: 3.1202290568003095\n",
      "[EPOCH #3, step #1820] loss: 3.120619101160375\n",
      "[EPOCH #3, step #1822] loss: 3.120424160248489\n",
      "[EPOCH #3, step #1824] loss: 3.120658019209561\n",
      "[EPOCH #3, step #1826] loss: 3.1207396355103323\n",
      "[EPOCH #3, step #1828] loss: 3.120525253477352\n",
      "[EPOCH #3, step #1830] loss: 3.120800906097479\n",
      "[EPOCH #3, step #1832] loss: 3.121008522056584\n",
      "[EPOCH #3, step #1834] loss: 3.12081780017884\n",
      "[EPOCH #3, step #1836] loss: 3.1210674545202948\n",
      "[EPOCH #3, step #1838] loss: 3.1210967803403307\n",
      "[EPOCH #3, step #1840] loss: 3.1213697795308457\n",
      "[EPOCH #3, step #1842] loss: 3.121289344954219\n",
      "[EPOCH #3, step #1844] loss: 3.121576479392323\n",
      "[EPOCH #3, step #1846] loss: 3.121298741055103\n",
      "[EPOCH #3, step #1848] loss: 3.121429899694341\n",
      "[EPOCH #3, step #1850] loss: 3.1212500105156633\n",
      "[EPOCH #3, step #1852] loss: 3.1214367216236836\n",
      "[EPOCH #3, step #1854] loss: 3.1216170119468094\n",
      "[EPOCH #3, step #1856] loss: 3.121672390094032\n",
      "[EPOCH #3, step #1858] loss: 3.1216763474083256\n",
      "[EPOCH #3, step #1860] loss: 3.121841289355254\n",
      "[EPOCH #3, step #1862] loss: 3.1213061385940493\n",
      "[EPOCH #3, step #1864] loss: 3.1211451070238376\n",
      "[EPOCH #3, step #1866] loss: 3.121318611028556\n",
      "[EPOCH #3, step #1868] loss: 3.12170600457393\n",
      "[EPOCH #3, step #1870] loss: 3.122186395970339\n",
      "[EPOCH #3, step #1872] loss: 3.122336626180149\n",
      "[EPOCH #3, step #1874] loss: 3.1223662415822346\n",
      "[EPOCH #3, step #1876] loss: 3.122545177147753\n",
      "[EPOCH #3, step #1878] loss: 3.1222502256467535\n",
      "[EPOCH #3, step #1880] loss: 3.1224694767890617\n",
      "[EPOCH #3, step #1882] loss: 3.122391746579746\n",
      "[EPOCH #3, step #1884] loss: 3.1222387836213454\n",
      "[EPOCH #3, step #1886] loss: 3.122411448802655\n",
      "[EPOCH #3, step #1888] loss: 3.1222962920409527\n",
      "[EPOCH #3, step #1890] loss: 3.1226057721344267\n",
      "[EPOCH #3, step #1892] loss: 3.122685631179205\n",
      "[EPOCH #3, step #1894] loss: 3.1227173829141583\n",
      "[EPOCH #3, step #1896] loss: 3.1228188760794646\n",
      "[EPOCH #3, step #1898] loss: 3.1227212003684786\n",
      "[EPOCH #3, step #1900] loss: 3.1228579570593675\n",
      "[EPOCH #3, step #1902] loss: 3.1227363458885\n",
      "[EPOCH #3, step #1904] loss: 3.1229002956330305\n",
      "[EPOCH #3, step #1906] loss: 3.1227004848104154\n",
      "[EPOCH #3, step #1908] loss: 3.122917301683541\n",
      "[EPOCH #3, step #1910] loss: 3.1231552202123924\n",
      "[EPOCH #3, step #1912] loss: 3.1231959021907794\n",
      "[EPOCH #3, step #1914] loss: 3.1234843000108206\n",
      "[EPOCH #3, step #1916] loss: 3.123256738570188\n",
      "[EPOCH #3, step #1918] loss: 3.1234525793104386\n",
      "[EPOCH #3, step #1920] loss: 3.12370559289268\n",
      "[EPOCH #3, step #1922] loss: 3.123862055894055\n",
      "[EPOCH #3, step #1924] loss: 3.12362487508105\n",
      "[EPOCH #3, step #1926] loss: 3.123516541660667\n",
      "[EPOCH #3, step #1928] loss: 3.1238624972960936\n",
      "[EPOCH #3, step #1930] loss: 3.124043120429159\n",
      "[EPOCH #3, step #1932] loss: 3.1241316636238947\n",
      "[EPOCH #3, step #1934] loss: 3.12420779358817\n",
      "[EPOCH #3, step #1936] loss: 3.1237641600646735\n",
      "[EPOCH #3, step #1938] loss: 3.123493166914178\n",
      "[EPOCH #3, step #1940] loss: 3.1231951937117817\n",
      "[EPOCH #3, step #1942] loss: 3.122833592218035\n",
      "[EPOCH #3, step #1944] loss: 3.122747152813909\n",
      "[EPOCH #3, step #1946] loss: 3.12289056719176\n",
      "[EPOCH #3, step #1948] loss: 3.122823575877605\n",
      "[EPOCH #3, step #1950] loss: 3.1229060767920185\n",
      "[EPOCH #3, step #1952] loss: 3.122768316156609\n",
      "[EPOCH #3, step #1954] loss: 3.122691951688293\n",
      "[EPOCH #3, step #1956] loss: 3.1224317154716217\n",
      "[EPOCH #3, step #1958] loss: 3.122780015603689\n",
      "[EPOCH #3, step #1960] loss: 3.122819628051688\n",
      "[EPOCH #3, step #1962] loss: 3.1227922335122837\n",
      "[EPOCH #3, step #1964] loss: 3.122617104945292\n",
      "[EPOCH #3, step #1966] loss: 3.122667089475856\n",
      "[EPOCH #3, step #1968] loss: 3.122663258053192\n",
      "[EPOCH #3, step #1970] loss: 3.122917816640156\n",
      "[EPOCH #3, step #1972] loss: 3.1228512639752877\n",
      "[EPOCH #3, step #1974] loss: 3.1229152975203114\n",
      "[EPOCH #3, step #1976] loss: 3.1227883095082816\n",
      "[EPOCH #3, step #1978] loss: 3.1226716535509444\n",
      "[EPOCH #3, step #1980] loss: 3.1226306581424983\n",
      "[EPOCH #3, step #1982] loss: 3.1228534557334835\n",
      "[EPOCH #3, step #1984] loss: 3.1229423426861125\n",
      "[EPOCH #3, step #1986] loss: 3.122497017802338\n",
      "[EPOCH #3, step #1988] loss: 3.122416706046847\n",
      "[EPOCH #3, step #1990] loss: 3.122271462818416\n",
      "[EPOCH #3, step #1992] loss: 3.122266822165122\n",
      "[EPOCH #3, step #1994] loss: 3.1221172559828987\n",
      "[EPOCH #3, step #1996] loss: 3.122270073482378\n",
      "[EPOCH #3, step #1998] loss: 3.1221049684712505\n",
      "[EPOCH #3, step #2000] loss: 3.1216368541784254\n",
      "[EPOCH #3, step #2002] loss: 3.121715877154917\n",
      "[EPOCH #3, step #2004] loss: 3.122238078914081\n",
      "[EPOCH #3, step #2006] loss: 3.1220073998122886\n",
      "[EPOCH #3, step #2008] loss: 3.121900679639111\n",
      "[EPOCH #3, step #2010] loss: 3.1222366455595982\n",
      "[EPOCH #3, step #2012] loss: 3.1221830226625307\n",
      "[EPOCH #3, step #2014] loss: 3.1221175768830935\n",
      "[EPOCH #3, step #2016] loss: 3.1220272088559122\n",
      "[EPOCH #3, step #2018] loss: 3.1223771067879587\n",
      "[EPOCH #3, step #2020] loss: 3.1221418203781406\n",
      "[EPOCH #3, step #2022] loss: 3.122207532940203\n",
      "[EPOCH #3, step #2024] loss: 3.1219416793776147\n",
      "[EPOCH #3, step #2026] loss: 3.1221889386226462\n",
      "[EPOCH #3, step #2028] loss: 3.1225582768377733\n",
      "[EPOCH #3, step #2030] loss: 3.1226719996425567\n",
      "[EPOCH #3, step #2032] loss: 3.1223101669851925\n",
      "[EPOCH #3, step #2034] loss: 3.122279673419362\n",
      "[EPOCH #3, step #2036] loss: 3.1220714493481387\n",
      "[EPOCH #3, step #2038] loss: 3.121524321442436\n",
      "[EPOCH #3, step #2040] loss: 3.1213454161713603\n",
      "[EPOCH #3, step #2042] loss: 3.121093706820913\n",
      "[EPOCH #3, step #2044] loss: 3.1210050212141938\n",
      "[EPOCH #3, step #2046] loss: 3.1212687378227857\n",
      "[EPOCH #3, step #2048] loss: 3.121301376162185\n",
      "[EPOCH #3, step #2050] loss: 3.1209119696084726\n",
      "[EPOCH #3, step #2052] loss: 3.120559843121885\n",
      "[EPOCH #3, step #2054] loss: 3.1204067345083195\n",
      "[EPOCH #3, step #2056] loss: 3.1201232729111124\n",
      "[EPOCH #3, step #2058] loss: 3.1199640675387723\n",
      "[EPOCH #3, step #2060] loss: 3.1200404873986316\n",
      "[EPOCH #3, step #2062] loss: 3.1201875729868265\n",
      "[EPOCH #3, step #2064] loss: 3.1205986634870997\n",
      "[EPOCH #3, step #2066] loss: 3.1205575122427236\n",
      "[EPOCH #3, step #2068] loss: 3.120507161706009\n",
      "[EPOCH #3, step #2070] loss: 3.120274545604166\n",
      "[EPOCH #3, step #2072] loss: 3.120573932171098\n",
      "[EPOCH #3, step #2074] loss: 3.12056387200413\n",
      "[EPOCH #3, step #2076] loss: 3.1202441797729406\n",
      "[EPOCH #3, step #2078] loss: 3.1197322557154843\n",
      "[EPOCH #3, step #2080] loss: 3.1200026338459037\n",
      "[EPOCH #3, step #2082] loss: 3.1200155558290814\n",
      "[EPOCH #3, step #2084] loss: 3.1204190381139303\n",
      "[EPOCH #3, step #2086] loss: 3.1200522913793427\n",
      "[EPOCH #3, step #2088] loss: 3.1198390937410743\n",
      "[EPOCH #3, step #2090] loss: 3.1197534555000077\n",
      "[EPOCH #3, step #2092] loss: 3.1198684616745003\n",
      "[EPOCH #3, step #2094] loss: 3.119925539021276\n",
      "[EPOCH #3, step #2096] loss: 3.1198145508709327\n",
      "[EPOCH #3, step #2098] loss: 3.119852904650982\n",
      "[EPOCH #3, step #2100] loss: 3.119221961594717\n",
      "[EPOCH #3, step #2102] loss: 3.1196448423156156\n",
      "[EPOCH #3, step #2104] loss: 3.1197250204244873\n",
      "[EPOCH #3, step #2106] loss: 3.1196903952728476\n",
      "[EPOCH #3, step #2108] loss: 3.1200973760352535\n",
      "[EPOCH #3, step #2110] loss: 3.120544275250383\n",
      "[EPOCH #3, step #2112] loss: 3.1206494379652883\n",
      "[EPOCH #3, step #2114] loss: 3.1207577280400773\n",
      "[EPOCH #3, step #2116] loss: 3.12047127231315\n",
      "[EPOCH #3, step #2118] loss: 3.1200757127935566\n",
      "[EPOCH #3, step #2120] loss: 3.119915979080524\n",
      "[EPOCH #3, step #2122] loss: 3.119690471137968\n",
      "[EPOCH #3, step #2124] loss: 3.119829055561739\n",
      "[EPOCH #3, step #2126] loss: 3.1199756601241253\n",
      "[EPOCH #3, step #2128] loss: 3.1202203760465137\n",
      "[EPOCH #3, step #2130] loss: 3.119814930136693\n",
      "[EPOCH #3, step #2132] loss: 3.1195204553054188\n",
      "[EPOCH #3, step #2134] loss: 3.1192696455211775\n",
      "[EPOCH #3, step #2136] loss: 3.119020394872827\n",
      "[EPOCH #3, step #2138] loss: 3.1188111817964943\n",
      "[EPOCH #3, step #2140] loss: 3.118721853734622\n",
      "[EPOCH #3, step #2142] loss: 3.118978876295459\n",
      "[EPOCH #3, step #2144] loss: 3.1187974645263385\n",
      "[EPOCH #3, step #2146] loss: 3.1185316729667747\n",
      "[EPOCH #3, step #2148] loss: 3.1182309215487076\n",
      "[EPOCH #3, step #2150] loss: 3.118186534254676\n",
      "[EPOCH #3, step #2152] loss: 3.1179588928479567\n",
      "[EPOCH #3, step #2154] loss: 3.1183297400684755\n",
      "[EPOCH #3, step #2156] loss: 3.1185043011763938\n",
      "[EPOCH #3, step #2158] loss: 3.1187160173021238\n",
      "[EPOCH #3, step #2160] loss: 3.118533909403577\n",
      "[EPOCH #3, step #2162] loss: 3.11839531055491\n",
      "[EPOCH #3, step #2164] loss: 3.1182968836733704\n",
      "[EPOCH #3, step #2166] loss: 3.117858589878267\n",
      "[EPOCH #3, step #2168] loss: 3.1178111450745796\n",
      "[EPOCH #3, step #2170] loss: 3.1178871854995593\n",
      "[EPOCH #3, step #2172] loss: 3.1179619509121594\n",
      "[EPOCH #3, step #2174] loss: 3.117785289808251\n",
      "[EPOCH #3, step #2176] loss: 3.1177465216961266\n",
      "[EPOCH #3, step #2178] loss: 3.117757579392936\n",
      "[EPOCH #3, step #2180] loss: 3.1178304824409326\n",
      "[EPOCH #3, step #2182] loss: 3.1178081791388896\n",
      "[EPOCH #3, step #2184] loss: 3.1177153682272274\n",
      "[EPOCH #3, step #2186] loss: 3.1178787270255106\n",
      "[EPOCH #3, step #2188] loss: 3.1176574653879916\n",
      "[EPOCH #3, step #2190] loss: 3.11778122276435\n",
      "[EPOCH #3, step #2192] loss: 3.1179278362194154\n",
      "[EPOCH #3, step #2194] loss: 3.1176185215795775\n",
      "[EPOCH #3, step #2196] loss: 3.1176915616950067\n",
      "[EPOCH #3, step #2198] loss: 3.1175660706477144\n",
      "[EPOCH #3, step #2200] loss: 3.1176457811300997\n",
      "[EPOCH #3, step #2202] loss: 3.117429008821547\n",
      "[EPOCH #3, step #2204] loss: 3.1173600723413655\n",
      "[EPOCH #3, step #2206] loss: 3.117457586767234\n",
      "[EPOCH #3, step #2208] loss: 3.117172604397229\n",
      "[EPOCH #3, step #2210] loss: 3.1168335330588737\n",
      "[EPOCH #3, step #2212] loss: 3.1167258247766783\n",
      "[EPOCH #3, step #2214] loss: 3.1167581467811463\n",
      "[EPOCH #3, step #2216] loss: 3.116765769473271\n",
      "[EPOCH #3, step #2218] loss: 3.116730673563486\n",
      "[EPOCH #3, step #2220] loss: 3.117158133293368\n",
      "[EPOCH #3, step #2222] loss: 3.1170528170979885\n",
      "[EPOCH #3, step #2224] loss: 3.1170962130085806\n",
      "[EPOCH #3, step #2226] loss: 3.1170667921099686\n",
      "[EPOCH #3, step #2228] loss: 3.117331116014449\n",
      "[EPOCH #3, step #2230] loss: 3.117205616285426\n",
      "[EPOCH #3, step #2232] loss: 3.117010961559072\n",
      "[EPOCH #3, step #2234] loss: 3.1175599451299747\n",
      "[EPOCH #3, step #2236] loss: 3.1174414379553013\n",
      "[EPOCH #3, step #2238] loss: 3.117717600656759\n",
      "[EPOCH #3, step #2240] loss: 3.1177850154721805\n",
      "[EPOCH #3, step #2242] loss: 3.1180650918971624\n",
      "[EPOCH #3, step #2244] loss: 3.117512845886842\n",
      "[EPOCH #3, step #2246] loss: 3.117607027570778\n",
      "[EPOCH #3, step #2248] loss: 3.117615349932001\n",
      "[EPOCH #3, step #2250] loss: 3.117559383308448\n",
      "[EPOCH #3, step #2252] loss: 3.117287295371545\n",
      "[EPOCH #3, step #2254] loss: 3.1174466220872628\n",
      "[EPOCH #3, step #2256] loss: 3.117454321869295\n",
      "[EPOCH #3, step #2258] loss: 3.1172962970784304\n",
      "[EPOCH #3, step #2260] loss: 3.1170326083806277\n",
      "[EPOCH #3, step #2262] loss: 3.1169266653334837\n",
      "[EPOCH #3, step #2264] loss: 3.1168684954148516\n",
      "[EPOCH #3, step #2266] loss: 3.1165099928544175\n",
      "[EPOCH #3, step #2268] loss: 3.1166062241558885\n",
      "[EPOCH #3, step #2270] loss: 3.1168027558236435\n",
      "[EPOCH #3, step #2272] loss: 3.1172311039575122\n",
      "[EPOCH #3, step #2274] loss: 3.11739760021587\n",
      "[EPOCH #3, step #2276] loss: 3.117379347483317\n",
      "[EPOCH #3, step #2278] loss: 3.1171752277724103\n",
      "[EPOCH #3, step #2280] loss: 3.1171897847837444\n",
      "[EPOCH #3, step #2282] loss: 3.117091907934823\n",
      "[EPOCH #3, step #2284] loss: 3.1169019692836235\n",
      "[EPOCH #3, step #2286] loss: 3.1167356831359196\n",
      "[EPOCH #3, step #2288] loss: 3.1169284962837014\n",
      "[EPOCH #3, step #2290] loss: 3.117032266818296\n",
      "[EPOCH #3, step #2292] loss: 3.116864728553296\n",
      "[EPOCH #3, step #2294] loss: 3.1167644970297554\n",
      "[EPOCH #3, step #2296] loss: 3.116951341259516\n",
      "[EPOCH #3, step #2298] loss: 3.117129482358059\n",
      "[EPOCH #3, step #2300] loss: 3.1170333143629647\n",
      "[EPOCH #3, step #2302] loss: 3.1169739006604003\n",
      "[EPOCH #3, step #2304] loss: 3.117143916049386\n",
      "[EPOCH #3, step #2306] loss: 3.1171628179413755\n",
      "[EPOCH #3, step #2308] loss: 3.1171267561295064\n",
      "[EPOCH #3, step #2310] loss: 3.116961177114489\n",
      "[EPOCH #3, step #2312] loss: 3.117337504661913\n",
      "[EPOCH #3, step #2314] loss: 3.117176078668681\n",
      "[EPOCH #3, step #2316] loss: 3.1175429499967375\n",
      "[EPOCH #3, step #2318] loss: 3.117508987354379\n",
      "[EPOCH #3, step #2320] loss: 3.1174178000207298\n",
      "[EPOCH #3, step #2322] loss: 3.117374874218445\n",
      "[EPOCH #3, step #2324] loss: 3.1172667191618233\n",
      "[EPOCH #3, step #2326] loss: 3.116975240867231\n",
      "[EPOCH #3, step #2328] loss: 3.1170052491118847\n",
      "[EPOCH #3, step #2330] loss: 3.1169727408083046\n",
      "[EPOCH #3, step #2332] loss: 3.116859344673729\n",
      "[EPOCH #3, step #2334] loss: 3.1172074529292497\n",
      "[EPOCH #3, step #2336] loss: 3.1168465906093403\n",
      "[EPOCH #3, step #2338] loss: 3.11700700164198\n",
      "[EPOCH #3, step #2340] loss: 3.11683634232884\n",
      "[EPOCH #3, step #2342] loss: 3.1168362892849344\n",
      "[EPOCH #3, step #2344] loss: 3.116428848827826\n",
      "[EPOCH #3, step #2346] loss: 3.1165778074154815\n",
      "[EPOCH #3, step #2348] loss: 3.11633401944212\n",
      "[EPOCH #3, step #2350] loss: 3.11630383566155\n",
      "[EPOCH #3, step #2352] loss: 3.1162578395919702\n",
      "[EPOCH #3, step #2354] loss: 3.1164921589464645\n",
      "[EPOCH #3, step #2356] loss: 3.116334869844407\n",
      "[EPOCH #3, step #2358] loss: 3.116247537025914\n",
      "[EPOCH #3, step #2360] loss: 3.1164741364640234\n",
      "[EPOCH #3, step #2362] loss: 3.116366316391359\n",
      "[EPOCH #3, step #2364] loss: 3.116239201846133\n",
      "[EPOCH #3, step #2366] loss: 3.1164280131339623\n",
      "[EPOCH #3, step #2368] loss: 3.116232470753526\n",
      "[EPOCH #3, step #2370] loss: 3.116209768434458\n",
      "[EPOCH #3, step #2372] loss: 3.116038353192972\n",
      "[EPOCH #3, step #2374] loss: 3.115734379015471\n",
      "[EPOCH #3, step #2376] loss: 3.1151612766861265\n",
      "[EPOCH #3, step #2378] loss: 3.1153329061128954\n",
      "[EPOCH #3, step #2380] loss: 3.1151873331418414\n",
      "[EPOCH #3, step #2382] loss: 3.115060450997715\n",
      "[EPOCH #3, step #2384] loss: 3.1152132438163838\n",
      "[EPOCH #3, step #2386] loss: 3.1152215216716166\n",
      "[EPOCH #3, step #2388] loss: 3.115446347779138\n",
      "[EPOCH #3, step #2390] loss: 3.1157404384270015\n",
      "[EPOCH #3, step #2392] loss: 3.115877605610396\n",
      "[EPOCH #3, step #2394] loss: 3.115646491020856\n",
      "[EPOCH #3, step #2396] loss: 3.1155367234770734\n",
      "[EPOCH #3, step #2398] loss: 3.115676167906697\n",
      "[EPOCH #3, step #2400] loss: 3.115573419922841\n",
      "[EPOCH #3, step #2402] loss: 3.11544842616846\n",
      "[EPOCH #3, step #2404] loss: 3.115407975250371\n",
      "[EPOCH #3, step #2406] loss: 3.1157153132152993\n",
      "[EPOCH #3, step #2408] loss: 3.1155296486410773\n",
      "[EPOCH #3, step #2410] loss: 3.1160378640047104\n",
      "[EPOCH #3, step #2412] loss: 3.116011421165672\n",
      "[EPOCH #3, step #2414] loss: 3.1159006221447427\n",
      "[EPOCH #3, step #2416] loss: 3.1155233998707024\n",
      "[EPOCH #3, step #2418] loss: 3.1155830786808703\n",
      "[EPOCH #3, step #2420] loss: 3.1155347359274006\n",
      "[EPOCH #3, step #2422] loss: 3.1157488783577776\n",
      "[EPOCH #3, step #2424] loss: 3.1159526678458933\n",
      "[EPOCH #3, step #2426] loss: 3.1162416360286906\n",
      "[EPOCH #3, step #2428] loss: 3.115875724903985\n",
      "[EPOCH #3, step #2430] loss: 3.1159720390434376\n",
      "[EPOCH #3, step #2432] loss: 3.1162480223193563\n",
      "[EPOCH #3, step #2434] loss: 3.1164231661653616\n",
      "[EPOCH #3, step #2436] loss: 3.1162838972050193\n",
      "[EPOCH #3, step #2438] loss: 3.1162980132632785\n",
      "[EPOCH #3, step #2440] loss: 3.1160792190976827\n",
      "[EPOCH #3, step #2442] loss: 3.1162961740450834\n",
      "[EPOCH #3, step #2444] loss: 3.11624693207458\n",
      "[EPOCH #3, step #2446] loss: 3.116175853821225\n",
      "[EPOCH #3, step #2448] loss: 3.116378611766742\n",
      "[EPOCH #3, step #2450] loss: 3.1163347068002203\n",
      "[EPOCH #3, step #2452] loss: 3.1163197551218773\n",
      "[EPOCH #3, step #2454] loss: 3.116157600107601\n",
      "[EPOCH #3, step #2456] loss: 3.1162366506089803\n",
      "[EPOCH #3, step #2458] loss: 3.1161226119971848\n",
      "[EPOCH #3, step #2460] loss: 3.1164442226684272\n",
      "[EPOCH #3, step #2462] loss: 3.116326154851352\n",
      "[EPOCH #3, step #2464] loss: 3.1162410394656974\n",
      "[EPOCH #3, step #2466] loss: 3.115768591305192\n",
      "[EPOCH #3, step #2468] loss: 3.1156772543612448\n",
      "[EPOCH #3, step #2470] loss: 3.115274887440032\n",
      "[EPOCH #3, step #2472] loss: 3.115214906141819\n",
      "[EPOCH #3, step #2474] loss: 3.1150167378512297\n",
      "[EPOCH #3, step #2476] loss: 3.11500140270207\n",
      "[EPOCH #3, step #2478] loss: 3.1150606078647614\n",
      "[EPOCH #3, step #2480] loss: 3.1151239251187137\n",
      "[EPOCH #3, step #2482] loss: 3.1153538531465874\n",
      "[EPOCH #3, step #2484] loss: 3.1155107209380244\n",
      "[EPOCH #3, step #2486] loss: 3.115833719235901\n",
      "[EPOCH #3, step #2488] loss: 3.1159352898932977\n",
      "[EPOCH #3, step #2490] loss: 3.1157189025212753\n",
      "[EPOCH #3, step #2492] loss: 3.1155135239456917\n",
      "[EPOCH #3, step #2494] loss: 3.1155532464235725\n",
      "[EPOCH #3, step #2496] loss: 3.1157003700804795\n",
      "[EPOCH #3, step #2498] loss: 3.115489513409429\n",
      "[EPOCH #3, step #2500] loss: 3.1151035783387147\n",
      "[EPOCH #3, step #2502] loss: 3.1148842173579023\n",
      "[EPOCH #3, step #2504] loss: 3.1146226372785435\n",
      "[EPOCH #3, step #2506] loss: 3.1147561881662984\n",
      "[EPOCH #3, step #2508] loss: 3.1148183688836157\n",
      "[EPOCH #3, step #2510] loss: 3.114655066846804\n",
      "[EPOCH #3, step #2512] loss: 3.1146291532793513\n",
      "[EPOCH #3, step #2514] loss: 3.114675682155086\n",
      "[EPOCH #3, step #2516] loss: 3.114680254227309\n",
      "[EPOCH #3, step #2518] loss: 3.1147095237094384\n",
      "[EPOCH #3, step #2520] loss: 3.1145731367226963\n",
      "[EPOCH #3, step #2522] loss: 3.1144704877314378\n",
      "[EPOCH #3, step #2524] loss: 3.114599856291667\n",
      "[EPOCH #3, step #2526] loss: 3.1148693468148974\n",
      "[EPOCH #3, step #2528] loss: 3.114969681731888\n",
      "[EPOCH #3, step #2530] loss: 3.1148728322059633\n",
      "[EPOCH #3, step #2532] loss: 3.1146291003507725\n",
      "[EPOCH #3, step #2534] loss: 3.1148646286958774\n",
      "[EPOCH #3, step #2536] loss: 3.11492002654085\n",
      "[EPOCH #3, step #2538] loss: 3.1146128064020733\n",
      "[EPOCH #3, step #2540] loss: 3.114057213243937\n",
      "[EPOCH #3, step #2542] loss: 3.1141354328902557\n",
      "[EPOCH #3, step #2544] loss: 3.114366964366441\n",
      "[EPOCH #3, step #2546] loss: 3.1142838529198134\n",
      "[EPOCH #3, step #2548] loss: 3.1139940300751032\n",
      "[EPOCH #3, step #2550] loss: 3.1142468996395647\n",
      "[EPOCH #3, step #2552] loss: 3.1141890344832674\n",
      "[EPOCH #3, step #2554] loss: 3.1139690556871447\n",
      "[EPOCH #3, step #2556] loss: 3.113950558644885\n",
      "[EPOCH #3, step #2558] loss: 3.113943081155518\n",
      "[EPOCH #3, step #2560] loss: 3.1141373655936624\n",
      "[EPOCH #3, step #2562] loss: 3.1145284607500665\n",
      "[EPOCH #3, step #2564] loss: 3.1143253899921914\n",
      "[EPOCH #3, step #2566] loss: 3.114162890048263\n",
      "[EPOCH #3, step #2568] loss: 3.113903460497056\n",
      "[EPOCH #3, step #2570] loss: 3.1140744023080096\n",
      "[EPOCH #3, step #2572] loss: 3.114127066364281\n",
      "[EPOCH #3, step #2574] loss: 3.1142057730850663\n",
      "[EPOCH #3, step #2576] loss: 3.114290368524595\n",
      "[EPOCH #3, step #2578] loss: 3.113929134415489\n",
      "[EPOCH #3, step #2580] loss: 3.1136156874357015\n",
      "[EPOCH #3, step #2582] loss: 3.113472165961714\n",
      "[EPOCH #3, step #2584] loss: 3.113438734740768\n",
      "[EPOCH #3, step #2586] loss: 3.1133277316696173\n",
      "[EPOCH #3, step #2588] loss: 3.113230395841985\n",
      "[EPOCH #3, step #2590] loss: 3.113042050648361\n",
      "[EPOCH #3, step #2592] loss: 3.1132306233364506\n",
      "[EPOCH #3, step #2594] loss: 3.113046551853246\n",
      "[EPOCH #3, step #2596] loss: 3.113109396107159\n",
      "[EPOCH #3, step #2598] loss: 3.11310313948763\n",
      "[EPOCH #3, step #2600] loss: 3.112969212878534\n",
      "[EPOCH #3, step #2602] loss: 3.113191185023205\n",
      "[EPOCH #3, step #2604] loss: 3.11280393179456\n",
      "[EPOCH #3, step #2606] loss: 3.112692439514968\n",
      "[EPOCH #3, step #2608] loss: 3.1127162879799095\n",
      "[EPOCH #3, step #2610] loss: 3.1127412167603556\n",
      "[EPOCH #3, step #2612] loss: 3.1128302188928765\n",
      "[EPOCH #3, step #2614] loss: 3.1131745799777613\n",
      "[EPOCH #3, step #2616] loss: 3.1132964229693063\n",
      "[EPOCH #3, step #2618] loss: 3.1131777577656923\n",
      "[EPOCH #3, step #2620] loss: 3.113174802972269\n",
      "[EPOCH #3, step #2622] loss: 3.1131238304746094\n",
      "[EPOCH #3, step #2624] loss: 3.1130967255546933\n",
      "[EPOCH #3, step #2626] loss: 3.1131156055383644\n",
      "[EPOCH #3, step #2628] loss: 3.112873364691954\n",
      "[EPOCH #3, step #2630] loss: 3.1125450722394628\n",
      "[EPOCH #3, step #2632] loss: 3.112455198984838\n",
      "[EPOCH #3, step #2634] loss: 3.1125864670444034\n",
      "[EPOCH #3, step #2636] loss: 3.1125732801789625\n",
      "[EPOCH #3, step #2638] loss: 3.1128391662119195\n",
      "[EPOCH #3, step #2640] loss: 3.1131299762552502\n",
      "[EPOCH #3, elapsed time: 1671.532[sec]] loss: 3.1131299762552502\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'coco2014_clf'\n",
    "model.train(dataloader.dataset.trainloader, epochs=epochs, lr=learning_rate, wd=weight_decay, output_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba45fa-de0f-4164-a9f4-8aab6c875907",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb3a83ec-4044-46fe-bdaf-bc2963063a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = model.predict(dataloader.dataset.trainloader)\n",
    "train_predictions, train_labels = train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50fcb205-1cff-48c0-9ad2-1881319b9056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.36412689772686935,\n",
      " 'classification_report': {'0': {'f1-score': 0.5377104284840015,\n",
      "                                 'precision': 0.36954174513496546,\n",
      "                                 'recall': 0.9867578530959804,\n",
      "                                 'support': 29829},\n",
      "                           '1': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 566},\n",
      "                           '10': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 376},\n",
      "                           '12': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 239},\n",
      "                           '13': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 157},\n",
      "                           '14': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1083},\n",
      "                           '15': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 697},\n",
      "                           '16': {'f1-score': 0.0020345879959308244,\n",
      "                                  'precision': 0.06666666666666667,\n",
      "                                  'recall': 0.0010330578512396695,\n",
      "                                  'support': 1936},\n",
      "                           '17': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1521},\n",
      "                           '18': {'f1-score': 0.0026455026455026454,\n",
      "                                  'precision': 0.11764705882352941,\n",
      "                                  'recall': 0.0013377926421404682,\n",
      "                                  'support': 1495},\n",
      "                           '19': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 612},\n",
      "                           '2': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 1217},\n",
      "                           '20': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 801},\n",
      "                           '21': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1394},\n",
      "                           '22': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 484},\n",
      "                           '23': {'f1-score': 0.3547911547911548,\n",
      "                                  'precision': 0.4088335220838052,\n",
      "                                  'recall': 0.3133680555555556,\n",
      "                                  'support': 1152},\n",
      "                           '24': {'f1-score': 0.06236275801493193,\n",
      "                                  'precision': 0.2,\n",
      "                                  'recall': 0.03694068678459938,\n",
      "                                  'support': 1922},\n",
      "                           '26': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 116},\n",
      "                           '27': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 955},\n",
      "                           '3': {'f1-score': 0.10548086866597724,\n",
      "                                 'precision': 0.18888888888888888,\n",
      "                                 'recall': 0.07317073170731707,\n",
      "                                 'support': 1394},\n",
      "                           '30': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 157},\n",
      "                           '31': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 451},\n",
      "                           '32': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 592},\n",
      "                           '33': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 55},\n",
      "                           '34': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 338},\n",
      "                           '35': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 309},\n",
      "                           '36': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 6},\n",
      "                           '37': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 268},\n",
      "                           '38': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 183},\n",
      "                           '39': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 17},\n",
      "                           '4': {'f1-score': 0.3133568350959655,\n",
      "                                 'precision': 0.5822416302765647,\n",
      "                                 'recall': 0.21436227224008575,\n",
      "                                 'support': 1866},\n",
      "                           '40': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 362},\n",
      "                           '41': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 657},\n",
      "                           '42': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 227},\n",
      "                           '43': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 447},\n",
      "                           '45': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 284},\n",
      "                           '46': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 327},\n",
      "                           '47': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 439},\n",
      "                           '48': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 433},\n",
      "                           '49': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 252},\n",
      "                           '5': {'f1-score': 0.013184584178498986,\n",
      "                                 'precision': 0.059907834101382486,\n",
      "                                 'recall': 0.007407407407407408,\n",
      "                                 'support': 1755},\n",
      "                           '50': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 902},\n",
      "                           '51': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 643},\n",
      "                           '52': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 146},\n",
      "                           '53': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 750},\n",
      "                           '54': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 166},\n",
      "                           '55': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 393},\n",
      "                           '56': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 178},\n",
      "                           '57': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 408},\n",
      "                           '58': {'f1-score': 0.18228941684665226,\n",
      "                                  'precision': 0.3528428093645485,\n",
      "                                  'recall': 0.12288875946418171,\n",
      "                                  'support': 1717},\n",
      "                           '59': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 298},\n",
      "                           '6': {'f1-score': 0.03985828166519043,\n",
      "                                 'precision': 0.19823788546255505,\n",
      "                                 'recall': 0.022156573116691284,\n",
      "                                 'support': 2031},\n",
      "                           '60': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 796},\n",
      "                           '61': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1153},\n",
      "                           '62': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1754},\n",
      "                           '63': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 422},\n",
      "                           '64': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1798},\n",
      "                           '66': {'f1-score': 0.051470588235294115,\n",
      "                                  'precision': 0.10475352112676056,\n",
      "                                  'recall': 0.03411697247706422,\n",
      "                                  'support': 3488},\n",
      "                           '69': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 878},\n",
      "                           '7': {'f1-score': 0.017291066282420747,\n",
      "                                 'precision': 0.096,\n",
      "                                 'recall': 0.009501187648456057,\n",
      "                                 'support': 1263},\n",
      "                           '71': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 611},\n",
      "                           '72': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1009},\n",
      "                           '73': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 41},\n",
      "                           '74': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 169},\n",
      "                           '75': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 530},\n",
      "                           '76': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 231},\n",
      "                           '77': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 144},\n",
      "                           '78': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 546},\n",
      "                           '79': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 15},\n",
      "                           '8': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 909},\n",
      "                           '80': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 501},\n",
      "                           '81': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 940},\n",
      "                           '83': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 413},\n",
      "                           '84': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 174},\n",
      "                           '85': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 475},\n",
      "                           '86': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 202},\n",
      "                           '87': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 766},\n",
      "                           '88': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 7},\n",
      "                           '89': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 167},\n",
      "                           '9': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 104},\n",
      "                           'accuracy': 0.36412689772686935,\n",
      "                           'macro avg': {'f1-score': 0.021030950911269015,\n",
      "                                         'precision': 0.03431951952412084,\n",
      "                                         'recall': 0.022788016874883986,\n",
      "                                         'support': 84509},\n",
      "                           'weighted avg': {'f1-score': 0.21212004933200435,\n",
      "                                            'precision': 0.1790741663666343,\n",
      "                                            'recall': 0.36412689772686935,\n",
      "                                            'support': 84509}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train_eval_result = model.evaluate(train_labels, train_predictions)\n",
    "pprint.pprint(train_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7768822-0900-4959-8618-20c89b0f5d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = model.predict(dataloader.dataset.testloader)\n",
    "test_predictions, test_labels = test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9521acc2-29d8-4dbb-81e2-0e264b4155a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3543477495865151,\n",
      " 'classification_report': {'0': {'f1-score': 0.5314964294737962,\n",
      "                                 'precision': 0.36518465727390675,\n",
      "                                 'recall': 0.9759718695057629,\n",
      "                                 'support': 20476},\n",
      "                           '1': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 331},\n",
      "                           '10': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 221},\n",
      "                           '12': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 117},\n",
      "                           '13': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 112},\n",
      "                           '14': {'f1-score': 0.0024009603841536617,\n",
      "                                  'precision': 0.3333333333333333,\n",
      "                                  'recall': 0.0012048192771084338,\n",
      "                                  'support': 830},\n",
      "                           '15': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 440},\n",
      "                           '16': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1122},\n",
      "                           '17': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 872},\n",
      "                           '18': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 731},\n",
      "                           '19': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 397},\n",
      "                           '2': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 870},\n",
      "                           '20': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 487},\n",
      "                           '21': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 750},\n",
      "                           '22': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 273},\n",
      "                           '23': {'f1-score': 0.018163471241170532,\n",
      "                                  'precision': 0.027439024390243903,\n",
      "                                  'recall': 0.013574660633484163,\n",
      "                                  'support': 663},\n",
      "                           '24': {'f1-score': 0.006962576153176674,\n",
      "                                  'precision': 0.02247191011235955,\n",
      "                                  'recall': 0.004119464469618949,\n",
      "                                  'support': 971},\n",
      "                           '26': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 119},\n",
      "                           '27': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 730},\n",
      "                           '3': {'f1-score': 0.009140767824497256,\n",
      "                                 'precision': 0.023474178403755867,\n",
      "                                 'recall': 0.0056753688989784334,\n",
      "                                 'support': 881},\n",
      "                           '30': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 161},\n",
      "                           '31': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 345},\n",
      "                           '32': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 462},\n",
      "                           '33': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 41},\n",
      "                           '34': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 311},\n",
      "                           '35': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 155},\n",
      "                           '36': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 20},\n",
      "                           '37': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 254},\n",
      "                           '38': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 82},\n",
      "                           '39': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 8},\n",
      "                           '4': {'f1-score': 0.1141011840688913,\n",
      "                                 'precision': 0.22268907563025211,\n",
      "                                 'recall': 0.07670043415340087,\n",
      "                                 'support': 691},\n",
      "                           '40': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 157},\n",
      "                           '41': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 502},\n",
      "                           '42': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 100},\n",
      "                           '43': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 379},\n",
      "                           '45': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 212},\n",
      "                           '46': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 284},\n",
      "                           '47': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 244},\n",
      "                           '48': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 241},\n",
      "                           '49': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 150},\n",
      "                           '5': {'f1-score': 0.007850834151128557,\n",
      "                                 'precision': 0.04,\n",
      "                                 'recall': 0.004352557127312296,\n",
      "                                 'support': 919},\n",
      "                           '50': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 774},\n",
      "                           '51': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 423},\n",
      "                           '52': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 146},\n",
      "                           '53': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 549},\n",
      "                           '54': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 172},\n",
      "                           '55': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 275},\n",
      "                           '56': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 178},\n",
      "                           '57': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 289},\n",
      "                           '58': {'f1-score': 0.01816680429397193,\n",
      "                                  'precision': 0.05238095238095238,\n",
      "                                  'recall': 0.01098901098901099,\n",
      "                                  'support': 1001},\n",
      "                           '59': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 250},\n",
      "                           '6': {'f1-score': 0.015232292460015232,\n",
      "                                 'precision': 0.09433962264150944,\n",
      "                                 'recall': 0.008285004142502071,\n",
      "                                 'support': 1207},\n",
      "                           '60': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 477},\n",
      "                           '61': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1248},\n",
      "                           '62': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1086},\n",
      "                           '63': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 432},\n",
      "                           '64': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 1228},\n",
      "                           '66': {'f1-score': 0.029283329052144876,\n",
      "                                  'precision': 0.09105431309904154,\n",
      "                                  'recall': 0.017447199265381085,\n",
      "                                  'support': 3267},\n",
      "                           '69': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 584},\n",
      "                           '7': {'f1-score': 0.002325581395348837,\n",
      "                                 'precision': 0.016666666666666666,\n",
      "                                 'recall': 0.00125,\n",
      "                                 'support': 800},\n",
      "                           '71': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 373},\n",
      "                           '72': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 562},\n",
      "                           '73': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 31},\n",
      "                           '74': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 104},\n",
      "                           '75': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 299},\n",
      "                           '76': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 211},\n",
      "                           '77': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 92},\n",
      "                           '78': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 456},\n",
      "                           '79': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 7},\n",
      "                           '8': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 662},\n",
      "                           '80': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 325},\n",
      "                           '81': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 561},\n",
      "                           '83': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 408},\n",
      "                           '84': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 118},\n",
      "                           '85': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 279},\n",
      "                           '86': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 133},\n",
      "                           '87': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 514},\n",
      "                           '88': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 15},\n",
      "                           '89': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 103},\n",
      "                           '9': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 84},\n",
      "                           'accuracy': 0.3543477495865151,\n",
      "                           'macro avg': {'f1-score': 0.009439052881228687,\n",
      "                                         'precision': 0.01611292167415027,\n",
      "                                         'recall': 0.013994629855782004,\n",
      "                                         'support': 56834},\n",
      "                           'weighted avg': {'f1-score': 0.1958674038490858,\n",
      "                                            'precision': 0.1492526998173763,\n",
      "                                            'recall': 0.3543477495865151,\n",
      "                                            'support': 56834}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_eval_result = model.evaluate(test_labels, test_predictions)\n",
    "pprint.pprint(test_eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
