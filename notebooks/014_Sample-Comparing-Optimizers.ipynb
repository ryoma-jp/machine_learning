{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31af5551-6b9f-4f84-988b-822f257fed68",
   "metadata": {},
   "source": [
    "# Comparing Optimizers\n",
    "\n",
    "This notebook explains about optimizers with comparing performance.\n",
    "\n",
    "|Method|Formula|Reference|\n",
    "|:--|:--|:--|\n",
    "|SGD|$w_{t+1} = w_{t} - \\alpha \\nabla L_{t}$ <br> where $w$ is the paramter, $t$ is the time, $\\alpha$ is the learning rate and $\\nabla L$ is the gradient of loss function|[【機械学習】Optimizer(最適化関数) – SGD、Momentum、AdaGrad、RMSProp、Adamとは何か](https://chefyushima.com/ai-ml_optimizer/2799/)|\n",
    "|Momentum|$w_{t+1} = w_{t} + V_{t}$ <br> $V_{t} = \\beta v_{t-1} - \\alpha \\nabla L_{t}$ <br> where $w$ is the paramter, $t$ is the time, $\\alpha$ is the learning rate, $\\beta$ is the coefficient of momentum and $\\nabla L$ is the gradient of loss function <br> $V_{0}=0.0$|[【機械学習】Optimizer(最適化関数) – SGD、Momentum、AdaGrad、RMSProp、Adamとは何か](https://chefyushima.com/ai-ml_optimizer/2799/)|\n",
    "|Adagrad|$w_{t+1} = w_{t} - \\frac{\\alpha \\nabla L_{t}}{\\sqrt{h_{t} + \\theta}}$ <br> $h_{t} = h_{t-1} + (\\nabla L_{t})^{2}$ <br> where $w$ is the paramter, $t$ is the time, $\\alpha$ is the learning rate, $\\nabla L$ is the gradient of the loss function and $\\theta$ is the hyperparameter to avoid zero-divide <br> $h_{0}=0.0$|[【機械学習】Optimizer(最適化関数) – SGD、Momentum、AdaGrad、RMSProp、Adamとは何か](https://chefyushima.com/ai-ml_optimizer/2799/)|\n",
    "|RMSprop|$w_{t+1} = w_{t} - \\frac{\\alpha \\nabla L_{t}}{\\sqrt{h_{t} + \\theta}}$ <br> $h_{t} = \\beta h_{t-1} + (1-\\beta)(\\nabla L_{t})^{2}$ <br> where $w$ is the paramter, $t$ is the time, $\\alpha$ is the learning rate, $\\nabla L$ is the gradient of the loss function, $\\theta$ is the hyperparameter to avoid zero-divide, $\\beta$ is the hyperparameter to set the balance between $h_{t-1}$ and $(\\nabla L)^{2}$ <br> $h_{0}=0.0$|[【機械学習】Optimizer(最適化関数) – SGD、Momentum、AdaGrad、RMSProp、Adamとは何か](https://chefyushima.com/ai-ml_optimizer/2799/)|\n",
    "|Adadelta|$w_{t+1} = w_{t} - \\frac{\\alpha}{RMS[\\nabla L]_{t}} \\nabla L_{t}$ <br> $RMS[\\nabla L]_{t} = \\sqrt{E[\\nabla L_{t}^{2}] + \\epsilon}$ <br> $E[\\nabla L_{t}^{2}] = \\rho E[\\nabla L_{t-1}^{2}] + (1 - \\rho) \\nabla L_{t}^{2}$ <br> where $w$ is the paramter, $t$ is the time, $\\alpha$ is the learning rate, $\\nabla L$ is the gradient of the loss function, $E[\\bullet]$ is the expected value, $\\rho$ is the decay constant, $\\epsilon$ is the hyperparameter to avoid zero-divide|[ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)|\n",
    "|Adam|$w_{t+1} = w_{t} - \\alpha \\frac{v_{t}}{\\sqrt{s_{t}} + \\theta}$ <br> $v_{t} = \\beta_{1} v_{t-1} + (1 - \\beta_{1}) \\nabla L_{t}$ <br> $s_{t} = \\beta_{2} s_{t-1} + (1 - \\beta_{2})(\\nabla L_{t})^{2}$ <br> where $w$ is the paramter, $t$ is the time, $\\alpha$ is the learning rate, $\\nabla L$ is the gradient of the loss function, $v_{t}$ is the Momentum method and $s_{t}$ is the RMSprop mothod|[【機械学習】Optimizer(最適化関数) – SGD、Momentum、AdaGrad、RMSProp、Adamとは何か](https://chefyushima.com/ai-ml_optimizer/2799/)|\n",
    "|Adan|$w_{t+1} = (1 + \\lambda_{t} \\alpha)^{-1}[w_{t} - \\alpha_{t} \\circ (m_{t} + (1 - \\beta_{2}) v_{t})]$ <br> $\\alpha_{t} = \\frac{\\alpha}{\\sqrt{n_{t}} + \\epsilon}$ <br> $n_{t} = (1 - \\beta_{3}) n_{t-1} + \\beta_{3} [\\nabla L_{t} + (1 - \\beta_{2})(\\nabla L_{t} - \\nabla L_{t-1})]^{2}$ <br> $v_{k} = (1 - \\beta_{2}) v_{t-1} + \\beta_{2} (\\nabla L_{t} - \\nabla L_{t-1})$ <br> $m_{t} = (1 - \\beta_{1}) m_{t-1} + \\beta_{1} \\nabla L_{t}$ <br> where $w$ is the paramter, $t$ is the time, $\\alpha$ is the learning rate, $\\nabla L$ is the gradient of the loss function, $\\beta_{1},\\beta_{2},\\beta_{3}$ are parameters of momentum, $\\epsilon$ is the hyperparameter to avoid zero-divide and $\\lambda$ is the weight decay|[Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models](https://arxiv.org/abs/2208.06677)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49c402-da6c-48bf-8600-332e98f30ffb",
   "metadata": {},
   "source": [
    "# Result of Measuring Performance\n",
    "\n",
    "T.B.D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
