{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "474b637f",
   "metadata": {},
   "source": [
    "# Training Image Classification Model with SGD\n",
    "\n",
    "|Item|Description|\n",
    "|---|---|\n",
    "|DeepLearning Framework|PyTorch|\n",
    "|Dataset|CIFAR-100|\n",
    "|Model Architecture|Simple CNN|\n",
    "|Optimizer|SGD|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc9d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c40f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from data_loader.data_loader import DataLoader\n",
    "from models.pytorch import simple_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080cc78",
   "metadata": {},
   "source": [
    "## Set Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d245650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fddf40f79d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b0797",
   "metadata": {},
   "source": [
    "## Device Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8918ce3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a9692",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa0fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 128\n",
    "n_trials = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb795be9-94f6-47ea-834c-049db0d6a1e7",
   "metadata": {},
   "source": [
    "## Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a1a56f-8c3e-4a6a-9eaf-05f759a442e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (batch_size, 3, 32, 32)\n",
    "num_classes = 100\n",
    "\n",
    "dataset_dir = '/tmp/dataset'\n",
    "model_name = 'cifar-100_model'\n",
    "output_dir = './014-2_ImageClassification-CIFAR100-SimpleCNN-SGD-PyTorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "324102d9-d3f9-4d97-b490-2e104302143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(dataset_dir).mkdir(exist_ok=True)\n",
    "Path(output_dir).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60045952",
   "metadata": {},
   "source": [
    "## Load Dataset and Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deef041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset_name='cifar100_pytorch', dataset_dir=dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef213c0a",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7f9975-c45f-4e79-86c4-6a45c2b3fc0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-11 00:19:16,205] A new study created in memory with name: no-name-88b3b509-a4cf-43f1-8287-9306ac37b77e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605131923275275\n",
      "[EPOCH #1, elapsed time: 11.479[sec]] loss: 4.611842111525289\n",
      "[EPOCH #2, elapsed time: 22.686[sec]] loss: 4.6121715919680115\n",
      "[EPOCH #3, elapsed time: 34.714[sec]] loss: 4.612154239732641\n",
      "[EPOCH #4, elapsed time: 45.417[sec]] loss: 4.612441755759739\n",
      "[EPOCH #5, elapsed time: 56.782[sec]] loss: 4.612210339029401\n",
      "[EPOCH #6, elapsed time: 67.971[sec]] loss: 4.6121394257475306\n",
      "[EPOCH #7, elapsed time: 79.010[sec]] loss: 4.612170376841715\n",
      "[EPOCH #8, elapsed time: 90.013[sec]] loss: 4.6115147885952865\n",
      "[EPOCH #9, elapsed time: 100.964[sec]] loss: 4.611025506994012\n",
      "[EPOCH #10, elapsed time: 111.872[sec]] loss: 4.611808711873821\n",
      "[EPOCH #11, elapsed time: 122.613[sec]] loss: 4.6122643573880575\n",
      "[EPOCH #12, elapsed time: 133.438[sec]] loss: 4.611592720200141\n",
      "[EPOCH #13, elapsed time: 143.956[sec]] loss: 4.611320600552355\n",
      "[EPOCH #14, elapsed time: 158.316[sec]] loss: 4.612137034544942\n",
      "[EPOCH #15, elapsed time: 170.469[sec]] loss: 4.611625948092606\n",
      "[EPOCH #16, elapsed time: 182.689[sec]] loss: 4.611621959500792\n",
      "[EPOCH #17, elapsed time: 193.312[sec]] loss: 4.612232532244955\n",
      "[EPOCH #18, elapsed time: 205.616[sec]] loss: 4.612241031722388\n",
      "[EPOCH #19, elapsed time: 220.797[sec]] loss: 4.612210103509065\n",
      "[EPOCH #20, elapsed time: 231.358[sec]] loss: 4.612297517248094\n",
      "[EPOCH #21, elapsed time: 243.614[sec]] loss: 4.612230419883801\n",
      "[EPOCH #22, elapsed time: 254.521[sec]] loss: 4.611971467409238\n",
      "[EPOCH #23, elapsed time: 266.607[sec]] loss: 4.6119120000496325\n",
      "[EPOCH #24, elapsed time: 278.016[sec]] loss: 4.61223022646425\n",
      "[EPOCH #25, elapsed time: 288.700[sec]] loss: 4.612783974435798\n",
      "[EPOCH #26, elapsed time: 302.306[sec]] loss: 4.6122290665571\n",
      "[EPOCH #27, elapsed time: 313.539[sec]] loss: 4.612679845464588\n",
      "[EPOCH #28, elapsed time: 324.201[sec]] loss: 4.612807759244093\n",
      "[EPOCH #29, elapsed time: 336.424[sec]] loss: 4.612210363740732\n",
      "[EPOCH #30, elapsed time: 346.980[sec]] loss: 4.612247668697677\n",
      "[EPOCH #31, elapsed time: 357.734[sec]] loss: 4.611562239841551\n",
      "[EPOCH #32, elapsed time: 369.768[sec]] loss: 4.612445509746451\n",
      "[EPOCH #33, elapsed time: 380.904[sec]] loss: 4.612406181511174\n",
      "[EPOCH #34, elapsed time: 392.143[sec]] loss: 4.612347740434487\n",
      "[EPOCH #35, elapsed time: 406.713[sec]] loss: 4.610223272559129\n",
      "[EPOCH #36, elapsed time: 421.466[sec]] loss: 4.612204136790485\n",
      "[EPOCH #37, elapsed time: 432.941[sec]] loss: 4.6125180710605225\n",
      "[EPOCH #38, elapsed time: 445.988[sec]] loss: 4.612216033618266\n",
      "[EPOCH #39, elapsed time: 460.077[sec]] loss: 4.612210201134075\n",
      "[EPOCH #40, elapsed time: 470.656[sec]] loss: 4.6120849967536754\n",
      "[EPOCH #41, elapsed time: 481.431[sec]] loss: 4.612208812418308\n",
      "[EPOCH #42, elapsed time: 495.820[sec]] loss: 4.612545831342271\n",
      "[EPOCH #43, elapsed time: 508.743[sec]] loss: 4.611187758540314\n",
      "[EPOCH #44, elapsed time: 520.323[sec]] loss: 4.612455355533788\n",
      "[EPOCH #45, elapsed time: 530.970[sec]] loss: 4.61172489073516\n",
      "[EPOCH #46, elapsed time: 541.386[sec]] loss: 4.611828525174679\n",
      "[EPOCH #47, elapsed time: 551.835[sec]] loss: 4.612276067813085\n",
      "[EPOCH #48, elapsed time: 564.493[sec]] loss: 4.612173995068648\n",
      "[EPOCH #49, elapsed time: 576.360[sec]] loss: 4.612136415236285\n",
      "[EPOCH #50, elapsed time: 586.930[sec]] loss: 4.6122750842411095\n",
      "[EPOCH #51, elapsed time: 597.693[sec]] loss: 4.612542316841911\n",
      "[EPOCH #52, elapsed time: 609.301[sec]] loss: 4.612318736654173\n",
      "[EPOCH #53, elapsed time: 621.276[sec]] loss: 4.61188825124056\n",
      "[EPOCH #54, elapsed time: 631.973[sec]] loss: 4.61142510904079\n",
      "[EPOCH #55, elapsed time: 642.908[sec]] loss: 4.611390705072033\n",
      "[EPOCH #56, elapsed time: 656.580[sec]] loss: 4.612256531218116\n",
      "[EPOCH #57, elapsed time: 667.249[sec]] loss: 4.612210363740732\n",
      "[EPOCH #58, elapsed time: 677.991[sec]] loss: 4.612218409566946\n",
      "[EPOCH #59, elapsed time: 690.237[sec]] loss: 4.6123293518486195\n",
      "[EPOCH #60, elapsed time: 703.375[sec]] loss: 4.61221035245284\n",
      "[EPOCH #61, elapsed time: 714.156[sec]] loss: 4.61230319505766\n",
      "[EPOCH #62, elapsed time: 725.079[sec]] loss: 4.610050098604677\n",
      "[EPOCH #63, elapsed time: 736.627[sec]] loss: 4.610304591446753\n",
      "[EPOCH #64, elapsed time: 747.702[sec]] loss: 4.611508823707175\n",
      "[EPOCH #65, elapsed time: 762.530[sec]] loss: 4.612202867665355\n",
      "[EPOCH #66, elapsed time: 776.596[sec]] loss: 4.612231113021372\n",
      "[EPOCH #67, elapsed time: 790.154[sec]] loss: 4.612263071178551\n",
      "[EPOCH #68, elapsed time: 802.403[sec]] loss: 4.612087646052385\n",
      "[EPOCH #69, elapsed time: 814.575[sec]] loss: 4.612308063799955\n",
      "[EPOCH #70, elapsed time: 826.763[sec]] loss: 4.612374119627423\n",
      "[EPOCH #71, elapsed time: 837.859[sec]] loss: 4.611785010656941\n",
      "[EPOCH #72, elapsed time: 849.888[sec]] loss: 4.612210051035622\n",
      "[EPOCH #73, elapsed time: 864.266[sec]] loss: 4.611964803587071\n",
      "[EPOCH #74, elapsed time: 877.049[sec]] loss: 4.612213846818042\n",
      "[EPOCH #75, elapsed time: 888.950[sec]] loss: 4.611660440839107\n",
      "[EPOCH #76, elapsed time: 901.222[sec]] loss: 4.6123067333541155\n",
      "[EPOCH #77, elapsed time: 913.487[sec]] loss: 4.611431349109398\n",
      "[EPOCH #78, elapsed time: 926.962[sec]] loss: 4.6122308570607995\n",
      "[EPOCH #79, elapsed time: 937.778[sec]] loss: 4.611957166870664\n",
      "[EPOCH #80, elapsed time: 950.326[sec]] loss: 4.612265001102967\n",
      "[EPOCH #81, elapsed time: 961.125[sec]] loss: 4.612210363740732\n",
      "[EPOCH #82, elapsed time: 974.594[sec]] loss: 4.610695935637998\n",
      "[EPOCH #83, elapsed time: 985.460[sec]] loss: 4.611931803282911\n",
      "[EPOCH #84, elapsed time: 996.102[sec]] loss: 4.612189804523783\n",
      "[EPOCH #85, elapsed time: 1006.736[sec]] loss: 4.61167525116328\n",
      "[EPOCH #86, elapsed time: 1017.333[sec]] loss: 4.612209975071161\n",
      "[EPOCH #87, elapsed time: 1027.822[sec]] loss: 4.612304159409711\n",
      "[EPOCH #88, elapsed time: 1038.482[sec]] loss: 4.611964770943708\n",
      "[EPOCH #89, elapsed time: 1049.235[sec]] loss: 4.611918669363206\n",
      "[EPOCH #90, elapsed time: 1063.933[sec]] loss: 4.612508964782637\n",
      "[EPOCH #91, elapsed time: 1076.151[sec]] loss: 4.611844065550879\n",
      "[EPOCH #92, elapsed time: 1086.752[sec]] loss: 4.61174074229108\n",
      "[EPOCH #93, elapsed time: 1097.903[sec]] loss: 4.612541382997675\n",
      "[EPOCH #94, elapsed time: 1108.611[sec]] loss: 4.6125429343201\n",
      "[EPOCH #95, elapsed time: 1119.740[sec]] loss: 4.612210095271954\n",
      "[EPOCH #96, elapsed time: 1130.386[sec]] loss: 4.612194380696127\n",
      "[EPOCH #97, elapsed time: 1141.277[sec]] loss: 4.612177896103032\n",
      "[EPOCH #98, elapsed time: 1155.858[sec]] loss: 4.612039872643586\n",
      "[EPOCH #99, elapsed time: 1166.459[sec]] loss: 4.612142268160712\n",
      "[EPOCH #100, elapsed time: 1178.740[sec]] loss: 4.612099583760638\n",
      "[EPOCH #101, elapsed time: 1190.842[sec]] loss: 4.612901775179501\n",
      "[EPOCH #102, elapsed time: 1205.608[sec]] loss: 4.612147504827264\n",
      "[EPOCH #103, elapsed time: 1217.358[sec]] loss: 4.612149401193083\n",
      "[EPOCH #104, elapsed time: 1228.136[sec]] loss: 4.612144288693341\n",
      "[EPOCH #105, elapsed time: 1239.250[sec]] loss: 4.612062175382198\n",
      "[EPOCH #106, elapsed time: 1251.481[sec]] loss: 4.612189531173755\n",
      "[EPOCH #107, elapsed time: 1262.343[sec]] loss: 4.612481886045489\n",
      "[EPOCH #108, elapsed time: 1274.491[sec]] loss: 4.61199011500646\n",
      "[EPOCH #109, elapsed time: 1285.457[sec]] loss: 4.611901407736048\n",
      "[EPOCH #110, elapsed time: 1297.595[sec]] loss: 4.608234184335915\n",
      "[EPOCH #111, elapsed time: 1308.737[sec]] loss: 4.6112949239544125\n",
      "[EPOCH #112, elapsed time: 1319.298[sec]] loss: 4.611967264957635\n",
      "[EPOCH #113, elapsed time: 1330.243[sec]] loss: 4.612247489921877\n",
      "[EPOCH #114, elapsed time: 1341.060[sec]] loss: 4.612181399925657\n",
      "[EPOCH #115, elapsed time: 1352.005[sec]] loss: 4.611960159382298\n",
      "[EPOCH #116, elapsed time: 1362.858[sec]] loss: 4.611879659629524\n",
      "[EPOCH #117, elapsed time: 1377.557[sec]] loss: 4.612085078209543\n",
      "[EPOCH #118, elapsed time: 1388.384[sec]] loss: 4.6122086107616465\n",
      "[EPOCH #119, elapsed time: 1399.869[sec]] loss: 4.612153282702465\n",
      "[EPOCH #120, elapsed time: 1414.337[sec]] loss: 4.612477798303273\n",
      "[EPOCH #121, elapsed time: 1426.218[sec]] loss: 4.612120548426456\n",
      "[EPOCH #122, elapsed time: 1436.822[sec]] loss: 4.612426294093702\n",
      "[EPOCH #123, elapsed time: 1447.097[sec]] loss: 4.612318455677191\n",
      "[EPOCH #124, elapsed time: 1457.428[sec]] loss: 4.6122509171698844\n",
      "[EPOCH #125, elapsed time: 1470.355[sec]] loss: 4.6123296535709155\n",
      "[EPOCH #126, elapsed time: 1480.656[sec]] loss: 4.612222299618517\n",
      "[EPOCH #127, elapsed time: 1492.678[sec]] loss: 4.61236651677469\n",
      "[EPOCH #128, elapsed time: 1506.796[sec]] loss: 4.612210363740732\n",
      "[EPOCH #129, elapsed time: 1518.765[sec]] loss: 4.612284820505388\n",
      "[EPOCH #130, elapsed time: 1529.428[sec]] loss: 4.612639474167095\n",
      "[EPOCH #131, elapsed time: 1541.504[sec]] loss: 4.6114417727147625\n",
      "[EPOCH #132, elapsed time: 1552.025[sec]] loss: 4.610520361328613\n",
      "[EPOCH #133, elapsed time: 1562.713[sec]] loss: 4.611872407616672\n",
      "[EPOCH #134, elapsed time: 1573.765[sec]] loss: 4.611800938482401\n",
      "[EPOCH #135, elapsed time: 1584.369[sec]] loss: 4.6125703684733\n",
      "[EPOCH #136, elapsed time: 1595.088[sec]] loss: 4.610854459739388\n",
      "[EPOCH #137, elapsed time: 1605.372[sec]] loss: 4.612188282183784\n",
      "[EPOCH #138, elapsed time: 1615.689[sec]] loss: 4.612066826603768\n",
      "[EPOCH #139, elapsed time: 1627.341[sec]] loss: 4.612295971111998\n",
      "[EPOCH #140, elapsed time: 1640.781[sec]] loss: 4.612224518146869\n",
      "[EPOCH #141, elapsed time: 1651.072[sec]] loss: 4.612257487638136\n",
      "[EPOCH #142, elapsed time: 1661.296[sec]] loss: 4.612002500264368\n",
      "[EPOCH #143, elapsed time: 1671.628[sec]] loss: 4.611700056458008\n",
      "[EPOCH #144, elapsed time: 1683.591[sec]] loss: 4.611148444033554\n",
      "[EPOCH #145, elapsed time: 1693.928[sec]] loss: 4.610875476878649\n",
      "[EPOCH #146, elapsed time: 1704.321[sec]] loss: 4.611783871800184\n",
      "[EPOCH #147, elapsed time: 1714.593[sec]] loss: 4.612209721551213\n",
      "[EPOCH #148, elapsed time: 1724.820[sec]] loss: 4.611851677250877\n",
      "[EPOCH #149, elapsed time: 1735.147[sec]] loss: 4.611359691741904\n",
      "[EPOCH #150, elapsed time: 1746.977[sec]] loss: 4.611096512409486\n",
      "[EPOCH #151, elapsed time: 1757.243[sec]] loss: 4.611665783672858\n",
      "[EPOCH #152, elapsed time: 1769.119[sec]] loss: 4.6117577979904825\n",
      "[EPOCH #153, elapsed time: 1780.545[sec]] loss: 4.612190035162869\n",
      "[EPOCH #154, elapsed time: 1792.530[sec]] loss: 4.612365708927733\n",
      "[EPOCH #155, elapsed time: 1804.342[sec]] loss: 4.612210363740732\n",
      "[EPOCH #156, elapsed time: 1818.970[sec]] loss: 4.612782568635653\n",
      "[EPOCH #157, elapsed time: 1829.209[sec]] loss: 4.61221022340478\n",
      "[EPOCH #158, elapsed time: 1839.529[sec]] loss: 4.6126411081656995\n",
      "[EPOCH #159, elapsed time: 1850.076[sec]] loss: 4.61232722880973\n",
      "[EPOCH #160, elapsed time: 1860.897[sec]] loss: 4.612348342963846\n",
      "[EPOCH #161, elapsed time: 1872.903[sec]] loss: 4.612210363740732\n",
      "[EPOCH #162, elapsed time: 1884.972[sec]] loss: 4.612168601897002\n",
      "[EPOCH #163, elapsed time: 1895.845[sec]] loss: 4.612135723013948\n",
      "[EPOCH #164, elapsed time: 1909.475[sec]] loss: 4.6123930082363875\n",
      "[EPOCH #165, elapsed time: 1919.792[sec]] loss: 4.612866876373975\n",
      "[EPOCH #166, elapsed time: 1930.448[sec]] loss: 4.611833704486537\n",
      "[EPOCH #167, elapsed time: 1940.746[sec]] loss: 4.612288953704248\n",
      "[EPOCH #168, elapsed time: 1951.102[sec]] loss: 4.612140715312897\n",
      "[EPOCH #169, elapsed time: 1961.756[sec]] loss: 4.612209683111366\n",
      "[EPOCH #170, elapsed time: 1972.045[sec]] loss: 4.612245822364676\n",
      "[EPOCH #171, elapsed time: 1985.478[sec]] loss: 4.611964624506193\n",
      "[EPOCH #172, elapsed time: 1995.744[sec]] loss: 4.612311939207774\n",
      "[EPOCH #173, elapsed time: 2005.910[sec]] loss: 4.612251530987135\n",
      "[EPOCH #174, elapsed time: 2016.137[sec]] loss: 4.612189731915182\n",
      "[EPOCH #175, elapsed time: 2026.782[sec]] loss: 4.612383813790915\n",
      "[EPOCH #176, elapsed time: 2037.162[sec]] loss: 4.611919238333967\n",
      "[EPOCH #177, elapsed time: 2050.501[sec]] loss: 4.613277268424983\n",
      "[EPOCH #178, elapsed time: 2061.927[sec]] loss: 4.612017621158104\n",
      "[EPOCH #179, elapsed time: 2072.544[sec]] loss: 4.612210363740732\n",
      "[EPOCH #180, elapsed time: 2084.227[sec]] loss: 4.6122685644158326\n",
      "[EPOCH #181, elapsed time: 2094.363[sec]] loss: 4.612189836556989\n",
      "[EPOCH #182, elapsed time: 2105.385[sec]] loss: 4.61194625514025\n",
      "[EPOCH #183, elapsed time: 2116.989[sec]] loss: 4.612210363740732\n",
      "[EPOCH #184, elapsed time: 2127.586[sec]] loss: 4.611441374892847\n",
      "[EPOCH #185, elapsed time: 2138.435[sec]] loss: 4.612210306691117\n",
      "[EPOCH #186, elapsed time: 2150.242[sec]] loss: 4.612428661805273\n",
      "[EPOCH #187, elapsed time: 2162.113[sec]] loss: 4.61043504957808\n",
      "[EPOCH #188, elapsed time: 2177.605[sec]] loss: 4.6111769392485815\n",
      "[EPOCH #189, elapsed time: 2190.476[sec]] loss: 4.611940302150187\n",
      "[EPOCH #190, elapsed time: 2202.748[sec]] loss: 4.612210313402836\n",
      "[EPOCH #191, elapsed time: 2214.896[sec]] loss: 4.611564673450004\n",
      "[EPOCH #192, elapsed time: 2231.923[sec]] loss: 4.6114566666303505\n",
      "[EPOCH #193, elapsed time: 2246.578[sec]] loss: 4.612528579172536\n",
      "[EPOCH #194, elapsed time: 2259.410[sec]] loss: 4.612459826759245\n",
      "[EPOCH #195, elapsed time: 2270.157[sec]] loss: 4.612663832217245\n",
      "[EPOCH #196, elapsed time: 2281.462[sec]] loss: 4.612330959610503\n",
      "[EPOCH #197, elapsed time: 2292.780[sec]] loss: 4.612389323807495\n",
      "[EPOCH #198, elapsed time: 2303.527[sec]] loss: 4.613476751404394\n",
      "[EPOCH #199, elapsed time: 2315.201[sec]] loss: 4.610977174376953\n",
      "[EPOCH #200, elapsed time: 2325.952[sec]] loss: 4.612086820510894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 00:58:21,132] Trial 0 finished with value: 0.0101 and parameters: {'learning_rate': 0.046406232322745745}. Best is trial 0 with value: 0.0101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.6051902691675775\n",
      "[EPOCH #1, elapsed time: 13.300[sec]] loss: 4.606916659471703\n",
      "[EPOCH #2, elapsed time: 24.489[sec]] loss: 4.610837829013856\n",
      "[EPOCH #3, elapsed time: 36.232[sec]] loss: 4.6122087224202515\n",
      "[EPOCH #4, elapsed time: 47.715[sec]] loss: 4.612496595998948\n",
      "[EPOCH #5, elapsed time: 58.837[sec]] loss: 4.612210043713746\n",
      "[EPOCH #6, elapsed time: 72.789[sec]] loss: 4.612213683601228\n",
      "[EPOCH #7, elapsed time: 84.059[sec]] loss: 4.610534142624165\n",
      "[EPOCH #8, elapsed time: 95.643[sec]] loss: 4.609834902879906\n",
      "[EPOCH #9, elapsed time: 106.651[sec]] loss: 4.609786377758532\n",
      "[EPOCH #10, elapsed time: 117.336[sec]] loss: 4.611966933032601\n",
      "[EPOCH #11, elapsed time: 129.586[sec]] loss: 4.6096068185182695\n",
      "[EPOCH #12, elapsed time: 140.733[sec]] loss: 4.610976971499979\n",
      "[EPOCH #13, elapsed time: 152.075[sec]] loss: 4.6087455233929635\n",
      "[EPOCH #14, elapsed time: 164.922[sec]] loss: 4.6110127007282475\n",
      "[EPOCH #15, elapsed time: 178.954[sec]] loss: 4.610535267752405\n",
      "[EPOCH #16, elapsed time: 192.512[sec]] loss: 4.606545431523924\n",
      "[EPOCH #17, elapsed time: 205.069[sec]] loss: 4.608071294871188\n",
      "[EPOCH #18, elapsed time: 215.482[sec]] loss: 4.612554695693179\n",
      "[EPOCH #19, elapsed time: 228.238[sec]] loss: 4.611406222567372\n",
      "[EPOCH #20, elapsed time: 239.645[sec]] loss: 4.61133452523464\n",
      "[EPOCH #21, elapsed time: 249.842[sec]] loss: 4.61147938824127\n",
      "[EPOCH #22, elapsed time: 260.131[sec]] loss: 4.610857307643975\n",
      "[EPOCH #23, elapsed time: 274.267[sec]] loss: 4.6111068414406216\n",
      "[EPOCH #24, elapsed time: 284.908[sec]] loss: 4.608931099384622\n",
      "[EPOCH #25, elapsed time: 295.222[sec]] loss: 4.610394673399337\n",
      "[EPOCH #26, elapsed time: 305.584[sec]] loss: 4.612247748628154\n",
      "[EPOCH #27, elapsed time: 318.923[sec]] loss: 4.6109974239395735\n",
      "[EPOCH #28, elapsed time: 329.224[sec]] loss: 4.611870559453202\n",
      "[EPOCH #29, elapsed time: 339.760[sec]] loss: 4.612236821643831\n",
      "[EPOCH #30, elapsed time: 351.318[sec]] loss: 4.6122103680118265\n",
      "[EPOCH #31, elapsed time: 361.843[sec]] loss: 4.612269611444065\n",
      "[EPOCH #32, elapsed time: 372.049[sec]] loss: 4.612306763861931\n",
      "[EPOCH #33, elapsed time: 384.860[sec]] loss: 4.612190370138684\n",
      "[EPOCH #34, elapsed time: 394.986[sec]] loss: 4.612323711563667\n",
      "[EPOCH #35, elapsed time: 405.455[sec]] loss: 4.611465039500348\n",
      "[EPOCH #36, elapsed time: 420.004[sec]] loss: 4.612252493813796\n",
      "[EPOCH #37, elapsed time: 434.691[sec]] loss: 4.611925734363148\n",
      "[EPOCH #38, elapsed time: 449.025[sec]] loss: 4.610382423291051\n",
      "[EPOCH #39, elapsed time: 464.232[sec]] loss: 4.611396508573799\n",
      "[EPOCH #40, elapsed time: 477.117[sec]] loss: 4.611600553691959\n",
      "[EPOCH #41, elapsed time: 489.914[sec]] loss: 4.612446852090339\n",
      "[EPOCH #42, elapsed time: 500.635[sec]] loss: 4.612439350218477\n",
      "[EPOCH #43, elapsed time: 512.665[sec]] loss: 4.612491848982837\n",
      "[EPOCH #44, elapsed time: 525.244[sec]] loss: 4.611487225699104\n",
      "[EPOCH #45, elapsed time: 538.260[sec]] loss: 4.611944989676058\n",
      "[EPOCH #46, elapsed time: 551.128[sec]] loss: 4.612189146165122\n",
      "[EPOCH #47, elapsed time: 563.689[sec]] loss: 4.611944710529545\n",
      "[EPOCH #48, elapsed time: 576.157[sec]] loss: 4.612591477441086\n",
      "[EPOCH #49, elapsed time: 586.580[sec]] loss: 4.612551343494398\n",
      "[EPOCH #50, elapsed time: 597.115[sec]] loss: 4.611731863296421\n",
      "[EPOCH #51, elapsed time: 607.560[sec]] loss: 4.612250746021039\n",
      "[EPOCH #52, elapsed time: 620.045[sec]] loss: 4.61096791891585\n",
      "[EPOCH #53, elapsed time: 631.321[sec]] loss: 4.608474052341337\n",
      "[EPOCH #54, elapsed time: 641.573[sec]] loss: 4.611402934435004\n",
      "[EPOCH #55, elapsed time: 651.825[sec]] loss: 4.610041863630006\n",
      "[EPOCH #56, elapsed time: 662.156[sec]] loss: 4.611513224459579\n",
      "[EPOCH #57, elapsed time: 672.469[sec]] loss: 4.612209148919514\n",
      "[EPOCH #58, elapsed time: 682.588[sec]] loss: 4.607483020899621\n",
      "[EPOCH #59, elapsed time: 692.751[sec]] loss: 4.607359664072536\n",
      "[EPOCH #60, elapsed time: 705.386[sec]] loss: 4.6112213927968835\n",
      "[EPOCH #61, elapsed time: 717.531[sec]] loss: 4.608761152851986\n",
      "[EPOCH #62, elapsed time: 728.208[sec]] loss: 4.607003546645835\n",
      "[EPOCH #63, elapsed time: 739.247[sec]] loss: 4.607707476356589\n",
      "[EPOCH #64, elapsed time: 750.151[sec]] loss: 4.608019362026807\n",
      "[EPOCH #65, elapsed time: 760.639[sec]] loss: 4.609795206415295\n",
      "[EPOCH #66, elapsed time: 772.750[sec]] loss: 4.6118600305958735\n",
      "[EPOCH #67, elapsed time: 783.151[sec]] loss: 4.611972529691378\n",
      "[EPOCH #68, elapsed time: 793.711[sec]] loss: 4.612195428029437\n",
      "[EPOCH #69, elapsed time: 805.264[sec]] loss: 4.611647436577619\n",
      "[EPOCH #70, elapsed time: 817.239[sec]] loss: 4.610691894267663\n",
      "[EPOCH #71, elapsed time: 830.116[sec]] loss: 4.611096199399298\n",
      "[EPOCH #72, elapsed time: 843.062[sec]] loss: 4.608489460618696\n",
      "[EPOCH #73, elapsed time: 857.835[sec]] loss: 4.608703226747226\n",
      "[EPOCH #74, elapsed time: 868.264[sec]] loss: 4.610295161175865\n",
      "[EPOCH #75, elapsed time: 879.623[sec]] loss: 4.612171919316873\n",
      "[EPOCH #76, elapsed time: 891.660[sec]] loss: 4.611582631265515\n",
      "[EPOCH #77, elapsed time: 904.380[sec]] loss: 4.608394990726991\n",
      "[EPOCH #78, elapsed time: 917.575[sec]] loss: 4.608214562013984\n",
      "[EPOCH #79, elapsed time: 930.202[sec]] loss: 4.607338369922308\n",
      "[EPOCH #80, elapsed time: 941.415[sec]] loss: 4.610827214124488\n",
      "[EPOCH #81, elapsed time: 952.803[sec]] loss: 4.610659052909221\n",
      "[EPOCH #82, elapsed time: 966.993[sec]] loss: 4.612246763225709\n",
      "[EPOCH #83, elapsed time: 977.179[sec]] loss: 4.61222925021415\n",
      "[EPOCH #84, elapsed time: 992.425[sec]] loss: 4.612246948713228\n",
      "[EPOCH #85, elapsed time: 1005.873[sec]] loss: 4.612694671042669\n",
      "[EPOCH #86, elapsed time: 1018.297[sec]] loss: 4.612210363740732\n",
      "[EPOCH #87, elapsed time: 1031.089[sec]] loss: 4.612210363740732\n",
      "[EPOCH #88, elapsed time: 1043.947[sec]] loss: 4.611822860483473\n",
      "[EPOCH #89, elapsed time: 1056.574[sec]] loss: 4.61221036404581\n",
      "[EPOCH #90, elapsed time: 1071.636[sec]] loss: 4.61142135963025\n",
      "[EPOCH #91, elapsed time: 1082.826[sec]] loss: 4.6118616017483784\n",
      "[EPOCH #92, elapsed time: 1093.179[sec]] loss: 4.612031988203716\n",
      "[EPOCH #93, elapsed time: 1105.245[sec]] loss: 4.612604625089308\n",
      "[EPOCH #94, elapsed time: 1117.038[sec]] loss: 4.612363215218884\n",
      "[EPOCH #95, elapsed time: 1127.093[sec]] loss: 4.612210234387594\n",
      "[EPOCH #96, elapsed time: 1137.372[sec]] loss: 4.612013897984285\n",
      "[EPOCH #97, elapsed time: 1147.748[sec]] loss: 4.612210346656355\n",
      "[EPOCH #98, elapsed time: 1158.026[sec]] loss: 4.612086808002689\n",
      "[EPOCH #99, elapsed time: 1168.326[sec]] loss: 4.612210363740732\n",
      "[EPOCH #100, elapsed time: 1180.352[sec]] loss: 4.6125739528365575\n",
      "[EPOCH #101, elapsed time: 1191.607[sec]] loss: 4.612016022548565\n",
      "[EPOCH #102, elapsed time: 1205.185[sec]] loss: 4.609777884992818\n",
      "[EPOCH #103, elapsed time: 1220.189[sec]] loss: 4.608416596667094\n",
      "[EPOCH #104, elapsed time: 1235.510[sec]] loss: 4.60933719288441\n",
      "[EPOCH #105, elapsed time: 1248.375[sec]] loss: 4.612210300284476\n",
      "[EPOCH #106, elapsed time: 1260.022[sec]] loss: 4.612210087339922\n",
      "[EPOCH #107, elapsed time: 1270.631[sec]] loss: 4.612296461372595\n",
      "[EPOCH #108, elapsed time: 1280.960[sec]] loss: 4.612230119991974\n",
      "[EPOCH #109, elapsed time: 1294.536[sec]] loss: 4.612210266725878\n",
      "[EPOCH #110, elapsed time: 1304.881[sec]] loss: 4.611547326400962\n",
      "[EPOCH #111, elapsed time: 1316.169[sec]] loss: 4.610736674249591\n",
      "[EPOCH #112, elapsed time: 1326.560[sec]] loss: 4.606570090076058\n",
      "[EPOCH #113, elapsed time: 1336.931[sec]] loss: 4.607182267531323\n",
      "[EPOCH #114, elapsed time: 1347.648[sec]] loss: 4.6073753826143795\n",
      "[EPOCH #115, elapsed time: 1357.855[sec]] loss: 4.606311785892577\n",
      "[EPOCH #116, elapsed time: 1368.246[sec]] loss: 4.607564572561878\n",
      "[EPOCH #117, elapsed time: 1379.343[sec]] loss: 4.609349140050086\n",
      "[EPOCH #118, elapsed time: 1389.744[sec]] loss: 4.607653409185428\n",
      "[EPOCH #119, elapsed time: 1401.736[sec]] loss: 4.608973775547587\n",
      "[EPOCH #120, elapsed time: 1413.717[sec]] loss: 4.6103711607207565\n",
      "[EPOCH #121, elapsed time: 1424.938[sec]] loss: 4.610602399285451\n",
      "[EPOCH #122, elapsed time: 1436.328[sec]] loss: 4.610777893359281\n",
      "[EPOCH #123, elapsed time: 1446.626[sec]] loss: 4.6118588374352045\n",
      "[EPOCH #124, elapsed time: 1457.011[sec]] loss: 4.612183947328261\n",
      "[EPOCH #125, elapsed time: 1467.642[sec]] loss: 4.6099871475576055\n",
      "[EPOCH #126, elapsed time: 1477.824[sec]] loss: 4.610096738648124\n",
      "[EPOCH #127, elapsed time: 1488.212[sec]] loss: 4.611016385462219\n",
      "[EPOCH #128, elapsed time: 1500.594[sec]] loss: 4.612595226546548\n",
      "[EPOCH #129, elapsed time: 1514.237[sec]] loss: 4.610668945068437\n",
      "[EPOCH #130, elapsed time: 1524.479[sec]] loss: 4.612335097385536\n",
      "[EPOCH #131, elapsed time: 1534.565[sec]] loss: 4.6121276909162505\n",
      "[EPOCH #132, elapsed time: 1547.138[sec]] loss: 4.612190958634448\n",
      "[EPOCH #133, elapsed time: 1557.574[sec]] loss: 4.611916942315764\n",
      "[EPOCH #134, elapsed time: 1567.685[sec]] loss: 4.61114746351236\n",
      "[EPOCH #135, elapsed time: 1582.577[sec]] loss: 4.605173496885782\n",
      "[EPOCH #136, elapsed time: 1592.628[sec]] loss: 4.605807629900717\n",
      "[EPOCH #137, elapsed time: 1603.396[sec]] loss: 4.612446043328147\n",
      "[EPOCH #138, elapsed time: 1614.191[sec]] loss: 4.612210336893854\n",
      "[EPOCH #139, elapsed time: 1628.164[sec]] loss: 4.612451851711163\n",
      "[EPOCH #140, elapsed time: 1640.185[sec]] loss: 4.611954243611771\n",
      "[EPOCH #141, elapsed time: 1650.391[sec]] loss: 4.612210363740732\n",
      "[EPOCH #142, elapsed time: 1660.589[sec]] loss: 4.612210363740732\n",
      "[EPOCH #143, elapsed time: 1671.308[sec]] loss: 4.612274735841855\n",
      "[EPOCH #144, elapsed time: 1683.751[sec]] loss: 4.612356780815491\n",
      "[EPOCH #145, elapsed time: 1694.294[sec]] loss: 4.612210356113778\n",
      "[EPOCH #146, elapsed time: 1704.491[sec]] loss: 4.612625152883206\n",
      "[EPOCH #147, elapsed time: 1717.994[sec]] loss: 4.612210363740732\n",
      "[EPOCH #148, elapsed time: 1729.973[sec]] loss: 4.612132955649993\n",
      "[EPOCH #149, elapsed time: 1740.290[sec]] loss: 4.612210363740732\n",
      "[EPOCH #150, elapsed time: 1753.957[sec]] loss: 4.6121089258639385\n",
      "[EPOCH #151, elapsed time: 1764.453[sec]] loss: 4.612573128820457\n",
      "[EPOCH #152, elapsed time: 1774.690[sec]] loss: 4.612479660195261\n",
      "[EPOCH #153, elapsed time: 1784.817[sec]] loss: 4.611635976926837\n",
      "[EPOCH #154, elapsed time: 1794.912[sec]] loss: 4.6110469073114375\n",
      "[EPOCH #155, elapsed time: 1805.266[sec]] loss: 4.611834239898701\n",
      "[EPOCH #156, elapsed time: 1817.233[sec]] loss: 4.611896291270289\n",
      "[EPOCH #157, elapsed time: 1827.896[sec]] loss: 4.611914898902273\n",
      "[EPOCH #158, elapsed time: 1838.169[sec]] loss: 4.607884987607188\n",
      "[EPOCH #159, elapsed time: 1848.810[sec]] loss: 4.607755575283781\n",
      "[EPOCH #160, elapsed time: 1860.867[sec]] loss: 4.607756048154922\n",
      "[EPOCH #161, elapsed time: 1871.381[sec]] loss: 4.607358753414239\n",
      "[EPOCH #162, elapsed time: 1881.700[sec]] loss: 4.60933984035265\n",
      "[EPOCH #163, elapsed time: 1893.631[sec]] loss: 4.6068559480033775\n",
      "[EPOCH #164, elapsed time: 1904.535[sec]] loss: 4.610489060614861\n",
      "[EPOCH #165, elapsed time: 1914.892[sec]] loss: 4.608144116111848\n",
      "[EPOCH #166, elapsed time: 1926.569[sec]] loss: 4.612432784936555\n",
      "[EPOCH #167, elapsed time: 1941.369[sec]] loss: 4.611977655614559\n",
      "[EPOCH #168, elapsed time: 1951.965[sec]] loss: 4.612297428775428\n",
      "[EPOCH #169, elapsed time: 1962.114[sec]] loss: 4.61221036068995\n",
      "[EPOCH #170, elapsed time: 1974.321[sec]] loss: 4.609649296380431\n",
      "[EPOCH #171, elapsed time: 1985.857[sec]] loss: 4.608644216089621\n",
      "[EPOCH #172, elapsed time: 1999.667[sec]] loss: 4.610166836456084\n",
      "[EPOCH #173, elapsed time: 2009.965[sec]] loss: 4.606122647808365\n",
      "[EPOCH #174, elapsed time: 2022.067[sec]] loss: 4.605351511515338\n",
      "[EPOCH #175, elapsed time: 2033.560[sec]] loss: 4.604203622614201\n",
      "[EPOCH #176, elapsed time: 2047.518[sec]] loss: 4.611251011309681\n",
      "[EPOCH #177, elapsed time: 2058.083[sec]] loss: 4.607594584930576\n",
      "[EPOCH #178, elapsed time: 2068.415[sec]] loss: 4.612124743861261\n",
      "[EPOCH #179, elapsed time: 2078.853[sec]] loss: 4.61221033933448\n",
      "[EPOCH #180, elapsed time: 2089.222[sec]] loss: 4.612168454239175\n",
      "[EPOCH #181, elapsed time: 2099.706[sec]] loss: 4.612490637517479\n",
      "[EPOCH #182, elapsed time: 2110.301[sec]] loss: 4.611780181574806\n",
      "[EPOCH #183, elapsed time: 2120.810[sec]] loss: 4.612392463061723\n",
      "[EPOCH #184, elapsed time: 2131.137[sec]] loss: 4.612238892819435\n",
      "[EPOCH #185, elapsed time: 2141.668[sec]] loss: 4.612190396985563\n",
      "[EPOCH #186, elapsed time: 2151.905[sec]] loss: 4.61212380422054\n",
      "[EPOCH #187, elapsed time: 2162.097[sec]] loss: 4.612210362825498\n",
      "[EPOCH #188, elapsed time: 2172.398[sec]] loss: 4.611532589600625\n",
      "[EPOCH #189, elapsed time: 2183.783[sec]] loss: 4.608721432896356\n",
      "[EPOCH #190, elapsed time: 2194.039[sec]] loss: 4.6055869055495995\n",
      "[EPOCH #191, elapsed time: 2205.368[sec]] loss: 4.6106209208853945\n",
      "[EPOCH #192, elapsed time: 2215.605[sec]] loss: 4.612330519077645\n",
      "[EPOCH #193, elapsed time: 2225.891[sec]] loss: 4.610642662890355\n",
      "[EPOCH #194, elapsed time: 2236.706[sec]] loss: 4.608948246912551\n",
      "[EPOCH #195, elapsed time: 2247.192[sec]] loss: 4.609911450955323\n",
      "[EPOCH #196, elapsed time: 2259.030[sec]] loss: 4.612972371485168\n",
      "[EPOCH #197, elapsed time: 2273.088[sec]] loss: 4.61220943081173\n",
      "[EPOCH #198, elapsed time: 2283.918[sec]] loss: 4.611770542935538\n",
      "[EPOCH #199, elapsed time: 2294.631[sec]] loss: 4.611762195692105\n",
      "[EPOCH #200, elapsed time: 2305.069[sec]] loss: 4.6127661935656175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 01:37:02,044] Trial 1 finished with value: 0.01002 and parameters: {'learning_rate': 0.025005872791129456}. Best is trial 0 with value: 0.0101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605166408089743\n",
      "[EPOCH #1, elapsed time: 10.459[sec]] loss: 4.6118685084127575\n",
      "[EPOCH #2, elapsed time: 24.856[sec]] loss: 4.612293385574624\n",
      "[EPOCH #3, elapsed time: 38.045[sec]] loss: 4.612294563176307\n",
      "[EPOCH #4, elapsed time: 48.100[sec]] loss: 4.6122905013657345\n",
      "[EPOCH #5, elapsed time: 58.488[sec]] loss: 4.612141901761847\n",
      "[EPOCH #6, elapsed time: 68.691[sec]] loss: 4.611624914487813\n",
      "[EPOCH #7, elapsed time: 83.799[sec]] loss: 4.612061457533296\n",
      "[EPOCH #8, elapsed time: 94.145[sec]] loss: 4.612034268662934\n",
      "[EPOCH #9, elapsed time: 104.432[sec]] loss: 4.6122996982518325\n",
      "[EPOCH #10, elapsed time: 115.305[sec]] loss: 4.612260521335321\n",
      "[EPOCH #11, elapsed time: 125.637[sec]] loss: 4.612210363740732\n",
      "[EPOCH #12, elapsed time: 138.132[sec]] loss: 4.612484829744618\n",
      "[EPOCH #13, elapsed time: 149.669[sec]] loss: 4.612210558380595\n",
      "[EPOCH #14, elapsed time: 161.316[sec]] loss: 4.612290575194649\n",
      "[EPOCH #15, elapsed time: 173.131[sec]] loss: 4.612467558354959\n",
      "[EPOCH #16, elapsed time: 186.062[sec]] loss: 4.612170269454205\n",
      "[EPOCH #17, elapsed time: 197.963[sec]] loss: 4.612382628562278\n",
      "[EPOCH #18, elapsed time: 209.598[sec]] loss: 4.612580680114981\n",
      "[EPOCH #19, elapsed time: 224.917[sec]] loss: 4.611928911447068\n",
      "[EPOCH #20, elapsed time: 236.696[sec]] loss: 4.612209995511397\n",
      "[EPOCH #21, elapsed time: 248.285[sec]] loss: 4.612272780595951\n",
      "[EPOCH #22, elapsed time: 261.600[sec]] loss: 4.6125121510189\n",
      "[EPOCH #23, elapsed time: 273.126[sec]] loss: 4.612182941790659\n",
      "[EPOCH #24, elapsed time: 288.917[sec]] loss: 4.612210363740732\n",
      "[EPOCH #25, elapsed time: 300.580[sec]] loss: 4.611841810718227\n",
      "[EPOCH #26, elapsed time: 312.398[sec]] loss: 4.611408345606262\n",
      "[EPOCH #27, elapsed time: 324.364[sec]] loss: 4.6125291722444715\n",
      "[EPOCH #28, elapsed time: 338.979[sec]] loss: 4.6122095726730725\n",
      "[EPOCH #29, elapsed time: 350.338[sec]] loss: 4.61174262035221\n",
      "[EPOCH #30, elapsed time: 361.751[sec]] loss: 4.61219017397343\n",
      "[EPOCH #31, elapsed time: 373.849[sec]] loss: 4.611883030743151\n",
      "[EPOCH #32, elapsed time: 387.073[sec]] loss: 4.612427971718484\n",
      "[EPOCH #33, elapsed time: 398.889[sec]] loss: 4.612210363740732\n",
      "[EPOCH #34, elapsed time: 410.363[sec]] loss: 4.611872198638135\n",
      "[EPOCH #35, elapsed time: 421.281[sec]] loss: 4.612210363740732\n",
      "[EPOCH #36, elapsed time: 432.597[sec]] loss: 4.612210364961045\n",
      "[EPOCH #37, elapsed time: 444.041[sec]] loss: 4.61143369180456\n",
      "[EPOCH #38, elapsed time: 456.496[sec]] loss: 4.611733716036063\n",
      "[EPOCH #39, elapsed time: 471.118[sec]] loss: 4.612506822828902\n",
      "[EPOCH #40, elapsed time: 482.080[sec]] loss: 4.6124435505345325\n",
      "[EPOCH #41, elapsed time: 493.214[sec]] loss: 4.611628145570566\n",
      "[EPOCH #42, elapsed time: 504.771[sec]] loss: 4.611743566704651\n",
      "[EPOCH #43, elapsed time: 516.217[sec]] loss: 4.611202325717196\n",
      "[EPOCH #44, elapsed time: 528.464[sec]] loss: 4.612459079622841\n",
      "[EPOCH #45, elapsed time: 540.422[sec]] loss: 4.612215675761588\n",
      "[EPOCH #46, elapsed time: 552.344[sec]] loss: 4.612190369528528\n",
      "[EPOCH #47, elapsed time: 565.152[sec]] loss: 4.611389032023425\n",
      "[EPOCH #48, elapsed time: 578.089[sec]] loss: 4.612258841269915\n",
      "[EPOCH #49, elapsed time: 590.803[sec]] loss: 4.612088287631747\n",
      "[EPOCH #50, elapsed time: 603.871[sec]] loss: 4.612209661450816\n",
      "[EPOCH #51, elapsed time: 616.007[sec]] loss: 4.612716150070259\n",
      "[EPOCH #52, elapsed time: 631.953[sec]] loss: 4.6123236862421795\n",
      "[EPOCH #53, elapsed time: 643.356[sec]] loss: 4.6121985450129594\n",
      "[EPOCH #54, elapsed time: 654.841[sec]] loss: 4.611989912129486\n",
      "[EPOCH #55, elapsed time: 670.805[sec]] loss: 4.612219422731504\n",
      "[EPOCH #56, elapsed time: 682.208[sec]] loss: 4.611983787075343\n",
      "[EPOCH #57, elapsed time: 693.586[sec]] loss: 4.6123035791510585\n",
      "[EPOCH #58, elapsed time: 707.854[sec]] loss: 4.612677134540092\n",
      "[EPOCH #59, elapsed time: 719.424[sec]] loss: 4.612210275573145\n",
      "[EPOCH #60, elapsed time: 731.323[sec]] loss: 4.61219880463447\n",
      "[EPOCH #61, elapsed time: 744.617[sec]] loss: 4.6122632789367755\n",
      "[EPOCH #62, elapsed time: 758.751[sec]] loss: 4.612515867786078\n",
      "[EPOCH #63, elapsed time: 773.485[sec]] loss: 4.612071287456569\n",
      "[EPOCH #64, elapsed time: 785.883[sec]] loss: 4.6116258529082215\n",
      "[EPOCH #65, elapsed time: 798.127[sec]] loss: 4.612053087103924\n",
      "[EPOCH #66, elapsed time: 810.109[sec]] loss: 4.6116038131469805\n",
      "[EPOCH #67, elapsed time: 822.163[sec]] loss: 4.612210363740732\n",
      "[EPOCH #68, elapsed time: 836.535[sec]] loss: 4.6120679193937235\n",
      "[EPOCH #69, elapsed time: 849.530[sec]] loss: 4.612290264930164\n",
      "[EPOCH #70, elapsed time: 860.976[sec]] loss: 4.612361211770632\n",
      "[EPOCH #71, elapsed time: 873.189[sec]] loss: 4.612018946112537\n",
      "[EPOCH #72, elapsed time: 884.652[sec]] loss: 4.611981405025099\n",
      "[EPOCH #73, elapsed time: 898.589[sec]] loss: 4.612568189605107\n",
      "[EPOCH #74, elapsed time: 911.313[sec]] loss: 4.612207882540087\n",
      "[EPOCH #75, elapsed time: 923.180[sec]] loss: 4.6122644123021255\n",
      "[EPOCH #76, elapsed time: 934.701[sec]] loss: 4.611773711172191\n",
      "[EPOCH #77, elapsed time: 946.482[sec]] loss: 4.612450214661777\n",
      "[EPOCH #78, elapsed time: 960.196[sec]] loss: 4.612210363740732\n",
      "[EPOCH #79, elapsed time: 971.962[sec]] loss: 4.612349418364346\n",
      "[EPOCH #80, elapsed time: 983.479[sec]] loss: 4.610203442173895\n",
      "[EPOCH #81, elapsed time: 995.567[sec]] loss: 4.611535544282568\n",
      "[EPOCH #82, elapsed time: 1011.378[sec]] loss: 4.612266691846109\n",
      "[EPOCH #83, elapsed time: 1024.138[sec]] loss: 4.6122100751367965\n",
      "[EPOCH #84, elapsed time: 1036.332[sec]] loss: 4.611989637559145\n",
      "[EPOCH #85, elapsed time: 1050.516[sec]] loss: 4.612159736325782\n",
      "[EPOCH #86, elapsed time: 1063.096[sec]] loss: 4.61218896433854\n",
      "[EPOCH #87, elapsed time: 1074.740[sec]] loss: 4.611620809661221\n",
      "[EPOCH #88, elapsed time: 1086.142[sec]] loss: 4.611803467885394\n",
      "[EPOCH #89, elapsed time: 1098.159[sec]] loss: 4.6121332027632995\n",
      "[EPOCH #90, elapsed time: 1109.730[sec]] loss: 4.613011449251279\n",
      "[EPOCH #91, elapsed time: 1124.251[sec]] loss: 4.612331781491056\n",
      "[EPOCH #92, elapsed time: 1135.769[sec]] loss: 4.6119969438759085\n",
      "[EPOCH #93, elapsed time: 1147.337[sec]] loss: 4.611885294117991\n",
      "[EPOCH #94, elapsed time: 1159.012[sec]] loss: 4.612223045534608\n",
      "[EPOCH #95, elapsed time: 1170.946[sec]] loss: 4.612176927174808\n",
      "[EPOCH #96, elapsed time: 1183.196[sec]] loss: 4.612229080285617\n",
      "[EPOCH #97, elapsed time: 1199.553[sec]] loss: 4.612489710384962\n",
      "[EPOCH #98, elapsed time: 1211.963[sec]] loss: 4.6123834257315\n",
      "[EPOCH #99, elapsed time: 1223.642[sec]] loss: 4.612548170071417\n",
      "[EPOCH #100, elapsed time: 1235.512[sec]] loss: 4.6117814253784495\n",
      "[EPOCH #101, elapsed time: 1247.072[sec]] loss: 4.61219899988449\n",
      "[EPOCH #102, elapsed time: 1258.747[sec]] loss: 4.61213148517328\n",
      "[EPOCH #103, elapsed time: 1270.376[sec]] loss: 4.611389218731256\n",
      "[EPOCH #104, elapsed time: 1281.600[sec]] loss: 4.612356151439254\n",
      "[EPOCH #105, elapsed time: 1293.409[sec]] loss: 4.612314215395897\n",
      "[EPOCH #106, elapsed time: 1305.334[sec]] loss: 4.612359233338789\n",
      "[EPOCH #107, elapsed time: 1316.730[sec]] loss: 4.61100864318877\n",
      "[EPOCH #108, elapsed time: 1329.669[sec]] loss: 4.61240741494216\n",
      "[EPOCH #109, elapsed time: 1342.154[sec]] loss: 4.61228383235724\n",
      "[EPOCH #110, elapsed time: 1353.797[sec]] loss: 4.611505072771244\n",
      "[EPOCH #111, elapsed time: 1365.734[sec]] loss: 4.612229492446207\n",
      "[EPOCH #112, elapsed time: 1377.611[sec]] loss: 4.611894774726775\n",
      "[EPOCH #113, elapsed time: 1389.166[sec]] loss: 4.6122099311399065\n",
      "[EPOCH #114, elapsed time: 1405.131[sec]] loss: 4.612325207056789\n",
      "[EPOCH #115, elapsed time: 1416.706[sec]] loss: 4.612221959456372\n",
      "[EPOCH #116, elapsed time: 1431.410[sec]] loss: 4.6123016602094555\n",
      "[EPOCH #117, elapsed time: 1445.992[sec]] loss: 4.612210363740732\n",
      "[EPOCH #118, elapsed time: 1459.468[sec]] loss: 4.612406219951022\n",
      "[EPOCH #119, elapsed time: 1471.578[sec]] loss: 4.610421188046973\n",
      "[EPOCH #120, elapsed time: 1487.234[sec]] loss: 4.612349176437368\n",
      "[EPOCH #121, elapsed time: 1502.152[sec]] loss: 4.6122211479484765\n",
      "[EPOCH #122, elapsed time: 1514.308[sec]] loss: 4.611014047953385\n",
      "[EPOCH #123, elapsed time: 1525.901[sec]] loss: 4.611922329080769\n",
      "[EPOCH #124, elapsed time: 1538.378[sec]] loss: 4.612199022155195\n",
      "[EPOCH #125, elapsed time: 1550.566[sec]] loss: 4.61214501355904\n",
      "[EPOCH #126, elapsed time: 1565.470[sec]] loss: 4.610783008909805\n",
      "[EPOCH #127, elapsed time: 1576.804[sec]] loss: 4.612067816277307\n",
      "[EPOCH #128, elapsed time: 1589.454[sec]] loss: 4.61196093184019\n",
      "[EPOCH #129, elapsed time: 1600.668[sec]] loss: 4.612222430802124\n",
      "[EPOCH #130, elapsed time: 1616.460[sec]] loss: 4.612245301901341\n",
      "[EPOCH #131, elapsed time: 1627.637[sec]] loss: 4.612495188368335\n",
      "[EPOCH #132, elapsed time: 1643.295[sec]] loss: 4.611759587884025\n",
      "[EPOCH #133, elapsed time: 1654.522[sec]] loss: 4.6125846601646066\n",
      "[EPOCH #134, elapsed time: 1667.893[sec]] loss: 4.61202389966656\n",
      "[EPOCH #135, elapsed time: 1679.802[sec]] loss: 4.6122213868246735\n",
      "[EPOCH #136, elapsed time: 1692.546[sec]] loss: 4.611748754558698\n",
      "[EPOCH #137, elapsed time: 1704.250[sec]] loss: 4.612262689525777\n",
      "[EPOCH #138, elapsed time: 1718.612[sec]] loss: 4.61240225729085\n",
      "[EPOCH #139, elapsed time: 1731.156[sec]] loss: 4.612223818297578\n",
      "[EPOCH #140, elapsed time: 1744.530[sec]] loss: 4.612017005205307\n",
      "[EPOCH #141, elapsed time: 1756.640[sec]] loss: 4.613143667073412\n",
      "[EPOCH #142, elapsed time: 1772.168[sec]] loss: 4.611930346229636\n",
      "[EPOCH #143, elapsed time: 1785.017[sec]] loss: 4.611450579710977\n",
      "[EPOCH #144, elapsed time: 1796.956[sec]] loss: 4.6115356342806235\n",
      "[EPOCH #145, elapsed time: 1808.367[sec]] loss: 4.612209017430828\n",
      "[EPOCH #146, elapsed time: 1821.153[sec]] loss: 4.612268877120942\n",
      "[EPOCH #147, elapsed time: 1833.163[sec]] loss: 4.6122282358292805\n",
      "[EPOCH #148, elapsed time: 1844.537[sec]] loss: 4.6124468270739305\n",
      "[EPOCH #149, elapsed time: 1860.614[sec]] loss: 4.612672447319299\n",
      "[EPOCH #150, elapsed time: 1872.174[sec]] loss: 4.611768292984135\n",
      "[EPOCH #151, elapsed time: 1883.466[sec]] loss: 4.612231167020206\n",
      "[EPOCH #152, elapsed time: 1894.736[sec]] loss: 4.611812836530494\n",
      "[EPOCH #153, elapsed time: 1906.798[sec]] loss: 4.613483558613295\n",
      "[EPOCH #154, elapsed time: 1922.231[sec]] loss: 4.612431885871228\n",
      "[EPOCH #155, elapsed time: 1933.657[sec]] loss: 4.611633168072252\n",
      "[EPOCH #156, elapsed time: 1945.977[sec]] loss: 4.612445573507786\n",
      "[EPOCH #157, elapsed time: 1957.883[sec]] loss: 4.6122102377434535\n",
      "[EPOCH #158, elapsed time: 1969.460[sec]] loss: 4.6122900968320995\n",
      "[EPOCH #159, elapsed time: 1980.798[sec]] loss: 4.612488685932513\n",
      "[EPOCH #160, elapsed time: 1992.575[sec]] loss: 4.611101682874078\n",
      "[EPOCH #161, elapsed time: 2004.951[sec]] loss: 4.612088537490757\n",
      "[EPOCH #162, elapsed time: 2017.290[sec]] loss: 4.610195772203969\n",
      "[EPOCH #163, elapsed time: 2032.826[sec]] loss: 4.609472676263127\n",
      "[EPOCH #164, elapsed time: 2046.964[sec]] loss: 4.612168989956417\n",
      "[EPOCH #165, elapsed time: 2059.220[sec]] loss: 4.612229936029846\n",
      "[EPOCH #166, elapsed time: 2070.859[sec]] loss: 4.611842994726551\n",
      "[EPOCH #167, elapsed time: 2082.526[sec]] loss: 4.611246013824404\n",
      "[EPOCH #168, elapsed time: 2094.208[sec]] loss: 4.612044411596394\n",
      "[EPOCH #169, elapsed time: 2106.087[sec]] loss: 4.612122102189506\n",
      "[EPOCH #170, elapsed time: 2117.506[sec]] loss: 4.612199131678254\n",
      "[EPOCH #171, elapsed time: 2128.807[sec]] loss: 4.61229903348653\n",
      "[EPOCH #172, elapsed time: 2141.381[sec]] loss: 4.610316186247159\n",
      "[EPOCH #173, elapsed time: 2156.041[sec]] loss: 4.609143990854079\n",
      "[EPOCH #174, elapsed time: 2167.596[sec]] loss: 4.610367391480136\n",
      "[EPOCH #175, elapsed time: 2180.160[sec]] loss: 4.612151658466361\n",
      "[EPOCH #176, elapsed time: 2194.203[sec]] loss: 4.612210363740732\n",
      "[EPOCH #177, elapsed time: 2205.892[sec]] loss: 4.612372948127303\n",
      "[EPOCH #178, elapsed time: 2217.379[sec]] loss: 4.612210340554792\n",
      "[EPOCH #179, elapsed time: 2229.090[sec]] loss: 4.611863918816975\n",
      "[EPOCH #180, elapsed time: 2241.400[sec]] loss: 4.611632578051098\n",
      "[EPOCH #181, elapsed time: 2252.961[sec]] loss: 4.612071533349562\n",
      "[EPOCH #182, elapsed time: 2264.551[sec]] loss: 4.612203028746621\n",
      "[EPOCH #183, elapsed time: 2277.156[sec]] loss: 4.611636560846428\n",
      "[EPOCH #184, elapsed time: 2288.668[sec]] loss: 4.6122005667659005\n",
      "[EPOCH #185, elapsed time: 2303.349[sec]] loss: 4.612190086721077\n",
      "[EPOCH #186, elapsed time: 2319.342[sec]] loss: 4.6117595317496445\n",
      "[EPOCH #187, elapsed time: 2336.066[sec]] loss: 4.612210363740732\n",
      "[EPOCH #188, elapsed time: 2352.345[sec]] loss: 4.611595081810149\n",
      "[EPOCH #189, elapsed time: 2367.200[sec]] loss: 4.609449473543954\n",
      "[EPOCH #190, elapsed time: 2384.143[sec]] loss: 4.60783467594813\n",
      "[EPOCH #191, elapsed time: 2398.895[sec]] loss: 4.610466653234441\n",
      "[EPOCH #192, elapsed time: 2412.653[sec]] loss: 4.61233740987796\n",
      "[EPOCH #193, elapsed time: 2424.904[sec]] loss: 4.612210363740732\n",
      "[EPOCH #194, elapsed time: 2440.768[sec]] loss: 4.61179416605241\n",
      "[EPOCH #195, elapsed time: 2455.880[sec]] loss: 4.6122103719778424\n",
      "[EPOCH #196, elapsed time: 2468.310[sec]] loss: 4.6123216095751705\n",
      "[EPOCH #197, elapsed time: 2480.244[sec]] loss: 4.611663391860112\n",
      "[EPOCH #198, elapsed time: 2491.702[sec]] loss: 4.612064078154659\n",
      "[EPOCH #199, elapsed time: 2504.132[sec]] loss: 4.61214213941773\n",
      "[EPOCH #200, elapsed time: 2515.703[sec]] loss: 4.612785645348859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 02:19:11,036] Trial 2 finished with value: 0.01 and parameters: {'learning_rate': 0.046820082377766165}. Best is trial 0 with value: 0.0101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605169239215034\n",
      "[EPOCH #1, elapsed time: 13.302[sec]] loss: 4.590551226549398\n",
      "[EPOCH #2, elapsed time: 25.155[sec]] loss: 4.589696965611141\n",
      "[EPOCH #3, elapsed time: 36.619[sec]] loss: 4.5944331602949555\n",
      "[EPOCH #4, elapsed time: 49.031[sec]] loss: 4.598609345278065\n",
      "[EPOCH #5, elapsed time: 61.403[sec]] loss: 4.5903693502405405\n",
      "[EPOCH #6, elapsed time: 72.849[sec]] loss: 4.59002034449074\n",
      "[EPOCH #7, elapsed time: 85.085[sec]] loss: 4.588324534000682\n",
      "[EPOCH #8, elapsed time: 96.707[sec]] loss: 4.585706269977494\n",
      "[EPOCH #9, elapsed time: 108.636[sec]] loss: 4.581728222579126\n",
      "[EPOCH #10, elapsed time: 120.191[sec]] loss: 4.580251186075534\n",
      "[EPOCH #11, elapsed time: 132.509[sec]] loss: 4.587945290429426\n",
      "[EPOCH #12, elapsed time: 145.098[sec]] loss: 4.576417883618551\n",
      "[EPOCH #13, elapsed time: 157.598[sec]] loss: 4.577759941647774\n",
      "[EPOCH #14, elapsed time: 171.353[sec]] loss: 4.574677011864504\n",
      "[EPOCH #15, elapsed time: 185.717[sec]] loss: 4.572487713355554\n",
      "[EPOCH #16, elapsed time: 199.303[sec]] loss: 4.570615490811495\n",
      "[EPOCH #17, elapsed time: 211.797[sec]] loss: 4.569864277037305\n",
      "[EPOCH #18, elapsed time: 223.845[sec]] loss: 4.567049298008817\n",
      "[EPOCH #19, elapsed time: 236.020[sec]] loss: 4.564812885670043\n",
      "[EPOCH #20, elapsed time: 247.578[sec]] loss: 4.563948626405371\n",
      "[EPOCH #21, elapsed time: 259.935[sec]] loss: 4.56051970718003\n",
      "[EPOCH #22, elapsed time: 275.826[sec]] loss: 4.558074017022561\n",
      "[EPOCH #23, elapsed time: 288.314[sec]] loss: 4.5557981757727175\n",
      "[EPOCH #24, elapsed time: 300.442[sec]] loss: 4.551302512029158\n",
      "[EPOCH #25, elapsed time: 312.445[sec]] loss: 4.551462075836904\n",
      "[EPOCH #26, elapsed time: 324.817[sec]] loss: 4.55105123806671\n",
      "[EPOCH #27, elapsed time: 336.602[sec]] loss: 4.549122461659437\n",
      "[EPOCH #28, elapsed time: 348.823[sec]] loss: 4.54656102820535\n",
      "[EPOCH #29, elapsed time: 363.140[sec]] loss: 4.545427802275635\n",
      "[EPOCH #30, elapsed time: 378.308[sec]] loss: 4.544333837158925\n",
      "[EPOCH #31, elapsed time: 393.735[sec]] loss: 4.546374248375286\n",
      "[EPOCH #32, elapsed time: 408.470[sec]] loss: 4.549780379787745\n",
      "[EPOCH #33, elapsed time: 421.944[sec]] loss: 4.539390343393337\n",
      "[EPOCH #34, elapsed time: 433.605[sec]] loss: 4.538005032908512\n",
      "[EPOCH #35, elapsed time: 448.285[sec]] loss: 4.544667826656798\n",
      "[EPOCH #36, elapsed time: 459.975[sec]] loss: 4.541021181083382\n",
      "[EPOCH #37, elapsed time: 473.657[sec]] loss: 4.5388577785235675\n",
      "[EPOCH #38, elapsed time: 484.825[sec]] loss: 4.535966896049807\n",
      "[EPOCH #39, elapsed time: 496.975[sec]] loss: 4.53863493784528\n",
      "[EPOCH #40, elapsed time: 508.690[sec]] loss: 4.535924223242703\n",
      "[EPOCH #41, elapsed time: 520.470[sec]] loss: 4.53633925858325\n",
      "[EPOCH #42, elapsed time: 532.577[sec]] loss: 4.542103552619998\n",
      "[EPOCH #43, elapsed time: 543.702[sec]] loss: 4.5313901721249\n",
      "[EPOCH #44, elapsed time: 555.153[sec]] loss: 4.525967053701995\n",
      "[EPOCH #45, elapsed time: 566.516[sec]] loss: 4.527175876778513\n",
      "[EPOCH #46, elapsed time: 578.914[sec]] loss: 4.529977825003714\n",
      "[EPOCH #47, elapsed time: 591.370[sec]] loss: 4.529140433362105\n",
      "[EPOCH #48, elapsed time: 603.255[sec]] loss: 4.527182236437758\n",
      "[EPOCH #49, elapsed time: 618.052[sec]] loss: 4.529463306117043\n",
      "[EPOCH #50, elapsed time: 630.944[sec]] loss: 4.523854287709476\n",
      "[EPOCH #51, elapsed time: 647.266[sec]] loss: 4.522750889881253\n",
      "[EPOCH #52, elapsed time: 663.091[sec]] loss: 4.52102891710883\n",
      "[EPOCH #53, elapsed time: 678.279[sec]] loss: 4.522107086193844\n",
      "[EPOCH #54, elapsed time: 690.167[sec]] loss: 4.519623184692227\n",
      "[EPOCH #55, elapsed time: 702.687[sec]] loss: 4.519832643422269\n",
      "[EPOCH #56, elapsed time: 714.644[sec]] loss: 4.517180942223001\n",
      "[EPOCH #57, elapsed time: 726.262[sec]] loss: 4.513436870855623\n",
      "[EPOCH #58, elapsed time: 737.457[sec]] loss: 4.512801104452239\n",
      "[EPOCH #59, elapsed time: 752.800[sec]] loss: 4.512124558251711\n",
      "[EPOCH #60, elapsed time: 764.223[sec]] loss: 4.5137394080158995\n",
      "[EPOCH #61, elapsed time: 776.348[sec]] loss: 4.511196818019211\n",
      "[EPOCH #62, elapsed time: 788.419[sec]] loss: 4.5115477698625694\n",
      "[EPOCH #63, elapsed time: 800.003[sec]] loss: 4.510271005880657\n",
      "[EPOCH #64, elapsed time: 812.517[sec]] loss: 4.508853959945708\n",
      "[EPOCH #65, elapsed time: 827.171[sec]] loss: 4.5093331526123555\n",
      "[EPOCH #66, elapsed time: 838.725[sec]] loss: 4.50915028632488\n",
      "[EPOCH #67, elapsed time: 853.258[sec]] loss: 4.509528384632738\n",
      "[EPOCH #68, elapsed time: 864.861[sec]] loss: 4.506936101980569\n",
      "[EPOCH #69, elapsed time: 877.380[sec]] loss: 4.507279090673895\n",
      "[EPOCH #70, elapsed time: 889.017[sec]] loss: 4.507476072317503\n",
      "[EPOCH #71, elapsed time: 901.241[sec]] loss: 4.50281900941601\n",
      "[EPOCH #72, elapsed time: 913.209[sec]] loss: 4.507136934320666\n",
      "[EPOCH #73, elapsed time: 925.097[sec]] loss: 4.502142389386568\n",
      "[EPOCH #74, elapsed time: 937.031[sec]] loss: 4.501690784632511\n",
      "[EPOCH #75, elapsed time: 951.258[sec]] loss: 4.498880816817818\n",
      "[EPOCH #76, elapsed time: 962.832[sec]] loss: 4.498438845211622\n",
      "[EPOCH #77, elapsed time: 974.325[sec]] loss: 4.496309126636117\n",
      "[EPOCH #78, elapsed time: 985.777[sec]] loss: 4.496963673345721\n",
      "[EPOCH #79, elapsed time: 997.270[sec]] loss: 4.495510472293398\n",
      "[EPOCH #80, elapsed time: 1008.897[sec]] loss: 4.493350889007021\n",
      "[EPOCH #81, elapsed time: 1024.699[sec]] loss: 4.489554232843244\n",
      "[EPOCH #82, elapsed time: 1036.857[sec]] loss: 4.493309005742186\n",
      "[EPOCH #83, elapsed time: 1052.998[sec]] loss: 4.491226093172646\n",
      "[EPOCH #84, elapsed time: 1065.537[sec]] loss: 4.492036521091571\n",
      "[EPOCH #85, elapsed time: 1076.912[sec]] loss: 4.491211503114902\n",
      "[EPOCH #86, elapsed time: 1093.556[sec]] loss: 4.489515577915458\n",
      "[EPOCH #87, elapsed time: 1104.964[sec]] loss: 4.50401226191359\n",
      "[EPOCH #88, elapsed time: 1116.825[sec]] loss: 4.49821006344132\n",
      "[EPOCH #89, elapsed time: 1131.346[sec]] loss: 4.499463114613382\n",
      "[EPOCH #90, elapsed time: 1143.345[sec]] loss: 4.492621412356542\n",
      "[EPOCH #91, elapsed time: 1155.713[sec]] loss: 4.4838016663159\n",
      "[EPOCH #92, elapsed time: 1168.129[sec]] loss: 4.488314204542437\n",
      "[EPOCH #93, elapsed time: 1179.813[sec]] loss: 4.4860568476730025\n",
      "[EPOCH #94, elapsed time: 1191.621[sec]] loss: 4.483302299135858\n",
      "[EPOCH #95, elapsed time: 1204.078[sec]] loss: 4.487866961795858\n",
      "[EPOCH #96, elapsed time: 1218.891[sec]] loss: 4.485921568086494\n",
      "[EPOCH #97, elapsed time: 1230.715[sec]] loss: 4.480200010008028\n",
      "[EPOCH #98, elapsed time: 1242.813[sec]] loss: 4.48176403985295\n",
      "[EPOCH #99, elapsed time: 1254.126[sec]] loss: 4.479060999491393\n",
      "[EPOCH #100, elapsed time: 1267.939[sec]] loss: 4.482170421956673\n",
      "[EPOCH #101, elapsed time: 1279.329[sec]] loss: 4.475857013475414\n",
      "[EPOCH #102, elapsed time: 1291.035[sec]] loss: 4.476042195306095\n",
      "[EPOCH #103, elapsed time: 1302.613[sec]] loss: 4.475239574642266\n",
      "[EPOCH #104, elapsed time: 1314.572[sec]] loss: 4.482671513743532\n",
      "[EPOCH #105, elapsed time: 1326.660[sec]] loss: 4.486527771501303\n",
      "[EPOCH #106, elapsed time: 1338.296[sec]] loss: 4.479611588378633\n",
      "[EPOCH #107, elapsed time: 1349.844[sec]] loss: 4.486165299601686\n",
      "[EPOCH #108, elapsed time: 1361.861[sec]] loss: 4.482738871339492\n",
      "[EPOCH #109, elapsed time: 1374.126[sec]] loss: 4.478514921489772\n",
      "[EPOCH #110, elapsed time: 1386.616[sec]] loss: 4.479535587308351\n",
      "[EPOCH #111, elapsed time: 1397.964[sec]] loss: 4.4803179727482325\n",
      "[EPOCH #112, elapsed time: 1409.711[sec]] loss: 4.474110531334075\n",
      "[EPOCH #113, elapsed time: 1421.387[sec]] loss: 4.478312876769082\n",
      "[EPOCH #114, elapsed time: 1433.877[sec]] loss: 4.492859037427359\n",
      "[EPOCH #115, elapsed time: 1446.577[sec]] loss: 4.475675346755249\n",
      "[EPOCH #116, elapsed time: 1460.276[sec]] loss: 4.477023497500332\n",
      "[EPOCH #117, elapsed time: 1473.562[sec]] loss: 4.47558644545513\n",
      "[EPOCH #118, elapsed time: 1485.198[sec]] loss: 4.4722698962772345\n",
      "[EPOCH #119, elapsed time: 1496.866[sec]] loss: 4.473505620230335\n",
      "[EPOCH #120, elapsed time: 1509.266[sec]] loss: 4.473337182614259\n",
      "[EPOCH #121, elapsed time: 1522.289[sec]] loss: 4.470396317355692\n",
      "[EPOCH #122, elapsed time: 1535.366[sec]] loss: 4.4690555632305085\n",
      "[EPOCH #123, elapsed time: 1547.523[sec]] loss: 4.46964254092499\n",
      "[EPOCH #124, elapsed time: 1559.912[sec]] loss: 4.4707648404805385\n",
      "[EPOCH #125, elapsed time: 1573.399[sec]] loss: 4.471360726457182\n",
      "[EPOCH #126, elapsed time: 1584.981[sec]] loss: 4.467790275068521\n",
      "[EPOCH #127, elapsed time: 1596.544[sec]] loss: 4.477107435789004\n",
      "[EPOCH #128, elapsed time: 1608.109[sec]] loss: 4.466118170981672\n",
      "[EPOCH #129, elapsed time: 1619.541[sec]] loss: 4.464499895189789\n",
      "[EPOCH #130, elapsed time: 1633.225[sec]] loss: 4.469056132811426\n",
      "[EPOCH #131, elapsed time: 1645.231[sec]] loss: 4.466119572510722\n",
      "[EPOCH #132, elapsed time: 1656.871[sec]] loss: 4.462317449041307\n",
      "[EPOCH #133, elapsed time: 1668.776[sec]] loss: 4.464755736477316\n",
      "[EPOCH #134, elapsed time: 1682.257[sec]] loss: 4.470874876375009\n",
      "[EPOCH #135, elapsed time: 1693.835[sec]] loss: 4.458834462949883\n",
      "[EPOCH #136, elapsed time: 1706.304[sec]] loss: 4.462984553377978\n",
      "[EPOCH #137, elapsed time: 1722.526[sec]] loss: 4.464236869616761\n",
      "[EPOCH #138, elapsed time: 1738.520[sec]] loss: 4.460730268645577\n",
      "[EPOCH #139, elapsed time: 1754.254[sec]] loss: 4.459288164506108\n",
      "[EPOCH #140, elapsed time: 1766.422[sec]] loss: 4.474245716651433\n",
      "[EPOCH #141, elapsed time: 1779.047[sec]] loss: 4.4605389754892695\n",
      "[EPOCH #142, elapsed time: 1793.867[sec]] loss: 4.4652056700132325\n",
      "[EPOCH #143, elapsed time: 1806.461[sec]] loss: 4.460467156125274\n",
      "[EPOCH #144, elapsed time: 1820.604[sec]] loss: 4.459006365033502\n",
      "[EPOCH #145, elapsed time: 1832.685[sec]] loss: 4.462259902758851\n",
      "[EPOCH #146, elapsed time: 1844.859[sec]] loss: 4.461577627190549\n",
      "[EPOCH #147, elapsed time: 1859.142[sec]] loss: 4.463711028974634\n",
      "[EPOCH #148, elapsed time: 1873.016[sec]] loss: 4.460435120173166\n",
      "[EPOCH #149, elapsed time: 1887.416[sec]] loss: 4.458776734230691\n",
      "[EPOCH #150, elapsed time: 1902.991[sec]] loss: 4.456296027812604\n",
      "[EPOCH #151, elapsed time: 1918.894[sec]] loss: 4.455188596546078\n",
      "[EPOCH #152, elapsed time: 1930.406[sec]] loss: 4.455277424932517\n",
      "[EPOCH #153, elapsed time: 1944.576[sec]] loss: 4.454139779030476\n",
      "[EPOCH #154, elapsed time: 1960.642[sec]] loss: 4.451300985601112\n",
      "[EPOCH #155, elapsed time: 1974.906[sec]] loss: 4.457452740184176\n",
      "[EPOCH #156, elapsed time: 1988.914[sec]] loss: 4.458298490967265\n",
      "[EPOCH #157, elapsed time: 2000.582[sec]] loss: 4.453595303604409\n",
      "[EPOCH #158, elapsed time: 2012.058[sec]] loss: 4.451878533024706\n",
      "[EPOCH #159, elapsed time: 2025.944[sec]] loss: 4.451305581908613\n",
      "[EPOCH #160, elapsed time: 2037.405[sec]] loss: 4.4616872112642705\n",
      "[EPOCH #161, elapsed time: 2049.036[sec]] loss: 4.498106447916647\n",
      "[EPOCH #162, elapsed time: 2060.024[sec]] loss: 4.459894156196677\n",
      "[EPOCH #163, elapsed time: 2071.322[sec]] loss: 4.45332638270109\n",
      "[EPOCH #164, elapsed time: 2086.674[sec]] loss: 4.452636572007407\n",
      "[EPOCH #165, elapsed time: 2098.434[sec]] loss: 4.455261679543796\n",
      "[EPOCH #166, elapsed time: 2113.631[sec]] loss: 4.451539991150586\n",
      "[EPOCH #167, elapsed time: 2125.815[sec]] loss: 4.449556446502549\n",
      "[EPOCH #168, elapsed time: 2137.178[sec]] loss: 4.466352461548242\n",
      "[EPOCH #169, elapsed time: 2148.783[sec]] loss: 4.4527106559665555\n",
      "[EPOCH #170, elapsed time: 2160.663[sec]] loss: 4.457561292178495\n",
      "[EPOCH #171, elapsed time: 2172.911[sec]] loss: 4.452604193147451\n",
      "[EPOCH #172, elapsed time: 2184.118[sec]] loss: 4.457678811029982\n",
      "[EPOCH #173, elapsed time: 2198.753[sec]] loss: 4.455400492240432\n",
      "[EPOCH #174, elapsed time: 2213.786[sec]] loss: 4.456556603913115\n",
      "[EPOCH #175, elapsed time: 2225.585[sec]] loss: 4.453365784765281\n",
      "[EPOCH #176, elapsed time: 2237.578[sec]] loss: 4.448077063642857\n",
      "[EPOCH #177, elapsed time: 2251.736[sec]] loss: 4.4477500982644536\n",
      "[EPOCH #178, elapsed time: 2264.751[sec]] loss: 4.450431043645623\n",
      "[EPOCH #179, elapsed time: 2278.174[sec]] loss: 4.45225946123754\n",
      "[EPOCH #180, elapsed time: 2292.856[sec]] loss: 4.447818571226153\n",
      "[EPOCH #181, elapsed time: 2308.063[sec]] loss: 4.443926415531893\n",
      "[EPOCH #182, elapsed time: 2321.311[sec]] loss: 4.448482284008961\n",
      "[EPOCH #183, elapsed time: 2333.344[sec]] loss: 4.450636761202236\n",
      "[EPOCH #184, elapsed time: 2345.455[sec]] loss: 4.451057305644127\n",
      "[EPOCH #185, elapsed time: 2360.783[sec]] loss: 4.450035777979758\n",
      "[EPOCH #186, elapsed time: 2374.767[sec]] loss: 4.4464013574982175\n",
      "[EPOCH #187, elapsed time: 2388.196[sec]] loss: 4.448936258610136\n",
      "[EPOCH #188, elapsed time: 2400.035[sec]] loss: 4.445316641435971\n",
      "[EPOCH #189, elapsed time: 2415.816[sec]] loss: 4.446111653755661\n",
      "[EPOCH #190, elapsed time: 2427.133[sec]] loss: 4.44319378071234\n",
      "[EPOCH #191, elapsed time: 2439.462[sec]] loss: 4.44465654626994\n",
      "[EPOCH #192, elapsed time: 2451.741[sec]] loss: 4.448673168970695\n",
      "[EPOCH #193, elapsed time: 2462.718[sec]] loss: 4.444495032707698\n",
      "[EPOCH #194, elapsed time: 2476.425[sec]] loss: 4.439676158182604\n",
      "[EPOCH #195, elapsed time: 2488.459[sec]] loss: 4.44404457138657\n",
      "[EPOCH #196, elapsed time: 2500.466[sec]] loss: 4.4378300124990275\n",
      "[EPOCH #197, elapsed time: 2515.670[sec]] loss: 4.442692183410976\n",
      "[EPOCH #198, elapsed time: 2531.988[sec]] loss: 4.439482990015949\n",
      "[EPOCH #199, elapsed time: 2545.244[sec]] loss: 4.442907151852521\n",
      "[EPOCH #200, elapsed time: 2558.100[sec]] loss: 4.441402329135536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 03:02:07,705] Trial 3 finished with value: 0.21588 and parameters: {'learning_rate': 0.0058453502791334395}. Best is trial 3 with value: 0.21588.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605155736760916\n",
      "[EPOCH #1, elapsed time: 11.943[sec]] loss: 4.5787288713973835\n",
      "[EPOCH #2, elapsed time: 23.922[sec]] loss: 4.563946440825459\n",
      "[EPOCH #3, elapsed time: 35.187[sec]] loss: 4.548327403883101\n",
      "[EPOCH #4, elapsed time: 46.166[sec]] loss: 4.538122799178384\n",
      "[EPOCH #5, elapsed time: 58.446[sec]] loss: 4.531388799273198\n",
      "[EPOCH #6, elapsed time: 69.647[sec]] loss: 4.52549477044543\n",
      "[EPOCH #7, elapsed time: 81.921[sec]] loss: 4.516663722479412\n",
      "[EPOCH #8, elapsed time: 94.708[sec]] loss: 4.508134435554841\n",
      "[EPOCH #9, elapsed time: 105.726[sec]] loss: 4.501199252774756\n",
      "[EPOCH #10, elapsed time: 117.609[sec]] loss: 4.497312365780255\n",
      "[EPOCH #11, elapsed time: 129.941[sec]] loss: 4.492112763783753\n",
      "[EPOCH #12, elapsed time: 142.223[sec]] loss: 4.484205846365491\n",
      "[EPOCH #13, elapsed time: 153.403[sec]] loss: 4.479701515046435\n",
      "[EPOCH #14, elapsed time: 164.587[sec]] loss: 4.478197662477988\n",
      "[EPOCH #15, elapsed time: 175.574[sec]] loss: 4.469623159004646\n",
      "[EPOCH #16, elapsed time: 189.704[sec]] loss: 4.470383867116136\n",
      "[EPOCH #17, elapsed time: 201.661[sec]] loss: 4.46159015766566\n",
      "[EPOCH #18, elapsed time: 212.636[sec]] loss: 4.4579050444824455\n",
      "[EPOCH #19, elapsed time: 224.105[sec]] loss: 4.45273759802869\n",
      "[EPOCH #20, elapsed time: 235.121[sec]] loss: 4.452203883319349\n",
      "[EPOCH #21, elapsed time: 246.322[sec]] loss: 4.444316438734722\n",
      "[EPOCH #22, elapsed time: 259.011[sec]] loss: 4.445295109934938\n",
      "[EPOCH #23, elapsed time: 272.231[sec]] loss: 4.435441425345452\n",
      "[EPOCH #24, elapsed time: 283.366[sec]] loss: 4.439099658092321\n",
      "[EPOCH #25, elapsed time: 297.160[sec]] loss: 4.430421800851365\n",
      "[EPOCH #26, elapsed time: 308.885[sec]] loss: 4.424636662349591\n",
      "[EPOCH #27, elapsed time: 320.764[sec]] loss: 4.422903613714705\n",
      "[EPOCH #28, elapsed time: 331.736[sec]] loss: 4.416882825523176\n",
      "[EPOCH #29, elapsed time: 343.042[sec]] loss: 4.4170543560216\n",
      "[EPOCH #30, elapsed time: 356.664[sec]] loss: 4.411249651942433\n",
      "[EPOCH #31, elapsed time: 369.140[sec]] loss: 4.420511261286525\n",
      "[EPOCH #32, elapsed time: 381.450[sec]] loss: 4.405789286222354\n",
      "[EPOCH #33, elapsed time: 392.527[sec]] loss: 4.4076547680645515\n",
      "[EPOCH #34, elapsed time: 404.065[sec]] loss: 4.402922132117429\n",
      "[EPOCH #35, elapsed time: 416.676[sec]] loss: 4.393149067481511\n",
      "[EPOCH #36, elapsed time: 427.795[sec]] loss: 4.388100024910973\n",
      "[EPOCH #37, elapsed time: 439.290[sec]] loss: 4.385271962910833\n",
      "[EPOCH #38, elapsed time: 450.545[sec]] loss: 4.383932995018254\n",
      "[EPOCH #39, elapsed time: 461.596[sec]] loss: 4.382664840341911\n",
      "[EPOCH #40, elapsed time: 473.929[sec]] loss: 4.379565913175362\n",
      "[EPOCH #41, elapsed time: 487.271[sec]] loss: 4.384205684857116\n",
      "[EPOCH #42, elapsed time: 499.395[sec]] loss: 4.384727960966065\n",
      "[EPOCH #43, elapsed time: 514.192[sec]] loss: 4.375630825052487\n",
      "[EPOCH #44, elapsed time: 529.290[sec]] loss: 4.3803160753451476\n",
      "[EPOCH #45, elapsed time: 541.616[sec]] loss: 4.36746497865068\n",
      "[EPOCH #46, elapsed time: 552.683[sec]] loss: 4.366539295865264\n",
      "[EPOCH #47, elapsed time: 563.803[sec]] loss: 4.367785076719786\n",
      "[EPOCH #48, elapsed time: 575.277[sec]] loss: 4.359942129614715\n",
      "[EPOCH #49, elapsed time: 587.345[sec]] loss: 4.352903713572887\n",
      "[EPOCH #50, elapsed time: 599.696[sec]] loss: 4.349416301102495\n",
      "[EPOCH #51, elapsed time: 611.776[sec]] loss: 4.348823445772255\n",
      "[EPOCH #52, elapsed time: 623.856[sec]] loss: 4.350245963283936\n",
      "[EPOCH #53, elapsed time: 634.953[sec]] loss: 4.3483819360543885\n",
      "[EPOCH #54, elapsed time: 647.035[sec]] loss: 4.342129629846574\n",
      "[EPOCH #55, elapsed time: 659.845[sec]] loss: 4.340049872700404\n",
      "[EPOCH #56, elapsed time: 671.414[sec]] loss: 4.3432015606934495\n",
      "[EPOCH #57, elapsed time: 683.209[sec]] loss: 4.333728530051536\n",
      "[EPOCH #58, elapsed time: 698.932[sec]] loss: 4.327336591402079\n",
      "[EPOCH #59, elapsed time: 710.641[sec]] loss: 4.33003408818846\n",
      "[EPOCH #60, elapsed time: 723.175[sec]] loss: 4.322519093084549\n",
      "[EPOCH #61, elapsed time: 735.927[sec]] loss: 4.31650692884234\n",
      "[EPOCH #62, elapsed time: 750.762[sec]] loss: 4.323205456700145\n",
      "[EPOCH #63, elapsed time: 762.579[sec]] loss: 4.311575987822569\n",
      "[EPOCH #64, elapsed time: 777.177[sec]] loss: 4.31396610616341\n",
      "[EPOCH #65, elapsed time: 788.778[sec]] loss: 4.312800485814754\n",
      "[EPOCH #66, elapsed time: 801.308[sec]] loss: 4.302152749596692\n",
      "[EPOCH #67, elapsed time: 812.954[sec]] loss: 4.300531475191611\n",
      "[EPOCH #68, elapsed time: 826.863[sec]] loss: 4.298486115912635\n",
      "[EPOCH #69, elapsed time: 838.169[sec]] loss: 4.291917156883333\n",
      "[EPOCH #70, elapsed time: 854.486[sec]] loss: 4.292957585359794\n",
      "[EPOCH #71, elapsed time: 865.894[sec]] loss: 4.288526115704254\n",
      "[EPOCH #72, elapsed time: 878.155[sec]] loss: 4.283763080091714\n",
      "[EPOCH #73, elapsed time: 890.092[sec]] loss: 4.280026031776033\n",
      "[EPOCH #74, elapsed time: 902.035[sec]] loss: 4.28108622245276\n",
      "[EPOCH #75, elapsed time: 913.357[sec]] loss: 4.274667774647074\n",
      "[EPOCH #76, elapsed time: 925.992[sec]] loss: 4.272008944076372\n",
      "[EPOCH #77, elapsed time: 937.504[sec]] loss: 4.278897922769694\n",
      "[EPOCH #78, elapsed time: 949.073[sec]] loss: 4.276613291913092\n",
      "[EPOCH #79, elapsed time: 960.437[sec]] loss: 4.278133097170868\n",
      "[EPOCH #80, elapsed time: 971.974[sec]] loss: 4.2683605949465315\n",
      "[EPOCH #81, elapsed time: 983.500[sec]] loss: 4.269627035388715\n",
      "[EPOCH #82, elapsed time: 997.663[sec]] loss: 4.273083690337012\n",
      "[EPOCH #83, elapsed time: 1009.437[sec]] loss: 4.2604830632664346\n",
      "[EPOCH #84, elapsed time: 1021.127[sec]] loss: 4.254218503441936\n",
      "[EPOCH #85, elapsed time: 1032.619[sec]] loss: 4.253980857930882\n",
      "[EPOCH #86, elapsed time: 1045.572[sec]] loss: 4.250345039733769\n",
      "[EPOCH #87, elapsed time: 1057.751[sec]] loss: 4.249991700653838\n",
      "[EPOCH #88, elapsed time: 1073.093[sec]] loss: 4.244411478878517\n",
      "[EPOCH #89, elapsed time: 1085.531[sec]] loss: 4.246293111100688\n",
      "[EPOCH #90, elapsed time: 1098.203[sec]] loss: 4.244549682944231\n",
      "[EPOCH #91, elapsed time: 1110.566[sec]] loss: 4.236787124543486\n",
      "[EPOCH #92, elapsed time: 1123.078[sec]] loss: 4.23567113034327\n",
      "[EPOCH #93, elapsed time: 1135.709[sec]] loss: 4.228823967492512\n",
      "[EPOCH #94, elapsed time: 1148.473[sec]] loss: 4.236577711575167\n",
      "[EPOCH #95, elapsed time: 1164.506[sec]] loss: 4.22771088495822\n",
      "[EPOCH #96, elapsed time: 1177.233[sec]] loss: 4.233759223995343\n",
      "[EPOCH #97, elapsed time: 1190.179[sec]] loss: 4.2302995001133326\n",
      "[EPOCH #98, elapsed time: 1203.556[sec]] loss: 4.228755406515765\n",
      "[EPOCH #99, elapsed time: 1217.366[sec]] loss: 4.222998439236017\n",
      "[EPOCH #100, elapsed time: 1233.625[sec]] loss: 4.221539844706969\n",
      "[EPOCH #101, elapsed time: 1245.771[sec]] loss: 4.2178496673789\n",
      "[EPOCH #102, elapsed time: 1258.145[sec]] loss: 4.217910058972779\n",
      "[EPOCH #103, elapsed time: 1269.966[sec]] loss: 4.211962561842271\n",
      "[EPOCH #104, elapsed time: 1282.533[sec]] loss: 4.211255809395876\n",
      "[EPOCH #105, elapsed time: 1294.132[sec]] loss: 4.2080506947432585\n",
      "[EPOCH #106, elapsed time: 1306.330[sec]] loss: 4.203843448380209\n",
      "[EPOCH #107, elapsed time: 1318.877[sec]] loss: 4.1986507046932\n",
      "[EPOCH #108, elapsed time: 1331.300[sec]] loss: 4.199769027402442\n",
      "[EPOCH #109, elapsed time: 1343.151[sec]] loss: 4.202115326910086\n",
      "[EPOCH #110, elapsed time: 1354.604[sec]] loss: 4.199545539805924\n",
      "[EPOCH #111, elapsed time: 1366.699[sec]] loss: 4.194903722422595\n",
      "[EPOCH #112, elapsed time: 1378.786[sec]] loss: 4.190795872963474\n",
      "[EPOCH #113, elapsed time: 1390.837[sec]] loss: 4.192311804948979\n",
      "[EPOCH #114, elapsed time: 1403.082[sec]] loss: 4.191182794436688\n",
      "[EPOCH #115, elapsed time: 1415.115[sec]] loss: 4.185144624417208\n",
      "[EPOCH #116, elapsed time: 1431.012[sec]] loss: 4.1844682498231425\n",
      "[EPOCH #117, elapsed time: 1443.742[sec]] loss: 4.181718516334539\n",
      "[EPOCH #118, elapsed time: 1458.346[sec]] loss: 4.185229875914805\n",
      "[EPOCH #119, elapsed time: 1469.539[sec]] loss: 4.180100022259235\n",
      "[EPOCH #120, elapsed time: 1480.876[sec]] loss: 4.187261434525766\n",
      "[EPOCH #121, elapsed time: 1493.033[sec]] loss: 4.1823960891230625\n",
      "[EPOCH #122, elapsed time: 1505.186[sec]] loss: 4.177946869867853\n",
      "[EPOCH #123, elapsed time: 1516.953[sec]] loss: 4.182691180850936\n",
      "[EPOCH #124, elapsed time: 1528.391[sec]] loss: 4.17379950943164\n",
      "[EPOCH #125, elapsed time: 1539.794[sec]] loss: 4.172399063866945\n",
      "[EPOCH #126, elapsed time: 1551.137[sec]] loss: 4.191706463379961\n",
      "[EPOCH #127, elapsed time: 1563.053[sec]] loss: 4.176350032070548\n",
      "[EPOCH #128, elapsed time: 1574.859[sec]] loss: 4.171715601086998\n",
      "[EPOCH #129, elapsed time: 1586.312[sec]] loss: 4.169977360174432\n",
      "[EPOCH #130, elapsed time: 1598.193[sec]] loss: 4.16512137487464\n",
      "[EPOCH #131, elapsed time: 1609.546[sec]] loss: 4.168087266304519\n",
      "[EPOCH #132, elapsed time: 1622.010[sec]] loss: 4.185897990060173\n",
      "[EPOCH #133, elapsed time: 1633.526[sec]] loss: 4.170660241322874\n",
      "[EPOCH #134, elapsed time: 1645.055[sec]] loss: 4.166710991777065\n",
      "[EPOCH #135, elapsed time: 1656.705[sec]] loss: 4.16642921365993\n",
      "[EPOCH #136, elapsed time: 1668.192[sec]] loss: 4.170584363199088\n",
      "[EPOCH #137, elapsed time: 1680.182[sec]] loss: 4.161598935923513\n",
      "[EPOCH #138, elapsed time: 1692.918[sec]] loss: 4.164924024696618\n",
      "[EPOCH #139, elapsed time: 1706.035[sec]] loss: 4.15744690916436\n",
      "[EPOCH #140, elapsed time: 1717.922[sec]] loss: 4.158601033405394\n",
      "[EPOCH #141, elapsed time: 1729.991[sec]] loss: 4.158046989355496\n",
      "[EPOCH #142, elapsed time: 1741.387[sec]] loss: 4.152000634851779\n",
      "[EPOCH #143, elapsed time: 1753.560[sec]] loss: 4.152517922627796\n",
      "[EPOCH #144, elapsed time: 1765.053[sec]] loss: 4.155836167582623\n",
      "[EPOCH #145, elapsed time: 1777.604[sec]] loss: 4.153996580621789\n",
      "[EPOCH #146, elapsed time: 1790.065[sec]] loss: 4.14970521841458\n",
      "[EPOCH #147, elapsed time: 1801.536[sec]] loss: 4.150841542870588\n",
      "[EPOCH #148, elapsed time: 1813.245[sec]] loss: 4.1495103244207945\n",
      "[EPOCH #149, elapsed time: 1825.783[sec]] loss: 4.1518962459539805\n",
      "[EPOCH #150, elapsed time: 1837.672[sec]] loss: 4.149547574463671\n",
      "[EPOCH #151, elapsed time: 1849.061[sec]] loss: 4.144708114789986\n",
      "[EPOCH #152, elapsed time: 1861.450[sec]] loss: 4.142389120997638\n",
      "[EPOCH #153, elapsed time: 1872.733[sec]] loss: 4.1410904330316445\n",
      "[EPOCH #154, elapsed time: 1883.958[sec]] loss: 4.142388448147765\n",
      "[EPOCH #155, elapsed time: 1895.376[sec]] loss: 4.140353151566694\n",
      "[EPOCH #156, elapsed time: 1906.830[sec]] loss: 4.1377453233519965\n",
      "[EPOCH #157, elapsed time: 1918.118[sec]] loss: 4.142670643764356\n",
      "[EPOCH #158, elapsed time: 1932.946[sec]] loss: 4.138720725182151\n",
      "[EPOCH #159, elapsed time: 1944.395[sec]] loss: 4.1418638534448275\n",
      "[EPOCH #160, elapsed time: 1958.859[sec]] loss: 4.1357487607139545\n",
      "[EPOCH #161, elapsed time: 1974.453[sec]] loss: 4.137686700143649\n",
      "[EPOCH #162, elapsed time: 1986.425[sec]] loss: 4.134329451404164\n",
      "[EPOCH #163, elapsed time: 1998.394[sec]] loss: 4.138901885625115\n",
      "[EPOCH #164, elapsed time: 2010.277[sec]] loss: 4.137045058697672\n",
      "[EPOCH #165, elapsed time: 2023.169[sec]] loss: 4.139396795765833\n",
      "[EPOCH #166, elapsed time: 2034.854[sec]] loss: 4.132935274280346\n",
      "[EPOCH #167, elapsed time: 2047.405[sec]] loss: 4.138570569222048\n",
      "[EPOCH #168, elapsed time: 2060.681[sec]] loss: 4.12917825997219\n",
      "[EPOCH #169, elapsed time: 2072.235[sec]] loss: 4.132209429890394\n",
      "[EPOCH #170, elapsed time: 2084.191[sec]] loss: 4.138988429281243\n",
      "[EPOCH #171, elapsed time: 2095.446[sec]] loss: 4.139037984499013\n",
      "[EPOCH #172, elapsed time: 2106.798[sec]] loss: 4.135212937914555\n",
      "[EPOCH #173, elapsed time: 2118.101[sec]] loss: 4.133425839650501\n",
      "[EPOCH #174, elapsed time: 2129.425[sec]] loss: 4.130464254398797\n",
      "[EPOCH #175, elapsed time: 2140.819[sec]] loss: 4.133163179408566\n",
      "[EPOCH #176, elapsed time: 2152.570[sec]] loss: 4.13059343928644\n",
      "[EPOCH #177, elapsed time: 2164.332[sec]] loss: 4.128698636535185\n",
      "[EPOCH #178, elapsed time: 2177.253[sec]] loss: 4.124797270531389\n",
      "[EPOCH #179, elapsed time: 2188.894[sec]] loss: 4.129262163481953\n",
      "[EPOCH #180, elapsed time: 2201.997[sec]] loss: 4.124623998799388\n",
      "[EPOCH #181, elapsed time: 2213.206[sec]] loss: 4.123128175582934\n",
      "[EPOCH #182, elapsed time: 2224.965[sec]] loss: 4.125695840563441\n",
      "[EPOCH #183, elapsed time: 2236.208[sec]] loss: 4.133128423837233\n",
      "[EPOCH #184, elapsed time: 2247.662[sec]] loss: 4.123896781710273\n",
      "[EPOCH #185, elapsed time: 2260.456[sec]] loss: 4.125037041521957\n",
      "[EPOCH #186, elapsed time: 2271.855[sec]] loss: 4.12227622393378\n",
      "[EPOCH #187, elapsed time: 2283.215[sec]] loss: 4.122616385162754\n",
      "[EPOCH #188, elapsed time: 2294.567[sec]] loss: 4.11985388430585\n",
      "[EPOCH #189, elapsed time: 2305.768[sec]] loss: 4.1169646692367525\n",
      "[EPOCH #190, elapsed time: 2320.846[sec]] loss: 4.11825225052739\n",
      "[EPOCH #191, elapsed time: 2334.115[sec]] loss: 4.11833276751708\n",
      "[EPOCH #192, elapsed time: 2345.463[sec]] loss: 4.121483001507633\n",
      "[EPOCH #193, elapsed time: 2356.724[sec]] loss: 4.119357388673954\n",
      "[EPOCH #194, elapsed time: 2368.038[sec]] loss: 4.118171570931042\n",
      "[EPOCH #195, elapsed time: 2379.414[sec]] loss: 4.122150242976935\n",
      "[EPOCH #196, elapsed time: 2391.953[sec]] loss: 4.116786530898003\n",
      "[EPOCH #197, elapsed time: 2404.465[sec]] loss: 4.116571894228954\n",
      "[EPOCH #198, elapsed time: 2416.373[sec]] loss: 4.128084253212312\n",
      "[EPOCH #199, elapsed time: 2427.796[sec]] loss: 4.126589324027395\n",
      "[EPOCH #200, elapsed time: 2439.546[sec]] loss: 4.121386085346732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 03:43:02,653] Trial 4 finished with value: 0.6144 and parameters: {'learning_rate': 0.001352398004624443}. Best is trial 4 with value: 0.6144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605149437507146\n",
      "[EPOCH #1, elapsed time: 11.989[sec]] loss: 4.589398439923541\n",
      "[EPOCH #2, elapsed time: 23.631[sec]] loss: 4.564069431863834\n",
      "[EPOCH #3, elapsed time: 35.683[sec]] loss: 4.53996369629736\n",
      "[EPOCH #4, elapsed time: 50.531[sec]] loss: 4.518478785885196\n",
      "[EPOCH #5, elapsed time: 64.544[sec]] loss: 4.50396799995437\n",
      "[EPOCH #6, elapsed time: 78.740[sec]] loss: 4.49046662032261\n",
      "[EPOCH #7, elapsed time: 91.911[sec]] loss: 4.479365001942054\n",
      "[EPOCH #8, elapsed time: 107.811[sec]] loss: 4.466392332517559\n",
      "[EPOCH #9, elapsed time: 121.093[sec]] loss: 4.456684047872259\n",
      "[EPOCH #10, elapsed time: 134.110[sec]] loss: 4.445337435868178\n",
      "[EPOCH #11, elapsed time: 145.999[sec]] loss: 4.431017467171736\n",
      "[EPOCH #12, elapsed time: 160.232[sec]] loss: 4.418223647985867\n",
      "[EPOCH #13, elapsed time: 171.990[sec]] loss: 4.407721465471381\n",
      "[EPOCH #14, elapsed time: 183.576[sec]] loss: 4.4000351885077436\n",
      "[EPOCH #15, elapsed time: 198.300[sec]] loss: 4.3913308513980605\n",
      "[EPOCH #16, elapsed time: 211.033[sec]] loss: 4.383726987027237\n",
      "[EPOCH #17, elapsed time: 222.526[sec]] loss: 4.3766784329332\n",
      "[EPOCH #18, elapsed time: 237.853[sec]] loss: 4.369440248053729\n",
      "[EPOCH #19, elapsed time: 251.009[sec]] loss: 4.363100235536933\n",
      "[EPOCH #20, elapsed time: 264.373[sec]] loss: 4.355270756717226\n",
      "[EPOCH #21, elapsed time: 281.421[sec]] loss: 4.349047649234667\n",
      "[EPOCH #22, elapsed time: 292.910[sec]] loss: 4.343983101982073\n",
      "[EPOCH #23, elapsed time: 304.292[sec]] loss: 4.334266894151977\n",
      "[EPOCH #24, elapsed time: 317.778[sec]] loss: 4.331812916546393\n",
      "[EPOCH #25, elapsed time: 331.281[sec]] loss: 4.326365823861657\n",
      "[EPOCH #26, elapsed time: 343.620[sec]] loss: 4.319619156806345\n",
      "[EPOCH #27, elapsed time: 356.040[sec]] loss: 4.313257907799094\n",
      "[EPOCH #28, elapsed time: 369.116[sec]] loss: 4.308664122988456\n",
      "[EPOCH #29, elapsed time: 382.565[sec]] loss: 4.301323874516893\n",
      "[EPOCH #30, elapsed time: 396.332[sec]] loss: 4.2984322442965714\n",
      "[EPOCH #31, elapsed time: 410.247[sec]] loss: 4.293380869098451\n",
      "[EPOCH #32, elapsed time: 422.461[sec]] loss: 4.286362171478173\n",
      "[EPOCH #33, elapsed time: 434.654[sec]] loss: 4.282850411330289\n",
      "[EPOCH #34, elapsed time: 450.730[sec]] loss: 4.276304800237361\n",
      "[EPOCH #35, elapsed time: 462.102[sec]] loss: 4.2720054779309\n",
      "[EPOCH #36, elapsed time: 473.443[sec]] loss: 4.266684238680341\n",
      "[EPOCH #37, elapsed time: 484.820[sec]] loss: 4.264782274676986\n",
      "[EPOCH #38, elapsed time: 496.045[sec]] loss: 4.258684252289267\n",
      "[EPOCH #39, elapsed time: 507.463[sec]] loss: 4.254675043292787\n",
      "[EPOCH #40, elapsed time: 523.350[sec]] loss: 4.2496714146565875\n",
      "[EPOCH #41, elapsed time: 538.866[sec]] loss: 4.245024744395026\n",
      "[EPOCH #42, elapsed time: 551.421[sec]] loss: 4.242709591689205\n",
      "[EPOCH #43, elapsed time: 564.075[sec]] loss: 4.238729028006204\n",
      "[EPOCH #44, elapsed time: 575.389[sec]] loss: 4.235975621529138\n",
      "[EPOCH #45, elapsed time: 591.085[sec]] loss: 4.232802715197787\n",
      "[EPOCH #46, elapsed time: 602.361[sec]] loss: 4.22885722131662\n",
      "[EPOCH #47, elapsed time: 613.898[sec]] loss: 4.226581994189106\n",
      "[EPOCH #48, elapsed time: 627.304[sec]] loss: 4.220288988419702\n",
      "[EPOCH #49, elapsed time: 638.555[sec]] loss: 4.215818220426543\n",
      "[EPOCH #50, elapsed time: 650.162[sec]] loss: 4.214299166576266\n",
      "[EPOCH #51, elapsed time: 662.382[sec]] loss: 4.210366140629188\n",
      "[EPOCH #52, elapsed time: 674.516[sec]] loss: 4.209821672677537\n",
      "[EPOCH #53, elapsed time: 686.665[sec]] loss: 4.204689117249814\n",
      "[EPOCH #54, elapsed time: 699.054[sec]] loss: 4.204015769793754\n",
      "[EPOCH #55, elapsed time: 714.139[sec]] loss: 4.2009828006771235\n",
      "[EPOCH #56, elapsed time: 726.332[sec]] loss: 4.197832791529172\n",
      "[EPOCH #57, elapsed time: 739.209[sec]] loss: 4.1950301161501855\n",
      "[EPOCH #58, elapsed time: 755.124[sec]] loss: 4.191896211315407\n",
      "[EPOCH #59, elapsed time: 766.673[sec]] loss: 4.186923426233341\n",
      "[EPOCH #60, elapsed time: 778.462[sec]] loss: 4.18598808970729\n",
      "[EPOCH #61, elapsed time: 789.837[sec]] loss: 4.181206255331301\n",
      "[EPOCH #62, elapsed time: 805.294[sec]] loss: 4.176827127935027\n",
      "[EPOCH #63, elapsed time: 816.664[sec]] loss: 4.1750451955593935\n",
      "[EPOCH #64, elapsed time: 827.949[sec]] loss: 4.167708362590329\n",
      "[EPOCH #65, elapsed time: 839.898[sec]] loss: 4.166425659194331\n",
      "[EPOCH #66, elapsed time: 851.429[sec]] loss: 4.1652151064772065\n",
      "[EPOCH #67, elapsed time: 862.850[sec]] loss: 4.160008518953623\n",
      "[EPOCH #68, elapsed time: 874.632[sec]] loss: 4.1590758454547965\n",
      "[EPOCH #69, elapsed time: 888.470[sec]] loss: 4.154363564322258\n",
      "[EPOCH #70, elapsed time: 900.994[sec]] loss: 4.153855605073564\n",
      "[EPOCH #71, elapsed time: 914.594[sec]] loss: 4.151063295030014\n",
      "[EPOCH #72, elapsed time: 926.285[sec]] loss: 4.1468152293622\n",
      "[EPOCH #73, elapsed time: 938.518[sec]] loss: 4.144188721517073\n",
      "[EPOCH #74, elapsed time: 950.730[sec]] loss: 4.140372585502864\n",
      "[EPOCH #75, elapsed time: 962.357[sec]] loss: 4.1419313982062365\n",
      "[EPOCH #76, elapsed time: 974.623[sec]] loss: 4.138621793674949\n",
      "[EPOCH #77, elapsed time: 990.263[sec]] loss: 4.135347575921701\n",
      "[EPOCH #78, elapsed time: 1005.199[sec]] loss: 4.13102487166265\n",
      "[EPOCH #79, elapsed time: 1017.313[sec]] loss: 4.130859109124387\n",
      "[EPOCH #80, elapsed time: 1030.633[sec]] loss: 4.128342367942259\n",
      "[EPOCH #81, elapsed time: 1043.578[sec]] loss: 4.123679794108951\n",
      "[EPOCH #82, elapsed time: 1057.385[sec]] loss: 4.120165548184249\n",
      "[EPOCH #83, elapsed time: 1073.399[sec]] loss: 4.1192301495747925\n",
      "[EPOCH #84, elapsed time: 1085.472[sec]] loss: 4.117015358430029\n",
      "[EPOCH #85, elapsed time: 1097.364[sec]] loss: 4.114235401763721\n",
      "[EPOCH #86, elapsed time: 1112.146[sec]] loss: 4.112429904236064\n",
      "[EPOCH #87, elapsed time: 1125.687[sec]] loss: 4.112282483911788\n",
      "[EPOCH #88, elapsed time: 1140.947[sec]] loss: 4.110903042978151\n",
      "[EPOCH #89, elapsed time: 1154.633[sec]] loss: 4.108357605076874\n",
      "[EPOCH #90, elapsed time: 1171.764[sec]] loss: 4.105593729232719\n",
      "[EPOCH #91, elapsed time: 1183.921[sec]] loss: 4.103906460168342\n",
      "[EPOCH #92, elapsed time: 1196.853[sec]] loss: 4.099380950781297\n",
      "[EPOCH #93, elapsed time: 1208.502[sec]] loss: 4.0979637109691796\n",
      "[EPOCH #94, elapsed time: 1225.030[sec]] loss: 4.096534724885351\n",
      "[EPOCH #95, elapsed time: 1238.938[sec]] loss: 4.092503204302992\n",
      "[EPOCH #96, elapsed time: 1250.398[sec]] loss: 4.090405644015936\n",
      "[EPOCH #97, elapsed time: 1263.214[sec]] loss: 4.089198669560506\n",
      "[EPOCH #98, elapsed time: 1274.961[sec]] loss: 4.086047900310328\n",
      "[EPOCH #99, elapsed time: 1288.057[sec]] loss: 4.089492206915174\n",
      "[EPOCH #100, elapsed time: 1299.773[sec]] loss: 4.0853971698233815\n",
      "[EPOCH #101, elapsed time: 1314.278[sec]] loss: 4.081431218011213\n",
      "[EPOCH #102, elapsed time: 1326.209[sec]] loss: 4.080893376509653\n",
      "[EPOCH #103, elapsed time: 1338.506[sec]] loss: 4.0801950899820945\n",
      "[EPOCH #104, elapsed time: 1351.162[sec]] loss: 4.077662058846736\n",
      "[EPOCH #105, elapsed time: 1363.099[sec]] loss: 4.077227687957724\n",
      "[EPOCH #106, elapsed time: 1375.575[sec]] loss: 4.073120258438687\n",
      "[EPOCH #107, elapsed time: 1387.637[sec]] loss: 4.068235106294306\n",
      "[EPOCH #108, elapsed time: 1400.420[sec]] loss: 4.069422955857739\n",
      "[EPOCH #109, elapsed time: 1412.098[sec]] loss: 4.073571399473946\n",
      "[EPOCH #110, elapsed time: 1423.584[sec]] loss: 4.061178690488721\n",
      "[EPOCH #111, elapsed time: 1435.182[sec]] loss: 4.066595787278979\n",
      "[EPOCH #112, elapsed time: 1447.409[sec]] loss: 4.059622662996376\n",
      "[EPOCH #113, elapsed time: 1459.352[sec]] loss: 4.061527152741787\n",
      "[EPOCH #114, elapsed time: 1475.029[sec]] loss: 4.060131369839093\n",
      "[EPOCH #115, elapsed time: 1489.898[sec]] loss: 4.057815991833053\n",
      "[EPOCH #116, elapsed time: 1501.436[sec]] loss: 4.053210244145213\n",
      "[EPOCH #117, elapsed time: 1516.834[sec]] loss: 4.055398916786325\n",
      "[EPOCH #118, elapsed time: 1530.616[sec]] loss: 4.054567971141081\n",
      "[EPOCH #119, elapsed time: 1544.091[sec]] loss: 4.051592892435066\n",
      "[EPOCH #120, elapsed time: 1557.224[sec]] loss: 4.049022134419671\n",
      "[EPOCH #121, elapsed time: 1571.157[sec]] loss: 4.0459730127875195\n",
      "[EPOCH #122, elapsed time: 1586.201[sec]] loss: 4.04846297634464\n",
      "[EPOCH #123, elapsed time: 1599.693[sec]] loss: 4.047027271219499\n",
      "[EPOCH #124, elapsed time: 1613.324[sec]] loss: 4.047058134603714\n",
      "[EPOCH #125, elapsed time: 1628.539[sec]] loss: 4.0399086121481655\n",
      "[EPOCH #126, elapsed time: 1640.272[sec]] loss: 4.04141453512952\n",
      "[EPOCH #127, elapsed time: 1652.899[sec]] loss: 4.0396287770738075\n",
      "[EPOCH #128, elapsed time: 1667.369[sec]] loss: 4.038197066489505\n",
      "[EPOCH #129, elapsed time: 1680.559[sec]] loss: 4.037435791085183\n",
      "[EPOCH #130, elapsed time: 1692.828[sec]] loss: 4.038433826358671\n",
      "[EPOCH #131, elapsed time: 1705.055[sec]] loss: 4.03121025701097\n",
      "[EPOCH #132, elapsed time: 1718.807[sec]] loss: 4.035219813338931\n",
      "[EPOCH #133, elapsed time: 1731.097[sec]] loss: 4.028218400989369\n",
      "[EPOCH #134, elapsed time: 1746.138[sec]] loss: 4.030604118272729\n",
      "[EPOCH #135, elapsed time: 1758.705[sec]] loss: 4.029000808547417\n",
      "[EPOCH #136, elapsed time: 1770.473[sec]] loss: 4.029324282916479\n",
      "[EPOCH #137, elapsed time: 1782.490[sec]] loss: 4.025000579831544\n",
      "[EPOCH #138, elapsed time: 1794.081[sec]] loss: 4.0219463830717235\n",
      "[EPOCH #139, elapsed time: 1806.599[sec]] loss: 4.02094518840885\n",
      "[EPOCH #140, elapsed time: 1818.098[sec]] loss: 4.020411663153045\n",
      "[EPOCH #141, elapsed time: 1829.813[sec]] loss: 4.020358987443354\n",
      "[EPOCH #142, elapsed time: 1841.349[sec]] loss: 4.01824638939636\n",
      "[EPOCH #143, elapsed time: 1853.979[sec]] loss: 4.0205459445238265\n",
      "[EPOCH #144, elapsed time: 1871.192[sec]] loss: 4.0165222051123814\n",
      "[EPOCH #145, elapsed time: 1884.550[sec]] loss: 4.012740458270638\n",
      "[EPOCH #146, elapsed time: 1897.808[sec]] loss: 4.014657326867317\n",
      "[EPOCH #147, elapsed time: 1910.095[sec]] loss: 4.012379256563925\n",
      "[EPOCH #148, elapsed time: 1926.328[sec]] loss: 4.008999644222736\n",
      "[EPOCH #149, elapsed time: 1938.201[sec]] loss: 4.012229399427876\n",
      "[EPOCH #150, elapsed time: 1949.824[sec]] loss: 4.008191106720605\n",
      "[EPOCH #151, elapsed time: 1962.506[sec]] loss: 4.006186586271397\n",
      "[EPOCH #152, elapsed time: 1977.197[sec]] loss: 4.006213819378092\n",
      "[EPOCH #153, elapsed time: 1991.083[sec]] loss: 4.003923943770366\n",
      "[EPOCH #154, elapsed time: 2006.319[sec]] loss: 4.00412938645156\n",
      "[EPOCH #155, elapsed time: 2019.339[sec]] loss: 4.001908866854257\n",
      "[EPOCH #156, elapsed time: 2031.195[sec]] loss: 4.001325441184749\n",
      "[EPOCH #157, elapsed time: 2043.969[sec]] loss: 3.9992996313140243\n",
      "[EPOCH #158, elapsed time: 2056.483[sec]] loss: 3.9979618716834833\n",
      "[EPOCH #159, elapsed time: 2068.311[sec]] loss: 3.9966142392661888\n",
      "[EPOCH #160, elapsed time: 2080.289[sec]] loss: 3.9970957538826832\n",
      "[EPOCH #161, elapsed time: 2092.159[sec]] loss: 3.9961365924305587\n",
      "[EPOCH #162, elapsed time: 2105.950[sec]] loss: 3.994220141638416\n",
      "[EPOCH #163, elapsed time: 2118.643[sec]] loss: 3.990399799206588\n",
      "[EPOCH #164, elapsed time: 2131.405[sec]] loss: 3.9914176302389386\n",
      "[EPOCH #165, elapsed time: 2143.890[sec]] loss: 3.9913410106226945\n",
      "[EPOCH #166, elapsed time: 2156.305[sec]] loss: 3.9887791566183686\n",
      "[EPOCH #167, elapsed time: 2170.878[sec]] loss: 3.988194960168898\n",
      "[EPOCH #168, elapsed time: 2183.239[sec]] loss: 3.986651446219827\n",
      "[EPOCH #169, elapsed time: 2195.149[sec]] loss: 3.988396851282736\n",
      "[EPOCH #170, elapsed time: 2211.165[sec]] loss: 3.981805474652896\n",
      "[EPOCH #171, elapsed time: 2223.592[sec]] loss: 3.9840777747080405\n",
      "[EPOCH #172, elapsed time: 2239.222[sec]] loss: 3.981945499425047\n",
      "[EPOCH #173, elapsed time: 2251.568[sec]] loss: 3.980874967971675\n",
      "[EPOCH #174, elapsed time: 2265.861[sec]] loss: 3.9804492940982033\n",
      "[EPOCH #175, elapsed time: 2279.575[sec]] loss: 3.981302311995513\n",
      "[EPOCH #176, elapsed time: 2293.495[sec]] loss: 3.981872936890664\n",
      "[EPOCH #177, elapsed time: 2305.319[sec]] loss: 3.9788359593521383\n",
      "[EPOCH #178, elapsed time: 2317.511[sec]] loss: 3.9771592040437196\n",
      "[EPOCH #179, elapsed time: 2331.154[sec]] loss: 3.9787680652762405\n",
      "[EPOCH #180, elapsed time: 2343.796[sec]] loss: 3.9759781276729727\n",
      "[EPOCH #181, elapsed time: 2356.241[sec]] loss: 3.97813333751144\n",
      "[EPOCH #182, elapsed time: 2369.112[sec]] loss: 3.9742222329552783\n",
      "[EPOCH #183, elapsed time: 2381.880[sec]] loss: 3.9713352121608194\n",
      "[EPOCH #184, elapsed time: 2394.198[sec]] loss: 3.9751887051478\n",
      "[EPOCH #185, elapsed time: 2406.527[sec]] loss: 3.9710015939430634\n",
      "[EPOCH #186, elapsed time: 2418.283[sec]] loss: 3.968255932942767\n",
      "[EPOCH #187, elapsed time: 2430.137[sec]] loss: 3.9661333054513865\n",
      "[EPOCH #188, elapsed time: 2441.881[sec]] loss: 3.9692911886665505\n",
      "[EPOCH #189, elapsed time: 2453.619[sec]] loss: 3.969280565387533\n",
      "[EPOCH #190, elapsed time: 2468.646[sec]] loss: 3.969056837541967\n",
      "[EPOCH #191, elapsed time: 2480.394[sec]] loss: 3.9681190719836352\n",
      "[EPOCH #192, elapsed time: 2492.332[sec]] loss: 3.965288416819167\n",
      "[EPOCH #193, elapsed time: 2504.070[sec]] loss: 3.9656427823345552\n",
      "[EPOCH #194, elapsed time: 2516.112[sec]] loss: 3.96257525823548\n",
      "[EPOCH #195, elapsed time: 2529.448[sec]] loss: 3.961887202351351\n",
      "[EPOCH #196, elapsed time: 2542.690[sec]] loss: 3.9619186485873836\n",
      "[EPOCH #197, elapsed time: 2555.447[sec]] loss: 3.963063506842117\n",
      "[EPOCH #198, elapsed time: 2568.273[sec]] loss: 3.959755970283113\n",
      "[EPOCH #199, elapsed time: 2580.578[sec]] loss: 3.9603567320188495\n",
      "[EPOCH #200, elapsed time: 2593.333[sec]] loss: 3.957754898742461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 04:26:32,659] Trial 5 finished with value: 0.76704 and parameters: {'learning_rate': 0.00010915610396069178}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605175100681649\n",
      "[EPOCH #1, elapsed time: 12.275[sec]] loss: 4.578782617016168\n",
      "[EPOCH #2, elapsed time: 25.226[sec]] loss: 4.548548619410051\n",
      "[EPOCH #3, elapsed time: 37.538[sec]] loss: 4.530927425306421\n",
      "[EPOCH #4, elapsed time: 49.706[sec]] loss: 4.517077719638993\n",
      "[EPOCH #5, elapsed time: 61.767[sec]] loss: 4.506120631729877\n",
      "[EPOCH #6, elapsed time: 73.965[sec]] loss: 4.497890572477744\n",
      "[EPOCH #7, elapsed time: 86.539[sec]] loss: 4.49284407425903\n",
      "[EPOCH #8, elapsed time: 99.836[sec]] loss: 4.483548545105215\n",
      "[EPOCH #9, elapsed time: 111.961[sec]] loss: 4.478472005230299\n",
      "[EPOCH #10, elapsed time: 123.961[sec]] loss: 4.478508659760615\n",
      "[EPOCH #11, elapsed time: 136.066[sec]] loss: 4.477858245334637\n",
      "[EPOCH #12, elapsed time: 148.535[sec]] loss: 4.467632700065276\n",
      "[EPOCH #13, elapsed time: 160.636[sec]] loss: 4.473126999323595\n",
      "[EPOCH #14, elapsed time: 176.145[sec]] loss: 4.457408474869097\n",
      "[EPOCH #15, elapsed time: 191.782[sec]] loss: 4.453261086213154\n",
      "[EPOCH #16, elapsed time: 204.426[sec]] loss: 4.458403773744062\n",
      "[EPOCH #17, elapsed time: 217.426[sec]] loss: 4.452187854513021\n",
      "[EPOCH #18, elapsed time: 229.670[sec]] loss: 4.44677258331045\n",
      "[EPOCH #19, elapsed time: 241.500[sec]] loss: 4.442145167904181\n",
      "[EPOCH #20, elapsed time: 254.504[sec]] loss: 4.441338866167318\n",
      "[EPOCH #21, elapsed time: 266.398[sec]] loss: 4.437975044518957\n",
      "[EPOCH #22, elapsed time: 278.091[sec]] loss: 4.429574977108399\n",
      "[EPOCH #23, elapsed time: 290.229[sec]] loss: 4.426509123159691\n",
      "[EPOCH #24, elapsed time: 304.297[sec]] loss: 4.428244472694031\n",
      "[EPOCH #25, elapsed time: 316.906[sec]] loss: 4.423961619574217\n",
      "[EPOCH #26, elapsed time: 329.769[sec]] loss: 4.415099061000675\n",
      "[EPOCH #27, elapsed time: 342.663[sec]] loss: 4.416827338518276\n",
      "[EPOCH #28, elapsed time: 358.225[sec]] loss: 4.415246017453614\n",
      "[EPOCH #29, elapsed time: 371.083[sec]] loss: 4.407931606051103\n",
      "[EPOCH #30, elapsed time: 382.888[sec]] loss: 4.405390625951844\n",
      "[EPOCH #31, elapsed time: 398.779[sec]] loss: 4.397406578979199\n",
      "[EPOCH #32, elapsed time: 410.380[sec]] loss: 4.392352587735889\n",
      "[EPOCH #33, elapsed time: 421.994[sec]] loss: 4.395173761986497\n",
      "[EPOCH #34, elapsed time: 434.079[sec]] loss: 4.3928970540096115\n",
      "[EPOCH #35, elapsed time: 446.879[sec]] loss: 4.386013713465695\n",
      "[EPOCH #36, elapsed time: 458.948[sec]] loss: 4.389114865262159\n",
      "[EPOCH #37, elapsed time: 471.277[sec]] loss: 4.38309498559338\n",
      "[EPOCH #38, elapsed time: 483.596[sec]] loss: 4.375188733245498\n",
      "[EPOCH #39, elapsed time: 495.383[sec]] loss: 4.37434696342727\n",
      "[EPOCH #40, elapsed time: 507.122[sec]] loss: 4.367060377288155\n",
      "[EPOCH #41, elapsed time: 519.195[sec]] loss: 4.365628351100499\n",
      "[EPOCH #42, elapsed time: 532.524[sec]] loss: 4.366893392454259\n",
      "[EPOCH #43, elapsed time: 544.411[sec]] loss: 4.3634247648662585\n",
      "[EPOCH #44, elapsed time: 556.421[sec]] loss: 4.356112475587402\n",
      "[EPOCH #45, elapsed time: 568.362[sec]] loss: 4.350468515358289\n",
      "[EPOCH #46, elapsed time: 580.868[sec]] loss: 4.351550830150368\n",
      "[EPOCH #47, elapsed time: 595.158[sec]] loss: 4.362802725149437\n",
      "[EPOCH #48, elapsed time: 608.030[sec]] loss: 4.3456864292928215\n",
      "[EPOCH #49, elapsed time: 621.986[sec]] loss: 4.344056356738792\n",
      "[EPOCH #50, elapsed time: 634.383[sec]] loss: 4.334125995635986\n",
      "[EPOCH #51, elapsed time: 646.355[sec]] loss: 4.345981004218299\n",
      "[EPOCH #52, elapsed time: 658.479[sec]] loss: 4.336303256065969\n",
      "[EPOCH #53, elapsed time: 670.643[sec]] loss: 4.331619649534415\n",
      "[EPOCH #54, elapsed time: 682.796[sec]] loss: 4.326138074780914\n",
      "[EPOCH #55, elapsed time: 694.746[sec]] loss: 4.323425488828926\n",
      "[EPOCH #56, elapsed time: 706.901[sec]] loss: 4.3178248844769085\n",
      "[EPOCH #57, elapsed time: 718.830[sec]] loss: 4.314038761441553\n",
      "[EPOCH #58, elapsed time: 734.848[sec]] loss: 4.308600014474861\n",
      "[EPOCH #59, elapsed time: 746.842[sec]] loss: 4.305727620042446\n",
      "[EPOCH #60, elapsed time: 759.582[sec]] loss: 4.303471103358254\n",
      "[EPOCH #61, elapsed time: 771.891[sec]] loss: 4.303400776589138\n",
      "[EPOCH #62, elapsed time: 784.418[sec]] loss: 4.298065161750779\n",
      "[EPOCH #63, elapsed time: 797.716[sec]] loss: 4.292949414756614\n",
      "[EPOCH #64, elapsed time: 809.513[sec]] loss: 4.290480897431181\n",
      "[EPOCH #65, elapsed time: 823.036[sec]] loss: 4.28351467447409\n",
      "[EPOCH #66, elapsed time: 835.705[sec]] loss: 4.287237278254309\n",
      "[EPOCH #67, elapsed time: 847.349[sec]] loss: 4.280361799116861\n",
      "[EPOCH #68, elapsed time: 861.422[sec]] loss: 4.278268236879042\n",
      "[EPOCH #69, elapsed time: 875.728[sec]] loss: 4.274391833132684\n",
      "[EPOCH #70, elapsed time: 887.881[sec]] loss: 4.2684825487191755\n",
      "[EPOCH #71, elapsed time: 899.714[sec]] loss: 4.264913451419911\n",
      "[EPOCH #72, elapsed time: 911.999[sec]] loss: 4.26945186637566\n",
      "[EPOCH #73, elapsed time: 924.186[sec]] loss: 4.260263488754887\n",
      "[EPOCH #74, elapsed time: 936.139[sec]] loss: 4.2573232039075135\n",
      "[EPOCH #75, elapsed time: 947.719[sec]] loss: 4.250983297557916\n",
      "[EPOCH #76, elapsed time: 959.519[sec]] loss: 4.251954237925114\n",
      "[EPOCH #77, elapsed time: 972.245[sec]] loss: 4.254612610726042\n",
      "[EPOCH #78, elapsed time: 983.777[sec]] loss: 4.2430755086839005\n",
      "[EPOCH #79, elapsed time: 996.172[sec]] loss: 4.240744959446229\n",
      "[EPOCH #80, elapsed time: 1008.196[sec]] loss: 4.244386405572629\n",
      "[EPOCH #81, elapsed time: 1019.847[sec]] loss: 4.235227526111322\n",
      "[EPOCH #82, elapsed time: 1031.675[sec]] loss: 4.235428089219946\n",
      "[EPOCH #83, elapsed time: 1043.724[sec]] loss: 4.232095125006775\n",
      "[EPOCH #84, elapsed time: 1055.559[sec]] loss: 4.231543878218492\n",
      "[EPOCH #85, elapsed time: 1067.454[sec]] loss: 4.223786084680929\n",
      "[EPOCH #86, elapsed time: 1079.561[sec]] loss: 4.232453410166315\n",
      "[EPOCH #87, elapsed time: 1092.459[sec]] loss: 4.222453899590998\n",
      "[EPOCH #88, elapsed time: 1104.609[sec]] loss: 4.222017605794369\n",
      "[EPOCH #89, elapsed time: 1117.395[sec]] loss: 4.2139174663631564\n",
      "[EPOCH #90, elapsed time: 1130.380[sec]] loss: 4.216080565217665\n",
      "[EPOCH #91, elapsed time: 1142.049[sec]] loss: 4.215348833887072\n",
      "[EPOCH #92, elapsed time: 1153.687[sec]] loss: 4.214471862778325\n",
      "[EPOCH #93, elapsed time: 1166.011[sec]] loss: 4.208137537681058\n",
      "[EPOCH #94, elapsed time: 1177.664[sec]] loss: 4.203087377456694\n",
      "[EPOCH #95, elapsed time: 1190.047[sec]] loss: 4.204606933160539\n",
      "[EPOCH #96, elapsed time: 1201.915[sec]] loss: 4.20175811669343\n",
      "[EPOCH #97, elapsed time: 1216.400[sec]] loss: 4.195922049359488\n",
      "[EPOCH #98, elapsed time: 1232.451[sec]] loss: 4.195972288867562\n",
      "[EPOCH #99, elapsed time: 1245.889[sec]] loss: 4.189997008726983\n",
      "[EPOCH #100, elapsed time: 1258.002[sec]] loss: 4.18883054056613\n",
      "[EPOCH #101, elapsed time: 1269.959[sec]] loss: 4.186495480747919\n",
      "[EPOCH #102, elapsed time: 1281.574[sec]] loss: 4.1833515454009795\n",
      "[EPOCH #103, elapsed time: 1293.543[sec]] loss: 4.181296621311649\n",
      "[EPOCH #104, elapsed time: 1305.628[sec]] loss: 4.179118837825167\n",
      "[EPOCH #105, elapsed time: 1317.746[sec]] loss: 4.18077821862751\n",
      "[EPOCH #106, elapsed time: 1329.315[sec]] loss: 4.174720342084527\n",
      "[EPOCH #107, elapsed time: 1342.019[sec]] loss: 4.1712496413379165\n",
      "[EPOCH #108, elapsed time: 1353.978[sec]] loss: 4.1727283931434425\n",
      "[EPOCH #109, elapsed time: 1367.090[sec]] loss: 4.169349316672034\n",
      "[EPOCH #110, elapsed time: 1378.955[sec]] loss: 4.16685857104706\n",
      "[EPOCH #111, elapsed time: 1390.596[sec]] loss: 4.164673548666087\n",
      "[EPOCH #112, elapsed time: 1402.530[sec]] loss: 4.164764270367565\n",
      "[EPOCH #113, elapsed time: 1414.533[sec]] loss: 4.163843673235014\n",
      "[EPOCH #114, elapsed time: 1427.029[sec]] loss: 4.158742361013812\n",
      "[EPOCH #115, elapsed time: 1439.472[sec]] loss: 4.157181493609057\n",
      "[EPOCH #116, elapsed time: 1451.910[sec]] loss: 4.154003519014296\n",
      "[EPOCH #117, elapsed time: 1464.330[sec]] loss: 4.152757286339026\n",
      "[EPOCH #118, elapsed time: 1476.816[sec]] loss: 4.148491995043275\n",
      "[EPOCH #119, elapsed time: 1489.414[sec]] loss: 4.148554607301055\n",
      "[EPOCH #120, elapsed time: 1501.024[sec]] loss: 4.1480142949715075\n",
      "[EPOCH #121, elapsed time: 1512.973[sec]] loss: 4.1421906423355175\n",
      "[EPOCH #122, elapsed time: 1525.630[sec]] loss: 4.14129816212108\n",
      "[EPOCH #123, elapsed time: 1537.205[sec]] loss: 4.146356731672281\n",
      "[EPOCH #124, elapsed time: 1548.712[sec]] loss: 4.141405847998514\n",
      "[EPOCH #125, elapsed time: 1560.846[sec]] loss: 4.136705697993781\n",
      "[EPOCH #126, elapsed time: 1572.667[sec]] loss: 4.133909837145571\n",
      "[EPOCH #127, elapsed time: 1584.221[sec]] loss: 4.140976264701009\n",
      "[EPOCH #128, elapsed time: 1596.992[sec]] loss: 4.135969163970313\n",
      "[EPOCH #129, elapsed time: 1608.565[sec]] loss: 4.132186145257736\n",
      "[EPOCH #130, elapsed time: 1622.330[sec]] loss: 4.128720429640738\n",
      "[EPOCH #131, elapsed time: 1634.911[sec]] loss: 4.133099783862621\n",
      "[EPOCH #132, elapsed time: 1647.205[sec]] loss: 4.130375056257632\n",
      "[EPOCH #133, elapsed time: 1659.113[sec]] loss: 4.125818686384615\n",
      "[EPOCH #134, elapsed time: 1671.137[sec]] loss: 4.126701547789864\n",
      "[EPOCH #135, elapsed time: 1683.238[sec]] loss: 4.123701909071959\n",
      "[EPOCH #136, elapsed time: 1694.952[sec]] loss: 4.124757808824419\n",
      "[EPOCH #137, elapsed time: 1706.719[sec]] loss: 4.121189274546891\n",
      "[EPOCH #138, elapsed time: 1719.737[sec]] loss: 4.1182354950248925\n",
      "[EPOCH #139, elapsed time: 1734.399[sec]] loss: 4.123344078936489\n",
      "[EPOCH #140, elapsed time: 1747.201[sec]] loss: 4.116617387788691\n",
      "[EPOCH #141, elapsed time: 1762.788[sec]] loss: 4.1164565107720215\n",
      "[EPOCH #142, elapsed time: 1775.675[sec]] loss: 4.11246594006788\n",
      "[EPOCH #143, elapsed time: 1792.094[sec]] loss: 4.1104894250307185\n",
      "[EPOCH #144, elapsed time: 1808.383[sec]] loss: 4.10673990679794\n",
      "[EPOCH #145, elapsed time: 1824.734[sec]] loss: 4.112328772962818\n",
      "[EPOCH #146, elapsed time: 1836.614[sec]] loss: 4.104127454818706\n",
      "[EPOCH #147, elapsed time: 1849.125[sec]] loss: 4.103320126341309\n",
      "[EPOCH #148, elapsed time: 1862.544[sec]] loss: 4.105746272734473\n",
      "[EPOCH #149, elapsed time: 1874.188[sec]] loss: 4.103710219864653\n",
      "[EPOCH #150, elapsed time: 1886.265[sec]] loss: 4.103817554292051\n",
      "[EPOCH #151, elapsed time: 1898.497[sec]] loss: 4.1006615575276655\n",
      "[EPOCH #152, elapsed time: 1913.372[sec]] loss: 4.095594686494755\n",
      "[EPOCH #153, elapsed time: 1925.042[sec]] loss: 4.09628696572834\n",
      "[EPOCH #154, elapsed time: 1936.566[sec]] loss: 4.096766040787358\n",
      "[EPOCH #155, elapsed time: 1948.745[sec]] loss: 4.097242939876427\n",
      "[EPOCH #156, elapsed time: 1960.515[sec]] loss: 4.091356768336574\n",
      "[EPOCH #157, elapsed time: 1972.071[sec]] loss: 4.091997457328548\n",
      "[EPOCH #158, elapsed time: 1983.620[sec]] loss: 4.0915550274949615\n",
      "[EPOCH #159, elapsed time: 1995.979[sec]] loss: 4.089042767300792\n",
      "[EPOCH #160, elapsed time: 2007.368[sec]] loss: 4.090810138448568\n",
      "[EPOCH #161, elapsed time: 2019.212[sec]] loss: 4.091610178608812\n",
      "[EPOCH #162, elapsed time: 2031.302[sec]] loss: 4.088663407494758\n",
      "[EPOCH #163, elapsed time: 2043.133[sec]] loss: 4.085552004462087\n",
      "[EPOCH #164, elapsed time: 2058.555[sec]] loss: 4.088423221902976\n",
      "[EPOCH #165, elapsed time: 2071.570[sec]] loss: 4.084818628150076\n",
      "[EPOCH #166, elapsed time: 2085.166[sec]] loss: 4.086448465488846\n",
      "[EPOCH #167, elapsed time: 2098.937[sec]] loss: 4.085458721171872\n",
      "[EPOCH #168, elapsed time: 2114.826[sec]] loss: 4.084870792701316\n",
      "[EPOCH #169, elapsed time: 2129.852[sec]] loss: 4.0781106428694285\n",
      "[EPOCH #170, elapsed time: 2145.808[sec]] loss: 4.085181850999574\n",
      "[EPOCH #171, elapsed time: 2160.497[sec]] loss: 4.079371666038791\n",
      "[EPOCH #172, elapsed time: 2172.443[sec]] loss: 4.078590608299045\n",
      "[EPOCH #173, elapsed time: 2186.884[sec]] loss: 4.077397830960694\n",
      "[EPOCH #174, elapsed time: 2199.269[sec]] loss: 4.080206362925046\n",
      "[EPOCH #175, elapsed time: 2214.786[sec]] loss: 4.07385485399555\n",
      "[EPOCH #176, elapsed time: 2230.806[sec]] loss: 4.074672115207558\n",
      "[EPOCH #177, elapsed time: 2245.066[sec]] loss: 4.074528189401023\n",
      "[EPOCH #178, elapsed time: 2258.521[sec]] loss: 4.073746628739936\n",
      "[EPOCH #179, elapsed time: 2274.136[sec]] loss: 4.0736038607050045\n",
      "[EPOCH #180, elapsed time: 2288.321[sec]] loss: 4.074195106137814\n",
      "[EPOCH #181, elapsed time: 2300.910[sec]] loss: 4.076042164005077\n",
      "[EPOCH #182, elapsed time: 2312.844[sec]] loss: 4.079310200112185\n",
      "[EPOCH #183, elapsed time: 2324.548[sec]] loss: 4.074811847867374\n",
      "[EPOCH #184, elapsed time: 2336.577[sec]] loss: 4.074436643530906\n",
      "[EPOCH #185, elapsed time: 2348.459[sec]] loss: 4.070510898579105\n",
      "[EPOCH #186, elapsed time: 2360.664[sec]] loss: 4.067586598149188\n",
      "[EPOCH #187, elapsed time: 2372.724[sec]] loss: 4.07254526451926\n",
      "[EPOCH #188, elapsed time: 2385.400[sec]] loss: 4.067288919816167\n",
      "[EPOCH #189, elapsed time: 2400.645[sec]] loss: 4.061804958741328\n",
      "[EPOCH #190, elapsed time: 2413.387[sec]] loss: 4.062697885132568\n",
      "[EPOCH #191, elapsed time: 2428.889[sec]] loss: 4.061810932171627\n",
      "[EPOCH #192, elapsed time: 2442.549[sec]] loss: 4.0595522868046\n",
      "[EPOCH #193, elapsed time: 2459.305[sec]] loss: 4.060598502918763\n",
      "[EPOCH #194, elapsed time: 2470.854[sec]] loss: 4.057576019338362\n",
      "[EPOCH #195, elapsed time: 2482.932[sec]] loss: 4.059027592341105\n",
      "[EPOCH #196, elapsed time: 2495.076[sec]] loss: 4.054119397338506\n",
      "[EPOCH #197, elapsed time: 2506.923[sec]] loss: 4.056833925418037\n",
      "[EPOCH #198, elapsed time: 2520.587[sec]] loss: 4.058174952740709\n",
      "[EPOCH #199, elapsed time: 2532.283[sec]] loss: 4.052980823236174\n",
      "[EPOCH #200, elapsed time: 2544.125[sec]] loss: 4.05826918665446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 05:09:11,696] Trial 6 finished with value: 0.67872 and parameters: {'learning_rate': 0.000717072545151194}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.60513796351769\n",
      "[EPOCH #1, elapsed time: 16.002[sec]] loss: 4.603634633853202\n",
      "[EPOCH #2, elapsed time: 31.975[sec]] loss: 4.596782467110525\n",
      "[EPOCH #3, elapsed time: 43.859[sec]] loss: 4.5910530801164136\n",
      "[EPOCH #4, elapsed time: 56.423[sec]] loss: 4.584991819341443\n",
      "[EPOCH #5, elapsed time: 68.449[sec]] loss: 4.5785329672898225\n",
      "[EPOCH #6, elapsed time: 80.473[sec]] loss: 4.571764251931081\n",
      "[EPOCH #7, elapsed time: 94.127[sec]] loss: 4.5648460122765595\n",
      "[EPOCH #8, elapsed time: 106.164[sec]] loss: 4.559423167203683\n",
      "[EPOCH #9, elapsed time: 118.553[sec]] loss: 4.555568541308968\n",
      "[EPOCH #10, elapsed time: 130.911[sec]] loss: 4.551449714985248\n",
      "[EPOCH #11, elapsed time: 142.711[sec]] loss: 4.54834086301612\n",
      "[EPOCH #12, elapsed time: 154.356[sec]] loss: 4.544986952136742\n",
      "[EPOCH #13, elapsed time: 166.108[sec]] loss: 4.542257679934999\n",
      "[EPOCH #14, elapsed time: 177.831[sec]] loss: 4.539290965404255\n",
      "[EPOCH #15, elapsed time: 192.158[sec]] loss: 4.536099396374312\n",
      "[EPOCH #16, elapsed time: 203.855[sec]] loss: 4.532410507238758\n",
      "[EPOCH #17, elapsed time: 216.111[sec]] loss: 4.528362478877365\n",
      "[EPOCH #18, elapsed time: 228.160[sec]] loss: 4.524016812605806\n",
      "[EPOCH #19, elapsed time: 240.244[sec]] loss: 4.519771755923847\n",
      "[EPOCH #20, elapsed time: 251.868[sec]] loss: 4.515567399718711\n",
      "[EPOCH #21, elapsed time: 263.678[sec]] loss: 4.511595010299829\n",
      "[EPOCH #22, elapsed time: 275.401[sec]] loss: 4.5091938286047295\n",
      "[EPOCH #23, elapsed time: 287.457[sec]] loss: 4.505521857883407\n",
      "[EPOCH #24, elapsed time: 300.181[sec]] loss: 4.502779647927214\n",
      "[EPOCH #25, elapsed time: 312.648[sec]] loss: 4.499084916239889\n",
      "[EPOCH #26, elapsed time: 324.559[sec]] loss: 4.497440502571892\n",
      "[EPOCH #27, elapsed time: 336.135[sec]] loss: 4.493394158546999\n",
      "[EPOCH #28, elapsed time: 351.919[sec]] loss: 4.491427085983853\n",
      "[EPOCH #29, elapsed time: 363.791[sec]] loss: 4.489162028941754\n",
      "[EPOCH #30, elapsed time: 375.863[sec]] loss: 4.486024518540786\n",
      "[EPOCH #31, elapsed time: 387.564[sec]] loss: 4.482456087989831\n",
      "[EPOCH #32, elapsed time: 400.381[sec]] loss: 4.480987904549255\n",
      "[EPOCH #33, elapsed time: 412.187[sec]] loss: 4.476473093795533\n",
      "[EPOCH #34, elapsed time: 424.586[sec]] loss: 4.474572196955568\n",
      "[EPOCH #35, elapsed time: 436.336[sec]] loss: 4.472313541978578\n",
      "[EPOCH #36, elapsed time: 448.735[sec]] loss: 4.470045359105692\n",
      "[EPOCH #37, elapsed time: 461.379[sec]] loss: 4.467278374057509\n",
      "[EPOCH #38, elapsed time: 474.346[sec]] loss: 4.464670839938154\n",
      "[EPOCH #39, elapsed time: 487.352[sec]] loss: 4.462593855342267\n",
      "[EPOCH #40, elapsed time: 499.496[sec]] loss: 4.459976978204377\n",
      "[EPOCH #41, elapsed time: 511.401[sec]] loss: 4.457482682079821\n",
      "[EPOCH #42, elapsed time: 524.162[sec]] loss: 4.453524866244462\n",
      "[EPOCH #43, elapsed time: 538.014[sec]] loss: 4.45111198815793\n",
      "[EPOCH #44, elapsed time: 552.713[sec]] loss: 4.449017334045391\n",
      "[EPOCH #45, elapsed time: 565.178[sec]] loss: 4.446347396494255\n",
      "[EPOCH #46, elapsed time: 577.383[sec]] loss: 4.443092591169166\n",
      "[EPOCH #47, elapsed time: 589.155[sec]] loss: 4.4412647205823825\n",
      "[EPOCH #48, elapsed time: 602.737[sec]] loss: 4.4387432761628585\n",
      "[EPOCH #49, elapsed time: 619.004[sec]] loss: 4.435014082541927\n",
      "[EPOCH #50, elapsed time: 631.435[sec]] loss: 4.432861207924207\n",
      "[EPOCH #51, elapsed time: 643.812[sec]] loss: 4.4302249812042565\n",
      "[EPOCH #52, elapsed time: 656.436[sec]] loss: 4.42856947641989\n",
      "[EPOCH #53, elapsed time: 669.256[sec]] loss: 4.426478838966355\n",
      "[EPOCH #54, elapsed time: 683.726[sec]] loss: 4.425156957586072\n",
      "[EPOCH #55, elapsed time: 695.441[sec]] loss: 4.42188828188261\n",
      "[EPOCH #56, elapsed time: 707.111[sec]] loss: 4.420209297978261\n",
      "[EPOCH #57, elapsed time: 719.165[sec]] loss: 4.418572324861415\n",
      "[EPOCH #58, elapsed time: 730.970[sec]] loss: 4.415964697693223\n",
      "[EPOCH #59, elapsed time: 743.345[sec]] loss: 4.4147928702243995\n",
      "[EPOCH #60, elapsed time: 755.081[sec]] loss: 4.412051863191376\n",
      "[EPOCH #61, elapsed time: 766.734[sec]] loss: 4.41100003485945\n",
      "[EPOCH #62, elapsed time: 778.717[sec]] loss: 4.409584398080505\n",
      "[EPOCH #63, elapsed time: 791.471[sec]] loss: 4.407323831788867\n",
      "[EPOCH #64, elapsed time: 803.992[sec]] loss: 4.404438804070002\n",
      "[EPOCH #65, elapsed time: 816.477[sec]] loss: 4.404620756038243\n",
      "[EPOCH #66, elapsed time: 829.736[sec]] loss: 4.401081982439935\n",
      "[EPOCH #67, elapsed time: 843.599[sec]] loss: 4.398456171393319\n",
      "[EPOCH #68, elapsed time: 858.282[sec]] loss: 4.397625892954001\n",
      "[EPOCH #69, elapsed time: 870.369[sec]] loss: 4.394745582963744\n",
      "[EPOCH #70, elapsed time: 882.276[sec]] loss: 4.39466033436439\n",
      "[EPOCH #71, elapsed time: 894.767[sec]] loss: 4.391704609053911\n",
      "[EPOCH #72, elapsed time: 907.750[sec]] loss: 4.389843552065292\n",
      "[EPOCH #73, elapsed time: 920.449[sec]] loss: 4.387239926607275\n",
      "[EPOCH #74, elapsed time: 937.257[sec]] loss: 4.385861778137246\n",
      "[EPOCH #75, elapsed time: 953.863[sec]] loss: 4.3831408757547194\n",
      "[EPOCH #76, elapsed time: 970.077[sec]] loss: 4.382494375481486\n",
      "[EPOCH #77, elapsed time: 982.204[sec]] loss: 4.3795834332037185\n",
      "[EPOCH #78, elapsed time: 994.109[sec]] loss: 4.378261113426126\n",
      "[EPOCH #79, elapsed time: 1006.087[sec]] loss: 4.3760367644878055\n",
      "[EPOCH #80, elapsed time: 1018.335[sec]] loss: 4.374598259965502\n",
      "[EPOCH #81, elapsed time: 1030.204[sec]] loss: 4.374163207531891\n",
      "[EPOCH #82, elapsed time: 1045.099[sec]] loss: 4.370358554659481\n",
      "[EPOCH #83, elapsed time: 1056.947[sec]] loss: 4.372584386277641\n",
      "[EPOCH #84, elapsed time: 1069.900[sec]] loss: 4.367273823999855\n",
      "[EPOCH #85, elapsed time: 1082.649[sec]] loss: 4.365840514348397\n",
      "[EPOCH #86, elapsed time: 1094.885[sec]] loss: 4.365074783430142\n",
      "[EPOCH #87, elapsed time: 1107.162[sec]] loss: 4.3629170583748165\n",
      "[EPOCH #88, elapsed time: 1119.932[sec]] loss: 4.360537410011219\n",
      "[EPOCH #89, elapsed time: 1133.087[sec]] loss: 4.359515380798359\n",
      "[EPOCH #90, elapsed time: 1147.045[sec]] loss: 4.358910825110671\n",
      "[EPOCH #91, elapsed time: 1160.162[sec]] loss: 4.35748355539655\n",
      "[EPOCH #92, elapsed time: 1174.433[sec]] loss: 4.354770496573421\n",
      "[EPOCH #93, elapsed time: 1189.080[sec]] loss: 4.353244128626886\n",
      "[EPOCH #94, elapsed time: 1202.409[sec]] loss: 4.351704123686768\n",
      "[EPOCH #95, elapsed time: 1215.140[sec]] loss: 4.34873170114372\n",
      "[EPOCH #96, elapsed time: 1226.741[sec]] loss: 4.348827107625365\n",
      "[EPOCH #97, elapsed time: 1238.929[sec]] loss: 4.346005862291547\n",
      "[EPOCH #98, elapsed time: 1251.422[sec]] loss: 4.344087925921323\n",
      "[EPOCH #99, elapsed time: 1263.408[sec]] loss: 4.343050609242054\n",
      "[EPOCH #100, elapsed time: 1276.139[sec]] loss: 4.341290568207139\n",
      "[EPOCH #101, elapsed time: 1289.067[sec]] loss: 4.340268198832136\n",
      "[EPOCH #102, elapsed time: 1301.700[sec]] loss: 4.339968639234663\n",
      "[EPOCH #103, elapsed time: 1317.252[sec]] loss: 4.33484475946701\n",
      "[EPOCH #104, elapsed time: 1333.112[sec]] loss: 4.33279993193926\n",
      "[EPOCH #105, elapsed time: 1344.808[sec]] loss: 4.332655671767066\n",
      "[EPOCH #106, elapsed time: 1359.390[sec]] loss: 4.331963996435691\n",
      "[EPOCH #107, elapsed time: 1371.187[sec]] loss: 4.330452285053939\n",
      "[EPOCH #108, elapsed time: 1386.323[sec]] loss: 4.328439766523248\n",
      "[EPOCH #109, elapsed time: 1399.253[sec]] loss: 4.32690957350679\n",
      "[EPOCH #110, elapsed time: 1411.812[sec]] loss: 4.326484774139853\n",
      "[EPOCH #111, elapsed time: 1424.435[sec]] loss: 4.326479958786235\n",
      "[EPOCH #112, elapsed time: 1440.491[sec]] loss: 4.3237408112045745\n",
      "[EPOCH #113, elapsed time: 1452.914[sec]] loss: 4.322207834960098\n",
      "[EPOCH #114, elapsed time: 1464.684[sec]] loss: 4.3214401942525855\n",
      "[EPOCH #115, elapsed time: 1476.860[sec]] loss: 4.322214096384177\n",
      "[EPOCH #116, elapsed time: 1488.465[sec]] loss: 4.319300887985864\n",
      "[EPOCH #117, elapsed time: 1500.315[sec]] loss: 4.318441463294734\n",
      "[EPOCH #118, elapsed time: 1514.738[sec]] loss: 4.316947867148821\n",
      "[EPOCH #119, elapsed time: 1528.719[sec]] loss: 4.315268639487024\n",
      "[EPOCH #120, elapsed time: 1540.498[sec]] loss: 4.313340471710673\n",
      "[EPOCH #121, elapsed time: 1552.906[sec]] loss: 4.314891119912429\n",
      "[EPOCH #122, elapsed time: 1565.642[sec]] loss: 4.311778587480425\n",
      "[EPOCH #123, elapsed time: 1577.364[sec]] loss: 4.3103469307424165\n",
      "[EPOCH #124, elapsed time: 1590.232[sec]] loss: 4.307945380207825\n",
      "[EPOCH #125, elapsed time: 1602.892[sec]] loss: 4.307716027331215\n",
      "[EPOCH #126, elapsed time: 1615.656[sec]] loss: 4.307627453990114\n",
      "[EPOCH #127, elapsed time: 1627.325[sec]] loss: 4.306190790309405\n",
      "[EPOCH #128, elapsed time: 1639.220[sec]] loss: 4.304922937660437\n",
      "[EPOCH #129, elapsed time: 1650.954[sec]] loss: 4.303321140360085\n",
      "[EPOCH #130, elapsed time: 1662.934[sec]] loss: 4.302789857733806\n",
      "[EPOCH #131, elapsed time: 1676.406[sec]] loss: 4.299189390010431\n",
      "[EPOCH #132, elapsed time: 1688.184[sec]] loss: 4.300186377187913\n",
      "[EPOCH #133, elapsed time: 1702.146[sec]] loss: 4.2991885434185475\n",
      "[EPOCH #134, elapsed time: 1714.117[sec]] loss: 4.298801757400034\n",
      "[EPOCH #135, elapsed time: 1726.861[sec]] loss: 4.296723463103623\n",
      "[EPOCH #136, elapsed time: 1738.916[sec]] loss: 4.29575336581991\n",
      "[EPOCH #137, elapsed time: 1751.718[sec]] loss: 4.293555900978874\n",
      "[EPOCH #138, elapsed time: 1763.570[sec]] loss: 4.294339253821589\n",
      "[EPOCH #139, elapsed time: 1775.506[sec]] loss: 4.292411288312056\n",
      "[EPOCH #140, elapsed time: 1787.350[sec]] loss: 4.290076607858532\n",
      "[EPOCH #141, elapsed time: 1799.653[sec]] loss: 4.2910717050768366\n",
      "[EPOCH #142, elapsed time: 1811.311[sec]] loss: 4.290037314859782\n",
      "[EPOCH #143, elapsed time: 1822.914[sec]] loss: 4.2890701605125034\n",
      "[EPOCH #144, elapsed time: 1834.777[sec]] loss: 4.28703405516924\n",
      "[EPOCH #145, elapsed time: 1846.450[sec]] loss: 4.286283477788085\n",
      "[EPOCH #146, elapsed time: 1858.293[sec]] loss: 4.284983769793276\n",
      "[EPOCH #147, elapsed time: 1872.652[sec]] loss: 4.284277108717788\n",
      "[EPOCH #148, elapsed time: 1885.347[sec]] loss: 4.282834239747382\n",
      "[EPOCH #149, elapsed time: 1898.558[sec]] loss: 4.281682277442703\n",
      "[EPOCH #150, elapsed time: 1911.310[sec]] loss: 4.28211270092545\n",
      "[EPOCH #151, elapsed time: 1924.012[sec]] loss: 4.278628257018057\n",
      "[EPOCH #152, elapsed time: 1939.365[sec]] loss: 4.280368647511312\n",
      "[EPOCH #153, elapsed time: 1951.231[sec]] loss: 4.277949128178397\n",
      "[EPOCH #154, elapsed time: 1962.937[sec]] loss: 4.276333968454802\n",
      "[EPOCH #155, elapsed time: 1975.140[sec]] loss: 4.278079467024166\n",
      "[EPOCH #156, elapsed time: 1987.758[sec]] loss: 4.2758226653969755\n",
      "[EPOCH #157, elapsed time: 2000.051[sec]] loss: 4.273336471538093\n",
      "[EPOCH #158, elapsed time: 2014.200[sec]] loss: 4.274142906594109\n",
      "[EPOCH #159, elapsed time: 2028.849[sec]] loss: 4.271348521256401\n",
      "[EPOCH #160, elapsed time: 2044.629[sec]] loss: 4.270555609094738\n",
      "[EPOCH #161, elapsed time: 2060.169[sec]] loss: 4.271302319000108\n",
      "[EPOCH #162, elapsed time: 2074.128[sec]] loss: 4.26963227968222\n",
      "[EPOCH #163, elapsed time: 2085.714[sec]] loss: 4.268607219212801\n",
      "[EPOCH #164, elapsed time: 2097.671[sec]] loss: 4.265487297940392\n",
      "[EPOCH #165, elapsed time: 2109.848[sec]] loss: 4.26587749656316\n",
      "[EPOCH #166, elapsed time: 2122.017[sec]] loss: 4.265488097702778\n",
      "[EPOCH #167, elapsed time: 2134.019[sec]] loss: 4.2642760627634315\n",
      "[EPOCH #168, elapsed time: 2146.477[sec]] loss: 4.26349576375306\n",
      "[EPOCH #169, elapsed time: 2158.243[sec]] loss: 4.261947195726713\n",
      "[EPOCH #170, elapsed time: 2171.970[sec]] loss: 4.261225566754185\n",
      "[EPOCH #171, elapsed time: 2183.902[sec]] loss: 4.261297150902922\n",
      "[EPOCH #172, elapsed time: 2195.729[sec]] loss: 4.260520064823153\n",
      "[EPOCH #173, elapsed time: 2207.536[sec]] loss: 4.260155728743462\n",
      "[EPOCH #174, elapsed time: 2219.480[sec]] loss: 4.2589164512399975\n",
      "[EPOCH #175, elapsed time: 2230.960[sec]] loss: 4.258872245415159\n",
      "[EPOCH #176, elapsed time: 2242.754[sec]] loss: 4.255275073451105\n",
      "[EPOCH #177, elapsed time: 2256.452[sec]] loss: 4.254655812691208\n",
      "[EPOCH #178, elapsed time: 2268.193[sec]] loss: 4.253530916768011\n",
      "[EPOCH #179, elapsed time: 2280.619[sec]] loss: 4.25271282040455\n",
      "[EPOCH #180, elapsed time: 2293.101[sec]] loss: 4.252059465177686\n",
      "[EPOCH #181, elapsed time: 2305.725[sec]] loss: 4.252136897217061\n",
      "[EPOCH #182, elapsed time: 2317.866[sec]] loss: 4.2522482569981905\n",
      "[EPOCH #183, elapsed time: 2331.253[sec]] loss: 4.250031369203798\n",
      "[EPOCH #184, elapsed time: 2344.570[sec]] loss: 4.248260829971909\n",
      "[EPOCH #185, elapsed time: 2356.072[sec]] loss: 4.250230368176715\n",
      "[EPOCH #186, elapsed time: 2368.813[sec]] loss: 4.2468888488093475\n",
      "[EPOCH #187, elapsed time: 2380.983[sec]] loss: 4.246262146430525\n",
      "[EPOCH #188, elapsed time: 2393.240[sec]] loss: 4.243578365500433\n",
      "[EPOCH #189, elapsed time: 2404.807[sec]] loss: 4.240858208271303\n",
      "[EPOCH #190, elapsed time: 2418.033[sec]] loss: 4.24361901899522\n",
      "[EPOCH #191, elapsed time: 2429.457[sec]] loss: 4.240340641348772\n",
      "[EPOCH #192, elapsed time: 2441.013[sec]] loss: 4.240964446552884\n",
      "[EPOCH #193, elapsed time: 2452.409[sec]] loss: 4.2397704514950725\n",
      "[EPOCH #194, elapsed time: 2464.859[sec]] loss: 4.238551629024366\n",
      "[EPOCH #195, elapsed time: 2476.688[sec]] loss: 4.237297033699217\n",
      "[EPOCH #196, elapsed time: 2488.435[sec]] loss: 4.238253929640952\n",
      "[EPOCH #197, elapsed time: 2501.087[sec]] loss: 4.236128804474707\n",
      "[EPOCH #198, elapsed time: 2513.424[sec]] loss: 4.234726308937341\n",
      "[EPOCH #199, elapsed time: 2526.614[sec]] loss: 4.23453184281567\n",
      "[EPOCH #200, elapsed time: 2538.460[sec]] loss: 4.232426092247893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 05:51:46,707] Trial 7 finished with value: 0.44936 and parameters: {'learning_rate': 1.5083167685079541e-05}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605194319690257\n",
      "[EPOCH #1, elapsed time: 12.236[sec]] loss: 4.588079422311911\n",
      "[EPOCH #2, elapsed time: 25.283[sec]] loss: 4.585370259641914\n",
      "[EPOCH #3, elapsed time: 37.380[sec]] loss: 4.584108236731433\n",
      "[EPOCH #4, elapsed time: 49.076[sec]] loss: 4.578665260771339\n",
      "[EPOCH #5, elapsed time: 60.860[sec]] loss: 4.571758483513303\n",
      "[EPOCH #6, elapsed time: 74.457[sec]] loss: 4.569913174354031\n",
      "[EPOCH #7, elapsed time: 86.628[sec]] loss: 4.567502515406008\n",
      "[EPOCH #8, elapsed time: 99.075[sec]] loss: 4.56667154871037\n",
      "[EPOCH #9, elapsed time: 110.823[sec]] loss: 4.5597258218190495\n",
      "[EPOCH #10, elapsed time: 126.194[sec]] loss: 4.560675445612775\n",
      "[EPOCH #11, elapsed time: 138.075[sec]] loss: 4.561987537035024\n",
      "[EPOCH #12, elapsed time: 149.629[sec]] loss: 4.554886501261003\n",
      "[EPOCH #13, elapsed time: 161.678[sec]] loss: 4.550827737199292\n",
      "[EPOCH #14, elapsed time: 173.563[sec]] loss: 4.558559276473423\n",
      "[EPOCH #15, elapsed time: 185.024[sec]] loss: 4.554626596332436\n",
      "[EPOCH #16, elapsed time: 196.642[sec]] loss: 4.548756283975456\n",
      "[EPOCH #17, elapsed time: 208.428[sec]] loss: 4.55112131573951\n",
      "[EPOCH #18, elapsed time: 219.825[sec]] loss: 4.545717251583009\n",
      "[EPOCH #19, elapsed time: 231.516[sec]] loss: 4.54292534347993\n",
      "[EPOCH #20, elapsed time: 243.423[sec]] loss: 4.538507190028293\n",
      "[EPOCH #21, elapsed time: 257.846[sec]] loss: 4.536031916136934\n",
      "[EPOCH #22, elapsed time: 269.977[sec]] loss: 4.537195051929086\n",
      "[EPOCH #23, elapsed time: 286.383[sec]] loss: 4.530201605932879\n",
      "[EPOCH #24, elapsed time: 297.916[sec]] loss: 4.529809891986908\n",
      "[EPOCH #25, elapsed time: 313.690[sec]] loss: 4.530669259933501\n",
      "[EPOCH #26, elapsed time: 326.332[sec]] loss: 4.5296539561685965\n",
      "[EPOCH #27, elapsed time: 340.512[sec]] loss: 4.524847728353392\n",
      "[EPOCH #28, elapsed time: 353.993[sec]] loss: 4.51858160698635\n",
      "[EPOCH #29, elapsed time: 365.745[sec]] loss: 4.519568972914019\n",
      "[EPOCH #30, elapsed time: 379.979[sec]] loss: 4.5156870428294\n",
      "[EPOCH #31, elapsed time: 393.707[sec]] loss: 4.5110201795972165\n",
      "[EPOCH #32, elapsed time: 406.151[sec]] loss: 4.510068190532545\n",
      "[EPOCH #33, elapsed time: 417.479[sec]] loss: 4.505599815419905\n",
      "[EPOCH #34, elapsed time: 430.068[sec]] loss: 4.503555532456665\n",
      "[EPOCH #35, elapsed time: 443.155[sec]] loss: 4.512747745977634\n",
      "[EPOCH #36, elapsed time: 454.704[sec]] loss: 4.50267983001543\n",
      "[EPOCH #37, elapsed time: 467.987[sec]] loss: 4.507341723219371\n",
      "[EPOCH #38, elapsed time: 480.339[sec]] loss: 4.496645349306093\n",
      "[EPOCH #39, elapsed time: 492.167[sec]] loss: 4.494196164173266\n",
      "[EPOCH #40, elapsed time: 504.114[sec]] loss: 4.499387777393168\n",
      "[EPOCH #41, elapsed time: 515.828[sec]] loss: 4.502903141734391\n",
      "[EPOCH #42, elapsed time: 527.510[sec]] loss: 4.501839055667226\n",
      "[EPOCH #43, elapsed time: 539.937[sec]] loss: 4.494994851159348\n",
      "[EPOCH #44, elapsed time: 551.575[sec]] loss: 4.493085334641157\n",
      "[EPOCH #45, elapsed time: 564.037[sec]] loss: 4.490951168026134\n",
      "[EPOCH #46, elapsed time: 576.357[sec]] loss: 4.487391244884645\n",
      "[EPOCH #47, elapsed time: 588.459[sec]] loss: 4.484784988432608\n",
      "[EPOCH #48, elapsed time: 600.866[sec]] loss: 4.480417926419796\n",
      "[EPOCH #49, elapsed time: 612.544[sec]] loss: 4.483107196468614\n",
      "[EPOCH #50, elapsed time: 624.104[sec]] loss: 4.479037313223342\n",
      "[EPOCH #51, elapsed time: 635.973[sec]] loss: 4.482382879299913\n",
      "[EPOCH #52, elapsed time: 647.808[sec]] loss: 4.472172023238697\n",
      "[EPOCH #53, elapsed time: 663.361[sec]] loss: 4.472663285102283\n",
      "[EPOCH #54, elapsed time: 675.642[sec]] loss: 4.4672580049042505\n",
      "[EPOCH #55, elapsed time: 686.910[sec]] loss: 4.461744749919772\n",
      "[EPOCH #56, elapsed time: 698.499[sec]] loss: 4.48035781381989\n",
      "[EPOCH #57, elapsed time: 710.070[sec]] loss: 4.470329994584838\n",
      "[EPOCH #58, elapsed time: 722.643[sec]] loss: 4.456613626071298\n",
      "[EPOCH #59, elapsed time: 735.027[sec]] loss: 4.461837249311666\n",
      "[EPOCH #60, elapsed time: 750.202[sec]] loss: 4.451631993265085\n",
      "[EPOCH #61, elapsed time: 764.069[sec]] loss: 4.454019726809979\n",
      "[EPOCH #62, elapsed time: 777.624[sec]] loss: 4.461667459589201\n",
      "[EPOCH #63, elapsed time: 789.487[sec]] loss: 4.45008118947347\n",
      "[EPOCH #64, elapsed time: 805.335[sec]] loss: 4.45414691816441\n",
      "[EPOCH #65, elapsed time: 821.398[sec]] loss: 4.450384831321751\n",
      "[EPOCH #66, elapsed time: 833.911[sec]] loss: 4.444550445273528\n",
      "[EPOCH #67, elapsed time: 849.682[sec]] loss: 4.448936935273486\n",
      "[EPOCH #68, elapsed time: 864.977[sec]] loss: 4.446735335098042\n",
      "[EPOCH #69, elapsed time: 879.082[sec]] loss: 4.440555662202744\n",
      "[EPOCH #70, elapsed time: 891.737[sec]] loss: 4.443874610210182\n",
      "[EPOCH #71, elapsed time: 904.293[sec]] loss: 4.47225851625185\n",
      "[EPOCH #72, elapsed time: 916.586[sec]] loss: 4.452041114360495\n",
      "[EPOCH #73, elapsed time: 928.169[sec]] loss: 4.4646437136088135\n",
      "[EPOCH #74, elapsed time: 939.490[sec]] loss: 4.45402838218235\n",
      "[EPOCH #75, elapsed time: 952.153[sec]] loss: 4.446296295292928\n",
      "[EPOCH #76, elapsed time: 965.800[sec]] loss: 4.446661912243258\n",
      "[EPOCH #77, elapsed time: 981.404[sec]] loss: 4.447732358274747\n",
      "[EPOCH #78, elapsed time: 997.227[sec]] loss: 4.4507546931295465\n",
      "[EPOCH #79, elapsed time: 1011.750[sec]] loss: 4.438338706528461\n",
      "[EPOCH #80, elapsed time: 1024.753[sec]] loss: 4.467109738445709\n",
      "[EPOCH #81, elapsed time: 1040.153[sec]] loss: 4.468447294436581\n",
      "[EPOCH #82, elapsed time: 1055.799[sec]] loss: 4.453233173392327\n",
      "[EPOCH #83, elapsed time: 1069.485[sec]] loss: 4.447011623333756\n",
      "[EPOCH #84, elapsed time: 1080.743[sec]] loss: 4.446724049951965\n",
      "[EPOCH #85, elapsed time: 1097.108[sec]] loss: 4.452031474500914\n",
      "[EPOCH #86, elapsed time: 1109.821[sec]] loss: 4.444263282221857\n",
      "[EPOCH #87, elapsed time: 1122.854[sec]] loss: 4.444322814868188\n",
      "[EPOCH #88, elapsed time: 1134.388[sec]] loss: 4.437330804874862\n",
      "[EPOCH #89, elapsed time: 1146.803[sec]] loss: 4.439245126679092\n",
      "[EPOCH #90, elapsed time: 1159.068[sec]] loss: 4.4388869374666315\n",
      "[EPOCH #91, elapsed time: 1170.359[sec]] loss: 4.438977934653685\n",
      "[EPOCH #92, elapsed time: 1181.633[sec]] loss: 4.4349116318056545\n",
      "[EPOCH #93, elapsed time: 1192.890[sec]] loss: 4.433871968465208\n",
      "[EPOCH #94, elapsed time: 1205.303[sec]] loss: 4.431904915427063\n",
      "[EPOCH #95, elapsed time: 1216.642[sec]] loss: 4.428788499655208\n",
      "[EPOCH #96, elapsed time: 1229.107[sec]] loss: 4.430951369548561\n",
      "[EPOCH #97, elapsed time: 1240.821[sec]] loss: 4.429216630475611\n",
      "[EPOCH #98, elapsed time: 1252.373[sec]] loss: 4.424176780214999\n",
      "[EPOCH #99, elapsed time: 1264.108[sec]] loss: 4.425002762543721\n",
      "[EPOCH #100, elapsed time: 1275.428[sec]] loss: 4.420448751382468\n",
      "[EPOCH #101, elapsed time: 1286.845[sec]] loss: 4.417037162121793\n",
      "[EPOCH #102, elapsed time: 1302.551[sec]] loss: 4.417655321549546\n",
      "[EPOCH #103, elapsed time: 1318.321[sec]] loss: 4.42224640153725\n",
      "[EPOCH #104, elapsed time: 1330.091[sec]] loss: 4.414221182131874\n",
      "[EPOCH #105, elapsed time: 1348.298[sec]] loss: 4.425779688609996\n",
      "[EPOCH #106, elapsed time: 1361.550[sec]] loss: 4.4216278564907086\n",
      "[EPOCH #107, elapsed time: 1375.575[sec]] loss: 4.425528414037391\n",
      "[EPOCH #108, elapsed time: 1388.866[sec]] loss: 4.412615585693242\n",
      "[EPOCH #109, elapsed time: 1403.805[sec]] loss: 4.411108359570543\n",
      "[EPOCH #110, elapsed time: 1418.269[sec]] loss: 4.4297576560778875\n",
      "[EPOCH #111, elapsed time: 1431.426[sec]] loss: 4.4108190887338905\n",
      "[EPOCH #112, elapsed time: 1444.497[sec]] loss: 4.420451254548739\n",
      "[EPOCH #113, elapsed time: 1457.430[sec]] loss: 4.41915025119208\n",
      "[EPOCH #114, elapsed time: 1470.629[sec]] loss: 4.413243612263802\n",
      "[EPOCH #115, elapsed time: 1482.960[sec]] loss: 4.417578451006518\n",
      "[EPOCH #116, elapsed time: 1495.223[sec]] loss: 4.423508167571923\n",
      "[EPOCH #117, elapsed time: 1507.347[sec]] loss: 4.413000140980277\n",
      "[EPOCH #118, elapsed time: 1524.211[sec]] loss: 4.413752423443248\n",
      "[EPOCH #119, elapsed time: 1538.378[sec]] loss: 4.414721823318296\n",
      "[EPOCH #120, elapsed time: 1553.701[sec]] loss: 4.4068389539907775\n",
      "[EPOCH #121, elapsed time: 1569.545[sec]] loss: 4.408599835516013\n",
      "[EPOCH #122, elapsed time: 1585.215[sec]] loss: 4.408344263002343\n",
      "[EPOCH #123, elapsed time: 1601.952[sec]] loss: 4.405016430813917\n",
      "[EPOCH #124, elapsed time: 1614.146[sec]] loss: 4.410236196340999\n",
      "[EPOCH #125, elapsed time: 1626.676[sec]] loss: 4.404984977561087\n",
      "[EPOCH #126, elapsed time: 1639.166[sec]] loss: 4.416343533069906\n",
      "[EPOCH #127, elapsed time: 1650.729[sec]] loss: 4.42476986862495\n",
      "[EPOCH #128, elapsed time: 1662.444[sec]] loss: 4.428898059017599\n",
      "[EPOCH #129, elapsed time: 1673.854[sec]] loss: 4.407990961752103\n",
      "[EPOCH #130, elapsed time: 1685.327[sec]] loss: 4.411488943045062\n",
      "[EPOCH #131, elapsed time: 1697.713[sec]] loss: 4.407902601355555\n",
      "[EPOCH #132, elapsed time: 1710.496[sec]] loss: 4.402360213542702\n",
      "[EPOCH #133, elapsed time: 1721.905[sec]] loss: 4.413743158219643\n",
      "[EPOCH #134, elapsed time: 1734.171[sec]] loss: 4.402291886408361\n",
      "[EPOCH #135, elapsed time: 1746.029[sec]] loss: 4.407746764077487\n",
      "[EPOCH #136, elapsed time: 1757.866[sec]] loss: 4.418717482878623\n",
      "[EPOCH #137, elapsed time: 1770.238[sec]] loss: 4.400902254491453\n",
      "[EPOCH #138, elapsed time: 1784.011[sec]] loss: 4.403083782049608\n",
      "[EPOCH #139, elapsed time: 1795.694[sec]] loss: 4.40259472254523\n",
      "[EPOCH #140, elapsed time: 1809.590[sec]] loss: 4.402834159780296\n",
      "[EPOCH #141, elapsed time: 1822.100[sec]] loss: 4.403342237856933\n",
      "[EPOCH #142, elapsed time: 1833.752[sec]] loss: 4.400243779900588\n",
      "[EPOCH #143, elapsed time: 1845.664[sec]] loss: 4.4009048439948435\n",
      "[EPOCH #144, elapsed time: 1857.233[sec]] loss: 4.406596110863176\n",
      "[EPOCH #145, elapsed time: 1871.269[sec]] loss: 4.407779909903929\n",
      "[EPOCH #146, elapsed time: 1885.162[sec]] loss: 4.39017708059007\n",
      "[EPOCH #147, elapsed time: 1897.360[sec]] loss: 4.391684908631972\n",
      "[EPOCH #148, elapsed time: 1908.776[sec]] loss: 4.389554832504868\n",
      "[EPOCH #149, elapsed time: 1921.515[sec]] loss: 4.409826810132672\n",
      "[EPOCH #150, elapsed time: 1933.526[sec]] loss: 4.39576398906842\n",
      "[EPOCH #151, elapsed time: 1948.608[sec]] loss: 4.391827456095397\n",
      "[EPOCH #152, elapsed time: 1960.809[sec]] loss: 4.3898531177908815\n",
      "[EPOCH #153, elapsed time: 1977.458[sec]] loss: 4.3891231341005055\n",
      "[EPOCH #154, elapsed time: 1989.150[sec]] loss: 4.3950897422419555\n",
      "[EPOCH #155, elapsed time: 2000.560[sec]] loss: 4.3891151977973495\n",
      "[EPOCH #156, elapsed time: 2014.161[sec]] loss: 4.3899581275227275\n",
      "[EPOCH #157, elapsed time: 2025.677[sec]] loss: 4.38959928605317\n",
      "[EPOCH #158, elapsed time: 2037.037[sec]] loss: 4.388938591103484\n",
      "[EPOCH #159, elapsed time: 2048.498[sec]] loss: 4.399468574170035\n",
      "[EPOCH #160, elapsed time: 2060.194[sec]] loss: 4.384956352541406\n",
      "[EPOCH #161, elapsed time: 2071.775[sec]] loss: 4.388827681770252\n",
      "[EPOCH #162, elapsed time: 2083.180[sec]] loss: 4.426793584744288\n",
      "[EPOCH #163, elapsed time: 2094.614[sec]] loss: 4.400455333602329\n",
      "[EPOCH #164, elapsed time: 2108.131[sec]] loss: 4.382161110239157\n",
      "[EPOCH #165, elapsed time: 2119.469[sec]] loss: 4.394072078392434\n",
      "[EPOCH #166, elapsed time: 2131.097[sec]] loss: 4.403710046793815\n",
      "[EPOCH #167, elapsed time: 2143.298[sec]] loss: 4.394300087399767\n",
      "[EPOCH #168, elapsed time: 2155.500[sec]] loss: 4.387652423392483\n",
      "[EPOCH #169, elapsed time: 2167.084[sec]] loss: 4.394449637017034\n",
      "[EPOCH #170, elapsed time: 2178.999[sec]] loss: 4.400468749719328\n",
      "[EPOCH #171, elapsed time: 2192.103[sec]] loss: 4.38597876645782\n",
      "[EPOCH #172, elapsed time: 2204.211[sec]] loss: 4.3906207914468345\n",
      "[EPOCH #173, elapsed time: 2216.533[sec]] loss: 4.401479553886507\n",
      "[EPOCH #174, elapsed time: 2228.352[sec]] loss: 4.389967614843231\n",
      "[EPOCH #175, elapsed time: 2244.492[sec]] loss: 4.3827091404359\n",
      "[EPOCH #176, elapsed time: 2258.006[sec]] loss: 4.400418072576639\n",
      "[EPOCH #177, elapsed time: 2272.857[sec]] loss: 4.391392000348462\n",
      "[EPOCH #178, elapsed time: 2286.054[sec]] loss: 4.392797204979856\n",
      "[EPOCH #179, elapsed time: 2299.458[sec]] loss: 4.38617444389231\n",
      "[EPOCH #180, elapsed time: 2312.934[sec]] loss: 4.381341233439577\n",
      "[EPOCH #181, elapsed time: 2324.970[sec]] loss: 4.408431381730796\n",
      "[EPOCH #182, elapsed time: 2339.539[sec]] loss: 4.396071877909713\n",
      "[EPOCH #183, elapsed time: 2352.705[sec]] loss: 4.385242023455814\n",
      "[EPOCH #184, elapsed time: 2365.492[sec]] loss: 4.3869763744693495\n",
      "[EPOCH #185, elapsed time: 2380.994[sec]] loss: 4.3835094119979265\n",
      "[EPOCH #186, elapsed time: 2396.963[sec]] loss: 4.384625837883733\n",
      "[EPOCH #187, elapsed time: 2412.413[sec]] loss: 4.387919514437021\n",
      "[EPOCH #188, elapsed time: 2428.183[sec]] loss: 4.388598441772559\n",
      "[EPOCH #189, elapsed time: 2440.621[sec]] loss: 4.402101810513897\n",
      "[EPOCH #190, elapsed time: 2452.882[sec]] loss: 4.3836236921397065\n",
      "[EPOCH #191, elapsed time: 2465.447[sec]] loss: 4.3845301763567495\n",
      "[EPOCH #192, elapsed time: 2477.653[sec]] loss: 4.384238271170218\n",
      "[EPOCH #193, elapsed time: 2488.937[sec]] loss: 4.3945550171137615\n",
      "[EPOCH #194, elapsed time: 2500.552[sec]] loss: 4.382723048949043\n",
      "[EPOCH #195, elapsed time: 2511.922[sec]] loss: 4.3822871740247225\n",
      "[EPOCH #196, elapsed time: 2524.279[sec]] loss: 4.383840097804445\n",
      "[EPOCH #197, elapsed time: 2536.790[sec]] loss: 4.38153499101723\n",
      "[EPOCH #198, elapsed time: 2548.247[sec]] loss: 4.398485101954875\n",
      "[EPOCH #199, elapsed time: 2559.631[sec]] loss: 4.40177166561095\n",
      "[EPOCH #200, elapsed time: 2571.575[sec]] loss: 4.383779621856455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 06:34:55,316] Trial 8 finished with value: 0.29028 and parameters: {'learning_rate': 0.0047306242724475525}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605179694853604\n",
      "[EPOCH #1, elapsed time: 11.790[sec]] loss: 4.589555421549772\n",
      "[EPOCH #2, elapsed time: 23.629[sec]] loss: 4.586787297339754\n",
      "[EPOCH #3, elapsed time: 39.954[sec]] loss: 4.580626970975733\n",
      "[EPOCH #4, elapsed time: 54.252[sec]] loss: 4.573810996569965\n",
      "[EPOCH #5, elapsed time: 66.215[sec]] loss: 4.570367159022784\n",
      "[EPOCH #6, elapsed time: 78.617[sec]] loss: 4.569387241883379\n",
      "[EPOCH #7, elapsed time: 92.665[sec]] loss: 4.56859034296037\n",
      "[EPOCH #8, elapsed time: 105.826[sec]] loss: 4.56183568194213\n",
      "[EPOCH #9, elapsed time: 122.006[sec]] loss: 4.560108615279732\n",
      "[EPOCH #10, elapsed time: 135.357[sec]] loss: 4.5577871294564645\n",
      "[EPOCH #11, elapsed time: 148.750[sec]] loss: 4.557878641920523\n",
      "[EPOCH #12, elapsed time: 160.352[sec]] loss: 4.554411193154519\n",
      "[EPOCH #13, elapsed time: 171.890[sec]] loss: 4.545902371177746\n",
      "[EPOCH #14, elapsed time: 183.736[sec]] loss: 4.545466430661622\n",
      "[EPOCH #15, elapsed time: 195.160[sec]] loss: 4.540725882817596\n",
      "[EPOCH #16, elapsed time: 207.952[sec]] loss: 4.537060593918052\n",
      "[EPOCH #17, elapsed time: 219.665[sec]] loss: 4.543040209371771\n",
      "[EPOCH #18, elapsed time: 233.624[sec]] loss: 4.533118405711247\n",
      "[EPOCH #19, elapsed time: 247.869[sec]] loss: 4.535199196767288\n",
      "[EPOCH #20, elapsed time: 263.734[sec]] loss: 4.528406589822898\n",
      "[EPOCH #21, elapsed time: 276.659[sec]] loss: 4.525974157141785\n",
      "[EPOCH #22, elapsed time: 289.846[sec]] loss: 4.52753162078955\n",
      "[EPOCH #23, elapsed time: 303.102[sec]] loss: 4.524576788442835\n",
      "[EPOCH #24, elapsed time: 315.078[sec]] loss: 4.521152133599963\n",
      "[EPOCH #25, elapsed time: 327.543[sec]] loss: 4.515395348451874\n",
      "[EPOCH #26, elapsed time: 339.784[sec]] loss: 4.512067764291989\n",
      "[EPOCH #27, elapsed time: 351.993[sec]] loss: 4.513776925002164\n",
      "[EPOCH #28, elapsed time: 366.838[sec]] loss: 4.506726488575902\n",
      "[EPOCH #29, elapsed time: 377.752[sec]] loss: 4.505032631196201\n",
      "[EPOCH #30, elapsed time: 389.090[sec]] loss: 4.505033982997511\n",
      "[EPOCH #31, elapsed time: 400.073[sec]] loss: 4.500724114596806\n",
      "[EPOCH #32, elapsed time: 412.365[sec]] loss: 4.499359986603603\n",
      "[EPOCH #33, elapsed time: 423.396[sec]] loss: 4.498407455720127\n",
      "[EPOCH #34, elapsed time: 434.474[sec]] loss: 4.501380533876132\n",
      "[EPOCH #35, elapsed time: 446.011[sec]] loss: 4.49845846478785\n",
      "[EPOCH #36, elapsed time: 461.708[sec]] loss: 4.49416242832567\n",
      "[EPOCH #37, elapsed time: 472.792[sec]] loss: 4.492097416216947\n",
      "[EPOCH #38, elapsed time: 484.998[sec]] loss: 4.492551757522065\n",
      "[EPOCH #39, elapsed time: 497.800[sec]] loss: 4.494203225207191\n",
      "[EPOCH #40, elapsed time: 510.028[sec]] loss: 4.482457127086032\n",
      "[EPOCH #41, elapsed time: 522.218[sec]] loss: 4.476910599972755\n",
      "[EPOCH #42, elapsed time: 534.371[sec]] loss: 4.47928308418601\n",
      "[EPOCH #43, elapsed time: 545.753[sec]] loss: 4.472993611526733\n",
      "[EPOCH #44, elapsed time: 560.529[sec]] loss: 4.471479475383795\n",
      "[EPOCH #45, elapsed time: 575.889[sec]] loss: 4.475363963853832\n",
      "[EPOCH #46, elapsed time: 588.087[sec]] loss: 4.471737607503196\n",
      "[EPOCH #47, elapsed time: 598.750[sec]] loss: 4.500656011084754\n",
      "[EPOCH #48, elapsed time: 611.438[sec]] loss: 4.485691209367202\n",
      "[EPOCH #49, elapsed time: 626.083[sec]] loss: 4.476525637711459\n",
      "[EPOCH #50, elapsed time: 636.784[sec]] loss: 4.473741555168166\n",
      "[EPOCH #51, elapsed time: 647.837[sec]] loss: 4.479440407499776\n",
      "[EPOCH #52, elapsed time: 660.494[sec]] loss: 4.486557110867588\n",
      "[EPOCH #53, elapsed time: 673.236[sec]] loss: 4.474980587388793\n",
      "[EPOCH #54, elapsed time: 686.340[sec]] loss: 4.474100000036121\n",
      "[EPOCH #55, elapsed time: 699.728[sec]] loss: 4.469063601734847\n",
      "[EPOCH #56, elapsed time: 711.855[sec]] loss: 4.460651999014124\n",
      "[EPOCH #57, elapsed time: 726.174[sec]] loss: 4.464042783469934\n",
      "[EPOCH #58, elapsed time: 739.584[sec]] loss: 4.4634724127048875\n",
      "[EPOCH #59, elapsed time: 752.101[sec]] loss: 4.474086775203126\n",
      "[EPOCH #60, elapsed time: 765.343[sec]] loss: 4.449023578690171\n",
      "[EPOCH #61, elapsed time: 776.386[sec]] loss: 4.447752229845531\n",
      "[EPOCH #62, elapsed time: 788.128[sec]] loss: 4.446133534571183\n",
      "[EPOCH #63, elapsed time: 802.921[sec]] loss: 4.446063319003056\n",
      "[EPOCH #64, elapsed time: 819.549[sec]] loss: 4.442844152603101\n",
      "[EPOCH #65, elapsed time: 831.936[sec]] loss: 4.448734005826143\n",
      "[EPOCH #66, elapsed time: 842.789[sec]] loss: 4.441549122981818\n",
      "[EPOCH #67, elapsed time: 854.448[sec]] loss: 4.435835966755775\n",
      "[EPOCH #68, elapsed time: 869.392[sec]] loss: 4.433161633638564\n",
      "[EPOCH #69, elapsed time: 881.906[sec]] loss: 4.438087199791379\n",
      "[EPOCH #70, elapsed time: 893.243[sec]] loss: 4.431852365104494\n",
      "[EPOCH #71, elapsed time: 908.286[sec]] loss: 4.434889699126845\n",
      "[EPOCH #72, elapsed time: 923.640[sec]] loss: 4.433726883971836\n",
      "[EPOCH #73, elapsed time: 938.046[sec]] loss: 4.446877436842281\n",
      "[EPOCH #74, elapsed time: 950.249[sec]] loss: 4.428742969486093\n",
      "[EPOCH #75, elapsed time: 962.710[sec]] loss: 4.422310378867239\n",
      "[EPOCH #76, elapsed time: 977.665[sec]] loss: 4.423940813243961\n",
      "[EPOCH #77, elapsed time: 990.549[sec]] loss: 4.428538329770606\n",
      "[EPOCH #78, elapsed time: 1002.078[sec]] loss: 4.422496219971816\n",
      "[EPOCH #79, elapsed time: 1013.096[sec]] loss: 4.418459780004188\n",
      "[EPOCH #80, elapsed time: 1026.812[sec]] loss: 4.4183888685527855\n",
      "[EPOCH #81, elapsed time: 1041.149[sec]] loss: 4.418999718307915\n",
      "[EPOCH #82, elapsed time: 1052.523[sec]] loss: 4.415999636769066\n",
      "[EPOCH #83, elapsed time: 1063.876[sec]] loss: 4.412580622211147\n",
      "[EPOCH #84, elapsed time: 1075.445[sec]] loss: 4.430421622990799\n",
      "[EPOCH #85, elapsed time: 1086.719[sec]] loss: 4.417217882794596\n",
      "[EPOCH #86, elapsed time: 1102.225[sec]] loss: 4.411968300453913\n",
      "[EPOCH #87, elapsed time: 1114.545[sec]] loss: 4.407779354051527\n",
      "[EPOCH #88, elapsed time: 1125.657[sec]] loss: 4.418902319360832\n",
      "[EPOCH #89, elapsed time: 1139.848[sec]] loss: 4.442258301867328\n",
      "[EPOCH #90, elapsed time: 1151.316[sec]] loss: 4.468544696434446\n",
      "[EPOCH #91, elapsed time: 1162.352[sec]] loss: 4.439159162366383\n",
      "[EPOCH #92, elapsed time: 1173.075[sec]] loss: 4.436203381531679\n",
      "[EPOCH #93, elapsed time: 1183.840[sec]] loss: 4.44682967533153\n",
      "[EPOCH #94, elapsed time: 1194.520[sec]] loss: 4.4586829474089775\n",
      "[EPOCH #95, elapsed time: 1209.090[sec]] loss: 4.448058576516745\n",
      "[EPOCH #96, elapsed time: 1221.555[sec]] loss: 4.43255799898183\n",
      "[EPOCH #97, elapsed time: 1233.503[sec]] loss: 4.431754070753023\n",
      "[EPOCH #98, elapsed time: 1244.371[sec]] loss: 4.429941669154152\n",
      "[EPOCH #99, elapsed time: 1255.564[sec]] loss: 4.42727405904427\n",
      "[EPOCH #100, elapsed time: 1270.798[sec]] loss: 4.459666603586267\n",
      "[EPOCH #101, elapsed time: 1282.073[sec]] loss: 4.448669035771834\n",
      "[EPOCH #102, elapsed time: 1293.733[sec]] loss: 4.433473724321303\n",
      "[EPOCH #103, elapsed time: 1304.614[sec]] loss: 4.4269784367854825\n",
      "[EPOCH #104, elapsed time: 1315.613[sec]] loss: 4.449115230574947\n",
      "[EPOCH #105, elapsed time: 1330.193[sec]] loss: 4.430224919883547\n",
      "[EPOCH #106, elapsed time: 1344.168[sec]] loss: 4.430100004412163\n",
      "[EPOCH #107, elapsed time: 1355.433[sec]] loss: 4.429121175181462\n",
      "[EPOCH #108, elapsed time: 1370.134[sec]] loss: 4.4280270474580945\n",
      "[EPOCH #109, elapsed time: 1381.086[sec]] loss: 4.426826005094873\n",
      "[EPOCH #110, elapsed time: 1391.961[sec]] loss: 4.423550791871601\n",
      "[EPOCH #111, elapsed time: 1403.083[sec]] loss: 4.425068315907464\n",
      "[EPOCH #112, elapsed time: 1413.946[sec]] loss: 4.416672875617302\n",
      "[EPOCH #113, elapsed time: 1425.076[sec]] loss: 4.425611065506401\n",
      "[EPOCH #114, elapsed time: 1437.291[sec]] loss: 4.420252593144803\n",
      "[EPOCH #115, elapsed time: 1449.268[sec]] loss: 4.414854321202176\n",
      "[EPOCH #116, elapsed time: 1460.033[sec]] loss: 4.41612441342989\n",
      "[EPOCH #117, elapsed time: 1471.050[sec]] loss: 4.408319601399427\n",
      "[EPOCH #118, elapsed time: 1481.795[sec]] loss: 4.410111117347722\n",
      "[EPOCH #119, elapsed time: 1492.585[sec]] loss: 4.402743406045612\n",
      "[EPOCH #120, elapsed time: 1503.358[sec]] loss: 4.4162236367443475\n",
      "[EPOCH #121, elapsed time: 1517.808[sec]] loss: 4.417313256327799\n",
      "[EPOCH #122, elapsed time: 1530.040[sec]] loss: 4.408468214731832\n",
      "[EPOCH #123, elapsed time: 1544.180[sec]] loss: 4.4051656414893525\n",
      "[EPOCH #124, elapsed time: 1558.369[sec]] loss: 4.411018303244524\n",
      "[EPOCH #125, elapsed time: 1572.196[sec]] loss: 4.41888117988523\n",
      "[EPOCH #126, elapsed time: 1583.102[sec]] loss: 4.411626816100976\n",
      "[EPOCH #127, elapsed time: 1597.451[sec]] loss: 4.401998780739285\n",
      "[EPOCH #128, elapsed time: 1612.236[sec]] loss: 4.413347432495918\n",
      "[EPOCH #129, elapsed time: 1624.631[sec]] loss: 4.404509560236622\n",
      "[EPOCH #130, elapsed time: 1635.726[sec]] loss: 4.400388853411147\n",
      "[EPOCH #131, elapsed time: 1646.703[sec]] loss: 4.408746587566588\n",
      "[EPOCH #132, elapsed time: 1658.685[sec]] loss: 4.418781367769931\n",
      "[EPOCH #133, elapsed time: 1669.698[sec]] loss: 4.405836833873317\n",
      "[EPOCH #134, elapsed time: 1681.611[sec]] loss: 4.4035459824731085\n",
      "[EPOCH #135, elapsed time: 1692.327[sec]] loss: 4.4369313910613055\n",
      "[EPOCH #136, elapsed time: 1703.109[sec]] loss: 4.439652236394217\n",
      "[EPOCH #137, elapsed time: 1714.109[sec]] loss: 4.407023576613199\n",
      "[EPOCH #138, elapsed time: 1725.290[sec]] loss: 4.41100794736651\n",
      "[EPOCH #139, elapsed time: 1736.366[sec]] loss: 4.4076816103661285\n",
      "[EPOCH #140, elapsed time: 1748.186[sec]] loss: 4.437871562618517\n",
      "[EPOCH #141, elapsed time: 1759.206[sec]] loss: 4.415004667988666\n",
      "[EPOCH #142, elapsed time: 1772.017[sec]] loss: 4.414197446746241\n",
      "[EPOCH #143, elapsed time: 1784.974[sec]] loss: 4.408932291691073\n",
      "[EPOCH #144, elapsed time: 1797.847[sec]] loss: 4.4161868775722235\n",
      "[EPOCH #145, elapsed time: 1811.332[sec]] loss: 4.406716834255616\n",
      "[EPOCH #146, elapsed time: 1824.560[sec]] loss: 4.4157360284967595\n",
      "[EPOCH #147, elapsed time: 1837.268[sec]] loss: 4.414343232004137\n",
      "[EPOCH #148, elapsed time: 1853.550[sec]] loss: 4.403041523538639\n",
      "[EPOCH #149, elapsed time: 1868.168[sec]] loss: 4.404801897718902\n",
      "[EPOCH #150, elapsed time: 1881.980[sec]] loss: 4.399506104884816\n",
      "[EPOCH #151, elapsed time: 1893.258[sec]] loss: 4.398775683712365\n",
      "[EPOCH #152, elapsed time: 1908.949[sec]] loss: 4.397561317670826\n",
      "[EPOCH #153, elapsed time: 1923.258[sec]] loss: 4.395791937583391\n",
      "[EPOCH #154, elapsed time: 1939.596[sec]] loss: 4.420065514643224\n",
      "[EPOCH #155, elapsed time: 1950.424[sec]] loss: 4.396994204225253\n",
      "[EPOCH #156, elapsed time: 1961.937[sec]] loss: 4.399935143007656\n",
      "[EPOCH #157, elapsed time: 1972.807[sec]] loss: 4.39306980329527\n",
      "[EPOCH #158, elapsed time: 1983.648[sec]] loss: 4.393471942982152\n",
      "[EPOCH #159, elapsed time: 1996.101[sec]] loss: 4.392153742979981\n",
      "[EPOCH #160, elapsed time: 2006.829[sec]] loss: 4.405861278870741\n",
      "[EPOCH #161, elapsed time: 2018.853[sec]] loss: 4.397892895220795\n",
      "[EPOCH #162, elapsed time: 2031.154[sec]] loss: 4.392281081992239\n",
      "[EPOCH #163, elapsed time: 2043.122[sec]] loss: 4.390261094843205\n",
      "[EPOCH #164, elapsed time: 2054.091[sec]] loss: 4.38825665996842\n",
      "[EPOCH #165, elapsed time: 2064.876[sec]] loss: 4.3894783966982125\n",
      "[EPOCH #166, elapsed time: 2075.628[sec]] loss: 4.3941842571558745\n",
      "[EPOCH #167, elapsed time: 2086.730[sec]] loss: 4.394480679634666\n",
      "[EPOCH #168, elapsed time: 2097.688[sec]] loss: 4.386337895921157\n",
      "[EPOCH #169, elapsed time: 2108.492[sec]] loss: 4.399951594042153\n",
      "[EPOCH #170, elapsed time: 2119.452[sec]] loss: 4.39835134226774\n",
      "[EPOCH #171, elapsed time: 2132.662[sec]] loss: 4.40435732105033\n",
      "[EPOCH #172, elapsed time: 2144.856[sec]] loss: 4.385060461682535\n",
      "[EPOCH #173, elapsed time: 2155.923[sec]] loss: 4.410540356517067\n",
      "[EPOCH #174, elapsed time: 2166.815[sec]] loss: 4.397542318623568\n",
      "[EPOCH #175, elapsed time: 2178.352[sec]] loss: 4.389717823865699\n",
      "[EPOCH #176, elapsed time: 2189.409[sec]] loss: 4.393526252385369\n",
      "[EPOCH #177, elapsed time: 2200.136[sec]] loss: 4.387290963437111\n",
      "[EPOCH #178, elapsed time: 2210.825[sec]] loss: 4.389404228232415\n",
      "[EPOCH #179, elapsed time: 2221.441[sec]] loss: 4.402030080537802\n",
      "[EPOCH #180, elapsed time: 2232.189[sec]] loss: 4.395701750073766\n",
      "[EPOCH #181, elapsed time: 2244.120[sec]] loss: 4.39301601374523\n",
      "[EPOCH #182, elapsed time: 2256.420[sec]] loss: 4.3886321953146865\n",
      "[EPOCH #183, elapsed time: 2270.490[sec]] loss: 4.384411849963382\n",
      "[EPOCH #184, elapsed time: 2281.721[sec]] loss: 4.401957643085463\n",
      "[EPOCH #185, elapsed time: 2295.843[sec]] loss: 4.383902744993672\n",
      "[EPOCH #186, elapsed time: 2308.249[sec]] loss: 4.39558296026668\n",
      "[EPOCH #187, elapsed time: 2323.074[sec]] loss: 4.382165184252855\n",
      "[EPOCH #188, elapsed time: 2335.351[sec]] loss: 4.3877506335423835\n",
      "[EPOCH #189, elapsed time: 2346.230[sec]] loss: 4.38513087951748\n",
      "[EPOCH #190, elapsed time: 2360.677[sec]] loss: 4.392160333278312\n",
      "[EPOCH #191, elapsed time: 2371.665[sec]] loss: 4.394570777451313\n",
      "[EPOCH #192, elapsed time: 2383.269[sec]] loss: 4.380685840443167\n",
      "[EPOCH #193, elapsed time: 2394.320[sec]] loss: 4.3813911237857015\n",
      "[EPOCH #194, elapsed time: 2405.509[sec]] loss: 4.405878644224473\n",
      "[EPOCH #195, elapsed time: 2417.256[sec]] loss: 4.3893256010493635\n",
      "[EPOCH #196, elapsed time: 2428.708[sec]] loss: 4.3977715207915695\n",
      "[EPOCH #197, elapsed time: 2440.322[sec]] loss: 4.380056270482215\n",
      "[EPOCH #198, elapsed time: 2451.495[sec]] loss: 4.397228628110977\n",
      "[EPOCH #199, elapsed time: 2462.826[sec]] loss: 4.423247114329176\n",
      "[EPOCH #200, elapsed time: 2478.559[sec]] loss: 4.404438426383245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 07:16:31,340] Trial 9 finished with value: 0.27778 and parameters: {'learning_rate': 0.004607938493015883}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.6051626049854475\n",
      "[EPOCH #1, elapsed time: 11.685[sec]] loss: 4.596874770642242\n",
      "[EPOCH #2, elapsed time: 26.024[sec]] loss: 4.577775291350127\n",
      "[EPOCH #3, elapsed time: 41.511[sec]] loss: 4.565423374517713\n",
      "[EPOCH #4, elapsed time: 53.218[sec]] loss: 4.548671049104619\n",
      "[EPOCH #5, elapsed time: 65.302[sec]] loss: 4.538735143816479\n",
      "[EPOCH #6, elapsed time: 76.026[sec]] loss: 4.5283333368966465\n",
      "[EPOCH #7, elapsed time: 86.693[sec]] loss: 4.518699990429332\n",
      "[EPOCH #8, elapsed time: 97.201[sec]] loss: 4.509864495033953\n",
      "[EPOCH #9, elapsed time: 108.061[sec]] loss: 4.499890998015401\n",
      "[EPOCH #10, elapsed time: 118.886[sec]] loss: 4.490460010804355\n",
      "[EPOCH #11, elapsed time: 130.572[sec]] loss: 4.483405631548003\n",
      "[EPOCH #12, elapsed time: 142.034[sec]] loss: 4.476083842745517\n",
      "[EPOCH #13, elapsed time: 152.688[sec]] loss: 4.467831691868856\n",
      "[EPOCH #14, elapsed time: 163.875[sec]] loss: 4.4609226466598075\n",
      "[EPOCH #15, elapsed time: 176.867[sec]] loss: 4.4517413107004975\n",
      "[EPOCH #16, elapsed time: 187.747[sec]] loss: 4.445534015418777\n",
      "[EPOCH #17, elapsed time: 198.558[sec]] loss: 4.438258974962484\n",
      "[EPOCH #18, elapsed time: 211.793[sec]] loss: 4.432090128070638\n",
      "[EPOCH #19, elapsed time: 222.536[sec]] loss: 4.4270710209891035\n",
      "[EPOCH #20, elapsed time: 233.749[sec]] loss: 4.419794854184259\n",
      "[EPOCH #21, elapsed time: 244.527[sec]] loss: 4.413860334468361\n",
      "[EPOCH #22, elapsed time: 256.352[sec]] loss: 4.406473671711185\n",
      "[EPOCH #23, elapsed time: 268.105[sec]] loss: 4.401934566973725\n",
      "[EPOCH #24, elapsed time: 280.434[sec]] loss: 4.395182533593645\n",
      "[EPOCH #25, elapsed time: 291.229[sec]] loss: 4.390309547355979\n",
      "[EPOCH #26, elapsed time: 306.644[sec]] loss: 4.384993021715473\n",
      "[EPOCH #27, elapsed time: 317.957[sec]] loss: 4.378851589451824\n",
      "[EPOCH #28, elapsed time: 329.151[sec]] loss: 4.375859692244673\n",
      "[EPOCH #29, elapsed time: 344.363[sec]] loss: 4.372988890320234\n",
      "[EPOCH #30, elapsed time: 357.510[sec]] loss: 4.365520344280846\n",
      "[EPOCH #31, elapsed time: 368.247[sec]] loss: 4.361070128030222\n",
      "[EPOCH #32, elapsed time: 381.652[sec]] loss: 4.355716041777276\n",
      "[EPOCH #33, elapsed time: 393.715[sec]] loss: 4.354837124117353\n",
      "[EPOCH #34, elapsed time: 406.174[sec]] loss: 4.348194942364537\n",
      "[EPOCH #35, elapsed time: 417.657[sec]] loss: 4.345286173768632\n",
      "[EPOCH #36, elapsed time: 429.985[sec]] loss: 4.341679673124717\n",
      "[EPOCH #37, elapsed time: 441.405[sec]] loss: 4.338365023363422\n",
      "[EPOCH #38, elapsed time: 452.563[sec]] loss: 4.332400152642072\n",
      "[EPOCH #39, elapsed time: 463.470[sec]] loss: 4.3314587925003645\n",
      "[EPOCH #40, elapsed time: 474.432[sec]] loss: 4.327723531179983\n",
      "[EPOCH #41, elapsed time: 486.615[sec]] loss: 4.323771765044463\n",
      "[EPOCH #42, elapsed time: 497.480[sec]] loss: 4.321195212527108\n",
      "[EPOCH #43, elapsed time: 508.631[sec]] loss: 4.318082183732944\n",
      "[EPOCH #44, elapsed time: 519.877[sec]] loss: 4.314633081146943\n",
      "[EPOCH #45, elapsed time: 530.851[sec]] loss: 4.312044366078734\n",
      "[EPOCH #46, elapsed time: 543.701[sec]] loss: 4.307132108045249\n",
      "[EPOCH #47, elapsed time: 554.408[sec]] loss: 4.306454170688329\n",
      "[EPOCH #48, elapsed time: 566.148[sec]] loss: 4.301147794082847\n",
      "[EPOCH #49, elapsed time: 581.266[sec]] loss: 4.29808173939271\n",
      "[EPOCH #50, elapsed time: 592.727[sec]] loss: 4.295497296333923\n",
      "[EPOCH #51, elapsed time: 604.813[sec]] loss: 4.291679293546475\n",
      "[EPOCH #52, elapsed time: 616.554[sec]] loss: 4.289273306870415\n",
      "[EPOCH #53, elapsed time: 627.817[sec]] loss: 4.2835619634187765\n",
      "[EPOCH #54, elapsed time: 640.077[sec]] loss: 4.28149804898126\n",
      "[EPOCH #55, elapsed time: 650.996[sec]] loss: 4.2796435319530755\n",
      "[EPOCH #56, elapsed time: 662.001[sec]] loss: 4.27415527324225\n",
      "[EPOCH #57, elapsed time: 672.994[sec]] loss: 4.268111748490971\n",
      "[EPOCH #58, elapsed time: 684.723[sec]] loss: 4.267062555729237\n",
      "[EPOCH #59, elapsed time: 697.022[sec]] loss: 4.263256565699269\n",
      "[EPOCH #60, elapsed time: 709.643[sec]] loss: 4.262611014981798\n",
      "[EPOCH #61, elapsed time: 720.651[sec]] loss: 4.258127779054551\n",
      "[EPOCH #62, elapsed time: 733.463[sec]] loss: 4.2559883419093\n",
      "[EPOCH #63, elapsed time: 744.188[sec]] loss: 4.253435454304525\n",
      "[EPOCH #64, elapsed time: 754.814[sec]] loss: 4.25026023212489\n",
      "[EPOCH #65, elapsed time: 766.509[sec]] loss: 4.246355908693447\n",
      "[EPOCH #66, elapsed time: 777.167[sec]] loss: 4.243214776862224\n",
      "[EPOCH #67, elapsed time: 787.947[sec]] loss: 4.240098531781597\n",
      "[EPOCH #68, elapsed time: 799.995[sec]] loss: 4.238851107013431\n",
      "[EPOCH #69, elapsed time: 810.975[sec]] loss: 4.238130133043705\n",
      "[EPOCH #70, elapsed time: 821.918[sec]] loss: 4.23494377673168\n",
      "[EPOCH #71, elapsed time: 832.583[sec]] loss: 4.2317087172851755\n",
      "[EPOCH #72, elapsed time: 843.586[sec]] loss: 4.228535012869368\n",
      "[EPOCH #73, elapsed time: 855.116[sec]] loss: 4.225179107846622\n",
      "[EPOCH #74, elapsed time: 867.622[sec]] loss: 4.224449729888926\n",
      "[EPOCH #75, elapsed time: 880.881[sec]] loss: 4.219587055140402\n",
      "[EPOCH #76, elapsed time: 896.378[sec]] loss: 4.217913871534497\n",
      "[EPOCH #77, elapsed time: 908.233[sec]] loss: 4.21422508383743\n",
      "[EPOCH #78, elapsed time: 921.619[sec]] loss: 4.213401351765189\n",
      "[EPOCH #79, elapsed time: 932.713[sec]] loss: 4.208625908013879\n",
      "[EPOCH #80, elapsed time: 943.599[sec]] loss: 4.209591316551409\n",
      "[EPOCH #81, elapsed time: 955.107[sec]] loss: 4.204508137260579\n",
      "[EPOCH #82, elapsed time: 967.782[sec]] loss: 4.204189798577199\n",
      "[EPOCH #83, elapsed time: 979.936[sec]] loss: 4.201240836239288\n",
      "[EPOCH #84, elapsed time: 991.050[sec]] loss: 4.199002841155039\n",
      "[EPOCH #85, elapsed time: 1003.402[sec]] loss: 4.1990039029795625\n",
      "[EPOCH #86, elapsed time: 1014.345[sec]] loss: 4.19885490021489\n",
      "[EPOCH #87, elapsed time: 1025.372[sec]] loss: 4.194497113340723\n",
      "[EPOCH #88, elapsed time: 1037.805[sec]] loss: 4.189230359676017\n",
      "[EPOCH #89, elapsed time: 1048.434[sec]] loss: 4.188358243886126\n",
      "[EPOCH #90, elapsed time: 1063.701[sec]] loss: 4.1864237271282665\n",
      "[EPOCH #91, elapsed time: 1074.973[sec]] loss: 4.181214461323548\n",
      "[EPOCH #92, elapsed time: 1085.885[sec]] loss: 4.181449345877288\n",
      "[EPOCH #93, elapsed time: 1098.156[sec]] loss: 4.179844917430377\n",
      "[EPOCH #94, elapsed time: 1108.806[sec]] loss: 4.177023036923838\n",
      "[EPOCH #95, elapsed time: 1119.451[sec]] loss: 4.174706388419817\n",
      "[EPOCH #96, elapsed time: 1133.130[sec]] loss: 4.17338788166156\n",
      "[EPOCH #97, elapsed time: 1143.958[sec]] loss: 4.172102879044038\n",
      "[EPOCH #98, elapsed time: 1157.304[sec]] loss: 4.169060915155588\n",
      "[EPOCH #99, elapsed time: 1168.156[sec]] loss: 4.166876941785855\n",
      "[EPOCH #100, elapsed time: 1178.763[sec]] loss: 4.165872936895545\n",
      "[EPOCH #101, elapsed time: 1189.768[sec]] loss: 4.163548320665317\n",
      "[EPOCH #102, elapsed time: 1200.440[sec]] loss: 4.158069557207026\n",
      "[EPOCH #103, elapsed time: 1212.308[sec]] loss: 4.158772219012962\n",
      "[EPOCH #104, elapsed time: 1224.062[sec]] loss: 4.1582539019032465\n",
      "[EPOCH #105, elapsed time: 1236.364[sec]] loss: 4.155389601499396\n",
      "[EPOCH #106, elapsed time: 1248.656[sec]] loss: 4.154426179935897\n",
      "[EPOCH #107, elapsed time: 1259.306[sec]] loss: 4.149030414057784\n",
      "[EPOCH #108, elapsed time: 1270.385[sec]] loss: 4.1503745576012845\n",
      "[EPOCH #109, elapsed time: 1281.121[sec]] loss: 4.148718886396782\n",
      "[EPOCH #110, elapsed time: 1292.022[sec]] loss: 4.14497033510922\n",
      "[EPOCH #111, elapsed time: 1305.741[sec]] loss: 4.143588107286625\n",
      "[EPOCH #112, elapsed time: 1317.841[sec]] loss: 4.145340506418806\n",
      "[EPOCH #113, elapsed time: 1330.628[sec]] loss: 4.138059039186074\n",
      "[EPOCH #114, elapsed time: 1345.709[sec]] loss: 4.136694209055495\n",
      "[EPOCH #115, elapsed time: 1357.187[sec]] loss: 4.137192552850861\n",
      "[EPOCH #116, elapsed time: 1371.719[sec]] loss: 4.135543795632614\n",
      "[EPOCH #117, elapsed time: 1386.469[sec]] loss: 4.135316039687613\n",
      "[EPOCH #118, elapsed time: 1403.444[sec]] loss: 4.131420450948861\n",
      "[EPOCH #119, elapsed time: 1414.933[sec]] loss: 4.130445770628546\n",
      "[EPOCH #120, elapsed time: 1427.188[sec]] loss: 4.129638570741591\n",
      "[EPOCH #121, elapsed time: 1439.505[sec]] loss: 4.127491423966255\n",
      "[EPOCH #122, elapsed time: 1450.295[sec]] loss: 4.125829806941027\n",
      "[EPOCH #123, elapsed time: 1461.193[sec]] loss: 4.125164767220778\n",
      "[EPOCH #124, elapsed time: 1477.683[sec]] loss: 4.122474698172269\n",
      "[EPOCH #125, elapsed time: 1488.752[sec]] loss: 4.121322955066244\n",
      "[EPOCH #126, elapsed time: 1499.690[sec]] loss: 4.118369638347809\n",
      "[EPOCH #127, elapsed time: 1511.438[sec]] loss: 4.117216110229492\n",
      "[EPOCH #128, elapsed time: 1522.235[sec]] loss: 4.116177032486568\n",
      "[EPOCH #129, elapsed time: 1532.892[sec]] loss: 4.1147187948989625\n",
      "[EPOCH #130, elapsed time: 1543.660[sec]] loss: 4.11496268124132\n",
      "[EPOCH #131, elapsed time: 1554.447[sec]] loss: 4.111581542899192\n",
      "[EPOCH #132, elapsed time: 1565.220[sec]] loss: 4.1110704460742\n",
      "[EPOCH #133, elapsed time: 1578.648[sec]] loss: 4.111164507618793\n",
      "[EPOCH #134, elapsed time: 1594.397[sec]] loss: 4.106621407577798\n",
      "[EPOCH #135, elapsed time: 1610.217[sec]] loss: 4.108471322196917\n",
      "[EPOCH #136, elapsed time: 1625.486[sec]] loss: 4.104530013530436\n",
      "[EPOCH #137, elapsed time: 1637.413[sec]] loss: 4.105671264235972\n",
      "[EPOCH #138, elapsed time: 1651.450[sec]] loss: 4.101329056940549\n",
      "[EPOCH #139, elapsed time: 1662.300[sec]] loss: 4.100465743113083\n",
      "[EPOCH #140, elapsed time: 1673.799[sec]] loss: 4.099338178518714\n",
      "[EPOCH #141, elapsed time: 1684.779[sec]] loss: 4.097348310363193\n",
      "[EPOCH #142, elapsed time: 1695.633[sec]] loss: 4.0974550460747095\n",
      "[EPOCH #143, elapsed time: 1707.199[sec]] loss: 4.09474460161884\n",
      "[EPOCH #144, elapsed time: 1722.966[sec]] loss: 4.092471960486316\n",
      "[EPOCH #145, elapsed time: 1733.952[sec]] loss: 4.0920409068646375\n",
      "[EPOCH #146, elapsed time: 1745.657[sec]] loss: 4.0889170538974895\n",
      "[EPOCH #147, elapsed time: 1756.626[sec]] loss: 4.089190753545047\n",
      "[EPOCH #148, elapsed time: 1767.313[sec]] loss: 4.087414151189881\n",
      "[EPOCH #149, elapsed time: 1778.200[sec]] loss: 4.086419973782218\n",
      "[EPOCH #150, elapsed time: 1788.894[sec]] loss: 4.083303336523621\n",
      "[EPOCH #151, elapsed time: 1800.166[sec]] loss: 4.083151882913581\n",
      "[EPOCH #152, elapsed time: 1811.606[sec]] loss: 4.079509088646809\n",
      "[EPOCH #153, elapsed time: 1823.748[sec]] loss: 4.0810281304465\n",
      "[EPOCH #154, elapsed time: 1834.442[sec]] loss: 4.080059416387146\n",
      "[EPOCH #155, elapsed time: 1847.113[sec]] loss: 4.074847930833764\n",
      "[EPOCH #156, elapsed time: 1860.344[sec]] loss: 4.076117361957113\n",
      "[EPOCH #157, elapsed time: 1875.228[sec]] loss: 4.0760259028817325\n",
      "[EPOCH #158, elapsed time: 1889.838[sec]] loss: 4.073069489925089\n",
      "[EPOCH #159, elapsed time: 1901.061[sec]] loss: 4.075168941391635\n",
      "[EPOCH #160, elapsed time: 1913.339[sec]] loss: 4.073291958713105\n",
      "[EPOCH #161, elapsed time: 1925.549[sec]] loss: 4.068901853384456\n",
      "[EPOCH #162, elapsed time: 1936.186[sec]] loss: 4.067664857102905\n",
      "[EPOCH #163, elapsed time: 1948.213[sec]] loss: 4.066989319033143\n",
      "[EPOCH #164, elapsed time: 1958.814[sec]] loss: 4.065908663713696\n",
      "[EPOCH #165, elapsed time: 1969.467[sec]] loss: 4.064776794619081\n",
      "[EPOCH #166, elapsed time: 1980.681[sec]] loss: 4.064684542493033\n",
      "[EPOCH #167, elapsed time: 1995.505[sec]] loss: 4.062238988858038\n",
      "[EPOCH #168, elapsed time: 2006.325[sec]] loss: 4.05966675228136\n",
      "[EPOCH #169, elapsed time: 2020.012[sec]] loss: 4.05899564348881\n",
      "[EPOCH #170, elapsed time: 2032.287[sec]] loss: 4.058697637959466\n",
      "[EPOCH #171, elapsed time: 2043.025[sec]] loss: 4.057157086929448\n",
      "[EPOCH #172, elapsed time: 2053.776[sec]] loss: 4.053657386826157\n",
      "[EPOCH #173, elapsed time: 2064.571[sec]] loss: 4.054668687202956\n",
      "[EPOCH #174, elapsed time: 2075.449[sec]] loss: 4.055063339966806\n",
      "[EPOCH #175, elapsed time: 2086.841[sec]] loss: 4.051041616359279\n",
      "[EPOCH #176, elapsed time: 2097.698[sec]] loss: 4.050340718057624\n",
      "[EPOCH #177, elapsed time: 2108.994[sec]] loss: 4.0481411895154\n",
      "[EPOCH #178, elapsed time: 2119.694[sec]] loss: 4.047534509263432\n",
      "[EPOCH #179, elapsed time: 2130.718[sec]] loss: 4.048425433579272\n",
      "[EPOCH #180, elapsed time: 2141.422[sec]] loss: 4.04491969079294\n",
      "[EPOCH #181, elapsed time: 2152.054[sec]] loss: 4.0454843877144375\n",
      "[EPOCH #182, elapsed time: 2162.710[sec]] loss: 4.042885674472353\n",
      "[EPOCH #183, elapsed time: 2173.432[sec]] loss: 4.043429483760265\n",
      "[EPOCH #184, elapsed time: 2184.310[sec]] loss: 4.04049890360158\n",
      "[EPOCH #185, elapsed time: 2195.922[sec]] loss: 4.038006454877799\n",
      "[EPOCH #186, elapsed time: 2211.257[sec]] loss: 4.0360353423171675\n",
      "[EPOCH #187, elapsed time: 2222.075[sec]] loss: 4.037141952618375\n",
      "[EPOCH #188, elapsed time: 2234.021[sec]] loss: 4.037455628639753\n",
      "[EPOCH #189, elapsed time: 2246.536[sec]] loss: 4.037107679833225\n",
      "[EPOCH #190, elapsed time: 2258.364[sec]] loss: 4.034953909048421\n",
      "[EPOCH #191, elapsed time: 2272.325[sec]] loss: 4.032005238822844\n",
      "[EPOCH #192, elapsed time: 2288.146[sec]] loss: 4.031634947465005\n",
      "[EPOCH #193, elapsed time: 2298.950[sec]] loss: 4.0330188097438215\n",
      "[EPOCH #194, elapsed time: 2311.625[sec]] loss: 4.030820829168162\n",
      "[EPOCH #195, elapsed time: 2322.518[sec]] loss: 4.030427257448759\n",
      "[EPOCH #196, elapsed time: 2333.182[sec]] loss: 4.027054680209853\n",
      "[EPOCH #197, elapsed time: 2345.351[sec]] loss: 4.028599267728955\n",
      "[EPOCH #198, elapsed time: 2356.027[sec]] loss: 4.025992380070213\n",
      "[EPOCH #199, elapsed time: 2368.308[sec]] loss: 4.024956267076811\n",
      "[EPOCH #200, elapsed time: 2379.045[sec]] loss: 4.02422175221312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 07:56:24,780] Trial 10 finished with value: 0.68916 and parameters: {'learning_rate': 5.028563487257306e-05}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.6051636462171945\n",
      "[EPOCH #1, elapsed time: 11.123[sec]] loss: 4.596917372061058\n",
      "[EPOCH #2, elapsed time: 21.916[sec]] loss: 4.580854289591198\n",
      "[EPOCH #3, elapsed time: 32.685[sec]] loss: 4.565809305097076\n",
      "[EPOCH #4, elapsed time: 47.359[sec]] loss: 4.551588860522153\n",
      "[EPOCH #5, elapsed time: 58.103[sec]] loss: 4.536254478278865\n",
      "[EPOCH #6, elapsed time: 69.690[sec]] loss: 4.5237132625860506\n",
      "[EPOCH #7, elapsed time: 80.468[sec]] loss: 4.511447805208193\n",
      "[EPOCH #8, elapsed time: 92.289[sec]] loss: 4.5030105588379685\n",
      "[EPOCH #9, elapsed time: 106.751[sec]] loss: 4.493412060838286\n",
      "[EPOCH #10, elapsed time: 119.057[sec]] loss: 4.486370917092663\n",
      "[EPOCH #11, elapsed time: 131.185[sec]] loss: 4.476929942232939\n",
      "[EPOCH #12, elapsed time: 147.038[sec]] loss: 4.4697910569374635\n",
      "[EPOCH #13, elapsed time: 158.106[sec]] loss: 4.461486690408972\n",
      "[EPOCH #14, elapsed time: 170.095[sec]] loss: 4.451575870782385\n",
      "[EPOCH #15, elapsed time: 182.838[sec]] loss: 4.443361019371262\n",
      "[EPOCH #16, elapsed time: 194.008[sec]] loss: 4.437157962845444\n",
      "[EPOCH #17, elapsed time: 205.091[sec]] loss: 4.428044472912223\n",
      "[EPOCH #18, elapsed time: 217.301[sec]] loss: 4.421408318283461\n",
      "[EPOCH #19, elapsed time: 229.609[sec]] loss: 4.413085464018701\n",
      "[EPOCH #20, elapsed time: 240.623[sec]] loss: 4.407610582069792\n",
      "[EPOCH #21, elapsed time: 251.707[sec]] loss: 4.401378856434398\n",
      "[EPOCH #22, elapsed time: 263.769[sec]] loss: 4.3956565728800765\n",
      "[EPOCH #23, elapsed time: 274.987[sec]] loss: 4.3900168401189745\n",
      "[EPOCH #24, elapsed time: 288.192[sec]] loss: 4.381760732988783\n",
      "[EPOCH #25, elapsed time: 298.913[sec]] loss: 4.378775348895189\n",
      "[EPOCH #26, elapsed time: 309.848[sec]] loss: 4.373233219178457\n",
      "[EPOCH #27, elapsed time: 320.836[sec]] loss: 4.368145456698485\n",
      "[EPOCH #28, elapsed time: 335.496[sec]] loss: 4.365562010940191\n",
      "[EPOCH #29, elapsed time: 346.608[sec]] loss: 4.3605222576944325\n",
      "[EPOCH #30, elapsed time: 358.700[sec]] loss: 4.355910374427253\n",
      "[EPOCH #31, elapsed time: 370.149[sec]] loss: 4.352752048238607\n",
      "[EPOCH #32, elapsed time: 380.848[sec]] loss: 4.348640870529341\n",
      "[EPOCH #33, elapsed time: 391.670[sec]] loss: 4.347106042155377\n",
      "[EPOCH #34, elapsed time: 402.721[sec]] loss: 4.342711974624175\n",
      "[EPOCH #35, elapsed time: 415.785[sec]] loss: 4.338163686118977\n",
      "[EPOCH #36, elapsed time: 426.678[sec]] loss: 4.337036520566836\n",
      "[EPOCH #37, elapsed time: 438.734[sec]] loss: 4.333625551530053\n",
      "[EPOCH #38, elapsed time: 450.119[sec]] loss: 4.327274086074195\n",
      "[EPOCH #39, elapsed time: 460.962[sec]] loss: 4.324258236875918\n",
      "[EPOCH #40, elapsed time: 472.097[sec]] loss: 4.320502577724933\n",
      "[EPOCH #41, elapsed time: 488.231[sec]] loss: 4.318821965008307\n",
      "[EPOCH #42, elapsed time: 504.189[sec]] loss: 4.313141487839125\n",
      "[EPOCH #43, elapsed time: 516.502[sec]] loss: 4.308271167069312\n",
      "[EPOCH #44, elapsed time: 527.448[sec]] loss: 4.3057487216883565\n",
      "[EPOCH #45, elapsed time: 543.673[sec]] loss: 4.301964684472355\n",
      "[EPOCH #46, elapsed time: 554.755[sec]] loss: 4.297479281727504\n",
      "[EPOCH #47, elapsed time: 567.777[sec]] loss: 4.2968311376931645\n",
      "[EPOCH #48, elapsed time: 578.813[sec]] loss: 4.292129298470681\n",
      "[EPOCH #49, elapsed time: 591.473[sec]] loss: 4.287342037974606\n",
      "[EPOCH #50, elapsed time: 604.552[sec]] loss: 4.285280936052612\n",
      "[EPOCH #51, elapsed time: 616.086[sec]] loss: 4.2824303730130575\n",
      "[EPOCH #52, elapsed time: 630.747[sec]] loss: 4.278968267843499\n",
      "[EPOCH #53, elapsed time: 642.161[sec]] loss: 4.275122029004917\n",
      "[EPOCH #54, elapsed time: 657.228[sec]] loss: 4.273204829093362\n",
      "[EPOCH #55, elapsed time: 670.689[sec]] loss: 4.269495746377029\n",
      "[EPOCH #56, elapsed time: 686.687[sec]] loss: 4.270640997267349\n",
      "[EPOCH #57, elapsed time: 700.459[sec]] loss: 4.264584066314112\n",
      "[EPOCH #58, elapsed time: 714.555[sec]] loss: 4.263116310288032\n",
      "[EPOCH #59, elapsed time: 728.894[sec]] loss: 4.261084944028848\n",
      "[EPOCH #60, elapsed time: 740.123[sec]] loss: 4.254013542784229\n",
      "[EPOCH #61, elapsed time: 751.150[sec]] loss: 4.251360760845592\n",
      "[EPOCH #62, elapsed time: 762.250[sec]] loss: 4.250463507683401\n",
      "[EPOCH #63, elapsed time: 773.020[sec]] loss: 4.249594754922565\n",
      "[EPOCH #64, elapsed time: 785.052[sec]] loss: 4.245985585149861\n",
      "[EPOCH #65, elapsed time: 795.997[sec]] loss: 4.242654266833344\n",
      "[EPOCH #66, elapsed time: 808.135[sec]] loss: 4.24267382036015\n",
      "[EPOCH #67, elapsed time: 819.576[sec]] loss: 4.238864110512224\n",
      "[EPOCH #68, elapsed time: 832.271[sec]] loss: 4.237118492962379\n",
      "[EPOCH #69, elapsed time: 843.285[sec]] loss: 4.233720965364082\n",
      "[EPOCH #70, elapsed time: 854.018[sec]] loss: 4.232167916959932\n",
      "[EPOCH #71, elapsed time: 865.202[sec]] loss: 4.229972855829689\n",
      "[EPOCH #72, elapsed time: 876.075[sec]] loss: 4.227262602352745\n",
      "[EPOCH #73, elapsed time: 886.872[sec]] loss: 4.223994869340785\n",
      "[EPOCH #74, elapsed time: 897.761[sec]] loss: 4.2206102494467395\n",
      "[EPOCH #75, elapsed time: 908.537[sec]] loss: 4.2176046891618215\n",
      "[EPOCH #76, elapsed time: 923.007[sec]] loss: 4.2162200085108505\n",
      "[EPOCH #77, elapsed time: 936.239[sec]] loss: 4.212520191170661\n",
      "[EPOCH #78, elapsed time: 949.509[sec]] loss: 4.211551477111309\n",
      "[EPOCH #79, elapsed time: 962.657[sec]] loss: 4.208428042101235\n",
      "[EPOCH #80, elapsed time: 977.165[sec]] loss: 4.204402463831203\n",
      "[EPOCH #81, elapsed time: 988.263[sec]] loss: 4.204588441763333\n",
      "[EPOCH #82, elapsed time: 999.065[sec]] loss: 4.2024228118279\n",
      "[EPOCH #83, elapsed time: 1011.395[sec]] loss: 4.1985600238721945\n",
      "[EPOCH #84, elapsed time: 1023.135[sec]] loss: 4.198683803079987\n",
      "[EPOCH #85, elapsed time: 1037.160[sec]] loss: 4.1945515563681735\n",
      "[EPOCH #86, elapsed time: 1049.463[sec]] loss: 4.190695574401055\n",
      "[EPOCH #87, elapsed time: 1060.196[sec]] loss: 4.190099058895636\n",
      "[EPOCH #88, elapsed time: 1072.355[sec]] loss: 4.188993155155438\n",
      "[EPOCH #89, elapsed time: 1083.097[sec]] loss: 4.186880506923087\n",
      "[EPOCH #90, elapsed time: 1093.812[sec]] loss: 4.182265284880566\n",
      "[EPOCH #91, elapsed time: 1105.028[sec]] loss: 4.181619130718502\n",
      "[EPOCH #92, elapsed time: 1120.464[sec]] loss: 4.180840269236403\n",
      "[EPOCH #93, elapsed time: 1131.828[sec]] loss: 4.177386175419227\n",
      "[EPOCH #94, elapsed time: 1144.232[sec]] loss: 4.173868815409245\n",
      "[EPOCH #95, elapsed time: 1155.903[sec]] loss: 4.170473310326584\n",
      "[EPOCH #96, elapsed time: 1167.468[sec]] loss: 4.170803359740069\n",
      "[EPOCH #97, elapsed time: 1179.143[sec]] loss: 4.170005682410144\n",
      "[EPOCH #98, elapsed time: 1189.884[sec]] loss: 4.169131738439402\n",
      "[EPOCH #99, elapsed time: 1200.664[sec]] loss: 4.165264397497293\n",
      "[EPOCH #100, elapsed time: 1212.932[sec]] loss: 4.1617537617149525\n",
      "[EPOCH #101, elapsed time: 1224.441[sec]] loss: 4.16001968496668\n",
      "[EPOCH #102, elapsed time: 1236.760[sec]] loss: 4.159917203112435\n",
      "[EPOCH #103, elapsed time: 1247.720[sec]] loss: 4.155282913380072\n",
      "[EPOCH #104, elapsed time: 1261.050[sec]] loss: 4.154814644189348\n",
      "[EPOCH #105, elapsed time: 1271.835[sec]] loss: 4.15116227382433\n",
      "[EPOCH #106, elapsed time: 1283.198[sec]] loss: 4.150463713946742\n",
      "[EPOCH #107, elapsed time: 1294.544[sec]] loss: 4.149310727342153\n",
      "[EPOCH #108, elapsed time: 1305.430[sec]] loss: 4.146098797853681\n",
      "[EPOCH #109, elapsed time: 1318.781[sec]] loss: 4.146140154553619\n",
      "[EPOCH #110, elapsed time: 1330.946[sec]] loss: 4.14357806121548\n",
      "[EPOCH #111, elapsed time: 1341.750[sec]] loss: 4.142230056145217\n",
      "[EPOCH #112, elapsed time: 1354.033[sec]] loss: 4.138982500239816\n",
      "[EPOCH #113, elapsed time: 1367.756[sec]] loss: 4.1368109546863945\n",
      "[EPOCH #114, elapsed time: 1380.059[sec]] loss: 4.134978708821234\n",
      "[EPOCH #115, elapsed time: 1391.235[sec]] loss: 4.132962673044479\n",
      "[EPOCH #116, elapsed time: 1402.239[sec]] loss: 4.131374209032407\n",
      "[EPOCH #117, elapsed time: 1413.093[sec]] loss: 4.131070965158581\n",
      "[EPOCH #118, elapsed time: 1424.843[sec]] loss: 4.127847536816783\n",
      "[EPOCH #119, elapsed time: 1435.854[sec]] loss: 4.125861861807981\n",
      "[EPOCH #120, elapsed time: 1449.065[sec]] loss: 4.125113007203174\n",
      "[EPOCH #121, elapsed time: 1459.854[sec]] loss: 4.121321424336595\n",
      "[EPOCH #122, elapsed time: 1471.102[sec]] loss: 4.120849199197419\n",
      "[EPOCH #123, elapsed time: 1482.309[sec]] loss: 4.120715200939166\n",
      "[EPOCH #124, elapsed time: 1494.099[sec]] loss: 4.11601182991926\n",
      "[EPOCH #125, elapsed time: 1509.084[sec]] loss: 4.116621972655762\n",
      "[EPOCH #126, elapsed time: 1523.042[sec]] loss: 4.11225018666024\n",
      "[EPOCH #127, elapsed time: 1535.917[sec]] loss: 4.11241636166417\n",
      "[EPOCH #128, elapsed time: 1548.279[sec]] loss: 4.111807285938519\n",
      "[EPOCH #129, elapsed time: 1560.383[sec]] loss: 4.109379233874653\n",
      "[EPOCH #130, elapsed time: 1573.064[sec]] loss: 4.107155847152836\n",
      "[EPOCH #131, elapsed time: 1585.374[sec]] loss: 4.104484133741753\n",
      "[EPOCH #132, elapsed time: 1597.270[sec]] loss: 4.106321426514853\n",
      "[EPOCH #133, elapsed time: 1609.433[sec]] loss: 4.104736199992175\n",
      "[EPOCH #134, elapsed time: 1621.139[sec]] loss: 4.1030785065771145\n",
      "[EPOCH #135, elapsed time: 1632.396[sec]] loss: 4.098145276555936\n",
      "[EPOCH #136, elapsed time: 1643.558[sec]] loss: 4.097245954506228\n",
      "[EPOCH #137, elapsed time: 1654.789[sec]] loss: 4.094923060899809\n",
      "[EPOCH #138, elapsed time: 1665.816[sec]] loss: 4.094947730739835\n",
      "[EPOCH #139, elapsed time: 1678.443[sec]] loss: 4.095190816404571\n",
      "[EPOCH #140, elapsed time: 1690.080[sec]] loss: 4.091625091286706\n",
      "[EPOCH #141, elapsed time: 1703.517[sec]] loss: 4.089365099304697\n",
      "[EPOCH #142, elapsed time: 1716.991[sec]] loss: 4.087637883154002\n",
      "[EPOCH #143, elapsed time: 1730.554[sec]] loss: 4.088229436258132\n",
      "[EPOCH #144, elapsed time: 1744.539[sec]] loss: 4.085216708467011\n",
      "[EPOCH #145, elapsed time: 1756.219[sec]] loss: 4.0847136814168685\n",
      "[EPOCH #146, elapsed time: 1769.799[sec]] loss: 4.08329757024139\n",
      "[EPOCH #147, elapsed time: 1783.444[sec]] loss: 4.082194082262572\n",
      "[EPOCH #148, elapsed time: 1795.208[sec]] loss: 4.081334791043136\n",
      "[EPOCH #149, elapsed time: 1807.365[sec]] loss: 4.078206572102494\n",
      "[EPOCH #150, elapsed time: 1819.400[sec]] loss: 4.076517623125248\n",
      "[EPOCH #151, elapsed time: 1831.436[sec]] loss: 4.078567510984375\n",
      "[EPOCH #152, elapsed time: 1843.735[sec]] loss: 4.073536388704736\n",
      "[EPOCH #153, elapsed time: 1854.751[sec]] loss: 4.070791546839288\n",
      "[EPOCH #154, elapsed time: 1867.698[sec]] loss: 4.070276431677361\n",
      "[EPOCH #155, elapsed time: 1882.525[sec]] loss: 4.070116492623484\n",
      "[EPOCH #156, elapsed time: 1895.630[sec]] loss: 4.068424395544744\n",
      "[EPOCH #157, elapsed time: 1907.119[sec]] loss: 4.0665610021150655\n",
      "[EPOCH #158, elapsed time: 1918.404[sec]] loss: 4.068275958394974\n",
      "[EPOCH #159, elapsed time: 1929.290[sec]] loss: 4.064739966499294\n",
      "[EPOCH #160, elapsed time: 1942.012[sec]] loss: 4.064191534667159\n",
      "[EPOCH #161, elapsed time: 1955.800[sec]] loss: 4.061406012765124\n",
      "[EPOCH #162, elapsed time: 1969.329[sec]] loss: 4.060969896676521\n",
      "[EPOCH #163, elapsed time: 1982.007[sec]] loss: 4.0596413981510295\n",
      "[EPOCH #164, elapsed time: 1993.768[sec]] loss: 4.057870176764383\n",
      "[EPOCH #165, elapsed time: 2006.158[sec]] loss: 4.052679042059569\n",
      "[EPOCH #166, elapsed time: 2017.354[sec]] loss: 4.05346527789086\n",
      "[EPOCH #167, elapsed time: 2033.309[sec]] loss: 4.051886355503202\n",
      "[EPOCH #168, elapsed time: 2046.589[sec]] loss: 4.050378307957567\n",
      "[EPOCH #169, elapsed time: 2061.788[sec]] loss: 4.05271088718529\n",
      "[EPOCH #170, elapsed time: 2074.325[sec]] loss: 4.050017527563787\n",
      "[EPOCH #171, elapsed time: 2086.986[sec]] loss: 4.046284896322191\n",
      "[EPOCH #172, elapsed time: 2100.471[sec]] loss: 4.049588272987996\n",
      "[EPOCH #173, elapsed time: 2112.301[sec]] loss: 4.0455020140625315\n",
      "[EPOCH #174, elapsed time: 2124.840[sec]] loss: 4.045773166765102\n",
      "[EPOCH #175, elapsed time: 2140.795[sec]] loss: 4.042842500879455\n",
      "[EPOCH #176, elapsed time: 2154.930[sec]] loss: 4.04156954137469\n",
      "[EPOCH #177, elapsed time: 2166.571[sec]] loss: 4.041108179916118\n",
      "[EPOCH #178, elapsed time: 2178.060[sec]] loss: 4.0405723638284385\n",
      "[EPOCH #179, elapsed time: 2189.641[sec]] loss: 4.041805990521754\n",
      "[EPOCH #180, elapsed time: 2201.357[sec]] loss: 4.039947920095745\n",
      "[EPOCH #181, elapsed time: 2212.858[sec]] loss: 4.035991948000224\n",
      "[EPOCH #182, elapsed time: 2225.470[sec]] loss: 4.034281844598547\n",
      "[EPOCH #183, elapsed time: 2241.434[sec]] loss: 4.036309058286407\n",
      "[EPOCH #184, elapsed time: 2253.938[sec]] loss: 4.036518773282101\n",
      "[EPOCH #185, elapsed time: 2266.600[sec]] loss: 4.034107532550033\n",
      "[EPOCH #186, elapsed time: 2279.290[sec]] loss: 4.029467731580777\n",
      "[EPOCH #187, elapsed time: 2291.589[sec]] loss: 4.0304174529995125\n",
      "[EPOCH #188, elapsed time: 2303.846[sec]] loss: 4.030043312318036\n",
      "[EPOCH #189, elapsed time: 2316.459[sec]] loss: 4.02729035933965\n",
      "[EPOCH #190, elapsed time: 2328.167[sec]] loss: 4.026752870660978\n",
      "[EPOCH #191, elapsed time: 2343.327[sec]] loss: 4.026595239065736\n",
      "[EPOCH #192, elapsed time: 2355.918[sec]] loss: 4.023947336242051\n",
      "[EPOCH #193, elapsed time: 2367.323[sec]] loss: 4.023310913310475\n",
      "[EPOCH #194, elapsed time: 2379.588[sec]] loss: 4.0236537271940165\n",
      "[EPOCH #195, elapsed time: 2391.264[sec]] loss: 4.022049582164713\n",
      "[EPOCH #196, elapsed time: 2402.791[sec]] loss: 4.022640197000974\n",
      "[EPOCH #197, elapsed time: 2414.721[sec]] loss: 4.021006591946973\n",
      "[EPOCH #198, elapsed time: 2426.743[sec]] loss: 4.019191293478469\n",
      "[EPOCH #199, elapsed time: 2438.338[sec]] loss: 4.018988379590113\n",
      "[EPOCH #200, elapsed time: 2449.848[sec]] loss: 4.019431775453681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 08:37:28,987] Trial 11 finished with value: 0.70448 and parameters: {'learning_rate': 4.946431505849851e-05}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605190153848034\n",
      "[EPOCH #1, elapsed time: 11.487[sec]] loss: 4.5879801593372935\n",
      "[EPOCH #2, elapsed time: 22.936[sec]] loss: 4.559663119105597\n",
      "[EPOCH #3, elapsed time: 39.336[sec]] loss: 4.537261855198036\n",
      "[EPOCH #4, elapsed time: 52.662[sec]] loss: 4.516207663889352\n",
      "[EPOCH #5, elapsed time: 64.549[sec]] loss: 4.494794599077904\n",
      "[EPOCH #6, elapsed time: 76.612[sec]] loss: 4.479632762633145\n",
      "[EPOCH #7, elapsed time: 88.381[sec]] loss: 4.467819120513272\n",
      "[EPOCH #8, elapsed time: 100.010[sec]] loss: 4.459863950408428\n",
      "[EPOCH #9, elapsed time: 111.845[sec]] loss: 4.44664790854573\n",
      "[EPOCH #10, elapsed time: 123.516[sec]] loss: 4.437582681061592\n",
      "[EPOCH #11, elapsed time: 135.380[sec]] loss: 4.423377084030376\n",
      "[EPOCH #12, elapsed time: 146.956[sec]] loss: 4.413322165312862\n",
      "[EPOCH #13, elapsed time: 158.267[sec]] loss: 4.404019192252034\n",
      "[EPOCH #14, elapsed time: 170.591[sec]] loss: 4.395934021633097\n",
      "[EPOCH #15, elapsed time: 181.829[sec]] loss: 4.388249553782926\n",
      "[EPOCH #16, elapsed time: 193.894[sec]] loss: 4.377892592741905\n",
      "[EPOCH #17, elapsed time: 205.474[sec]] loss: 4.370552976392281\n",
      "[EPOCH #18, elapsed time: 217.061[sec]] loss: 4.362132004111223\n",
      "[EPOCH #19, elapsed time: 228.994[sec]] loss: 4.3537769531181665\n",
      "[EPOCH #20, elapsed time: 243.488[sec]] loss: 4.3477306661892605\n",
      "[EPOCH #21, elapsed time: 256.872[sec]] loss: 4.339528153969246\n",
      "[EPOCH #22, elapsed time: 270.676[sec]] loss: 4.331063715677877\n",
      "[EPOCH #23, elapsed time: 286.989[sec]] loss: 4.32976321661541\n",
      "[EPOCH #24, elapsed time: 300.121[sec]] loss: 4.322590042060564\n",
      "[EPOCH #25, elapsed time: 315.789[sec]] loss: 4.317612140360202\n",
      "[EPOCH #26, elapsed time: 330.988[sec]] loss: 4.31104568144639\n",
      "[EPOCH #27, elapsed time: 342.580[sec]] loss: 4.3043835481923125\n",
      "[EPOCH #28, elapsed time: 357.152[sec]] loss: 4.2980606947964155\n",
      "[EPOCH #29, elapsed time: 370.547[sec]] loss: 4.292306505863436\n",
      "[EPOCH #30, elapsed time: 382.186[sec]] loss: 4.289720883067418\n",
      "[EPOCH #31, elapsed time: 393.155[sec]] loss: 4.283247470550635\n",
      "[EPOCH #32, elapsed time: 404.235[sec]] loss: 4.277633584621086\n",
      "[EPOCH #33, elapsed time: 415.205[sec]] loss: 4.271230921406664\n",
      "[EPOCH #34, elapsed time: 426.558[sec]] loss: 4.267325104922723\n",
      "[EPOCH #35, elapsed time: 437.742[sec]] loss: 4.259845673237103\n",
      "[EPOCH #36, elapsed time: 450.181[sec]] loss: 4.259976758914198\n",
      "[EPOCH #37, elapsed time: 461.518[sec]] loss: 4.2535811817501115\n",
      "[EPOCH #38, elapsed time: 473.799[sec]] loss: 4.249347347063051\n",
      "[EPOCH #39, elapsed time: 486.615[sec]] loss: 4.2451478368718885\n",
      "[EPOCH #40, elapsed time: 497.816[sec]] loss: 4.240724883015463\n",
      "[EPOCH #41, elapsed time: 508.987[sec]] loss: 4.236956670050886\n",
      "[EPOCH #42, elapsed time: 520.220[sec]] loss: 4.229765102639079\n",
      "[EPOCH #43, elapsed time: 531.374[sec]] loss: 4.22965429535449\n",
      "[EPOCH #44, elapsed time: 544.793[sec]] loss: 4.221816771776342\n",
      "[EPOCH #45, elapsed time: 556.455[sec]] loss: 4.21968093379064\n",
      "[EPOCH #46, elapsed time: 568.000[sec]] loss: 4.217020830586409\n",
      "[EPOCH #47, elapsed time: 580.181[sec]] loss: 4.213487461142561\n",
      "[EPOCH #48, elapsed time: 594.382[sec]] loss: 4.209492602870012\n",
      "[EPOCH #49, elapsed time: 609.676[sec]] loss: 4.207081551591479\n",
      "[EPOCH #50, elapsed time: 621.859[sec]] loss: 4.2019511742692535\n",
      "[EPOCH #51, elapsed time: 637.169[sec]] loss: 4.198300397937602\n",
      "[EPOCH #52, elapsed time: 649.743[sec]] loss: 4.196157053961483\n",
      "[EPOCH #53, elapsed time: 662.845[sec]] loss: 4.192136975335373\n",
      "[EPOCH #54, elapsed time: 674.131[sec]] loss: 4.188199042968848\n",
      "[EPOCH #55, elapsed time: 685.988[sec]] loss: 4.18591088525622\n",
      "[EPOCH #56, elapsed time: 698.435[sec]] loss: 4.181508932980069\n",
      "[EPOCH #57, elapsed time: 712.044[sec]] loss: 4.1777967892620556\n",
      "[EPOCH #58, elapsed time: 723.591[sec]] loss: 4.17475968420086\n",
      "[EPOCH #59, elapsed time: 734.732[sec]] loss: 4.169910207285914\n",
      "[EPOCH #60, elapsed time: 745.943[sec]] loss: 4.167641508525866\n",
      "[EPOCH #61, elapsed time: 757.136[sec]] loss: 4.1623941514252545\n",
      "[EPOCH #62, elapsed time: 768.455[sec]] loss: 4.159255032499708\n",
      "[EPOCH #63, elapsed time: 780.010[sec]] loss: 4.159365427471168\n",
      "[EPOCH #64, elapsed time: 791.885[sec]] loss: 4.157510105646808\n",
      "[EPOCH #65, elapsed time: 803.567[sec]] loss: 4.154728086804703\n",
      "[EPOCH #66, elapsed time: 814.632[sec]] loss: 4.1489299565496465\n",
      "[EPOCH #67, elapsed time: 826.051[sec]] loss: 4.145037595385248\n",
      "[EPOCH #68, elapsed time: 837.384[sec]] loss: 4.144365487614276\n",
      "[EPOCH #69, elapsed time: 849.997[sec]] loss: 4.141505871838053\n",
      "[EPOCH #70, elapsed time: 861.868[sec]] loss: 4.140537522804714\n",
      "[EPOCH #71, elapsed time: 872.988[sec]] loss: 4.134126137405805\n",
      "[EPOCH #72, elapsed time: 885.068[sec]] loss: 4.133667628580534\n",
      "[EPOCH #73, elapsed time: 896.306[sec]] loss: 4.131019137871243\n",
      "[EPOCH #74, elapsed time: 911.509[sec]] loss: 4.134693043398232\n",
      "[EPOCH #75, elapsed time: 925.077[sec]] loss: 4.129329790156847\n",
      "[EPOCH #76, elapsed time: 936.548[sec]] loss: 4.123782931271076\n",
      "[EPOCH #77, elapsed time: 948.982[sec]] loss: 4.1224719686380045\n",
      "[EPOCH #78, elapsed time: 960.453[sec]] loss: 4.118229851231542\n",
      "[EPOCH #79, elapsed time: 976.841[sec]] loss: 4.119850596326021\n",
      "[EPOCH #80, elapsed time: 988.869[sec]] loss: 4.114206105108377\n",
      "[EPOCH #81, elapsed time: 999.878[sec]] loss: 4.110961920469141\n",
      "[EPOCH #82, elapsed time: 1010.968[sec]] loss: 4.113919620550526\n",
      "[EPOCH #83, elapsed time: 1023.891[sec]] loss: 4.110234548247783\n",
      "[EPOCH #84, elapsed time: 1039.925[sec]] loss: 4.105403937060942\n",
      "[EPOCH #85, elapsed time: 1051.032[sec]] loss: 4.105660173577219\n",
      "[EPOCH #86, elapsed time: 1062.181[sec]] loss: 4.10163825822807\n",
      "[EPOCH #87, elapsed time: 1074.613[sec]] loss: 4.101204163739869\n",
      "[EPOCH #88, elapsed time: 1087.568[sec]] loss: 4.0960900228296575\n",
      "[EPOCH #89, elapsed time: 1098.865[sec]] loss: 4.097011622601569\n",
      "[EPOCH #90, elapsed time: 1111.274[sec]] loss: 4.0935167685122495\n",
      "[EPOCH #91, elapsed time: 1122.548[sec]] loss: 4.093563674126233\n",
      "[EPOCH #92, elapsed time: 1133.641[sec]] loss: 4.0900887212002806\n",
      "[EPOCH #93, elapsed time: 1148.117[sec]] loss: 4.089833375512219\n",
      "[EPOCH #94, elapsed time: 1160.021[sec]] loss: 4.085285499777767\n",
      "[EPOCH #95, elapsed time: 1171.361[sec]] loss: 4.087654942514343\n",
      "[EPOCH #96, elapsed time: 1184.291[sec]] loss: 4.083392432158526\n",
      "[EPOCH #97, elapsed time: 1196.036[sec]] loss: 4.082404611206787\n",
      "[EPOCH #98, elapsed time: 1207.640[sec]] loss: 4.07965883931058\n",
      "[EPOCH #99, elapsed time: 1219.240[sec]] loss: 4.077609089651858\n",
      "[EPOCH #100, elapsed time: 1231.880[sec]] loss: 4.077486438623088\n",
      "[EPOCH #101, elapsed time: 1242.994[sec]] loss: 4.074723470691527\n",
      "[EPOCH #102, elapsed time: 1254.022[sec]] loss: 4.071025343789401\n",
      "[EPOCH #103, elapsed time: 1265.085[sec]] loss: 4.072356136960245\n",
      "[EPOCH #104, elapsed time: 1275.979[sec]] loss: 4.0665609028121255\n",
      "[EPOCH #105, elapsed time: 1287.040[sec]] loss: 4.067579048990212\n",
      "[EPOCH #106, elapsed time: 1300.525[sec]] loss: 4.06493806396626\n",
      "[EPOCH #107, elapsed time: 1313.652[sec]] loss: 4.063756424573775\n",
      "[EPOCH #108, elapsed time: 1326.095[sec]] loss: 4.0641567475202365\n",
      "[EPOCH #109, elapsed time: 1340.734[sec]] loss: 4.061185570458762\n",
      "[EPOCH #110, elapsed time: 1352.827[sec]] loss: 4.058118807072069\n",
      "[EPOCH #111, elapsed time: 1365.624[sec]] loss: 4.057753483759464\n",
      "[EPOCH #112, elapsed time: 1380.188[sec]] loss: 4.056600937687733\n",
      "[EPOCH #113, elapsed time: 1395.350[sec]] loss: 4.055565499374673\n",
      "[EPOCH #114, elapsed time: 1407.727[sec]] loss: 4.052291886133791\n",
      "[EPOCH #115, elapsed time: 1418.661[sec]] loss: 4.05165250226617\n",
      "[EPOCH #116, elapsed time: 1430.281[sec]] loss: 4.049806269787857\n",
      "[EPOCH #117, elapsed time: 1442.674[sec]] loss: 4.0481524694751485\n",
      "[EPOCH #118, elapsed time: 1453.857[sec]] loss: 4.045261074927703\n",
      "[EPOCH #119, elapsed time: 1464.912[sec]] loss: 4.044993932019879\n",
      "[EPOCH #120, elapsed time: 1476.069[sec]] loss: 4.043790191697983\n",
      "[EPOCH #121, elapsed time: 1487.295[sec]] loss: 4.043991293269552\n",
      "[EPOCH #122, elapsed time: 1498.755[sec]] loss: 4.0421331737259605\n",
      "[EPOCH #123, elapsed time: 1511.277[sec]] loss: 4.038539222014385\n",
      "[EPOCH #124, elapsed time: 1522.357[sec]] loss: 4.039109142636613\n",
      "[EPOCH #125, elapsed time: 1534.823[sec]] loss: 4.03743070177138\n",
      "[EPOCH #126, elapsed time: 1545.920[sec]] loss: 4.035387372787534\n",
      "[EPOCH #127, elapsed time: 1556.962[sec]] loss: 4.035316025837064\n",
      "[EPOCH #128, elapsed time: 1568.799[sec]] loss: 4.030195449455686\n",
      "[EPOCH #129, elapsed time: 1584.125[sec]] loss: 4.031304240608093\n",
      "[EPOCH #130, elapsed time: 1595.428[sec]] loss: 4.032187637425507\n",
      "[EPOCH #131, elapsed time: 1606.631[sec]] loss: 4.031921986654944\n",
      "[EPOCH #132, elapsed time: 1620.456[sec]] loss: 4.0237836851497075\n",
      "[EPOCH #133, elapsed time: 1632.577[sec]] loss: 4.024810605985723\n",
      "[EPOCH #134, elapsed time: 1644.125[sec]] loss: 4.024819213308285\n",
      "[EPOCH #135, elapsed time: 1655.115[sec]] loss: 4.022766936534655\n",
      "[EPOCH #136, elapsed time: 1666.226[sec]] loss: 4.020557605221114\n",
      "[EPOCH #137, elapsed time: 1677.471[sec]] loss: 4.019848671160824\n",
      "[EPOCH #138, elapsed time: 1688.613[sec]] loss: 4.020278812293738\n",
      "[EPOCH #139, elapsed time: 1699.870[sec]] loss: 4.019216624270322\n",
      "[EPOCH #140, elapsed time: 1710.980[sec]] loss: 4.022386900827965\n",
      "[EPOCH #141, elapsed time: 1722.293[sec]] loss: 4.015765775874572\n",
      "[EPOCH #142, elapsed time: 1737.795[sec]] loss: 4.015415226124222\n",
      "[EPOCH #143, elapsed time: 1748.884[sec]] loss: 4.0146841853380355\n",
      "[EPOCH #144, elapsed time: 1761.705[sec]] loss: 4.014195166866671\n",
      "[EPOCH #145, elapsed time: 1775.493[sec]] loss: 4.01293407551234\n",
      "[EPOCH #146, elapsed time: 1786.576[sec]] loss: 4.0083656248494135\n",
      "[EPOCH #147, elapsed time: 1800.895[sec]] loss: 4.011846556086915\n",
      "[EPOCH #148, elapsed time: 1812.867[sec]] loss: 4.008347636373548\n",
      "[EPOCH #149, elapsed time: 1824.540[sec]] loss: 4.007411375765761\n",
      "[EPOCH #150, elapsed time: 1835.642[sec]] loss: 4.007217118927705\n",
      "[EPOCH #151, elapsed time: 1851.831[sec]] loss: 4.00273598239579\n",
      "[EPOCH #152, elapsed time: 1864.287[sec]] loss: 4.003759904771147\n",
      "[EPOCH #153, elapsed time: 1875.722[sec]] loss: 4.003597837492052\n",
      "[EPOCH #154, elapsed time: 1887.973[sec]] loss: 4.005039351305287\n",
      "[EPOCH #155, elapsed time: 1899.117[sec]] loss: 4.0034783519542305\n",
      "[EPOCH #156, elapsed time: 1910.278[sec]] loss: 3.999765363169723\n",
      "[EPOCH #157, elapsed time: 1921.596[sec]] loss: 4.001000046653772\n",
      "[EPOCH #158, elapsed time: 1933.431[sec]] loss: 3.9994852825074494\n",
      "[EPOCH #159, elapsed time: 1945.655[sec]] loss: 3.9988754126633577\n",
      "[EPOCH #160, elapsed time: 1958.144[sec]] loss: 3.9998240188688934\n",
      "[EPOCH #161, elapsed time: 1970.252[sec]] loss: 3.9970902498151277\n",
      "[EPOCH #162, elapsed time: 1982.965[sec]] loss: 3.994414693792127\n",
      "[EPOCH #163, elapsed time: 1995.649[sec]] loss: 3.994828767526325\n",
      "[EPOCH #164, elapsed time: 2007.856[sec]] loss: 3.9904475501921417\n",
      "[EPOCH #165, elapsed time: 2020.757[sec]] loss: 3.9921429585891888\n",
      "[EPOCH #166, elapsed time: 2033.726[sec]] loss: 3.993030712075212\n",
      "[EPOCH #167, elapsed time: 2047.257[sec]] loss: 3.991161084831028\n",
      "[EPOCH #168, elapsed time: 2063.023[sec]] loss: 3.9921419948472936\n",
      "[EPOCH #169, elapsed time: 2074.163[sec]] loss: 3.9867355945548106\n",
      "[EPOCH #170, elapsed time: 2087.283[sec]] loss: 3.986362767997493\n",
      "[EPOCH #171, elapsed time: 2099.301[sec]] loss: 3.9849825152508815\n",
      "[EPOCH #172, elapsed time: 2112.033[sec]] loss: 3.984109721577328\n",
      "[EPOCH #173, elapsed time: 2125.217[sec]] loss: 3.9835780990939833\n",
      "[EPOCH #174, elapsed time: 2138.015[sec]] loss: 3.981922567462738\n",
      "[EPOCH #175, elapsed time: 2151.326[sec]] loss: 3.9819380288236963\n",
      "[EPOCH #176, elapsed time: 2164.366[sec]] loss: 3.982150454286269\n",
      "[EPOCH #177, elapsed time: 2177.771[sec]] loss: 3.9808253303828027\n",
      "[EPOCH #178, elapsed time: 2192.128[sec]] loss: 3.9802173581248432\n",
      "[EPOCH #179, elapsed time: 2207.488[sec]] loss: 3.981290353999562\n",
      "[EPOCH #180, elapsed time: 2222.039[sec]] loss: 3.978416549190831\n",
      "[EPOCH #181, elapsed time: 2234.203[sec]] loss: 3.9793790790108785\n",
      "[EPOCH #182, elapsed time: 2245.610[sec]] loss: 3.9778564102895277\n",
      "[EPOCH #183, elapsed time: 2256.639[sec]] loss: 3.975381042586636\n",
      "[EPOCH #184, elapsed time: 2267.534[sec]] loss: 3.976623948994769\n",
      "[EPOCH #185, elapsed time: 2279.448[sec]] loss: 3.974668824359994\n",
      "[EPOCH #186, elapsed time: 2290.342[sec]] loss: 3.974252220460107\n",
      "[EPOCH #187, elapsed time: 2302.785[sec]] loss: 3.970945639253349\n",
      "[EPOCH #188, elapsed time: 2313.871[sec]] loss: 3.972081005458868\n",
      "[EPOCH #189, elapsed time: 2326.016[sec]] loss: 3.969869855574439\n",
      "[EPOCH #190, elapsed time: 2338.638[sec]] loss: 3.9695233731260684\n",
      "[EPOCH #191, elapsed time: 2349.930[sec]] loss: 3.9677728017171225\n",
      "[EPOCH #192, elapsed time: 2362.291[sec]] loss: 3.967034513799334\n",
      "[EPOCH #193, elapsed time: 2374.225[sec]] loss: 3.967175904711469\n",
      "[EPOCH #194, elapsed time: 2385.568[sec]] loss: 3.9667165154306385\n",
      "[EPOCH #195, elapsed time: 2400.554[sec]] loss: 3.965674913623588\n",
      "[EPOCH #196, elapsed time: 2411.915[sec]] loss: 3.9663151300494977\n",
      "[EPOCH #197, elapsed time: 2424.280[sec]] loss: 3.9618377536058578\n",
      "[EPOCH #198, elapsed time: 2440.210[sec]] loss: 3.963605971345517\n",
      "[EPOCH #199, elapsed time: 2452.751[sec]] loss: 3.965497967682812\n",
      "[EPOCH #200, elapsed time: 2467.994[sec]] loss: 3.9630246128855955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 09:18:52,965] Trial 12 finished with value: 0.75428 and parameters: {'learning_rate': 0.00012508215710255074}. Best is trial 5 with value: 0.76704.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.6051733388552964\n",
      "[EPOCH #1, elapsed time: 11.438[sec]] loss: 4.58107230789907\n",
      "[EPOCH #2, elapsed time: 22.520[sec]] loss: 4.543825358209592\n",
      "[EPOCH #3, elapsed time: 33.826[sec]] loss: 4.51767137457908\n",
      "[EPOCH #4, elapsed time: 45.048[sec]] loss: 4.502322219536233\n",
      "[EPOCH #5, elapsed time: 56.383[sec]] loss: 4.491946416868282\n",
      "[EPOCH #6, elapsed time: 67.580[sec]] loss: 4.4779728064533995\n",
      "[EPOCH #7, elapsed time: 78.642[sec]] loss: 4.462320393655671\n",
      "[EPOCH #8, elapsed time: 91.251[sec]] loss: 4.451755832115657\n",
      "[EPOCH #9, elapsed time: 106.084[sec]] loss: 4.434394507856607\n",
      "[EPOCH #10, elapsed time: 117.985[sec]] loss: 4.42523009854864\n",
      "[EPOCH #11, elapsed time: 129.287[sec]] loss: 4.417482369081835\n",
      "[EPOCH #12, elapsed time: 140.818[sec]] loss: 4.407171248779492\n",
      "[EPOCH #13, elapsed time: 152.210[sec]] loss: 4.392185090065612\n",
      "[EPOCH #14, elapsed time: 163.615[sec]] loss: 4.383289024758171\n",
      "[EPOCH #15, elapsed time: 174.894[sec]] loss: 4.376098284413246\n",
      "[EPOCH #16, elapsed time: 186.193[sec]] loss: 4.367746889171735\n",
      "[EPOCH #17, elapsed time: 197.500[sec]] loss: 4.357387658044153\n",
      "[EPOCH #18, elapsed time: 210.014[sec]] loss: 4.347605542044417\n",
      "[EPOCH #19, elapsed time: 221.982[sec]] loss: 4.343847411760366\n",
      "[EPOCH #20, elapsed time: 233.174[sec]] loss: 4.334267190077789\n",
      "[EPOCH #21, elapsed time: 247.057[sec]] loss: 4.327519803648184\n",
      "[EPOCH #22, elapsed time: 261.114[sec]] loss: 4.324435961986305\n",
      "[EPOCH #23, elapsed time: 273.590[sec]] loss: 4.317610140572888\n",
      "[EPOCH #24, elapsed time: 285.101[sec]] loss: 4.312568150799166\n",
      "[EPOCH #25, elapsed time: 296.612[sec]] loss: 4.307609103539016\n",
      "[EPOCH #26, elapsed time: 307.554[sec]] loss: 4.30507594777923\n",
      "[EPOCH #27, elapsed time: 319.087[sec]] loss: 4.2967547952404255\n",
      "[EPOCH #28, elapsed time: 330.297[sec]] loss: 4.289029454544273\n",
      "[EPOCH #29, elapsed time: 342.097[sec]] loss: 4.28434982714711\n",
      "[EPOCH #30, elapsed time: 354.429[sec]] loss: 4.278079763407594\n",
      "[EPOCH #31, elapsed time: 365.491[sec]] loss: 4.275932955726629\n",
      "[EPOCH #32, elapsed time: 380.867[sec]] loss: 4.264739621699962\n",
      "[EPOCH #33, elapsed time: 393.002[sec]] loss: 4.2578345333546\n",
      "[EPOCH #34, elapsed time: 404.092[sec]] loss: 4.255428856485407\n",
      "[EPOCH #35, elapsed time: 415.299[sec]] loss: 4.251037900598859\n",
      "[EPOCH #36, elapsed time: 426.598[sec]] loss: 4.2517325498321\n",
      "[EPOCH #37, elapsed time: 438.162[sec]] loss: 4.242637472585921\n",
      "[EPOCH #38, elapsed time: 449.754[sec]] loss: 4.23982533002159\n",
      "[EPOCH #39, elapsed time: 461.608[sec]] loss: 4.233790453778424\n",
      "[EPOCH #40, elapsed time: 473.371[sec]] loss: 4.230037240591556\n",
      "[EPOCH #41, elapsed time: 485.720[sec]] loss: 4.227178461339637\n",
      "[EPOCH #42, elapsed time: 496.879[sec]] loss: 4.226076605528955\n",
      "[EPOCH #43, elapsed time: 509.737[sec]] loss: 4.217955998861858\n",
      "[EPOCH #44, elapsed time: 522.091[sec]] loss: 4.216843358843432\n",
      "[EPOCH #45, elapsed time: 534.643[sec]] loss: 4.214560644487807\n",
      "[EPOCH #46, elapsed time: 549.106[sec]] loss: 4.213298970891815\n",
      "[EPOCH #47, elapsed time: 564.670[sec]] loss: 4.207135258770416\n",
      "[EPOCH #48, elapsed time: 580.172[sec]] loss: 4.207989815634485\n",
      "[EPOCH #49, elapsed time: 592.546[sec]] loss: 4.202555793413198\n",
      "[EPOCH #50, elapsed time: 603.699[sec]] loss: 4.201967872569596\n",
      "[EPOCH #51, elapsed time: 614.885[sec]] loss: 4.194628120612732\n",
      "[EPOCH #52, elapsed time: 627.506[sec]] loss: 4.190843612661136\n",
      "[EPOCH #53, elapsed time: 638.717[sec]] loss: 4.189584546720684\n",
      "[EPOCH #54, elapsed time: 650.143[sec]] loss: 4.187426358709256\n",
      "[EPOCH #55, elapsed time: 662.511[sec]] loss: 4.184339532772852\n",
      "[EPOCH #56, elapsed time: 678.532[sec]] loss: 4.1793504107555215\n",
      "[EPOCH #57, elapsed time: 693.416[sec]] loss: 4.180329081040503\n",
      "[EPOCH #58, elapsed time: 706.613[sec]] loss: 4.1832310090214495\n",
      "[EPOCH #59, elapsed time: 717.568[sec]] loss: 4.175337912711438\n",
      "[EPOCH #60, elapsed time: 729.362[sec]] loss: 4.166632508895981\n",
      "[EPOCH #61, elapsed time: 743.521[sec]] loss: 4.166877154730408\n",
      "[EPOCH #62, elapsed time: 757.955[sec]] loss: 4.165929896970323\n",
      "[EPOCH #63, elapsed time: 769.110[sec]] loss: 4.160036411334213\n",
      "[EPOCH #64, elapsed time: 781.491[sec]] loss: 4.157336377975503\n",
      "[EPOCH #65, elapsed time: 793.178[sec]] loss: 4.159414508597483\n",
      "[EPOCH #66, elapsed time: 805.663[sec]] loss: 4.153425970065311\n",
      "[EPOCH #67, elapsed time: 816.722[sec]] loss: 4.150817747842175\n",
      "[EPOCH #68, elapsed time: 827.946[sec]] loss: 4.149892538690597\n",
      "[EPOCH #69, elapsed time: 839.832[sec]] loss: 4.147502791172254\n",
      "[EPOCH #70, elapsed time: 851.254[sec]] loss: 4.1428434447607625\n",
      "[EPOCH #71, elapsed time: 862.579[sec]] loss: 4.142209920986906\n",
      "[EPOCH #72, elapsed time: 873.747[sec]] loss: 4.139010822170451\n",
      "[EPOCH #73, elapsed time: 885.190[sec]] loss: 4.140572711739567\n",
      "[EPOCH #74, elapsed time: 897.648[sec]] loss: 4.13670869965776\n",
      "[EPOCH #75, elapsed time: 909.019[sec]] loss: 4.130841940851145\n",
      "[EPOCH #76, elapsed time: 920.486[sec]] loss: 4.130952713051745\n",
      "[EPOCH #77, elapsed time: 932.078[sec]] loss: 4.1253666732834455\n",
      "[EPOCH #78, elapsed time: 943.794[sec]] loss: 4.124696861988294\n",
      "[EPOCH #79, elapsed time: 955.856[sec]] loss: 4.127563338667174\n",
      "[EPOCH #80, elapsed time: 967.671[sec]] loss: 4.122812949459444\n",
      "[EPOCH #81, elapsed time: 979.949[sec]] loss: 4.116460816340041\n",
      "[EPOCH #82, elapsed time: 991.064[sec]] loss: 4.117763711943965\n",
      "[EPOCH #83, elapsed time: 1002.537[sec]] loss: 4.115682683384579\n",
      "[EPOCH #84, elapsed time: 1013.813[sec]] loss: 4.114950504809408\n",
      "[EPOCH #85, elapsed time: 1025.420[sec]] loss: 4.111267356634597\n",
      "[EPOCH #86, elapsed time: 1038.435[sec]] loss: 4.1102645568030045\n",
      "[EPOCH #87, elapsed time: 1050.710[sec]] loss: 4.105396411087905\n",
      "[EPOCH #88, elapsed time: 1062.012[sec]] loss: 4.104698083222256\n",
      "[EPOCH #89, elapsed time: 1076.602[sec]] loss: 4.10103834918578\n",
      "[EPOCH #90, elapsed time: 1087.780[sec]] loss: 4.1011364278469955\n",
      "[EPOCH #91, elapsed time: 1099.247[sec]] loss: 4.100691258716644\n",
      "[EPOCH #92, elapsed time: 1110.411[sec]] loss: 4.095662368846413\n",
      "[EPOCH #93, elapsed time: 1122.699[sec]] loss: 4.09620683908615\n",
      "[EPOCH #94, elapsed time: 1134.105[sec]] loss: 4.092918115171651\n",
      "[EPOCH #95, elapsed time: 1148.602[sec]] loss: 4.091852362157439\n",
      "[EPOCH #96, elapsed time: 1165.076[sec]] loss: 4.087456762218658\n",
      "[EPOCH #97, elapsed time: 1176.383[sec]] loss: 4.090865920006428\n",
      "[EPOCH #98, elapsed time: 1187.446[sec]] loss: 4.086538691285781\n",
      "[EPOCH #99, elapsed time: 1199.736[sec]] loss: 4.084949141958167\n",
      "[EPOCH #100, elapsed time: 1214.836[sec]] loss: 4.0841155187944835\n",
      "[EPOCH #101, elapsed time: 1227.679[sec]] loss: 4.082063726790045\n",
      "[EPOCH #102, elapsed time: 1239.180[sec]] loss: 4.078779372204898\n",
      "[EPOCH #103, elapsed time: 1250.308[sec]] loss: 4.079312645161068\n",
      "[EPOCH #104, elapsed time: 1261.391[sec]] loss: 4.078218942259034\n",
      "[EPOCH #105, elapsed time: 1273.724[sec]] loss: 4.074365172566165\n",
      "[EPOCH #106, elapsed time: 1284.752[sec]] loss: 4.073322819351616\n",
      "[EPOCH #107, elapsed time: 1295.790[sec]] loss: 4.0701488771273855\n",
      "[EPOCH #108, elapsed time: 1307.430[sec]] loss: 4.0684988982236625\n",
      "[EPOCH #109, elapsed time: 1318.488[sec]] loss: 4.066258667984302\n",
      "[EPOCH #110, elapsed time: 1329.591[sec]] loss: 4.0678542098095996\n",
      "[EPOCH #111, elapsed time: 1340.809[sec]] loss: 4.063026062433947\n",
      "[EPOCH #112, elapsed time: 1353.798[sec]] loss: 4.059773671802464\n",
      "[EPOCH #113, elapsed time: 1368.882[sec]] loss: 4.061292551910732\n",
      "[EPOCH #114, elapsed time: 1383.410[sec]] loss: 4.0586508610884655\n",
      "[EPOCH #115, elapsed time: 1395.284[sec]] loss: 4.055819863626305\n",
      "[EPOCH #116, elapsed time: 1406.476[sec]] loss: 4.056470918106255\n",
      "[EPOCH #117, elapsed time: 1421.555[sec]] loss: 4.0571004815080265\n",
      "[EPOCH #118, elapsed time: 1433.975[sec]] loss: 4.053815522715593\n",
      "[EPOCH #119, elapsed time: 1445.123[sec]] loss: 4.05124039933686\n",
      "[EPOCH #120, elapsed time: 1456.075[sec]] loss: 4.052050452314732\n",
      "[EPOCH #121, elapsed time: 1468.161[sec]] loss: 4.047923464235097\n",
      "[EPOCH #122, elapsed time: 1479.324[sec]] loss: 4.045422679860852\n",
      "[EPOCH #123, elapsed time: 1494.541[sec]] loss: 4.0478923959909\n",
      "[EPOCH #124, elapsed time: 1506.368[sec]] loss: 4.0425249868223325\n",
      "[EPOCH #125, elapsed time: 1517.395[sec]] loss: 4.041914002832814\n",
      "[EPOCH #126, elapsed time: 1529.017[sec]] loss: 4.041884857648775\n",
      "[EPOCH #127, elapsed time: 1541.949[sec]] loss: 4.039071123644242\n",
      "[EPOCH #128, elapsed time: 1553.331[sec]] loss: 4.034645044414645\n",
      "[EPOCH #129, elapsed time: 1564.461[sec]] loss: 4.0356699669887375\n",
      "[EPOCH #130, elapsed time: 1575.392[sec]] loss: 4.033816701238611\n",
      "[EPOCH #131, elapsed time: 1586.642[sec]] loss: 4.034445896105971\n",
      "[EPOCH #132, elapsed time: 1597.602[sec]] loss: 4.031557916908484\n",
      "[EPOCH #133, elapsed time: 1608.614[sec]] loss: 4.031385409092186\n",
      "[EPOCH #134, elapsed time: 1619.459[sec]] loss: 4.028390385596628\n",
      "[EPOCH #135, elapsed time: 1630.507[sec]] loss: 4.028174700373956\n",
      "[EPOCH #136, elapsed time: 1643.477[sec]] loss: 4.027580003897044\n",
      "[EPOCH #137, elapsed time: 1654.916[sec]] loss: 4.0236859625147\n",
      "[EPOCH #138, elapsed time: 1666.838[sec]] loss: 4.022953835040121\n",
      "[EPOCH #139, elapsed time: 1680.483[sec]] loss: 4.019801159356545\n",
      "[EPOCH #140, elapsed time: 1692.184[sec]] loss: 4.02060829624486\n",
      "[EPOCH #141, elapsed time: 1703.506[sec]] loss: 4.018925511981918\n",
      "[EPOCH #142, elapsed time: 1716.803[sec]] loss: 4.017555834006897\n",
      "[EPOCH #143, elapsed time: 1728.521[sec]] loss: 4.014000242212531\n",
      "[EPOCH #144, elapsed time: 1739.763[sec]] loss: 4.014820218009973\n",
      "[EPOCH #145, elapsed time: 1751.407[sec]] loss: 4.0154980144207855\n",
      "[EPOCH #146, elapsed time: 1762.447[sec]] loss: 4.013262062139871\n",
      "[EPOCH #147, elapsed time: 1773.426[sec]] loss: 4.012421828390159\n",
      "[EPOCH #148, elapsed time: 1787.638[sec]] loss: 4.010045645256799\n",
      "[EPOCH #149, elapsed time: 1799.940[sec]] loss: 4.0104152286807775\n",
      "[EPOCH #150, elapsed time: 1810.877[sec]] loss: 4.008581522253944\n",
      "[EPOCH #151, elapsed time: 1822.149[sec]] loss: 4.002945049405479\n",
      "[EPOCH #152, elapsed time: 1836.687[sec]] loss: 4.0055949803887465\n",
      "[EPOCH #153, elapsed time: 1850.332[sec]] loss: 4.003401274568213\n",
      "[EPOCH #154, elapsed time: 1862.219[sec]] loss: 4.004325199950908\n",
      "[EPOCH #155, elapsed time: 1874.014[sec]] loss: 4.002406493868495\n",
      "[EPOCH #156, elapsed time: 1885.082[sec]] loss: 4.000635797368816\n",
      "[EPOCH #157, elapsed time: 1896.264[sec]] loss: 3.998372953211125\n",
      "[EPOCH #158, elapsed time: 1907.645[sec]] loss: 3.9965044233330684\n",
      "[EPOCH #159, elapsed time: 1919.360[sec]] loss: 3.995305931880851\n",
      "[EPOCH #160, elapsed time: 1931.682[sec]] loss: 3.996301530952722\n",
      "[EPOCH #161, elapsed time: 1943.232[sec]] loss: 3.9929596721859064\n",
      "[EPOCH #162, elapsed time: 1958.974[sec]] loss: 3.9908073579967596\n",
      "[EPOCH #163, elapsed time: 1974.117[sec]] loss: 3.9919772527954627\n",
      "[EPOCH #164, elapsed time: 1989.172[sec]] loss: 3.990341640174656\n",
      "[EPOCH #165, elapsed time: 2003.647[sec]] loss: 3.990712424386257\n",
      "[EPOCH #166, elapsed time: 2018.033[sec]] loss: 3.9903415481935918\n",
      "[EPOCH #167, elapsed time: 2029.011[sec]] loss: 3.989089918075581\n",
      "[EPOCH #168, elapsed time: 2040.046[sec]] loss: 3.985326523820483\n",
      "[EPOCH #169, elapsed time: 2051.028[sec]] loss: 3.9880638600005907\n",
      "[EPOCH #170, elapsed time: 2062.690[sec]] loss: 3.9885743596350927\n",
      "[EPOCH #171, elapsed time: 2074.053[sec]] loss: 3.98379892290058\n",
      "[EPOCH #172, elapsed time: 2090.817[sec]] loss: 3.9839872386466215\n",
      "[EPOCH #173, elapsed time: 2102.174[sec]] loss: 3.9855081385248226\n",
      "[EPOCH #174, elapsed time: 2113.363[sec]] loss: 3.9825523093352313\n",
      "[EPOCH #175, elapsed time: 2124.653[sec]] loss: 3.981813803896718\n",
      "[EPOCH #176, elapsed time: 2140.512[sec]] loss: 3.9806954406120805\n",
      "[EPOCH #177, elapsed time: 2152.296[sec]] loss: 3.981868882249428\n",
      "[EPOCH #178, elapsed time: 2163.738[sec]] loss: 3.9789756430469105\n",
      "[EPOCH #179, elapsed time: 2175.341[sec]] loss: 3.9777106369296784\n",
      "[EPOCH #180, elapsed time: 2186.539[sec]] loss: 3.9785907686481243\n",
      "[EPOCH #181, elapsed time: 2198.040[sec]] loss: 3.9742119403809824\n",
      "[EPOCH #182, elapsed time: 2209.541[sec]] loss: 3.975496999590502\n",
      "[EPOCH #183, elapsed time: 2224.950[sec]] loss: 3.9706146447077364\n",
      "[EPOCH #184, elapsed time: 2236.167[sec]] loss: 3.9709623036900163\n",
      "[EPOCH #185, elapsed time: 2248.578[sec]] loss: 3.9704741684199143\n",
      "[EPOCH #186, elapsed time: 2259.699[sec]] loss: 3.968124425495121\n",
      "[EPOCH #187, elapsed time: 2270.891[sec]] loss: 3.9665086545474395\n",
      "[EPOCH #188, elapsed time: 2283.000[sec]] loss: 3.966581582717993\n",
      "[EPOCH #189, elapsed time: 2299.369[sec]] loss: 3.9671892221356844\n",
      "[EPOCH #190, elapsed time: 2311.801[sec]] loss: 3.966268574207621\n",
      "[EPOCH #191, elapsed time: 2324.320[sec]] loss: 3.962989675640221\n",
      "[EPOCH #192, elapsed time: 2335.726[sec]] loss: 3.9622156322574433\n",
      "[EPOCH #193, elapsed time: 2351.450[sec]] loss: 3.9627544706018782\n",
      "[EPOCH #194, elapsed time: 2363.015[sec]] loss: 3.961294556762344\n",
      "[EPOCH #195, elapsed time: 2375.329[sec]] loss: 3.9617214928966873\n",
      "[EPOCH #196, elapsed time: 2386.587[sec]] loss: 3.9590031597298534\n",
      "[EPOCH #197, elapsed time: 2398.398[sec]] loss: 3.9568670581566243\n",
      "[EPOCH #198, elapsed time: 2410.934[sec]] loss: 3.956544751169128\n",
      "[EPOCH #199, elapsed time: 2423.029[sec]] loss: 3.9563731579771426\n",
      "[EPOCH #200, elapsed time: 2435.216[sec]] loss: 3.9580023166695546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 09:59:45,627] Trial 13 finished with value: 0.77218 and parameters: {'learning_rate': 0.00023987726353680947}. Best is trial 13 with value: 0.77218.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605176959522855\n",
      "[EPOCH #1, elapsed time: 15.674[sec]] loss: 4.578300949860595\n",
      "[EPOCH #2, elapsed time: 31.974[sec]] loss: 4.541870157762895\n",
      "[EPOCH #3, elapsed time: 43.358[sec]] loss: 4.5197564742539225\n",
      "[EPOCH #4, elapsed time: 58.234[sec]] loss: 4.500330758110041\n",
      "[EPOCH #5, elapsed time: 70.496[sec]] loss: 4.481314224992436\n",
      "[EPOCH #6, elapsed time: 81.527[sec]] loss: 4.467416910963491\n",
      "[EPOCH #7, elapsed time: 92.957[sec]] loss: 4.456046316460471\n",
      "[EPOCH #8, elapsed time: 105.479[sec]] loss: 4.443030857383938\n",
      "[EPOCH #9, elapsed time: 117.538[sec]] loss: 4.432108532215492\n",
      "[EPOCH #10, elapsed time: 129.494[sec]] loss: 4.417282270759783\n",
      "[EPOCH #11, elapsed time: 140.835[sec]] loss: 4.402840870584499\n",
      "[EPOCH #12, elapsed time: 152.768[sec]] loss: 4.393563168367665\n",
      "[EPOCH #13, elapsed time: 164.622[sec]] loss: 4.3841080699146975\n",
      "[EPOCH #14, elapsed time: 176.159[sec]] loss: 4.377670946139521\n",
      "[EPOCH #15, elapsed time: 187.171[sec]] loss: 4.367548086516612\n",
      "[EPOCH #16, elapsed time: 198.717[sec]] loss: 4.357461878373237\n",
      "[EPOCH #17, elapsed time: 210.375[sec]] loss: 4.3529517148750685\n",
      "[EPOCH #18, elapsed time: 221.533[sec]] loss: 4.346574096289188\n",
      "[EPOCH #19, elapsed time: 233.672[sec]] loss: 4.336988122968131\n",
      "[EPOCH #20, elapsed time: 245.974[sec]] loss: 4.328830290969488\n",
      "[EPOCH #21, elapsed time: 257.388[sec]] loss: 4.324479648263044\n",
      "[EPOCH #22, elapsed time: 268.593[sec]] loss: 4.315763546119343\n",
      "[EPOCH #23, elapsed time: 279.865[sec]] loss: 4.309750137463336\n",
      "[EPOCH #24, elapsed time: 290.944[sec]] loss: 4.302813765488598\n",
      "[EPOCH #25, elapsed time: 301.879[sec]] loss: 4.298365845256178\n",
      "[EPOCH #26, elapsed time: 313.334[sec]] loss: 4.290943282732045\n",
      "[EPOCH #27, elapsed time: 324.506[sec]] loss: 4.285737637137268\n",
      "[EPOCH #28, elapsed time: 335.665[sec]] loss: 4.281726875292972\n",
      "[EPOCH #29, elapsed time: 347.208[sec]] loss: 4.2761496496902245\n",
      "[EPOCH #30, elapsed time: 358.241[sec]] loss: 4.273541803475915\n",
      "[EPOCH #31, elapsed time: 369.545[sec]] loss: 4.271425477526391\n",
      "[EPOCH #32, elapsed time: 380.907[sec]] loss: 4.260505146348774\n",
      "[EPOCH #33, elapsed time: 392.275[sec]] loss: 4.255112817481169\n",
      "[EPOCH #34, elapsed time: 404.219[sec]] loss: 4.250711283619711\n",
      "[EPOCH #35, elapsed time: 415.430[sec]] loss: 4.246570710409778\n",
      "[EPOCH #36, elapsed time: 426.591[sec]] loss: 4.24299266532073\n",
      "[EPOCH #37, elapsed time: 439.457[sec]] loss: 4.236252510463742\n",
      "[EPOCH #38, elapsed time: 455.127[sec]] loss: 4.236534763740105\n",
      "[EPOCH #39, elapsed time: 467.518[sec]] loss: 4.225195170821705\n",
      "[EPOCH #40, elapsed time: 483.245[sec]] loss: 4.224393600236889\n",
      "[EPOCH #41, elapsed time: 497.171[sec]] loss: 4.222168785291685\n",
      "[EPOCH #42, elapsed time: 511.124[sec]] loss: 4.222815978855028\n",
      "[EPOCH #43, elapsed time: 522.219[sec]] loss: 4.215456603050842\n",
      "[EPOCH #44, elapsed time: 538.545[sec]] loss: 4.211011894223634\n",
      "[EPOCH #45, elapsed time: 550.129[sec]] loss: 4.20927601232791\n",
      "[EPOCH #46, elapsed time: 562.873[sec]] loss: 4.20323463059814\n",
      "[EPOCH #47, elapsed time: 578.389[sec]] loss: 4.2033481661966805\n",
      "[EPOCH #48, elapsed time: 590.976[sec]] loss: 4.201334658907685\n",
      "[EPOCH #49, elapsed time: 603.307[sec]] loss: 4.198009952550048\n",
      "[EPOCH #50, elapsed time: 617.451[sec]] loss: 4.194945894138827\n",
      "[EPOCH #51, elapsed time: 628.913[sec]] loss: 4.190172922435817\n",
      "[EPOCH #52, elapsed time: 641.061[sec]] loss: 4.18944152425057\n",
      "[EPOCH #53, elapsed time: 652.051[sec]] loss: 4.18257050138975\n",
      "[EPOCH #54, elapsed time: 663.690[sec]] loss: 4.184148928177944\n",
      "[EPOCH #55, elapsed time: 675.231[sec]] loss: 4.178305320379754\n",
      "[EPOCH #56, elapsed time: 686.360[sec]] loss: 4.1739183646779905\n",
      "[EPOCH #57, elapsed time: 697.642[sec]] loss: 4.1725782585388105\n",
      "[EPOCH #58, elapsed time: 709.232[sec]] loss: 4.1686127403952415\n",
      "[EPOCH #59, elapsed time: 720.304[sec]] loss: 4.170585220621247\n",
      "[EPOCH #60, elapsed time: 731.503[sec]] loss: 4.1639962066691885\n",
      "[EPOCH #61, elapsed time: 742.898[sec]] loss: 4.160515497223506\n",
      "[EPOCH #62, elapsed time: 754.455[sec]] loss: 4.157445483991753\n",
      "[EPOCH #63, elapsed time: 765.970[sec]] loss: 4.1554679451123\n",
      "[EPOCH #64, elapsed time: 777.189[sec]] loss: 4.1530764485808875\n",
      "[EPOCH #65, elapsed time: 788.319[sec]] loss: 4.1505205007371275\n",
      "[EPOCH #66, elapsed time: 804.544[sec]] loss: 4.144645019898564\n",
      "[EPOCH #67, elapsed time: 816.974[sec]] loss: 4.1483832197317465\n",
      "[EPOCH #68, elapsed time: 829.407[sec]] loss: 4.141062215132662\n",
      "[EPOCH #69, elapsed time: 840.588[sec]] loss: 4.139564261555443\n",
      "[EPOCH #70, elapsed time: 851.777[sec]] loss: 4.136232003445665\n",
      "[EPOCH #71, elapsed time: 862.686[sec]] loss: 4.132824579874675\n",
      "[EPOCH #72, elapsed time: 874.940[sec]] loss: 4.132337242994107\n",
      "[EPOCH #73, elapsed time: 887.359[sec]] loss: 4.131579719898568\n",
      "[EPOCH #74, elapsed time: 901.767[sec]] loss: 4.130475782387087\n",
      "[EPOCH #75, elapsed time: 914.205[sec]] loss: 4.123558553761575\n",
      "[EPOCH #76, elapsed time: 925.549[sec]] loss: 4.1229759103124595\n",
      "[EPOCH #77, elapsed time: 936.829[sec]] loss: 4.120382518853733\n",
      "[EPOCH #78, elapsed time: 948.203[sec]] loss: 4.1151282285469435\n",
      "[EPOCH #79, elapsed time: 961.031[sec]] loss: 4.118284091382018\n",
      "[EPOCH #80, elapsed time: 972.148[sec]] loss: 4.11422359112052\n",
      "[EPOCH #81, elapsed time: 983.250[sec]] loss: 4.114209699691753\n",
      "[EPOCH #82, elapsed time: 996.192[sec]] loss: 4.115693866024396\n",
      "[EPOCH #83, elapsed time: 1008.123[sec]] loss: 4.109121788181103\n",
      "[EPOCH #84, elapsed time: 1019.768[sec]] loss: 4.103220560424083\n",
      "[EPOCH #85, elapsed time: 1032.361[sec]] loss: 4.101919250768953\n",
      "[EPOCH #86, elapsed time: 1043.474[sec]] loss: 4.104888138218866\n",
      "[EPOCH #87, elapsed time: 1054.513[sec]] loss: 4.101863192047587\n",
      "[EPOCH #88, elapsed time: 1065.772[sec]] loss: 4.097988562635787\n",
      "[EPOCH #89, elapsed time: 1080.590[sec]] loss: 4.094410267077572\n",
      "[EPOCH #90, elapsed time: 1092.039[sec]] loss: 4.098066569442407\n",
      "[EPOCH #91, elapsed time: 1106.172[sec]] loss: 4.0918179522396585\n",
      "[EPOCH #92, elapsed time: 1117.729[sec]] loss: 4.092117201877723\n",
      "[EPOCH #93, elapsed time: 1128.931[sec]] loss: 4.09066984169924\n",
      "[EPOCH #94, elapsed time: 1140.309[sec]] loss: 4.08713938483655\n",
      "[EPOCH #95, elapsed time: 1151.603[sec]] loss: 4.086387556634953\n",
      "[EPOCH #96, elapsed time: 1165.477[sec]] loss: 4.083841434900988\n",
      "[EPOCH #97, elapsed time: 1181.280[sec]] loss: 4.083526234556602\n",
      "[EPOCH #98, elapsed time: 1192.346[sec]] loss: 4.082581386913951\n",
      "[EPOCH #99, elapsed time: 1203.361[sec]] loss: 4.081273677176721\n",
      "[EPOCH #100, elapsed time: 1215.841[sec]] loss: 4.076184542150125\n",
      "[EPOCH #101, elapsed time: 1226.849[sec]] loss: 4.074171350159404\n",
      "[EPOCH #102, elapsed time: 1238.430[sec]] loss: 4.075007747093684\n",
      "[EPOCH #103, elapsed time: 1249.533[sec]] loss: 4.071873308982288\n",
      "[EPOCH #104, elapsed time: 1261.584[sec]] loss: 4.0706271675253864\n",
      "[EPOCH #105, elapsed time: 1274.023[sec]] loss: 4.071914772154503\n",
      "[EPOCH #106, elapsed time: 1285.202[sec]] loss: 4.067432086435709\n",
      "[EPOCH #107, elapsed time: 1296.617[sec]] loss: 4.063723128648881\n",
      "[EPOCH #108, elapsed time: 1307.975[sec]] loss: 4.064982837694086\n",
      "[EPOCH #109, elapsed time: 1319.254[sec]] loss: 4.061055631570456\n",
      "[EPOCH #110, elapsed time: 1330.566[sec]] loss: 4.062657280145207\n",
      "[EPOCH #111, elapsed time: 1342.069[sec]] loss: 4.0589904060595625\n",
      "[EPOCH #112, elapsed time: 1353.590[sec]] loss: 4.055631865771703\n",
      "[EPOCH #113, elapsed time: 1364.801[sec]] loss: 4.0560766771979155\n",
      "[EPOCH #114, elapsed time: 1376.242[sec]] loss: 4.05332563507656\n",
      "[EPOCH #115, elapsed time: 1387.390[sec]] loss: 4.0500328861134065\n",
      "[EPOCH #116, elapsed time: 1399.336[sec]] loss: 4.050867998211031\n",
      "[EPOCH #117, elapsed time: 1410.869[sec]] loss: 4.047499511460043\n",
      "[EPOCH #118, elapsed time: 1422.802[sec]] loss: 4.049956598269657\n",
      "[EPOCH #119, elapsed time: 1434.246[sec]] loss: 4.044500991158659\n",
      "[EPOCH #120, elapsed time: 1445.469[sec]] loss: 4.045626900398952\n",
      "[EPOCH #121, elapsed time: 1457.387[sec]] loss: 4.042104181996234\n",
      "[EPOCH #122, elapsed time: 1468.874[sec]] loss: 4.042939375549726\n",
      "[EPOCH #123, elapsed time: 1480.845[sec]] loss: 4.044126535483987\n",
      "[EPOCH #124, elapsed time: 1495.126[sec]] loss: 4.039766087413063\n",
      "[EPOCH #125, elapsed time: 1506.422[sec]] loss: 4.039009433180113\n",
      "[EPOCH #126, elapsed time: 1517.799[sec]] loss: 4.035574093890053\n",
      "[EPOCH #127, elapsed time: 1530.169[sec]] loss: 4.0362974110125585\n",
      "[EPOCH #128, elapsed time: 1542.411[sec]] loss: 4.03342534636963\n",
      "[EPOCH #129, elapsed time: 1554.904[sec]] loss: 4.0315535756463206\n",
      "[EPOCH #130, elapsed time: 1566.376[sec]] loss: 4.0319762103312\n",
      "[EPOCH #131, elapsed time: 1578.707[sec]] loss: 4.030375624191128\n",
      "[EPOCH #132, elapsed time: 1591.181[sec]] loss: 4.027619442265536\n",
      "[EPOCH #133, elapsed time: 1602.695[sec]] loss: 4.026644727776467\n",
      "[EPOCH #134, elapsed time: 1614.965[sec]] loss: 4.025687446215941\n",
      "[EPOCH #135, elapsed time: 1626.841[sec]] loss: 4.027565403009025\n",
      "[EPOCH #136, elapsed time: 1639.316[sec]] loss: 4.025352405571281\n",
      "[EPOCH #137, elapsed time: 1654.044[sec]] loss: 4.021277105358267\n",
      "[EPOCH #138, elapsed time: 1667.709[sec]] loss: 4.021378213140496\n",
      "[EPOCH #139, elapsed time: 1681.855[sec]] loss: 4.022661938853395\n",
      "[EPOCH #140, elapsed time: 1692.696[sec]] loss: 4.020070210985853\n",
      "[EPOCH #141, elapsed time: 1703.284[sec]] loss: 4.019441376873414\n",
      "[EPOCH #142, elapsed time: 1714.456[sec]] loss: 4.016897060744517\n",
      "[EPOCH #143, elapsed time: 1725.086[sec]] loss: 4.018528633520379\n",
      "[EPOCH #144, elapsed time: 1735.781[sec]] loss: 4.016732527061067\n",
      "[EPOCH #145, elapsed time: 1748.843[sec]] loss: 4.017736982399275\n",
      "[EPOCH #146, elapsed time: 1759.508[sec]] loss: 4.013337147365529\n",
      "[EPOCH #147, elapsed time: 1770.367[sec]] loss: 4.010781156810826\n",
      "[EPOCH #148, elapsed time: 1780.907[sec]] loss: 4.011411614396675\n",
      "[EPOCH #149, elapsed time: 1792.932[sec]] loss: 4.011885188438918\n",
      "[EPOCH #150, elapsed time: 1803.476[sec]] loss: 4.0095990193782525\n",
      "[EPOCH #151, elapsed time: 1814.509[sec]] loss: 4.008738596318856\n",
      "[EPOCH #152, elapsed time: 1826.673[sec]] loss: 4.008573942892229\n",
      "[EPOCH #153, elapsed time: 1837.373[sec]] loss: 4.006003931403084\n",
      "[EPOCH #154, elapsed time: 1850.593[sec]] loss: 4.0053364757689165\n",
      "[EPOCH #155, elapsed time: 1861.030[sec]] loss: 4.002761480980627\n",
      "[EPOCH #156, elapsed time: 1871.567[sec]] loss: 4.0035892829480115\n",
      "[EPOCH #157, elapsed time: 1882.299[sec]] loss: 4.000576149250404\n",
      "[EPOCH #158, elapsed time: 1892.951[sec]] loss: 3.9996636665866534\n",
      "[EPOCH #159, elapsed time: 1903.423[sec]] loss: 4.001341767137202\n",
      "[EPOCH #160, elapsed time: 1914.254[sec]] loss: 3.9973360805731133\n",
      "[EPOCH #161, elapsed time: 1926.554[sec]] loss: 3.996126374905489\n",
      "[EPOCH #162, elapsed time: 1937.070[sec]] loss: 3.9977903948788147\n",
      "[EPOCH #163, elapsed time: 1949.042[sec]] loss: 3.995426455904716\n",
      "[EPOCH #164, elapsed time: 1961.149[sec]] loss: 3.9923120952918603\n",
      "[EPOCH #165, elapsed time: 1973.411[sec]] loss: 3.9914117193496614\n",
      "[EPOCH #166, elapsed time: 1984.032[sec]] loss: 3.990534095678738\n",
      "[EPOCH #167, elapsed time: 1994.736[sec]] loss: 3.990266871925203\n",
      "[EPOCH #168, elapsed time: 2005.247[sec]] loss: 3.988069353695489\n",
      "[EPOCH #169, elapsed time: 2015.925[sec]] loss: 3.988491970838375\n",
      "[EPOCH #170, elapsed time: 2026.431[sec]] loss: 3.9858903537098596\n",
      "[EPOCH #171, elapsed time: 2036.967[sec]] loss: 3.9834557754140745\n",
      "[EPOCH #172, elapsed time: 2047.600[sec]] loss: 3.9848200451160802\n",
      "[EPOCH #173, elapsed time: 2057.984[sec]] loss: 3.9853806893793498\n",
      "[EPOCH #174, elapsed time: 2068.737[sec]] loss: 3.982454538345337\n",
      "[EPOCH #175, elapsed time: 2079.296[sec]] loss: 3.9841498269992077\n",
      "[EPOCH #176, elapsed time: 2091.361[sec]] loss: 3.9809317222712366\n",
      "[EPOCH #177, elapsed time: 2101.857[sec]] loss: 3.9796783789868395\n",
      "[EPOCH #178, elapsed time: 2112.570[sec]] loss: 3.9789054930705867\n",
      "[EPOCH #179, elapsed time: 2126.297[sec]] loss: 3.978346750359465\n",
      "[EPOCH #180, elapsed time: 2138.295[sec]] loss: 3.979471500760382\n",
      "[EPOCH #181, elapsed time: 2148.679[sec]] loss: 3.977293754340896\n",
      "[EPOCH #182, elapsed time: 2159.533[sec]] loss: 3.9783009625518466\n",
      "[EPOCH #183, elapsed time: 2170.906[sec]] loss: 3.978126416203309\n",
      "[EPOCH #184, elapsed time: 2181.525[sec]] loss: 3.980077110188021\n",
      "[EPOCH #185, elapsed time: 2192.467[sec]] loss: 3.9747685717987236\n",
      "[EPOCH #186, elapsed time: 2203.290[sec]] loss: 3.97267091022572\n",
      "[EPOCH #187, elapsed time: 2213.926[sec]] loss: 3.972864231083993\n",
      "[EPOCH #188, elapsed time: 2224.440[sec]] loss: 3.9730496363844234\n",
      "[EPOCH #189, elapsed time: 2235.641[sec]] loss: 3.970416147588387\n",
      "[EPOCH #190, elapsed time: 2247.274[sec]] loss: 3.971770089784648\n",
      "[EPOCH #191, elapsed time: 2261.460[sec]] loss: 3.9696269895659757\n",
      "[EPOCH #192, elapsed time: 2273.713[sec]] loss: 3.966544195237407\n",
      "[EPOCH #193, elapsed time: 2288.821[sec]] loss: 3.9660684549877145\n",
      "[EPOCH #194, elapsed time: 2301.744[sec]] loss: 3.9667587923988346\n",
      "[EPOCH #195, elapsed time: 2316.275[sec]] loss: 3.9688547409579913\n",
      "[EPOCH #196, elapsed time: 2329.934[sec]] loss: 3.9666962692238776\n",
      "[EPOCH #197, elapsed time: 2345.016[sec]] loss: 3.964411239019969\n",
      "[EPOCH #198, elapsed time: 2355.588[sec]] loss: 3.963893569896256\n",
      "[EPOCH #199, elapsed time: 2366.180[sec]] loss: 3.9623765365786072\n",
      "[EPOCH #200, elapsed time: 2376.693[sec]] loss: 3.9653709924762555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 10:39:36,013] Trial 14 finished with value: 0.77282 and parameters: {'learning_rate': 0.0003798483309943936}. Best is trial 14 with value: 0.77282.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605164511418846\n",
      "[EPOCH #1, elapsed time: 11.191[sec]] loss: 4.579440359724536\n",
      "[EPOCH #2, elapsed time: 24.701[sec]] loss: 4.559722755173422\n",
      "[EPOCH #3, elapsed time: 35.173[sec]] loss: 4.5473132252464366\n",
      "[EPOCH #4, elapsed time: 46.489[sec]] loss: 4.533581943902463\n",
      "[EPOCH #5, elapsed time: 57.068[sec]] loss: 4.52318201992501\n",
      "[EPOCH #6, elapsed time: 67.487[sec]] loss: 4.51277132882419\n",
      "[EPOCH #7, elapsed time: 78.110[sec]] loss: 4.5013693204234215\n",
      "[EPOCH #8, elapsed time: 88.811[sec]] loss: 4.494489167031003\n",
      "[EPOCH #9, elapsed time: 99.448[sec]] loss: 4.4910378620857925\n",
      "[EPOCH #10, elapsed time: 115.410[sec]] loss: 4.482921336144114\n",
      "[EPOCH #11, elapsed time: 126.410[sec]] loss: 4.474499221955517\n",
      "[EPOCH #12, elapsed time: 137.017[sec]] loss: 4.472537439142521\n",
      "[EPOCH #13, elapsed time: 147.542[sec]] loss: 4.465824329464083\n",
      "[EPOCH #14, elapsed time: 157.867[sec]] loss: 4.4599855411380664\n",
      "[EPOCH #15, elapsed time: 168.163[sec]] loss: 4.456829249210565\n",
      "[EPOCH #16, elapsed time: 178.949[sec]] loss: 4.451540416124457\n",
      "[EPOCH #17, elapsed time: 189.363[sec]] loss: 4.440434322552428\n",
      "[EPOCH #18, elapsed time: 201.511[sec]] loss: 4.437737409990717\n",
      "[EPOCH #19, elapsed time: 212.219[sec]] loss: 4.438222053183704\n",
      "[EPOCH #20, elapsed time: 222.894[sec]] loss: 4.429639494731803\n",
      "[EPOCH #21, elapsed time: 233.493[sec]] loss: 4.425415238278536\n",
      "[EPOCH #22, elapsed time: 246.020[sec]] loss: 4.418584262264591\n",
      "[EPOCH #23, elapsed time: 261.133[sec]] loss: 4.418091599177033\n",
      "[EPOCH #24, elapsed time: 273.252[sec]] loss: 4.4158849371448206\n",
      "[EPOCH #25, elapsed time: 283.825[sec]] loss: 4.405114203176663\n",
      "[EPOCH #26, elapsed time: 294.708[sec]] loss: 4.40006488847641\n",
      "[EPOCH #27, elapsed time: 305.874[sec]] loss: 4.396915584669156\n",
      "[EPOCH #28, elapsed time: 316.509[sec]] loss: 4.391309532231424\n",
      "[EPOCH #29, elapsed time: 329.414[sec]] loss: 4.387183563722988\n",
      "[EPOCH #30, elapsed time: 343.500[sec]] loss: 4.381518277920597\n",
      "[EPOCH #31, elapsed time: 356.029[sec]] loss: 4.378211325281184\n",
      "[EPOCH #32, elapsed time: 368.651[sec]] loss: 4.378414032936706\n",
      "[EPOCH #33, elapsed time: 380.386[sec]] loss: 4.36738604669455\n",
      "[EPOCH #34, elapsed time: 392.802[sec]] loss: 4.360342563609626\n",
      "[EPOCH #35, elapsed time: 404.204[sec]] loss: 4.360656127209703\n",
      "[EPOCH #36, elapsed time: 415.749[sec]] loss: 4.354729773825891\n",
      "[EPOCH #37, elapsed time: 427.258[sec]] loss: 4.348242402152991\n",
      "[EPOCH #38, elapsed time: 440.378[sec]] loss: 4.345240949592908\n",
      "[EPOCH #39, elapsed time: 452.888[sec]] loss: 4.337307885146187\n",
      "[EPOCH #40, elapsed time: 465.352[sec]] loss: 4.329747762576327\n",
      "[EPOCH #41, elapsed time: 477.614[sec]] loss: 4.330696654182478\n",
      "[EPOCH #42, elapsed time: 489.117[sec]] loss: 4.3242339990265615\n",
      "[EPOCH #43, elapsed time: 500.586[sec]] loss: 4.318065773883998\n",
      "[EPOCH #44, elapsed time: 512.100[sec]] loss: 4.316152434431431\n",
      "[EPOCH #45, elapsed time: 526.073[sec]] loss: 4.309379027580803\n",
      "[EPOCH #46, elapsed time: 537.296[sec]] loss: 4.302441771489569\n",
      "[EPOCH #47, elapsed time: 549.809[sec]] loss: 4.302312584618918\n",
      "[EPOCH #48, elapsed time: 565.299[sec]] loss: 4.301534939788506\n",
      "[EPOCH #49, elapsed time: 577.153[sec]] loss: 4.295864429522689\n",
      "[EPOCH #50, elapsed time: 589.591[sec]] loss: 4.296860315978184\n",
      "[EPOCH #51, elapsed time: 601.527[sec]] loss: 4.289707644810985\n",
      "[EPOCH #52, elapsed time: 613.805[sec]] loss: 4.2861161137420405\n",
      "[EPOCH #53, elapsed time: 625.844[sec]] loss: 4.2863355631715425\n",
      "[EPOCH #54, elapsed time: 636.832[sec]] loss: 4.287050444425411\n",
      "[EPOCH #55, elapsed time: 648.278[sec]] loss: 4.279511911779051\n",
      "[EPOCH #56, elapsed time: 659.647[sec]] loss: 4.27896758767175\n",
      "[EPOCH #57, elapsed time: 670.750[sec]] loss: 4.277875131166523\n",
      "[EPOCH #58, elapsed time: 681.951[sec]] loss: 4.268781374451143\n",
      "[EPOCH #59, elapsed time: 693.185[sec]] loss: 4.266935173700959\n",
      "[EPOCH #60, elapsed time: 704.317[sec]] loss: 4.26119621243907\n",
      "[EPOCH #61, elapsed time: 715.505[sec]] loss: 4.258714473636503\n",
      "[EPOCH #62, elapsed time: 727.170[sec]] loss: 4.2553077845411735\n",
      "[EPOCH #63, elapsed time: 741.501[sec]] loss: 4.2560685307874335\n",
      "[EPOCH #64, elapsed time: 754.668[sec]] loss: 4.253530694671113\n",
      "[EPOCH #65, elapsed time: 768.831[sec]] loss: 4.249379221781354\n",
      "[EPOCH #66, elapsed time: 780.394[sec]] loss: 4.247308054415751\n",
      "[EPOCH #67, elapsed time: 793.339[sec]] loss: 4.2517434916127135\n",
      "[EPOCH #68, elapsed time: 806.801[sec]] loss: 4.250138281555566\n",
      "[EPOCH #69, elapsed time: 819.011[sec]] loss: 4.24143633747894\n",
      "[EPOCH #70, elapsed time: 830.324[sec]] loss: 4.237212564422012\n",
      "[EPOCH #71, elapsed time: 844.181[sec]] loss: 4.2325609804191275\n",
      "[EPOCH #72, elapsed time: 859.587[sec]] loss: 4.235664964103576\n",
      "[EPOCH #73, elapsed time: 875.059[sec]] loss: 4.227628029697001\n",
      "[EPOCH #74, elapsed time: 886.886[sec]] loss: 4.225066585717717\n",
      "[EPOCH #75, elapsed time: 901.125[sec]] loss: 4.231369257583423\n",
      "[EPOCH #76, elapsed time: 914.541[sec]] loss: 4.225035274936386\n",
      "[EPOCH #77, elapsed time: 926.621[sec]] loss: 4.220379009204115\n",
      "[EPOCH #78, elapsed time: 938.018[sec]] loss: 4.217920493255879\n",
      "[EPOCH #79, elapsed time: 950.209[sec]] loss: 4.213689458881214\n",
      "[EPOCH #80, elapsed time: 961.593[sec]] loss: 4.21396092688206\n",
      "[EPOCH #81, elapsed time: 977.966[sec]] loss: 4.2122663429587295\n",
      "[EPOCH #82, elapsed time: 990.288[sec]] loss: 4.2086072584336485\n",
      "[EPOCH #83, elapsed time: 1002.130[sec]] loss: 4.2059508884708166\n",
      "[EPOCH #84, elapsed time: 1014.158[sec]] loss: 4.208713204297818\n",
      "[EPOCH #85, elapsed time: 1026.144[sec]] loss: 4.202296493302075\n",
      "[EPOCH #86, elapsed time: 1037.538[sec]] loss: 4.202608940621179\n",
      "[EPOCH #87, elapsed time: 1049.099[sec]] loss: 4.19949497432184\n",
      "[EPOCH #88, elapsed time: 1062.256[sec]] loss: 4.1937079513522955\n",
      "[EPOCH #89, elapsed time: 1073.752[sec]] loss: 4.192356584778369\n",
      "[EPOCH #90, elapsed time: 1085.017[sec]] loss: 4.187264674455785\n",
      "[EPOCH #91, elapsed time: 1096.513[sec]] loss: 4.183454995725831\n",
      "[EPOCH #92, elapsed time: 1107.871[sec]] loss: 4.18523632480941\n",
      "[EPOCH #93, elapsed time: 1123.689[sec]] loss: 4.184430879274394\n",
      "[EPOCH #94, elapsed time: 1137.828[sec]] loss: 4.178700692517896\n",
      "[EPOCH #95, elapsed time: 1151.903[sec]] loss: 4.170879914222127\n",
      "[EPOCH #96, elapsed time: 1163.247[sec]] loss: 4.173064695438817\n",
      "[EPOCH #97, elapsed time: 1175.905[sec]] loss: 4.170592032101241\n",
      "[EPOCH #98, elapsed time: 1187.921[sec]] loss: 4.170564853450998\n",
      "[EPOCH #99, elapsed time: 1199.847[sec]] loss: 4.167842116633517\n",
      "[EPOCH #100, elapsed time: 1211.798[sec]] loss: 4.164772681372332\n",
      "[EPOCH #101, elapsed time: 1223.649[sec]] loss: 4.163097760498867\n",
      "[EPOCH #102, elapsed time: 1239.364[sec]] loss: 4.161491617055101\n",
      "[EPOCH #103, elapsed time: 1255.380[sec]] loss: 4.160387050625001\n",
      "[EPOCH #104, elapsed time: 1270.660[sec]] loss: 4.157789131348818\n",
      "[EPOCH #105, elapsed time: 1283.016[sec]] loss: 4.157357060444027\n",
      "[EPOCH #106, elapsed time: 1295.389[sec]] loss: 4.153874346634858\n",
      "[EPOCH #107, elapsed time: 1307.856[sec]] loss: 4.15075683273418\n",
      "[EPOCH #108, elapsed time: 1320.217[sec]] loss: 4.1579744434478725\n",
      "[EPOCH #109, elapsed time: 1332.892[sec]] loss: 4.148951903109511\n",
      "[EPOCH #110, elapsed time: 1344.614[sec]] loss: 4.1527026384515935\n",
      "[EPOCH #111, elapsed time: 1360.426[sec]] loss: 4.146123794127334\n",
      "[EPOCH #112, elapsed time: 1374.458[sec]] loss: 4.1451908672611\n",
      "[EPOCH #113, elapsed time: 1386.745[sec]] loss: 4.145982382774963\n",
      "[EPOCH #114, elapsed time: 1400.452[sec]] loss: 4.14197061615576\n",
      "[EPOCH #115, elapsed time: 1413.405[sec]] loss: 4.138083921360496\n",
      "[EPOCH #116, elapsed time: 1425.634[sec]] loss: 4.137522891478437\n",
      "[EPOCH #117, elapsed time: 1438.239[sec]] loss: 4.132093029760506\n",
      "[EPOCH #118, elapsed time: 1450.843[sec]] loss: 4.131604473330009\n",
      "[EPOCH #119, elapsed time: 1463.221[sec]] loss: 4.129033191495421\n",
      "[EPOCH #120, elapsed time: 1480.549[sec]] loss: 4.130137289172933\n",
      "[EPOCH #121, elapsed time: 1497.019[sec]] loss: 4.130146787323711\n",
      "[EPOCH #122, elapsed time: 1512.292[sec]] loss: 4.132834111278971\n",
      "[EPOCH #123, elapsed time: 1524.888[sec]] loss: 4.130754824105701\n",
      "[EPOCH #124, elapsed time: 1537.384[sec]] loss: 4.124430788226869\n",
      "[EPOCH #125, elapsed time: 1550.860[sec]] loss: 4.123489729959997\n",
      "[EPOCH #126, elapsed time: 1563.576[sec]] loss: 4.120792509879505\n",
      "[EPOCH #127, elapsed time: 1576.342[sec]] loss: 4.120226138994164\n",
      "[EPOCH #128, elapsed time: 1588.843[sec]] loss: 4.1191247888505265\n",
      "[EPOCH #129, elapsed time: 1600.692[sec]] loss: 4.117743735273756\n",
      "[EPOCH #130, elapsed time: 1613.052[sec]] loss: 4.1177325573626495\n",
      "[EPOCH #131, elapsed time: 1629.190[sec]] loss: 4.112393471039951\n",
      "[EPOCH #132, elapsed time: 1644.772[sec]] loss: 4.11248730987749\n",
      "[EPOCH #133, elapsed time: 1658.646[sec]] loss: 4.111080155796678\n",
      "[EPOCH #134, elapsed time: 1671.224[sec]] loss: 4.111662792991692\n",
      "[EPOCH #135, elapsed time: 1685.080[sec]] loss: 4.109759295970602\n",
      "[EPOCH #136, elapsed time: 1700.032[sec]] loss: 4.109671037775236\n",
      "[EPOCH #137, elapsed time: 1711.830[sec]] loss: 4.108661164248973\n",
      "[EPOCH #138, elapsed time: 1724.329[sec]] loss: 4.104275704497949\n",
      "[EPOCH #139, elapsed time: 1736.784[sec]] loss: 4.1067271845812074\n",
      "[EPOCH #140, elapsed time: 1749.292[sec]] loss: 4.1026549973857\n",
      "[EPOCH #141, elapsed time: 1761.369[sec]] loss: 4.09801761111005\n",
      "[EPOCH #142, elapsed time: 1773.658[sec]] loss: 4.098533278310901\n",
      "[EPOCH #143, elapsed time: 1789.074[sec]] loss: 4.101015334547291\n",
      "[EPOCH #144, elapsed time: 1803.872[sec]] loss: 4.098231021364911\n",
      "[EPOCH #145, elapsed time: 1819.209[sec]] loss: 4.104424661958515\n",
      "[EPOCH #146, elapsed time: 1831.538[sec]] loss: 4.096961748424587\n",
      "[EPOCH #147, elapsed time: 1847.252[sec]] loss: 4.0967185718465595\n",
      "[EPOCH #148, elapsed time: 1862.805[sec]] loss: 4.091323710067564\n",
      "[EPOCH #149, elapsed time: 1875.393[sec]] loss: 4.096060058968386\n",
      "[EPOCH #150, elapsed time: 1891.143[sec]] loss: 4.094752771611864\n",
      "[EPOCH #151, elapsed time: 1903.308[sec]] loss: 4.097612456488289\n",
      "[EPOCH #152, elapsed time: 1916.166[sec]] loss: 4.09716323485225\n",
      "[EPOCH #153, elapsed time: 1927.992[sec]] loss: 4.092379194913731\n",
      "[EPOCH #154, elapsed time: 1940.204[sec]] loss: 4.086992862967445\n",
      "[EPOCH #155, elapsed time: 1952.449[sec]] loss: 4.089180023641214\n",
      "[EPOCH #156, elapsed time: 1968.126[sec]] loss: 4.084812246982821\n",
      "[EPOCH #157, elapsed time: 1982.068[sec]] loss: 4.084596015975327\n",
      "[EPOCH #158, elapsed time: 1994.801[sec]] loss: 4.083328918089717\n",
      "[EPOCH #159, elapsed time: 2007.404[sec]] loss: 4.083533880272815\n",
      "[EPOCH #160, elapsed time: 2019.851[sec]] loss: 4.078360557403613\n",
      "[EPOCH #161, elapsed time: 2032.600[sec]] loss: 4.078756681864489\n",
      "[EPOCH #162, elapsed time: 2045.313[sec]] loss: 4.079264532810438\n",
      "[EPOCH #163, elapsed time: 2057.678[sec]] loss: 4.077886788721505\n",
      "[EPOCH #164, elapsed time: 2069.736[sec]] loss: 4.0795629852792805\n",
      "[EPOCH #165, elapsed time: 2082.157[sec]] loss: 4.079397054642953\n",
      "[EPOCH #166, elapsed time: 2094.609[sec]] loss: 4.0756919149092505\n",
      "[EPOCH #167, elapsed time: 2107.370[sec]] loss: 4.078678986237588\n",
      "[EPOCH #168, elapsed time: 2123.674[sec]] loss: 4.077704005262749\n",
      "[EPOCH #169, elapsed time: 2135.786[sec]] loss: 4.0799754819119505\n",
      "[EPOCH #170, elapsed time: 2148.245[sec]] loss: 4.070245759882228\n",
      "[EPOCH #171, elapsed time: 2160.161[sec]] loss: 4.0719682502197445\n",
      "[EPOCH #172, elapsed time: 2172.911[sec]] loss: 4.067456898899774\n",
      "[EPOCH #173, elapsed time: 2184.840[sec]] loss: 4.069437526390481\n",
      "[EPOCH #174, elapsed time: 2196.700[sec]] loss: 4.067379185578339\n",
      "[EPOCH #175, elapsed time: 2209.044[sec]] loss: 4.071291657952414\n",
      "[EPOCH #176, elapsed time: 2221.056[sec]] loss: 4.076153155251794\n",
      "[EPOCH #177, elapsed time: 2234.470[sec]] loss: 4.070295282609174\n",
      "[EPOCH #178, elapsed time: 2246.082[sec]] loss: 4.067414177585243\n",
      "[EPOCH #179, elapsed time: 2262.103[sec]] loss: 4.067066215660354\n",
      "[EPOCH #180, elapsed time: 2275.321[sec]] loss: 4.069622288738698\n",
      "[EPOCH #181, elapsed time: 2289.083[sec]] loss: 4.064418231990958\n",
      "[EPOCH #182, elapsed time: 2300.831[sec]] loss: 4.066645422579764\n",
      "[EPOCH #183, elapsed time: 2312.944[sec]] loss: 4.06284361654417\n",
      "[EPOCH #184, elapsed time: 2329.848[sec]] loss: 4.066790750678045\n",
      "[EPOCH #185, elapsed time: 2342.062[sec]] loss: 4.062445359739522\n",
      "[EPOCH #186, elapsed time: 2353.948[sec]] loss: 4.061694348537228\n",
      "[EPOCH #187, elapsed time: 2366.068[sec]] loss: 4.059658508306883\n",
      "[EPOCH #188, elapsed time: 2377.621[sec]] loss: 4.056595431637169\n",
      "[EPOCH #189, elapsed time: 2389.607[sec]] loss: 4.0593294224827545\n",
      "[EPOCH #190, elapsed time: 2401.778[sec]] loss: 4.0617742022870065\n",
      "[EPOCH #191, elapsed time: 2415.703[sec]] loss: 4.059110823000995\n",
      "[EPOCH #192, elapsed time: 2427.298[sec]] loss: 4.056401069089532\n",
      "[EPOCH #193, elapsed time: 2439.698[sec]] loss: 4.052095138332589\n",
      "[EPOCH #194, elapsed time: 2452.378[sec]] loss: 4.052003829813278\n",
      "[EPOCH #195, elapsed time: 2464.679[sec]] loss: 4.051521676820742\n",
      "[EPOCH #196, elapsed time: 2477.537[sec]] loss: 4.054487008737282\n",
      "[EPOCH #197, elapsed time: 2489.507[sec]] loss: 4.0516361493143\n",
      "[EPOCH #198, elapsed time: 2501.629[sec]] loss: 4.051295675990373\n",
      "[EPOCH #199, elapsed time: 2513.353[sec]] loss: 4.054286081212801\n",
      "[EPOCH #200, elapsed time: 2525.010[sec]] loss: 4.053483344466733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 11:21:55,734] Trial 15 finished with value: 0.67776 and parameters: {'learning_rate': 0.0008109213652090158}. Best is trial 14 with value: 0.77282.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605189525387033\n",
      "[EPOCH #1, elapsed time: 12.078[sec]] loss: 4.576528242285711\n",
      "[EPOCH #2, elapsed time: 24.278[sec]] loss: 4.541121554847566\n",
      "[EPOCH #3, elapsed time: 35.878[sec]] loss: 4.522682567933241\n",
      "[EPOCH #4, elapsed time: 47.788[sec]] loss: 4.5052506470634475\n",
      "[EPOCH #5, elapsed time: 59.191[sec]] loss: 4.488712787628174\n",
      "[EPOCH #6, elapsed time: 70.591[sec]] loss: 4.471219079889552\n",
      "[EPOCH #7, elapsed time: 82.697[sec]] loss: 4.454565365499056\n",
      "[EPOCH #8, elapsed time: 94.841[sec]] loss: 4.443526053230349\n",
      "[EPOCH #9, elapsed time: 106.312[sec]] loss: 4.428443250027667\n",
      "[EPOCH #10, elapsed time: 119.264[sec]] loss: 4.419987989097395\n",
      "[EPOCH #11, elapsed time: 134.812[sec]] loss: 4.40763879935862\n",
      "[EPOCH #12, elapsed time: 147.546[sec]] loss: 4.396275944383344\n",
      "[EPOCH #13, elapsed time: 163.005[sec]] loss: 4.384058435224068\n",
      "[EPOCH #14, elapsed time: 178.404[sec]] loss: 4.374720594780764\n",
      "[EPOCH #15, elapsed time: 194.370[sec]] loss: 4.3650861387441955\n",
      "[EPOCH #16, elapsed time: 210.376[sec]] loss: 4.356979835056298\n",
      "[EPOCH #17, elapsed time: 222.732[sec]] loss: 4.347224158349895\n",
      "[EPOCH #18, elapsed time: 234.729[sec]] loss: 4.34582146237618\n",
      "[EPOCH #19, elapsed time: 246.923[sec]] loss: 4.3407278649713925\n",
      "[EPOCH #20, elapsed time: 258.401[sec]] loss: 4.3332099136601485\n",
      "[EPOCH #21, elapsed time: 270.506[sec]] loss: 4.328366300652443\n",
      "[EPOCH #22, elapsed time: 283.708[sec]] loss: 4.321010872712138\n",
      "[EPOCH #23, elapsed time: 296.621[sec]] loss: 4.318363674161378\n",
      "[EPOCH #24, elapsed time: 308.464[sec]] loss: 4.306463932579172\n",
      "[EPOCH #25, elapsed time: 320.431[sec]] loss: 4.306139326827768\n",
      "[EPOCH #26, elapsed time: 332.493[sec]] loss: 4.299499972234226\n",
      "[EPOCH #27, elapsed time: 345.337[sec]] loss: 4.293423369536397\n",
      "[EPOCH #28, elapsed time: 357.856[sec]] loss: 4.285577337633549\n",
      "[EPOCH #29, elapsed time: 371.036[sec]] loss: 4.283295380634447\n",
      "[EPOCH #30, elapsed time: 382.681[sec]] loss: 4.2812162964906895\n",
      "[EPOCH #31, elapsed time: 394.072[sec]] loss: 4.270473846318397\n",
      "[EPOCH #32, elapsed time: 405.582[sec]] loss: 4.271571281699134\n",
      "[EPOCH #33, elapsed time: 417.812[sec]] loss: 4.264640519043915\n",
      "[EPOCH #34, elapsed time: 429.742[sec]] loss: 4.263067089893538\n",
      "[EPOCH #35, elapsed time: 441.257[sec]] loss: 4.252984950730073\n",
      "[EPOCH #36, elapsed time: 452.988[sec]] loss: 4.248984176686995\n",
      "[EPOCH #37, elapsed time: 464.655[sec]] loss: 4.245726577761229\n",
      "[EPOCH #38, elapsed time: 476.160[sec]] loss: 4.2408326889411505\n",
      "[EPOCH #39, elapsed time: 487.911[sec]] loss: 4.241210505509331\n",
      "[EPOCH #40, elapsed time: 500.069[sec]] loss: 4.233769565534683\n",
      "[EPOCH #41, elapsed time: 516.916[sec]] loss: 4.233897579227284\n",
      "[EPOCH #42, elapsed time: 528.576[sec]] loss: 4.22982991359513\n",
      "[EPOCH #43, elapsed time: 541.828[sec]] loss: 4.229670867810094\n",
      "[EPOCH #44, elapsed time: 554.110[sec]] loss: 4.220941190908753\n",
      "[EPOCH #45, elapsed time: 565.818[sec]] loss: 4.221027452672664\n",
      "[EPOCH #46, elapsed time: 580.875[sec]] loss: 4.220544515324188\n",
      "[EPOCH #47, elapsed time: 593.161[sec]] loss: 4.218454601973657\n",
      "[EPOCH #48, elapsed time: 605.054[sec]] loss: 4.209410770993467\n",
      "[EPOCH #49, elapsed time: 616.614[sec]] loss: 4.208059756937351\n",
      "[EPOCH #50, elapsed time: 628.200[sec]] loss: 4.207194135193633\n",
      "[EPOCH #51, elapsed time: 644.219[sec]] loss: 4.201315478338924\n",
      "[EPOCH #52, elapsed time: 655.822[sec]] loss: 4.1939981596941225\n",
      "[EPOCH #53, elapsed time: 668.010[sec]] loss: 4.198056828723988\n",
      "[EPOCH #54, elapsed time: 680.290[sec]] loss: 4.197139128155992\n",
      "[EPOCH #55, elapsed time: 692.052[sec]] loss: 4.191109200280519\n",
      "[EPOCH #56, elapsed time: 704.497[sec]] loss: 4.186197642859021\n",
      "[EPOCH #57, elapsed time: 716.183[sec]] loss: 4.186215263410631\n",
      "[EPOCH #58, elapsed time: 729.579[sec]] loss: 4.187015283130639\n",
      "[EPOCH #59, elapsed time: 741.185[sec]] loss: 4.1832180110140635\n",
      "[EPOCH #60, elapsed time: 752.638[sec]] loss: 4.180665875579482\n",
      "[EPOCH #61, elapsed time: 764.037[sec]] loss: 4.175824090142473\n",
      "[EPOCH #62, elapsed time: 775.535[sec]] loss: 4.173660668210196\n",
      "[EPOCH #63, elapsed time: 787.507[sec]] loss: 4.1679274393668635\n",
      "[EPOCH #64, elapsed time: 799.440[sec]] loss: 4.164899323586082\n",
      "[EPOCH #65, elapsed time: 811.288[sec]] loss: 4.163766039996595\n",
      "[EPOCH #66, elapsed time: 823.144[sec]] loss: 4.159656491099606\n",
      "[EPOCH #67, elapsed time: 835.514[sec]] loss: 4.162700392997044\n",
      "[EPOCH #68, elapsed time: 849.695[sec]] loss: 4.15706694820182\n",
      "[EPOCH #69, elapsed time: 861.915[sec]] loss: 4.1541194729673805\n",
      "[EPOCH #70, elapsed time: 876.305[sec]] loss: 4.151565882767612\n",
      "[EPOCH #71, elapsed time: 889.939[sec]] loss: 4.150996091803601\n",
      "[EPOCH #72, elapsed time: 903.661[sec]] loss: 4.149233178610384\n",
      "[EPOCH #73, elapsed time: 917.450[sec]] loss: 4.1470087807070195\n",
      "[EPOCH #74, elapsed time: 931.978[sec]] loss: 4.144607072447022\n",
      "[EPOCH #75, elapsed time: 943.624[sec]] loss: 4.140288893869879\n",
      "[EPOCH #76, elapsed time: 955.296[sec]] loss: 4.140415305749621\n",
      "[EPOCH #77, elapsed time: 966.936[sec]] loss: 4.143376875747417\n",
      "[EPOCH #78, elapsed time: 978.345[sec]] loss: 4.136023786536257\n",
      "[EPOCH #79, elapsed time: 990.153[sec]] loss: 4.131686600064591\n",
      "[EPOCH #80, elapsed time: 1002.350[sec]] loss: 4.1285701412767155\n",
      "[EPOCH #81, elapsed time: 1014.816[sec]] loss: 4.129207387004079\n",
      "[EPOCH #82, elapsed time: 1029.504[sec]] loss: 4.128789225832743\n",
      "[EPOCH #83, elapsed time: 1040.909[sec]] loss: 4.126494669105789\n",
      "[EPOCH #84, elapsed time: 1052.761[sec]] loss: 4.122341374746897\n",
      "[EPOCH #85, elapsed time: 1064.566[sec]] loss: 4.120214206472239\n",
      "[EPOCH #86, elapsed time: 1076.906[sec]] loss: 4.118242346317587\n",
      "[EPOCH #87, elapsed time: 1088.710[sec]] loss: 4.112626373653448\n",
      "[EPOCH #88, elapsed time: 1100.643[sec]] loss: 4.1176123408575664\n",
      "[EPOCH #89, elapsed time: 1113.939[sec]] loss: 4.1129685630420045\n",
      "[EPOCH #90, elapsed time: 1126.234[sec]] loss: 4.110930007768608\n",
      "[EPOCH #91, elapsed time: 1138.377[sec]] loss: 4.10461857482095\n",
      "[EPOCH #92, elapsed time: 1149.818[sec]] loss: 4.10551561305558\n",
      "[EPOCH #93, elapsed time: 1162.862[sec]] loss: 4.104838612898755\n",
      "[EPOCH #94, elapsed time: 1174.440[sec]] loss: 4.100486985705102\n",
      "[EPOCH #95, elapsed time: 1187.119[sec]] loss: 4.101082198831872\n",
      "[EPOCH #96, elapsed time: 1203.707[sec]] loss: 4.097149019430481\n",
      "[EPOCH #97, elapsed time: 1217.903[sec]] loss: 4.094245581922818\n",
      "[EPOCH #98, elapsed time: 1233.807[sec]] loss: 4.094385702947127\n",
      "[EPOCH #99, elapsed time: 1245.926[sec]] loss: 4.093305726274037\n",
      "[EPOCH #100, elapsed time: 1257.846[sec]] loss: 4.0910680901142396\n",
      "[EPOCH #101, elapsed time: 1269.866[sec]] loss: 4.087925847951068\n",
      "[EPOCH #102, elapsed time: 1281.251[sec]] loss: 4.086968428190137\n",
      "[EPOCH #103, elapsed time: 1297.349[sec]] loss: 4.084145810919852\n",
      "[EPOCH #104, elapsed time: 1313.576[sec]] loss: 4.082281061570307\n",
      "[EPOCH #105, elapsed time: 1325.761[sec]] loss: 4.0817482382993395\n",
      "[EPOCH #106, elapsed time: 1338.003[sec]] loss: 4.0799477153760995\n",
      "[EPOCH #107, elapsed time: 1350.399[sec]] loss: 4.080405869242936\n",
      "[EPOCH #108, elapsed time: 1363.606[sec]] loss: 4.074260913479122\n",
      "[EPOCH #109, elapsed time: 1375.038[sec]] loss: 4.071818120648902\n",
      "[EPOCH #110, elapsed time: 1387.028[sec]] loss: 4.073117483600316\n",
      "[EPOCH #111, elapsed time: 1398.581[sec]] loss: 4.072202131981585\n",
      "[EPOCH #112, elapsed time: 1410.414[sec]] loss: 4.070590828140806\n",
      "[EPOCH #113, elapsed time: 1422.170[sec]] loss: 4.066874210299091\n",
      "[EPOCH #114, elapsed time: 1435.263[sec]] loss: 4.062361001510767\n",
      "[EPOCH #115, elapsed time: 1447.667[sec]] loss: 4.064269237768475\n",
      "[EPOCH #116, elapsed time: 1461.320[sec]] loss: 4.059658520204931\n",
      "[EPOCH #117, elapsed time: 1476.131[sec]] loss: 4.056254680997198\n",
      "[EPOCH #118, elapsed time: 1489.661[sec]] loss: 4.056700043394561\n",
      "[EPOCH #119, elapsed time: 1503.565[sec]] loss: 4.054611166089449\n",
      "[EPOCH #120, elapsed time: 1516.791[sec]] loss: 4.05351448394668\n",
      "[EPOCH #121, elapsed time: 1531.109[sec]] loss: 4.051406458868709\n",
      "[EPOCH #122, elapsed time: 1544.507[sec]] loss: 4.049202539946738\n",
      "[EPOCH #123, elapsed time: 1556.625[sec]] loss: 4.045817851257568\n",
      "[EPOCH #124, elapsed time: 1569.068[sec]] loss: 4.045257726084782\n",
      "[EPOCH #125, elapsed time: 1581.019[sec]] loss: 4.048532524096683\n",
      "[EPOCH #126, elapsed time: 1592.421[sec]] loss: 4.042693747668714\n",
      "[EPOCH #127, elapsed time: 1604.333[sec]] loss: 4.0446013248050665\n",
      "[EPOCH #128, elapsed time: 1616.478[sec]] loss: 4.03952819356839\n",
      "[EPOCH #129, elapsed time: 1627.914[sec]] loss: 4.042415957380698\n",
      "[EPOCH #130, elapsed time: 1639.373[sec]] loss: 4.037918681299084\n",
      "[EPOCH #131, elapsed time: 1651.464[sec]] loss: 4.035904815238177\n",
      "[EPOCH #132, elapsed time: 1666.841[sec]] loss: 4.033412705151148\n",
      "[EPOCH #133, elapsed time: 1679.898[sec]] loss: 4.033689972840283\n",
      "[EPOCH #134, elapsed time: 1693.205[sec]] loss: 4.031881814726026\n",
      "[EPOCH #135, elapsed time: 1705.132[sec]] loss: 4.029993151062509\n",
      "[EPOCH #136, elapsed time: 1720.952[sec]] loss: 4.029403972686748\n",
      "[EPOCH #137, elapsed time: 1735.741[sec]] loss: 4.0262518446184625\n",
      "[EPOCH #138, elapsed time: 1747.308[sec]] loss: 4.025447192286652\n",
      "[EPOCH #139, elapsed time: 1758.747[sec]] loss: 4.024050312475447\n",
      "[EPOCH #140, elapsed time: 1770.213[sec]] loss: 4.022884883105717\n",
      "[EPOCH #141, elapsed time: 1781.782[sec]] loss: 4.020489955360281\n",
      "[EPOCH #142, elapsed time: 1793.813[sec]] loss: 4.022170005817864\n",
      "[EPOCH #143, elapsed time: 1805.455[sec]] loss: 4.022611877510964\n",
      "[EPOCH #144, elapsed time: 1817.080[sec]] loss: 4.019652664699542\n",
      "[EPOCH #145, elapsed time: 1828.709[sec]] loss: 4.017110662435921\n",
      "[EPOCH #146, elapsed time: 1840.539[sec]] loss: 4.0154474309370904\n",
      "[EPOCH #147, elapsed time: 1852.970[sec]] loss: 4.015700258052433\n",
      "[EPOCH #148, elapsed time: 1864.907[sec]] loss: 4.011720049938024\n",
      "[EPOCH #149, elapsed time: 1877.612[sec]] loss: 4.01291290095275\n",
      "[EPOCH #150, elapsed time: 1889.045[sec]] loss: 4.010381446003685\n",
      "[EPOCH #151, elapsed time: 1903.453[sec]] loss: 4.01165486724424\n",
      "[EPOCH #152, elapsed time: 1916.074[sec]] loss: 4.0077371604917\n",
      "[EPOCH #153, elapsed time: 1928.562[sec]] loss: 4.006948276276933\n",
      "[EPOCH #154, elapsed time: 1939.981[sec]] loss: 4.003805936946368\n",
      "[EPOCH #155, elapsed time: 1956.234[sec]] loss: 4.004068015709338\n",
      "[EPOCH #156, elapsed time: 1967.595[sec]] loss: 4.002959355282921\n",
      "[EPOCH #157, elapsed time: 1979.124[sec]] loss: 4.002673084279778\n",
      "[EPOCH #158, elapsed time: 1991.290[sec]] loss: 3.9989213983141605\n",
      "[EPOCH #159, elapsed time: 2003.077[sec]] loss: 3.9989930239535263\n",
      "[EPOCH #160, elapsed time: 2015.661[sec]] loss: 3.9962594078964555\n",
      "[EPOCH #161, elapsed time: 2027.357[sec]] loss: 3.998690376354957\n",
      "[EPOCH #162, elapsed time: 2041.048[sec]] loss: 3.994329943232863\n",
      "[EPOCH #163, elapsed time: 2053.882[sec]] loss: 3.9938321900688067\n",
      "[EPOCH #164, elapsed time: 2066.326[sec]] loss: 3.9934046521678006\n",
      "[EPOCH #165, elapsed time: 2078.954[sec]] loss: 3.992598056793213\n",
      "[EPOCH #166, elapsed time: 2090.811[sec]] loss: 3.990275721784898\n",
      "[EPOCH #167, elapsed time: 2102.975[sec]] loss: 3.9892100157527226\n",
      "[EPOCH #168, elapsed time: 2114.554[sec]] loss: 3.988390872666108\n",
      "[EPOCH #169, elapsed time: 2125.964[sec]] loss: 3.9858236436422865\n",
      "[EPOCH #170, elapsed time: 2139.199[sec]] loss: 3.9865094662017726\n",
      "[EPOCH #171, elapsed time: 2151.470[sec]] loss: 3.986958439656732\n",
      "[EPOCH #172, elapsed time: 2167.885[sec]] loss: 3.983917115669714\n",
      "[EPOCH #173, elapsed time: 2179.737[sec]] loss: 3.984988270702838\n",
      "[EPOCH #174, elapsed time: 2193.872[sec]] loss: 3.9825671980644906\n",
      "[EPOCH #175, elapsed time: 2205.544[sec]] loss: 3.9821984487241915\n",
      "[EPOCH #176, elapsed time: 2219.832[sec]] loss: 3.9808148437787994\n",
      "[EPOCH #177, elapsed time: 2235.689[sec]] loss: 3.980535842940659\n",
      "[EPOCH #178, elapsed time: 2247.776[sec]] loss: 3.9761941278888413\n",
      "[EPOCH #179, elapsed time: 2259.204[sec]] loss: 3.9773392538496566\n",
      "[EPOCH #180, elapsed time: 2271.067[sec]] loss: 3.9790891707134186\n",
      "[EPOCH #181, elapsed time: 2283.818[sec]] loss: 3.973706183949115\n",
      "[EPOCH #182, elapsed time: 2296.489[sec]] loss: 3.9729073436612587\n",
      "[EPOCH #183, elapsed time: 2308.691[sec]] loss: 3.972750489710236\n",
      "[EPOCH #184, elapsed time: 2321.277[sec]] loss: 3.973340592472811\n",
      "[EPOCH #185, elapsed time: 2332.838[sec]] loss: 3.9738080750195093\n",
      "[EPOCH #186, elapsed time: 2344.789[sec]] loss: 3.9678172824173803\n",
      "[EPOCH #187, elapsed time: 2356.963[sec]] loss: 3.967904296656564\n",
      "[EPOCH #188, elapsed time: 2370.210[sec]] loss: 3.96853325874929\n",
      "[EPOCH #189, elapsed time: 2382.402[sec]] loss: 3.9657081820914475\n",
      "[EPOCH #190, elapsed time: 2395.154[sec]] loss: 3.9667088100716463\n",
      "[EPOCH #191, elapsed time: 2407.006[sec]] loss: 3.966155424075331\n",
      "[EPOCH #192, elapsed time: 2418.437[sec]] loss: 3.9655005771688216\n",
      "[EPOCH #193, elapsed time: 2432.773[sec]] loss: 3.9634115514431865\n",
      "[EPOCH #194, elapsed time: 2448.328[sec]] loss: 3.9646798278457145\n",
      "[EPOCH #195, elapsed time: 2462.028[sec]] loss: 3.966325198543888\n",
      "[EPOCH #196, elapsed time: 2475.750[sec]] loss: 3.962847815975499\n",
      "[EPOCH #197, elapsed time: 2487.872[sec]] loss: 3.964042810469351\n",
      "[EPOCH #198, elapsed time: 2500.740[sec]] loss: 3.9604049204864795\n",
      "[EPOCH #199, elapsed time: 2512.829[sec]] loss: 3.9593813933398736\n",
      "[EPOCH #200, elapsed time: 2525.049[sec]] loss: 3.956730012777747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 12:04:17,799] Trial 16 finished with value: 0.77024 and parameters: {'learning_rate': 0.0002586024207947874}. Best is trial 14 with value: 0.77282.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605159853180479\n",
      "[EPOCH #1, elapsed time: 15.282[sec]] loss: 4.575519315874584\n",
      "[EPOCH #2, elapsed time: 31.512[sec]] loss: 4.539517956060701\n",
      "[EPOCH #3, elapsed time: 43.423[sec]] loss: 4.51700139427063\n",
      "[EPOCH #4, elapsed time: 55.840[sec]] loss: 4.499621002321738\n",
      "[EPOCH #5, elapsed time: 67.720[sec]] loss: 4.48332374643532\n",
      "[EPOCH #6, elapsed time: 79.421[sec]] loss: 4.466968852743001\n",
      "[EPOCH #7, elapsed time: 91.398[sec]] loss: 4.455134883875734\n",
      "[EPOCH #8, elapsed time: 103.615[sec]] loss: 4.44512137768746\n",
      "[EPOCH #9, elapsed time: 119.550[sec]] loss: 4.436585082202406\n",
      "[EPOCH #10, elapsed time: 135.714[sec]] loss: 4.427215132283158\n",
      "[EPOCH #11, elapsed time: 147.396[sec]] loss: 4.418741052911896\n",
      "[EPOCH #12, elapsed time: 162.822[sec]] loss: 4.409590920041329\n",
      "[EPOCH #13, elapsed time: 174.393[sec]] loss: 4.405465900325958\n",
      "[EPOCH #14, elapsed time: 185.924[sec]] loss: 4.392287463312033\n",
      "[EPOCH #15, elapsed time: 197.493[sec]] loss: 4.386286605571374\n",
      "[EPOCH #16, elapsed time: 209.361[sec]] loss: 4.378465713481452\n",
      "[EPOCH #17, elapsed time: 220.853[sec]] loss: 4.369448774378039\n",
      "[EPOCH #18, elapsed time: 235.493[sec]] loss: 4.360523662274264\n",
      "[EPOCH #19, elapsed time: 247.008[sec]] loss: 4.357635209137861\n",
      "[EPOCH #20, elapsed time: 261.681[sec]] loss: 4.3470506271489215\n",
      "[EPOCH #21, elapsed time: 273.228[sec]] loss: 4.339892502099524\n",
      "[EPOCH #22, elapsed time: 284.934[sec]] loss: 4.33518616053361\n",
      "[EPOCH #23, elapsed time: 297.545[sec]] loss: 4.3287626517863895\n",
      "[EPOCH #24, elapsed time: 309.862[sec]] loss: 4.32918159975429\n",
      "[EPOCH #25, elapsed time: 321.305[sec]] loss: 4.3209877319238315\n",
      "[EPOCH #26, elapsed time: 337.775[sec]] loss: 4.311163597204558\n",
      "[EPOCH #27, elapsed time: 349.279[sec]] loss: 4.311631938546267\n",
      "[EPOCH #28, elapsed time: 361.784[sec]] loss: 4.307998479060919\n",
      "[EPOCH #29, elapsed time: 378.300[sec]] loss: 4.300289255643761\n",
      "[EPOCH #30, elapsed time: 389.781[sec]] loss: 4.297068037593204\n",
      "[EPOCH #31, elapsed time: 401.487[sec]] loss: 4.293095591734864\n",
      "[EPOCH #32, elapsed time: 414.065[sec]] loss: 4.287432043657651\n",
      "[EPOCH #33, elapsed time: 426.608[sec]] loss: 4.285211257727117\n",
      "[EPOCH #34, elapsed time: 441.984[sec]] loss: 4.272253937699897\n",
      "[EPOCH #35, elapsed time: 454.599[sec]] loss: 4.2724429276686635\n",
      "[EPOCH #36, elapsed time: 467.102[sec]] loss: 4.2743138237329\n",
      "[EPOCH #37, elapsed time: 479.847[sec]] loss: 4.264131645628526\n",
      "[EPOCH #38, elapsed time: 495.294[sec]] loss: 4.2598442654539515\n",
      "[EPOCH #39, elapsed time: 508.010[sec]] loss: 4.257323456664766\n",
      "[EPOCH #40, elapsed time: 520.313[sec]] loss: 4.2503372713761385\n",
      "[EPOCH #41, elapsed time: 532.666[sec]] loss: 4.248356844474319\n",
      "[EPOCH #42, elapsed time: 544.541[sec]] loss: 4.2417843245727775\n",
      "[EPOCH #43, elapsed time: 556.509[sec]] loss: 4.2408856052049675\n",
      "[EPOCH #44, elapsed time: 569.141[sec]] loss: 4.2408765009100895\n",
      "[EPOCH #45, elapsed time: 581.204[sec]] loss: 4.23450340434518\n",
      "[EPOCH #46, elapsed time: 596.371[sec]] loss: 4.226227814771392\n",
      "[EPOCH #47, elapsed time: 609.005[sec]] loss: 4.227966310424219\n",
      "[EPOCH #48, elapsed time: 621.977[sec]] loss: 4.221578069322016\n",
      "[EPOCH #49, elapsed time: 634.097[sec]] loss: 4.218893710726435\n",
      "[EPOCH #50, elapsed time: 649.389[sec]] loss: 4.216095356321914\n",
      "[EPOCH #51, elapsed time: 665.619[sec]] loss: 4.209400297507825\n",
      "[EPOCH #52, elapsed time: 679.541[sec]] loss: 4.209565282859485\n",
      "[EPOCH #53, elapsed time: 691.111[sec]] loss: 4.205645659758506\n",
      "[EPOCH #54, elapsed time: 703.091[sec]] loss: 4.201123958814625\n",
      "[EPOCH #55, elapsed time: 719.321[sec]] loss: 4.1981883206126485\n",
      "[EPOCH #56, elapsed time: 732.159[sec]] loss: 4.1918377972381355\n",
      "[EPOCH #57, elapsed time: 745.488[sec]] loss: 4.189595287607331\n",
      "[EPOCH #58, elapsed time: 758.090[sec]] loss: 4.187560300833128\n",
      "[EPOCH #59, elapsed time: 773.656[sec]] loss: 4.184642403841171\n",
      "[EPOCH #60, elapsed time: 785.367[sec]] loss: 4.181034938540126\n",
      "[EPOCH #61, elapsed time: 798.027[sec]] loss: 4.180037248920189\n",
      "[EPOCH #62, elapsed time: 809.659[sec]] loss: 4.178348526463475\n",
      "[EPOCH #63, elapsed time: 821.543[sec]] loss: 4.172542742865252\n",
      "[EPOCH #64, elapsed time: 835.711[sec]] loss: 4.173503795496867\n",
      "[EPOCH #65, elapsed time: 847.695[sec]] loss: 4.1726024791512515\n",
      "[EPOCH #66, elapsed time: 859.223[sec]] loss: 4.167764956876397\n",
      "[EPOCH #67, elapsed time: 871.476[sec]] loss: 4.16791894827908\n",
      "[EPOCH #68, elapsed time: 883.132[sec]] loss: 4.162429554219896\n",
      "[EPOCH #69, elapsed time: 894.847[sec]] loss: 4.155992572611139\n",
      "[EPOCH #70, elapsed time: 906.252[sec]] loss: 4.155367029376771\n",
      "[EPOCH #71, elapsed time: 918.406[sec]] loss: 4.150586035796182\n",
      "[EPOCH #72, elapsed time: 930.301[sec]] loss: 4.15210348127442\n",
      "[EPOCH #73, elapsed time: 941.800[sec]] loss: 4.148744224663049\n",
      "[EPOCH #74, elapsed time: 953.098[sec]] loss: 4.144700778728102\n",
      "[EPOCH #75, elapsed time: 964.666[sec]] loss: 4.1441187469606895\n",
      "[EPOCH #76, elapsed time: 976.100[sec]] loss: 4.140228486411936\n",
      "[EPOCH #77, elapsed time: 987.525[sec]] loss: 4.14464011286896\n",
      "[EPOCH #78, elapsed time: 999.017[sec]] loss: 4.136906331117841\n",
      "[EPOCH #79, elapsed time: 1011.402[sec]] loss: 4.134989286948684\n",
      "[EPOCH #80, elapsed time: 1023.231[sec]] loss: 4.139398317495677\n",
      "[EPOCH #81, elapsed time: 1036.256[sec]] loss: 4.1306411158329235\n",
      "[EPOCH #82, elapsed time: 1047.967[sec]] loss: 4.129297200640424\n",
      "[EPOCH #83, elapsed time: 1059.453[sec]] loss: 4.126564450090083\n",
      "[EPOCH #84, elapsed time: 1071.234[sec]] loss: 4.124586230733802\n",
      "[EPOCH #85, elapsed time: 1083.520[sec]] loss: 4.121197020633817\n",
      "[EPOCH #86, elapsed time: 1095.218[sec]] loss: 4.116662332055207\n",
      "[EPOCH #87, elapsed time: 1111.522[sec]] loss: 4.116375698314137\n",
      "[EPOCH #88, elapsed time: 1124.109[sec]] loss: 4.1190811873855155\n",
      "[EPOCH #89, elapsed time: 1135.934[sec]] loss: 4.112866426688772\n",
      "[EPOCH #90, elapsed time: 1151.252[sec]] loss: 4.11253400071645\n",
      "[EPOCH #91, elapsed time: 1162.882[sec]] loss: 4.110188208096163\n",
      "[EPOCH #92, elapsed time: 1174.618[sec]] loss: 4.109626565922245\n",
      "[EPOCH #93, elapsed time: 1188.050[sec]] loss: 4.1090107069363295\n",
      "[EPOCH #94, elapsed time: 1200.543[sec]] loss: 4.105007729618807\n",
      "[EPOCH #95, elapsed time: 1212.094[sec]] loss: 4.101101461619196\n",
      "[EPOCH #96, elapsed time: 1223.821[sec]] loss: 4.102802724160983\n",
      "[EPOCH #97, elapsed time: 1236.500[sec]] loss: 4.098925127711574\n",
      "[EPOCH #98, elapsed time: 1248.853[sec]] loss: 4.098434401107612\n",
      "[EPOCH #99, elapsed time: 1261.297[sec]] loss: 4.095715307228396\n",
      "[EPOCH #100, elapsed time: 1273.624[sec]] loss: 4.093374516517019\n",
      "[EPOCH #101, elapsed time: 1286.326[sec]] loss: 4.095123380403525\n",
      "[EPOCH #102, elapsed time: 1299.193[sec]] loss: 4.0934879651682845\n",
      "[EPOCH #103, elapsed time: 1312.416[sec]] loss: 4.088114473809055\n",
      "[EPOCH #104, elapsed time: 1324.934[sec]] loss: 4.087228279577488\n",
      "[EPOCH #105, elapsed time: 1337.372[sec]] loss: 4.0847995420030045\n",
      "[EPOCH #106, elapsed time: 1349.715[sec]] loss: 4.084680813516628\n",
      "[EPOCH #107, elapsed time: 1362.262[sec]] loss: 4.082922572137756\n",
      "[EPOCH #108, elapsed time: 1375.376[sec]] loss: 4.081366365259455\n",
      "[EPOCH #109, elapsed time: 1388.306[sec]] loss: 4.077751671436576\n",
      "[EPOCH #110, elapsed time: 1401.297[sec]] loss: 4.075169393670002\n",
      "[EPOCH #111, elapsed time: 1414.425[sec]] loss: 4.07530889782628\n",
      "[EPOCH #112, elapsed time: 1427.475[sec]] loss: 4.072231338791411\n",
      "[EPOCH #113, elapsed time: 1440.344[sec]] loss: 4.071096250207014\n",
      "[EPOCH #114, elapsed time: 1453.804[sec]] loss: 4.06855638943951\n",
      "[EPOCH #115, elapsed time: 1466.342[sec]] loss: 4.066011562152162\n",
      "[EPOCH #116, elapsed time: 1478.851[sec]] loss: 4.0687019721255115\n",
      "[EPOCH #117, elapsed time: 1495.302[sec]] loss: 4.0638047207339945\n",
      "[EPOCH #118, elapsed time: 1508.097[sec]] loss: 4.064505201078575\n",
      "[EPOCH #119, elapsed time: 1523.226[sec]] loss: 4.064105428950724\n",
      "[EPOCH #120, elapsed time: 1536.145[sec]] loss: 4.05900392330997\n",
      "[EPOCH #121, elapsed time: 1548.855[sec]] loss: 4.0577003397548035\n",
      "[EPOCH #122, elapsed time: 1561.406[sec]] loss: 4.056774473617417\n",
      "[EPOCH #123, elapsed time: 1573.860[sec]] loss: 4.054552556304541\n",
      "[EPOCH #124, elapsed time: 1586.853[sec]] loss: 4.053303474656909\n",
      "[EPOCH #125, elapsed time: 1600.321[sec]] loss: 4.0495178090862485\n",
      "[EPOCH #126, elapsed time: 1613.218[sec]] loss: 4.049320571329528\n",
      "[EPOCH #127, elapsed time: 1625.804[sec]] loss: 4.049915067217553\n",
      "[EPOCH #128, elapsed time: 1638.728[sec]] loss: 4.045523228892438\n",
      "[EPOCH #129, elapsed time: 1651.668[sec]] loss: 4.047161164683405\n",
      "[EPOCH #130, elapsed time: 1664.927[sec]] loss: 4.046214669161093\n",
      "[EPOCH #131, elapsed time: 1678.536[sec]] loss: 4.045624969406762\n",
      "[EPOCH #132, elapsed time: 1691.142[sec]] loss: 4.042676747646076\n",
      "[EPOCH #133, elapsed time: 1704.544[sec]] loss: 4.04670229067958\n",
      "[EPOCH #134, elapsed time: 1716.631[sec]] loss: 4.040513700654853\n",
      "[EPOCH #135, elapsed time: 1728.466[sec]] loss: 4.0386058127963995\n",
      "[EPOCH #136, elapsed time: 1740.196[sec]] loss: 4.038058921914031\n",
      "[EPOCH #137, elapsed time: 1753.256[sec]] loss: 4.036743316601578\n",
      "[EPOCH #138, elapsed time: 1765.081[sec]] loss: 4.038883583254335\n",
      "[EPOCH #139, elapsed time: 1779.298[sec]] loss: 4.0336222223036575\n",
      "[EPOCH #140, elapsed time: 1794.593[sec]] loss: 4.033327240556459\n",
      "[EPOCH #141, elapsed time: 1806.104[sec]] loss: 4.033104802123721\n",
      "[EPOCH #142, elapsed time: 1817.602[sec]] loss: 4.027724567469464\n",
      "[EPOCH #143, elapsed time: 1830.214[sec]] loss: 4.027152488724361\n",
      "[EPOCH #144, elapsed time: 1842.122[sec]] loss: 4.026518802496385\n",
      "[EPOCH #145, elapsed time: 1853.647[sec]] loss: 4.027419560701513\n",
      "[EPOCH #146, elapsed time: 1865.233[sec]] loss: 4.023232443700291\n",
      "[EPOCH #147, elapsed time: 1877.490[sec]] loss: 4.023527554953167\n",
      "[EPOCH #148, elapsed time: 1890.950[sec]] loss: 4.022694369424099\n",
      "[EPOCH #149, elapsed time: 1903.609[sec]] loss: 4.02253838189504\n",
      "[EPOCH #150, elapsed time: 1915.208[sec]] loss: 4.0206027789064045\n",
      "[EPOCH #151, elapsed time: 1926.675[sec]] loss: 4.017741122462394\n",
      "[EPOCH #152, elapsed time: 1939.546[sec]] loss: 4.019357599818546\n",
      "[EPOCH #153, elapsed time: 1951.833[sec]] loss: 4.013998552842241\n",
      "[EPOCH #154, elapsed time: 1963.439[sec]] loss: 4.016917375441324\n",
      "[EPOCH #155, elapsed time: 1979.352[sec]] loss: 4.0176878041055675\n",
      "[EPOCH #156, elapsed time: 1991.111[sec]] loss: 4.011515428069609\n",
      "[EPOCH #157, elapsed time: 2003.722[sec]] loss: 4.015010252718886\n",
      "[EPOCH #158, elapsed time: 2015.510[sec]] loss: 4.015197834294344\n",
      "[EPOCH #159, elapsed time: 2031.511[sec]] loss: 4.010971667746741\n",
      "[EPOCH #160, elapsed time: 2043.902[sec]] loss: 4.008226816271332\n",
      "[EPOCH #161, elapsed time: 2055.597[sec]] loss: 4.006410379556227\n",
      "[EPOCH #162, elapsed time: 2068.842[sec]] loss: 4.006679545742384\n",
      "[EPOCH #163, elapsed time: 2082.689[sec]] loss: 4.009208039145247\n",
      "[EPOCH #164, elapsed time: 2095.381[sec]] loss: 4.002327228004324\n",
      "[EPOCH #165, elapsed time: 2107.694[sec]] loss: 4.003469285489044\n",
      "[EPOCH #166, elapsed time: 2119.797[sec]] loss: 4.001343996800906\n",
      "[EPOCH #167, elapsed time: 2134.381[sec]] loss: 3.999442279148163\n",
      "[EPOCH #168, elapsed time: 2147.246[sec]] loss: 4.0019093743517695\n",
      "[EPOCH #169, elapsed time: 2160.434[sec]] loss: 3.9996584189372877\n",
      "[EPOCH #170, elapsed time: 2172.423[sec]] loss: 4.000327922408579\n",
      "[EPOCH #171, elapsed time: 2188.864[sec]] loss: 3.9978245380057484\n",
      "[EPOCH #172, elapsed time: 2203.293[sec]] loss: 3.9970398792759854\n",
      "[EPOCH #173, elapsed time: 2215.297[sec]] loss: 3.9980152568524265\n",
      "[EPOCH #174, elapsed time: 2228.083[sec]] loss: 3.993792608466121\n",
      "[EPOCH #175, elapsed time: 2239.979[sec]] loss: 3.9941669108390196\n",
      "[EPOCH #176, elapsed time: 2251.570[sec]] loss: 3.9937996158062914\n",
      "[EPOCH #177, elapsed time: 2263.404[sec]] loss: 3.9918941313535528\n",
      "[EPOCH #178, elapsed time: 2275.932[sec]] loss: 3.9908514126553722\n",
      "[EPOCH #179, elapsed time: 2288.543[sec]] loss: 3.99127213312736\n",
      "[EPOCH #180, elapsed time: 2300.041[sec]] loss: 3.9876378105759085\n",
      "[EPOCH #181, elapsed time: 2311.796[sec]] loss: 3.988854769476697\n",
      "[EPOCH #182, elapsed time: 2323.237[sec]] loss: 3.987967654214177\n",
      "[EPOCH #183, elapsed time: 2336.195[sec]] loss: 3.9855953681491845\n",
      "[EPOCH #184, elapsed time: 2347.504[sec]] loss: 3.985037102122682\n",
      "[EPOCH #185, elapsed time: 2358.869[sec]] loss: 3.984542203269856\n",
      "[EPOCH #186, elapsed time: 2371.290[sec]] loss: 3.984352910053402\n",
      "[EPOCH #187, elapsed time: 2382.793[sec]] loss: 3.980662727691543\n",
      "[EPOCH #188, elapsed time: 2394.156[sec]] loss: 3.983582813924349\n",
      "[EPOCH #189, elapsed time: 2405.827[sec]] loss: 3.9801778012334883\n",
      "[EPOCH #190, elapsed time: 2421.252[sec]] loss: 3.982326461501558\n",
      "[EPOCH #191, elapsed time: 2437.622[sec]] loss: 3.9818964515522515\n",
      "[EPOCH #192, elapsed time: 2449.090[sec]] loss: 3.9775689719048204\n",
      "[EPOCH #193, elapsed time: 2462.188[sec]] loss: 3.978506650058261\n",
      "[EPOCH #194, elapsed time: 2477.408[sec]] loss: 3.9804766103386955\n",
      "[EPOCH #195, elapsed time: 2493.721[sec]] loss: 3.9741340328773016\n",
      "[EPOCH #196, elapsed time: 2505.800[sec]] loss: 3.9747689069270784\n",
      "[EPOCH #197, elapsed time: 2517.510[sec]] loss: 3.9731149461432596\n",
      "[EPOCH #198, elapsed time: 2529.398[sec]] loss: 3.972024654472629\n",
      "[EPOCH #199, elapsed time: 2540.896[sec]] loss: 3.972135918611757\n",
      "[EPOCH #200, elapsed time: 2552.882[sec]] loss: 3.971548506180903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 12:47:07,933] Trial 17 finished with value: 0.7566 and parameters: {'learning_rate': 0.00032243109603497677}. Best is trial 14 with value: 0.77282.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605177215483428\n",
      "[EPOCH #1, elapsed time: 12.226[sec]] loss: 4.60447886595723\n",
      "[EPOCH #2, elapsed time: 27.711[sec]] loss: 4.601205963395913\n",
      "[EPOCH #3, elapsed time: 39.152[sec]] loss: 4.59412411185159\n",
      "[EPOCH #4, elapsed time: 51.779[sec]] loss: 4.586851033047843\n",
      "[EPOCH #5, elapsed time: 67.953[sec]] loss: 4.582473865931261\n",
      "[EPOCH #6, elapsed time: 80.029[sec]] loss: 4.578643774421873\n",
      "[EPOCH #7, elapsed time: 96.222[sec]] loss: 4.57544752122192\n",
      "[EPOCH #8, elapsed time: 108.266[sec]] loss: 4.572517749520348\n",
      "[EPOCH #9, elapsed time: 120.895[sec]] loss: 4.5697012850663175\n",
      "[EPOCH #10, elapsed time: 133.627[sec]] loss: 4.567684073213271\n",
      "[EPOCH #11, elapsed time: 149.701[sec]] loss: 4.565114598509141\n",
      "[EPOCH #12, elapsed time: 162.558[sec]] loss: 4.561870979484807\n",
      "[EPOCH #13, elapsed time: 174.794[sec]] loss: 4.557110494173115\n",
      "[EPOCH #14, elapsed time: 187.363[sec]] loss: 4.553571187603268\n",
      "[EPOCH #15, elapsed time: 203.823[sec]] loss: 4.549699970338105\n",
      "[EPOCH #16, elapsed time: 216.608[sec]] loss: 4.546355771011675\n",
      "[EPOCH #17, elapsed time: 232.245[sec]] loss: 4.543913777791302\n",
      "[EPOCH #18, elapsed time: 245.907[sec]] loss: 4.5413268489556975\n",
      "[EPOCH #19, elapsed time: 262.694[sec]] loss: 4.538518644187668\n",
      "[EPOCH #20, elapsed time: 279.202[sec]] loss: 4.536134934013498\n",
      "[EPOCH #21, elapsed time: 292.435[sec]] loss: 4.533765818778323\n",
      "[EPOCH #22, elapsed time: 305.365[sec]] loss: 4.53053683008205\n",
      "[EPOCH #23, elapsed time: 319.863[sec]] loss: 4.528461285149983\n",
      "[EPOCH #24, elapsed time: 332.731[sec]] loss: 4.524723780589918\n",
      "[EPOCH #25, elapsed time: 349.593[sec]] loss: 4.521567342224902\n",
      "[EPOCH #26, elapsed time: 362.702[sec]] loss: 4.519186864353798\n",
      "[EPOCH #27, elapsed time: 374.516[sec]] loss: 4.515626932059964\n",
      "[EPOCH #28, elapsed time: 386.148[sec]] loss: 4.513121588140135\n",
      "[EPOCH #29, elapsed time: 397.500[sec]] loss: 4.510191460412355\n",
      "[EPOCH #30, elapsed time: 410.226[sec]] loss: 4.50753911000677\n",
      "[EPOCH #31, elapsed time: 421.651[sec]] loss: 4.50511562038673\n",
      "[EPOCH #32, elapsed time: 434.264[sec]] loss: 4.503131001558505\n",
      "[EPOCH #33, elapsed time: 446.217[sec]] loss: 4.500655164797949\n",
      "[EPOCH #34, elapsed time: 460.310[sec]] loss: 4.498632539943175\n",
      "[EPOCH #35, elapsed time: 475.569[sec]] loss: 4.495355504182998\n",
      "[EPOCH #36, elapsed time: 489.193[sec]] loss: 4.493346662759324\n",
      "[EPOCH #37, elapsed time: 504.746[sec]] loss: 4.489545271172404\n",
      "[EPOCH #38, elapsed time: 516.963[sec]] loss: 4.487595419050865\n",
      "[EPOCH #39, elapsed time: 529.561[sec]] loss: 4.485694679936307\n",
      "[EPOCH #40, elapsed time: 541.502[sec]] loss: 4.482906047457392\n",
      "[EPOCH #41, elapsed time: 554.060[sec]] loss: 4.480180684527142\n",
      "[EPOCH #42, elapsed time: 566.049[sec]] loss: 4.478175806983953\n",
      "[EPOCH #43, elapsed time: 579.900[sec]] loss: 4.474495736437582\n",
      "[EPOCH #44, elapsed time: 591.643[sec]] loss: 4.472732290120287\n",
      "[EPOCH #45, elapsed time: 603.731[sec]] loss: 4.471062628794235\n",
      "[EPOCH #46, elapsed time: 617.074[sec]] loss: 4.46890440219042\n",
      "[EPOCH #47, elapsed time: 629.089[sec]] loss: 4.466515219829362\n",
      "[EPOCH #48, elapsed time: 641.055[sec]] loss: 4.463586365192728\n",
      "[EPOCH #49, elapsed time: 655.172[sec]] loss: 4.4627283078618945\n",
      "[EPOCH #50, elapsed time: 668.139[sec]] loss: 4.46146501308058\n",
      "[EPOCH #51, elapsed time: 681.616[sec]] loss: 4.458636781456985\n",
      "[EPOCH #52, elapsed time: 695.215[sec]] loss: 4.457676802090323\n",
      "[EPOCH #53, elapsed time: 706.693[sec]] loss: 4.454150264871784\n",
      "[EPOCH #54, elapsed time: 719.574[sec]] loss: 4.451870089986731\n",
      "[EPOCH #55, elapsed time: 731.943[sec]] loss: 4.450052185998234\n",
      "[EPOCH #56, elapsed time: 745.973[sec]] loss: 4.448892667212703\n",
      "[EPOCH #57, elapsed time: 762.301[sec]] loss: 4.447591150409505\n",
      "[EPOCH #58, elapsed time: 775.127[sec]] loss: 4.445726127099777\n",
      "[EPOCH #59, elapsed time: 787.800[sec]] loss: 4.444548129120166\n",
      "[EPOCH #60, elapsed time: 800.062[sec]] loss: 4.443515853247273\n",
      "[EPOCH #61, elapsed time: 811.898[sec]] loss: 4.4415483142196255\n",
      "[EPOCH #62, elapsed time: 824.012[sec]] loss: 4.439293329027778\n",
      "[EPOCH #63, elapsed time: 836.270[sec]] loss: 4.438230003825534\n",
      "[EPOCH #64, elapsed time: 847.874[sec]] loss: 4.436391396013041\n",
      "[EPOCH #65, elapsed time: 859.278[sec]] loss: 4.436019082597182\n",
      "[EPOCH #66, elapsed time: 871.116[sec]] loss: 4.434563466851252\n",
      "[EPOCH #67, elapsed time: 882.751[sec]] loss: 4.433369195697709\n",
      "[EPOCH #68, elapsed time: 894.553[sec]] loss: 4.431200118989267\n",
      "[EPOCH #69, elapsed time: 906.164[sec]] loss: 4.431200956428806\n",
      "[EPOCH #70, elapsed time: 918.682[sec]] loss: 4.429239046703297\n",
      "[EPOCH #71, elapsed time: 930.483[sec]] loss: 4.427847956817881\n",
      "[EPOCH #72, elapsed time: 942.234[sec]] loss: 4.426077183438506\n",
      "[EPOCH #73, elapsed time: 953.830[sec]] loss: 4.424492612071169\n",
      "[EPOCH #74, elapsed time: 966.155[sec]] loss: 4.423013142264202\n",
      "[EPOCH #75, elapsed time: 979.483[sec]] loss: 4.421902752349717\n",
      "[EPOCH #76, elapsed time: 991.870[sec]] loss: 4.418717442303229\n",
      "[EPOCH #77, elapsed time: 1003.390[sec]] loss: 4.418387102760417\n",
      "[EPOCH #78, elapsed time: 1014.851[sec]] loss: 4.416028537738041\n",
      "[EPOCH #79, elapsed time: 1026.553[sec]] loss: 4.414685154754385\n",
      "[EPOCH #80, elapsed time: 1040.959[sec]] loss: 4.412258839500462\n",
      "[EPOCH #81, elapsed time: 1053.448[sec]] loss: 4.410472383578466\n",
      "[EPOCH #82, elapsed time: 1065.021[sec]] loss: 4.407472045163809\n",
      "[EPOCH #83, elapsed time: 1076.725[sec]] loss: 4.406624081648853\n",
      "[EPOCH #84, elapsed time: 1088.394[sec]] loss: 4.405842776490722\n",
      "[EPOCH #85, elapsed time: 1100.277[sec]] loss: 4.40337645481278\n",
      "[EPOCH #86, elapsed time: 1112.884[sec]] loss: 4.402205844605801\n",
      "[EPOCH #87, elapsed time: 1125.209[sec]] loss: 4.399107825656922\n",
      "[EPOCH #88, elapsed time: 1139.691[sec]] loss: 4.397879644761235\n",
      "[EPOCH #89, elapsed time: 1155.779[sec]] loss: 4.39553847651564\n",
      "[EPOCH #90, elapsed time: 1171.070[sec]] loss: 4.3947495733860285\n",
      "[EPOCH #91, elapsed time: 1186.481[sec]] loss: 4.392977018350222\n",
      "[EPOCH #92, elapsed time: 1200.518[sec]] loss: 4.391693045371477\n",
      "[EPOCH #93, elapsed time: 1213.053[sec]] loss: 4.389251890810956\n",
      "[EPOCH #94, elapsed time: 1224.510[sec]] loss: 4.3873380266239606\n",
      "[EPOCH #95, elapsed time: 1236.016[sec]] loss: 4.386793939562387\n",
      "[EPOCH #96, elapsed time: 1248.021[sec]] loss: 4.384907287431694\n",
      "[EPOCH #97, elapsed time: 1260.590[sec]] loss: 4.383280125933432\n",
      "[EPOCH #98, elapsed time: 1272.586[sec]] loss: 4.3815090105614445\n",
      "[EPOCH #99, elapsed time: 1284.400[sec]] loss: 4.381328338701147\n",
      "[EPOCH #100, elapsed time: 1296.460[sec]] loss: 4.378645673899489\n",
      "[EPOCH #101, elapsed time: 1308.626[sec]] loss: 4.377901353976395\n",
      "[EPOCH #102, elapsed time: 1321.241[sec]] loss: 4.374892285140142\n",
      "[EPOCH #103, elapsed time: 1333.530[sec]] loss: 4.374830178244329\n",
      "[EPOCH #104, elapsed time: 1346.520[sec]] loss: 4.375154200838837\n",
      "[EPOCH #105, elapsed time: 1358.900[sec]] loss: 4.373805226077655\n",
      "[EPOCH #106, elapsed time: 1371.004[sec]] loss: 4.371677760199256\n",
      "[EPOCH #107, elapsed time: 1384.630[sec]] loss: 4.368951210209901\n",
      "[EPOCH #108, elapsed time: 1400.562[sec]] loss: 4.3692686318283425\n",
      "[EPOCH #109, elapsed time: 1413.242[sec]] loss: 4.36884520500803\n",
      "[EPOCH #110, elapsed time: 1425.738[sec]] loss: 4.367516172900844\n",
      "[EPOCH #111, elapsed time: 1442.446[sec]] loss: 4.365420288103022\n",
      "[EPOCH #112, elapsed time: 1454.766[sec]] loss: 4.364868934995001\n",
      "[EPOCH #113, elapsed time: 1467.235[sec]] loss: 4.363140621478178\n",
      "[EPOCH #114, elapsed time: 1479.354[sec]] loss: 4.3630695297255855\n",
      "[EPOCH #115, elapsed time: 1492.205[sec]] loss: 4.3620537835973545\n",
      "[EPOCH #116, elapsed time: 1504.705[sec]] loss: 4.3608566599630505\n",
      "[EPOCH #117, elapsed time: 1519.614[sec]] loss: 4.358934456769732\n",
      "[EPOCH #118, elapsed time: 1531.816[sec]] loss: 4.3582871367514935\n",
      "[EPOCH #119, elapsed time: 1544.363[sec]] loss: 4.357802318443645\n",
      "[EPOCH #120, elapsed time: 1556.096[sec]] loss: 4.355774235740657\n",
      "[EPOCH #121, elapsed time: 1568.754[sec]] loss: 4.35536940953553\n",
      "[EPOCH #122, elapsed time: 1582.405[sec]] loss: 4.3538266482142705\n",
      "[EPOCH #123, elapsed time: 1595.357[sec]] loss: 4.353830067835324\n",
      "[EPOCH #124, elapsed time: 1607.095[sec]] loss: 4.352109036839169\n",
      "[EPOCH #125, elapsed time: 1620.994[sec]] loss: 4.3515136145813225\n",
      "[EPOCH #126, elapsed time: 1634.188[sec]] loss: 4.348686980041875\n",
      "[EPOCH #127, elapsed time: 1647.881[sec]] loss: 4.349778715037262\n",
      "[EPOCH #128, elapsed time: 1660.041[sec]] loss: 4.347293521224575\n",
      "[EPOCH #129, elapsed time: 1673.028[sec]] loss: 4.3457399052225165\n",
      "[EPOCH #130, elapsed time: 1684.919[sec]] loss: 4.345386206455438\n",
      "[EPOCH #131, elapsed time: 1697.066[sec]] loss: 4.344499917497104\n",
      "[EPOCH #132, elapsed time: 1709.020[sec]] loss: 4.343968493009483\n",
      "[EPOCH #133, elapsed time: 1720.835[sec]] loss: 4.342375224168989\n",
      "[EPOCH #134, elapsed time: 1733.872[sec]] loss: 4.340886842418922\n",
      "[EPOCH #135, elapsed time: 1745.714[sec]] loss: 4.340373804991778\n",
      "[EPOCH #136, elapsed time: 1757.808[sec]] loss: 4.338274743110037\n",
      "[EPOCH #137, elapsed time: 1770.241[sec]] loss: 4.339287738348533\n",
      "[EPOCH #138, elapsed time: 1783.469[sec]] loss: 4.3368091894431675\n",
      "[EPOCH #139, elapsed time: 1796.405[sec]] loss: 4.336631671785927\n",
      "[EPOCH #140, elapsed time: 1808.133[sec]] loss: 4.334932392252155\n",
      "[EPOCH #141, elapsed time: 1819.647[sec]] loss: 4.333498886435442\n",
      "[EPOCH #142, elapsed time: 1831.308[sec]] loss: 4.333364000094677\n",
      "[EPOCH #143, elapsed time: 1843.204[sec]] loss: 4.331184623337524\n",
      "[EPOCH #144, elapsed time: 1855.497[sec]] loss: 4.330951016756181\n",
      "[EPOCH #145, elapsed time: 1868.029[sec]] loss: 4.328081713375646\n",
      "[EPOCH #146, elapsed time: 1884.401[sec]] loss: 4.327908313968436\n",
      "[EPOCH #147, elapsed time: 1896.960[sec]] loss: 4.3273312388058285\n",
      "[EPOCH #148, elapsed time: 1909.380[sec]] loss: 4.32623353892233\n",
      "[EPOCH #149, elapsed time: 1922.687[sec]] loss: 4.323507848643219\n",
      "[EPOCH #150, elapsed time: 1935.155[sec]] loss: 4.323139030202718\n",
      "[EPOCH #151, elapsed time: 1946.462[sec]] loss: 4.322046361363094\n",
      "[EPOCH #152, elapsed time: 1958.532[sec]] loss: 4.322512956742514\n",
      "[EPOCH #153, elapsed time: 1970.783[sec]] loss: 4.321368138453019\n",
      "[EPOCH #154, elapsed time: 1982.867[sec]] loss: 4.321546439856043\n",
      "[EPOCH #155, elapsed time: 1998.694[sec]] loss: 4.318905093772092\n",
      "[EPOCH #156, elapsed time: 2014.724[sec]] loss: 4.318933608359583\n",
      "[EPOCH #157, elapsed time: 2028.256[sec]] loss: 4.316313375361974\n",
      "[EPOCH #158, elapsed time: 2039.676[sec]] loss: 4.316874247518626\n",
      "[EPOCH #159, elapsed time: 2051.499[sec]] loss: 4.313576154043792\n",
      "[EPOCH #160, elapsed time: 2064.007[sec]] loss: 4.316596075120219\n",
      "[EPOCH #161, elapsed time: 2076.061[sec]] loss: 4.312822010909146\n",
      "[EPOCH #162, elapsed time: 2088.498[sec]] loss: 4.3132680266313805\n",
      "[EPOCH #163, elapsed time: 2101.117[sec]] loss: 4.31158984874352\n",
      "[EPOCH #164, elapsed time: 2113.166[sec]] loss: 4.3117683701078935\n",
      "[EPOCH #165, elapsed time: 2125.264[sec]] loss: 4.30958387230881\n",
      "[EPOCH #166, elapsed time: 2137.153[sec]] loss: 4.308163448853594\n",
      "[EPOCH #167, elapsed time: 2148.848[sec]] loss: 4.307082877888256\n",
      "[EPOCH #168, elapsed time: 2161.586[sec]] loss: 4.305310295700493\n",
      "[EPOCH #169, elapsed time: 2174.528[sec]] loss: 4.304954083699564\n",
      "[EPOCH #170, elapsed time: 2186.861[sec]] loss: 4.304367309187135\n",
      "[EPOCH #171, elapsed time: 2199.416[sec]] loss: 4.303288376186417\n",
      "[EPOCH #172, elapsed time: 2211.227[sec]] loss: 4.3019832737996495\n",
      "[EPOCH #173, elapsed time: 2223.506[sec]] loss: 4.302968122527451\n",
      "[EPOCH #174, elapsed time: 2235.789[sec]] loss: 4.299685397364738\n",
      "[EPOCH #175, elapsed time: 2247.895[sec]] loss: 4.299532133725997\n",
      "[EPOCH #176, elapsed time: 2260.578[sec]] loss: 4.299013415133427\n",
      "[EPOCH #177, elapsed time: 2272.987[sec]] loss: 4.2981326127006545\n",
      "[EPOCH #178, elapsed time: 2287.824[sec]] loss: 4.2963623786994605\n",
      "[EPOCH #179, elapsed time: 2302.489[sec]] loss: 4.294890149312376\n",
      "[EPOCH #180, elapsed time: 2314.669[sec]] loss: 4.296323037955979\n",
      "[EPOCH #181, elapsed time: 2327.469[sec]] loss: 4.294056076616029\n",
      "[EPOCH #182, elapsed time: 2340.369[sec]] loss: 4.293490577949139\n",
      "[EPOCH #183, elapsed time: 2356.434[sec]] loss: 4.292768650457635\n",
      "[EPOCH #184, elapsed time: 2372.973[sec]] loss: 4.2904504875456455\n",
      "[EPOCH #185, elapsed time: 2388.590[sec]] loss: 4.289605592506785\n",
      "[EPOCH #186, elapsed time: 2405.041[sec]] loss: 4.288174098070356\n",
      "[EPOCH #187, elapsed time: 2417.505[sec]] loss: 4.289430987125623\n",
      "[EPOCH #188, elapsed time: 2430.181[sec]] loss: 4.287630126938481\n",
      "[EPOCH #189, elapsed time: 2446.138[sec]] loss: 4.285665589727352\n",
      "[EPOCH #190, elapsed time: 2462.913[sec]] loss: 4.284102592724527\n",
      "[EPOCH #191, elapsed time: 2477.126[sec]] loss: 4.284302197735201\n",
      "[EPOCH #192, elapsed time: 2489.167[sec]] loss: 4.28363769632536\n",
      "[EPOCH #193, elapsed time: 2501.902[sec]] loss: 4.283126053105389\n",
      "[EPOCH #194, elapsed time: 2514.703[sec]] loss: 4.282022515246293\n",
      "[EPOCH #195, elapsed time: 2527.879[sec]] loss: 4.28182536733509\n",
      "[EPOCH #196, elapsed time: 2540.055[sec]] loss: 4.280455821763988\n",
      "[EPOCH #197, elapsed time: 2551.683[sec]] loss: 4.279632669645323\n",
      "[EPOCH #198, elapsed time: 2563.456[sec]] loss: 4.280039818410453\n",
      "[EPOCH #199, elapsed time: 2574.979[sec]] loss: 4.2786866982472835\n",
      "[EPOCH #200, elapsed time: 2587.433[sec]] loss: 4.2774419876069345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 13:30:34,422] Trial 18 finished with value: 0.3879 and parameters: {'learning_rate': 1.2013989706095827e-05}. Best is trial 14 with value: 0.77282.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n",
      "[EPOCH #0] loss: 4.605157305472796\n",
      "[EPOCH #1, elapsed time: 12.663[sec]] loss: 4.581784421331365\n",
      "[EPOCH #2, elapsed time: 24.474[sec]] loss: 4.563827446921087\n",
      "[EPOCH #3, elapsed time: 36.203[sec]] loss: 4.556712096574897\n",
      "[EPOCH #4, elapsed time: 48.013[sec]] loss: 4.549782001278146\n",
      "[EPOCH #5, elapsed time: 59.631[sec]] loss: 4.543754609669925\n",
      "[EPOCH #6, elapsed time: 72.123[sec]] loss: 4.5405969601446285\n",
      "[EPOCH #7, elapsed time: 85.170[sec]] loss: 4.537830271327335\n",
      "[EPOCH #8, elapsed time: 96.905[sec]] loss: 4.534004849649284\n",
      "[EPOCH #9, elapsed time: 109.202[sec]] loss: 4.5287711219153035\n",
      "[EPOCH #10, elapsed time: 122.545[sec]] loss: 4.524152734686912\n",
      "[EPOCH #11, elapsed time: 134.567[sec]] loss: 4.522239888850802\n",
      "[EPOCH #12, elapsed time: 146.242[sec]] loss: 4.515577488653338\n",
      "[EPOCH #13, elapsed time: 157.882[sec]] loss: 4.5098775761751355\n",
      "[EPOCH #14, elapsed time: 169.552[sec]] loss: 4.509706951148679\n",
      "[EPOCH #15, elapsed time: 181.774[sec]] loss: 4.505803342210278\n",
      "[EPOCH #16, elapsed time: 193.748[sec]] loss: 4.496764887774974\n",
      "[EPOCH #17, elapsed time: 205.502[sec]] loss: 4.497799333058636\n",
      "[EPOCH #18, elapsed time: 217.387[sec]] loss: 4.49301693733884\n",
      "[EPOCH #19, elapsed time: 230.023[sec]] loss: 4.4898817360744365\n",
      "[EPOCH #20, elapsed time: 241.749[sec]] loss: 4.487022761419959\n",
      "[EPOCH #21, elapsed time: 257.215[sec]] loss: 4.485582933773692\n",
      "[EPOCH #22, elapsed time: 269.081[sec]] loss: 4.475463113644454\n",
      "[EPOCH #23, elapsed time: 280.947[sec]] loss: 4.476272424062093\n",
      "[EPOCH #24, elapsed time: 293.179[sec]] loss: 4.478219386483337\n",
      "[EPOCH #25, elapsed time: 306.184[sec]] loss: 4.46925588334438\n",
      "[EPOCH #26, elapsed time: 319.295[sec]] loss: 4.469731363820023\n",
      "[EPOCH #27, elapsed time: 335.226[sec]] loss: 4.462526729910784\n",
      "[EPOCH #28, elapsed time: 346.863[sec]] loss: 4.46102036837958\n",
      "[EPOCH #29, elapsed time: 358.625[sec]] loss: 4.4574019120278\n",
      "[EPOCH #30, elapsed time: 371.180[sec]] loss: 4.451988011846463\n",
      "[EPOCH #31, elapsed time: 383.368[sec]] loss: 4.455120964379778\n",
      "[EPOCH #32, elapsed time: 394.990[sec]] loss: 4.448074520206268\n",
      "[EPOCH #33, elapsed time: 407.563[sec]] loss: 4.448444598619555\n",
      "[EPOCH #34, elapsed time: 419.837[sec]] loss: 4.4425308303198445\n",
      "[EPOCH #35, elapsed time: 431.421[sec]] loss: 4.444821622229812\n",
      "[EPOCH #36, elapsed time: 447.965[sec]] loss: 4.442427955524935\n",
      "[EPOCH #37, elapsed time: 459.597[sec]] loss: 4.4337685219538345\n",
      "[EPOCH #38, elapsed time: 474.929[sec]] loss: 4.439820126089924\n",
      "[EPOCH #39, elapsed time: 488.103[sec]] loss: 4.426575761381358\n",
      "[EPOCH #40, elapsed time: 500.125[sec]] loss: 4.424759008147666\n",
      "[EPOCH #41, elapsed time: 513.738[sec]] loss: 4.4212183470308055\n",
      "[EPOCH #42, elapsed time: 529.911[sec]] loss: 4.424125666810546\n",
      "[EPOCH #43, elapsed time: 542.004[sec]] loss: 4.4265740492827454\n",
      "[EPOCH #44, elapsed time: 553.915[sec]] loss: 4.424978163786904\n",
      "[EPOCH #45, elapsed time: 566.450[sec]] loss: 4.425109477357382\n",
      "[EPOCH #46, elapsed time: 578.522[sec]] loss: 4.415306323167992\n",
      "[EPOCH #47, elapsed time: 590.894[sec]] loss: 4.410496485362965\n",
      "[EPOCH #48, elapsed time: 602.438[sec]] loss: 4.406680774322627\n",
      "[EPOCH #49, elapsed time: 614.050[sec]] loss: 4.410627143015407\n",
      "[EPOCH #50, elapsed time: 625.744[sec]] loss: 4.396952046084999\n",
      "[EPOCH #51, elapsed time: 638.313[sec]] loss: 4.399603244210388\n",
      "[EPOCH #52, elapsed time: 650.064[sec]] loss: 4.394660389583536\n",
      "[EPOCH #53, elapsed time: 663.244[sec]] loss: 4.39093862278524\n",
      "[EPOCH #54, elapsed time: 675.189[sec]] loss: 4.397119796054911\n",
      "[EPOCH #55, elapsed time: 688.071[sec]] loss: 4.392169978324222\n",
      "[EPOCH #56, elapsed time: 700.077[sec]] loss: 4.387832244694882\n",
      "[EPOCH #57, elapsed time: 711.803[sec]] loss: 4.3811995316528005\n",
      "[EPOCH #58, elapsed time: 723.377[sec]] loss: 4.374156574827696\n",
      "[EPOCH #59, elapsed time: 734.786[sec]] loss: 4.374653186846908\n",
      "[EPOCH #60, elapsed time: 747.181[sec]] loss: 4.378413004213163\n",
      "[EPOCH #61, elapsed time: 759.305[sec]] loss: 4.369898459885415\n",
      "[EPOCH #62, elapsed time: 771.582[sec]] loss: 4.368783899552533\n",
      "[EPOCH #63, elapsed time: 783.041[sec]] loss: 4.361812621145315\n",
      "[EPOCH #64, elapsed time: 795.529[sec]] loss: 4.359666904576375\n",
      "[EPOCH #65, elapsed time: 807.775[sec]] loss: 4.354775496194245\n",
      "[EPOCH #66, elapsed time: 820.357[sec]] loss: 4.3574502854033\n",
      "[EPOCH #67, elapsed time: 832.852[sec]] loss: 4.3470288833134925\n",
      "[EPOCH #68, elapsed time: 844.371[sec]] loss: 4.34424709892395\n",
      "[EPOCH #69, elapsed time: 855.931[sec]] loss: 4.343850325866914\n",
      "[EPOCH #70, elapsed time: 867.612[sec]] loss: 4.337555043604309\n",
      "[EPOCH #71, elapsed time: 883.972[sec]] loss: 4.34105713796097\n",
      "[EPOCH #72, elapsed time: 896.425[sec]] loss: 4.35050117946632\n",
      "[EPOCH #73, elapsed time: 907.785[sec]] loss: 4.3385633659606855\n",
      "[EPOCH #74, elapsed time: 919.246[sec]] loss: 4.333244005533952\n",
      "[EPOCH #75, elapsed time: 931.076[sec]] loss: 4.333594124819022\n",
      "[EPOCH #76, elapsed time: 942.510[sec]] loss: 4.327902082442017\n",
      "[EPOCH #77, elapsed time: 954.098[sec]] loss: 4.32850238808591\n",
      "[EPOCH #78, elapsed time: 966.639[sec]] loss: 4.320564656248477\n",
      "[EPOCH #79, elapsed time: 983.539[sec]] loss: 4.3189670138075344\n",
      "[EPOCH #80, elapsed time: 995.099[sec]] loss: 4.331050553996061\n",
      "[EPOCH #81, elapsed time: 1007.571[sec]] loss: 4.3184050387933475\n",
      "[EPOCH #82, elapsed time: 1020.283[sec]] loss: 4.313065666154799\n",
      "[EPOCH #83, elapsed time: 1032.125[sec]] loss: 4.306000168630121\n",
      "[EPOCH #84, elapsed time: 1043.903[sec]] loss: 4.307798421314262\n",
      "[EPOCH #85, elapsed time: 1056.026[sec]] loss: 4.317041536210366\n",
      "[EPOCH #86, elapsed time: 1067.801[sec]] loss: 4.304669222462581\n",
      "[EPOCH #87, elapsed time: 1079.621[sec]] loss: 4.3002708560750795\n",
      "[EPOCH #88, elapsed time: 1091.966[sec]] loss: 4.296732727106916\n",
      "[EPOCH #89, elapsed time: 1103.942[sec]] loss: 4.297112604020424\n",
      "[EPOCH #90, elapsed time: 1116.431[sec]] loss: 4.290034198638955\n",
      "[EPOCH #91, elapsed time: 1128.290[sec]] loss: 4.286291992519425\n",
      "[EPOCH #92, elapsed time: 1140.636[sec]] loss: 4.286693931464881\n",
      "[EPOCH #93, elapsed time: 1153.023[sec]] loss: 4.277810707202113\n",
      "[EPOCH #94, elapsed time: 1165.481[sec]] loss: 4.2789473349058085\n",
      "[EPOCH #95, elapsed time: 1177.171[sec]] loss: 4.273269176178077\n",
      "[EPOCH #96, elapsed time: 1189.927[sec]] loss: 4.2721523215049215\n",
      "[EPOCH #97, elapsed time: 1201.506[sec]] loss: 4.2666464243954145\n",
      "[EPOCH #98, elapsed time: 1215.926[sec]] loss: 4.269330649214224\n",
      "[EPOCH #99, elapsed time: 1228.417[sec]] loss: 4.260966758734129\n",
      "[EPOCH #100, elapsed time: 1240.950[sec]] loss: 4.26539109505222\n",
      "[EPOCH #101, elapsed time: 1252.374[sec]] loss: 4.25910501837044\n",
      "[EPOCH #102, elapsed time: 1264.808[sec]] loss: 4.258436003786894\n",
      "[EPOCH #103, elapsed time: 1277.349[sec]] loss: 4.255332892015815\n",
      "[EPOCH #104, elapsed time: 1288.907[sec]] loss: 4.251061927486671\n",
      "[EPOCH #105, elapsed time: 1300.369[sec]] loss: 4.2505885445758915\n",
      "[EPOCH #106, elapsed time: 1312.244[sec]] loss: 4.249724425037016\n",
      "[EPOCH #107, elapsed time: 1327.271[sec]] loss: 4.2403591238987115\n",
      "[EPOCH #108, elapsed time: 1340.184[sec]] loss: 4.251715839176093\n",
      "[EPOCH #109, elapsed time: 1354.311[sec]] loss: 4.240149211822529\n",
      "[EPOCH #110, elapsed time: 1370.062[sec]] loss: 4.237202712685652\n",
      "[EPOCH #111, elapsed time: 1382.213[sec]] loss: 4.25702678005587\n",
      "[EPOCH #112, elapsed time: 1396.249[sec]] loss: 4.2568209525185825\n",
      "[EPOCH #113, elapsed time: 1412.004[sec]] loss: 4.2634200880486315\n",
      "[EPOCH #114, elapsed time: 1424.515[sec]] loss: 4.252288697090808\n",
      "[EPOCH #115, elapsed time: 1436.326[sec]] loss: 4.250739486112247\n",
      "[EPOCH #116, elapsed time: 1448.470[sec]] loss: 4.247113832509144\n",
      "[EPOCH #117, elapsed time: 1461.571[sec]] loss: 4.251153420578267\n",
      "[EPOCH #118, elapsed time: 1473.073[sec]] loss: 4.240113595168101\n",
      "[EPOCH #119, elapsed time: 1484.532[sec]] loss: 4.255086694401347\n",
      "[EPOCH #120, elapsed time: 1497.128[sec]] loss: 4.24104741484556\n",
      "[EPOCH #121, elapsed time: 1508.647[sec]] loss: 4.244811470205023\n",
      "[EPOCH #122, elapsed time: 1520.474[sec]] loss: 4.2395032613001336\n",
      "[EPOCH #123, elapsed time: 1536.443[sec]] loss: 4.228873276664748\n",
      "[EPOCH #124, elapsed time: 1548.154[sec]] loss: 4.2433314497319845\n",
      "[EPOCH #125, elapsed time: 1559.695[sec]] loss: 4.251188464295917\n",
      "[EPOCH #126, elapsed time: 1571.268[sec]] loss: 4.240748131343819\n",
      "[EPOCH #127, elapsed time: 1583.829[sec]] loss: 4.232397817299295\n",
      "[EPOCH #128, elapsed time: 1595.654[sec]] loss: 4.232932261603045\n",
      "[EPOCH #129, elapsed time: 1607.249[sec]] loss: 4.237964846274827\n",
      "[EPOCH #130, elapsed time: 1619.638[sec]] loss: 4.2375665406576735\n",
      "[EPOCH #131, elapsed time: 1630.920[sec]] loss: 4.233928126245451\n",
      "[EPOCH #132, elapsed time: 1642.839[sec]] loss: 4.225259459178873\n",
      "[EPOCH #133, elapsed time: 1655.316[sec]] loss: 4.244884469763865\n",
      "[EPOCH #134, elapsed time: 1666.375[sec]] loss: 4.2384521862061755\n",
      "[EPOCH #135, elapsed time: 1677.438[sec]] loss: 4.234004395479433\n",
      "[EPOCH #136, elapsed time: 1688.531[sec]] loss: 4.239961717651963\n",
      "[EPOCH #137, elapsed time: 1700.892[sec]] loss: 4.252851207517159\n",
      "[EPOCH #138, elapsed time: 1712.522[sec]] loss: 4.242875596612063\n",
      "[EPOCH #139, elapsed time: 1724.951[sec]] loss: 4.224682550741478\n",
      "[EPOCH #140, elapsed time: 1736.215[sec]] loss: 4.222811354480336\n",
      "[EPOCH #141, elapsed time: 1747.475[sec]] loss: 4.218890603352874\n",
      "[EPOCH #142, elapsed time: 1758.937[sec]] loss: 4.217004353925347\n",
      "[EPOCH #143, elapsed time: 1771.477[sec]] loss: 4.215412003370103\n",
      "[EPOCH #144, elapsed time: 1783.434[sec]] loss: 4.214088011916753\n",
      "[EPOCH #145, elapsed time: 1794.983[sec]] loss: 4.211279874570996\n",
      "[EPOCH #146, elapsed time: 1807.478[sec]] loss: 4.2300065137298155\n",
      "[EPOCH #147, elapsed time: 1820.048[sec]] loss: 4.217829248650442\n",
      "[EPOCH #148, elapsed time: 1832.628[sec]] loss: 4.210361794028157\n",
      "[EPOCH #149, elapsed time: 1844.596[sec]] loss: 4.205836838663044\n",
      "[EPOCH #150, elapsed time: 1857.671[sec]] loss: 4.20460568050963\n",
      "[EPOCH #151, elapsed time: 1869.725[sec]] loss: 4.20486808174021\n",
      "[EPOCH #152, elapsed time: 1882.265[sec]] loss: 4.203421653575495\n",
      "[EPOCH #153, elapsed time: 1894.992[sec]] loss: 4.203249631291083\n",
      "[EPOCH #154, elapsed time: 1909.479[sec]] loss: 4.203198706882548\n",
      "[EPOCH #155, elapsed time: 1921.600[sec]] loss: 4.21017629217049\n",
      "[EPOCH #156, elapsed time: 1933.841[sec]] loss: 4.198805890171785\n",
      "[EPOCH #157, elapsed time: 1945.951[sec]] loss: 4.198176337600288\n",
      "[EPOCH #158, elapsed time: 1958.657[sec]] loss: 4.218873363538805\n",
      "[EPOCH #159, elapsed time: 1971.455[sec]] loss: 4.199744813959338\n",
      "[EPOCH #160, elapsed time: 1986.737[sec]] loss: 4.1942498529864976\n",
      "[EPOCH #161, elapsed time: 2001.497[sec]] loss: 4.192365261811289\n",
      "[EPOCH #162, elapsed time: 2013.670[sec]] loss: 4.194212577469594\n",
      "[EPOCH #163, elapsed time: 2025.847[sec]] loss: 4.197661385807713\n",
      "[EPOCH #164, elapsed time: 2037.423[sec]] loss: 4.188603350083491\n",
      "[EPOCH #165, elapsed time: 2049.401[sec]] loss: 4.193884549656512\n",
      "[EPOCH #166, elapsed time: 2060.990[sec]] loss: 4.188341215033601\n",
      "[EPOCH #167, elapsed time: 2073.207[sec]] loss: 4.188355620213983\n",
      "[EPOCH #168, elapsed time: 2088.728[sec]] loss: 4.196638404445929\n",
      "[EPOCH #169, elapsed time: 2101.209[sec]] loss: 4.194700341001964\n",
      "[EPOCH #170, elapsed time: 2113.531[sec]] loss: 4.183519359894921\n",
      "[EPOCH #171, elapsed time: 2124.991[sec]] loss: 4.180656697150613\n",
      "[EPOCH #172, elapsed time: 2136.893[sec]] loss: 4.187446557781442\n",
      "[EPOCH #173, elapsed time: 2149.617[sec]] loss: 4.181634571029067\n",
      "[EPOCH #174, elapsed time: 2165.615[sec]] loss: 4.182124689612263\n",
      "[EPOCH #175, elapsed time: 2177.146[sec]] loss: 4.18213766611164\n",
      "[EPOCH #176, elapsed time: 2188.629[sec]] loss: 4.183211962534537\n",
      "[EPOCH #177, elapsed time: 2200.214[sec]] loss: 4.18092096545951\n",
      "[EPOCH #178, elapsed time: 2212.187[sec]] loss: 4.179499294234634\n",
      "[EPOCH #179, elapsed time: 2224.166[sec]] loss: 4.187097557065431\n",
      "[EPOCH #180, elapsed time: 2235.954[sec]] loss: 4.192876382737456\n",
      "[EPOCH #181, elapsed time: 2248.606[sec]] loss: 4.175372863227713\n",
      "[EPOCH #182, elapsed time: 2260.593[sec]] loss: 4.179604911834707\n",
      "[EPOCH #183, elapsed time: 2275.228[sec]] loss: 4.176815116550399\n",
      "[EPOCH #184, elapsed time: 2289.305[sec]] loss: 4.178631715109466\n",
      "[EPOCH #185, elapsed time: 2304.889[sec]] loss: 4.173633545389254\n",
      "[EPOCH #186, elapsed time: 2321.030[sec]] loss: 4.17526363838352\n",
      "[EPOCH #187, elapsed time: 2332.545[sec]] loss: 4.176574312107577\n",
      "[EPOCH #188, elapsed time: 2347.546[sec]] loss: 4.1817035203703075\n",
      "[EPOCH #189, elapsed time: 2361.674[sec]] loss: 4.170972401258355\n",
      "[EPOCH #190, elapsed time: 2373.082[sec]] loss: 4.181350808378526\n",
      "[EPOCH #191, elapsed time: 2384.728[sec]] loss: 4.1749844389395\n",
      "[EPOCH #192, elapsed time: 2396.571[sec]] loss: 4.175170858014645\n",
      "[EPOCH #193, elapsed time: 2408.119[sec]] loss: 4.179551272688199\n",
      "[EPOCH #194, elapsed time: 2419.494[sec]] loss: 4.168631856287433\n",
      "[EPOCH #195, elapsed time: 2431.339[sec]] loss: 4.178338742759544\n",
      "[EPOCH #196, elapsed time: 2443.104[sec]] loss: 4.169567006105654\n",
      "[EPOCH #197, elapsed time: 2459.609[sec]] loss: 4.166992502798275\n",
      "[EPOCH #198, elapsed time: 2471.822[sec]] loss: 4.171725296470803\n",
      "[EPOCH #199, elapsed time: 2484.475[sec]] loss: 4.164432198743521\n",
      "[EPOCH #200, elapsed time: 2496.156[sec]] loss: 4.169422770339712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "[I 2024-05-11 14:12:25,649] Trial 19 finished with value: 0.5303 and parameters: {'learning_rate': 0.0017706426910962046}. Best is trial 14 with value: 0.77282.\n"
     ]
    }
   ],
   "source": [
    "def objective_lr(dataloader):\n",
    "    def objective(trial):\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.00001, 0.1, log=True)\n",
    "        \n",
    "        model = simple_cnn.SimpleCNN(device, input_size=input_size, num_classes=num_classes)\n",
    "        model_dir = Path(output_dir, f'{model_name}-{trial.number}')\n",
    "        \n",
    "        train_result = model.train(dataloader.dataset.trainloader, epochs=epochs, lr=learning_rate, output_dir=model_dir)\n",
    "        train_result = model.predict(dataloader.dataset.trainloader)\n",
    "        train_predictions, train_labels = train_result\n",
    "        train_eval_result = model.evaluate(train_labels, train_predictions)\n",
    "        trial.set_user_attr(\"train_accuracy\", train_eval_result['accuracy'])\n",
    "\n",
    "        test_result = model.predict(dataloader.dataset.testloader)\n",
    "        test_predictions, test_labels = test_result\n",
    "        test_eval_result = model.evaluate(test_labels, test_predictions)\n",
    "        trial.set_user_attr(\"test_accuracy\", test_eval_result['accuracy'])\n",
    "        \n",
    "        return train_eval_result['accuracy']\n",
    "\n",
    "    return objective\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_lr(dataloader), n_trials=n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ea18805-7cfc-4e1e-8ab1-96096dcdb16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[0.0101], datetime_start=datetime.datetime(2024, 5, 11, 0, 19, 16, 206822), datetime_complete=datetime.datetime(2024, 5, 11, 0, 58, 21, 131896), params={'learning_rate': 0.046406232322745745}, user_attrs={'train_accuracy': 0.0101, 'test_accuracy': 0.0101}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None)}, trial_id=0, value=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3710cb38-e83c-4d08-8665-0b87409eeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(output_dir, 'study.pkl'), 'wb') as f:\n",
    "    pickle.dump(study, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f3fb0-c611-45f9-b7ec-74c8c7a6e02b",
   "metadata": {},
   "source": [
    "## Optimization History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061f26c7-d874-438f-8b77-2333c35e1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path(output_dir, 'study.pkl'), 'rb') as f:\n",
    "    study = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6e8923-c8b1-4442-96a7-2a961e86bfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, state=TrialState.COMPLETE, values=[0.0101], datetime_start=datetime.datetime(2024, 5, 11, 0, 19, 16, 206822), datetime_complete=datetime.datetime(2024, 5, 11, 0, 58, 21, 131896), params={'learning_rate': 0.046406232322745745}, user_attrs={'train_accuracy': 0.0101, 'test_accuracy': 0.0101}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None)}, trial_id=0, value=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5407d2-f8c4-4347-a11a-65716e1bd4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0.0101,
          0.01002,
          0.01,
          0.21588,
          0.6144,
          0.76704,
          0.67872,
          0.44936,
          0.29028,
          0.27778,
          0.68916,
          0.70448,
          0.75428,
          0.77218,
          0.77282,
          0.67776,
          0.77024,
          0.7566,
          0.3879,
          0.5303
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0.0101,
          0.0101,
          0.0101,
          0.21588,
          0.6144,
          0.76704,
          0.76704,
          0.76704,
          0.76704,
          0.76704,
          0.76704,
          0.76704,
          0.76704,
          0.77218,
          0.77282,
          0.77282,
          0.77282,
          0.77282,
          0.77282,
          0.77282
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "autosize": true,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "autorange": true,
         "range": [
          -1.16006600660066,
          20.16006600660066
         ],
         "title": {
          "text": "Trial"
         },
         "type": "linear"
        },
        "yaxis": {
         "autorange": true,
         "range": [
          -0.052951165048543684,
          0.8357711650485437
         ],
         "title": {
          "text": "Objective Value"
         },
         "type": "linear"
        }
       }
      },
      "image/png": "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",
      "text/html": [
       "<div>                            <div id=\"ed28d877-ae33-4fa6-9cb4-ad240e452467\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ed28d877-ae33-4fa6-9cb4-ad240e452467\")) {                    Plotly.newPlot(                        \"ed28d877-ae33-4fa6-9cb4-ad240e452467\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[0.0101,0.01002,0.01,0.21588,0.6144,0.76704,0.67872,0.44936,0.29028,0.27778,0.68916,0.70448,0.75428,0.77218,0.77282,0.67776,0.77024,0.7566,0.3879,0.5303],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19],\"y\":[0.0101,0.0101,0.0101,0.21588,0.6144,0.76704,0.76704,0.76704,0.76704,0.76704,0.76704,0.76704,0.76704,0.77218,0.77282,0.77282,0.77282,0.77282,0.77282,0.77282],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('ed28d877-ae33-4fa6-9cb4-ad240e452467');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e916e0b",
   "metadata": {},
   "source": [
    "## Test Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a4097f-e8d1-49ca-bb18-4d98550748c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=14, state=TrialState.COMPLETE, values=[0.77282], datetime_start=datetime.datetime(2024, 5, 11, 9, 59, 45, 628431), datetime_complete=datetime.datetime(2024, 5, 11, 10, 39, 36, 13219), params={'learning_rate': 0.0003798483309943936}, user_attrs={'train_accuracy': 0.77282, 'test_accuracy': 0.5541}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None)}, trial_id=14, value=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "385e104a-9d48-4c24-a704-130a56534597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "Net                                      [128, 100]                --\n",
      "├─Conv2d: 1-1                            [128, 64, 32, 32]         1,792\n",
      "├─ReLU: 1-2                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-3                       [128, 64, 32, 32]         128\n",
      "├─Conv2d: 1-4                            [128, 64, 32, 32]         36,928\n",
      "├─ReLU: 1-5                              [128, 64, 32, 32]         --\n",
      "├─BatchNorm2d: 1-6                       [128, 64, 32, 32]         128\n",
      "├─MaxPool2d: 1-7                         [128, 64, 16, 16]         --\n",
      "├─Dropout: 1-8                           [128, 64, 16, 16]         --\n",
      "├─Conv2d: 1-9                            [128, 128, 16, 16]        73,856\n",
      "├─ReLU: 1-10                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-11                      [128, 128, 16, 16]        256\n",
      "├─Conv2d: 1-12                           [128, 128, 16, 16]        147,584\n",
      "├─ReLU: 1-13                             [128, 128, 16, 16]        --\n",
      "├─BatchNorm2d: 1-14                      [128, 128, 16, 16]        256\n",
      "├─MaxPool2d: 1-15                        [128, 128, 8, 8]          --\n",
      "├─Dropout: 1-16                          [128, 128, 8, 8]          --\n",
      "├─Conv2d: 1-17                           [128, 256, 8, 8]          295,168\n",
      "├─ReLU: 1-18                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-19                      [128, 256, 8, 8]          512\n",
      "├─Conv2d: 1-20                           [128, 256, 8, 8]          590,080\n",
      "├─ReLU: 1-21                             [128, 256, 8, 8]          --\n",
      "├─BatchNorm2d: 1-22                      [128, 256, 8, 8]          512\n",
      "├─MaxPool2d: 1-23                        [128, 256, 4, 4]          --\n",
      "├─AdaptiveAvgPool2d: 1-24                [128, 256, 2, 2]          --\n",
      "├─Linear: 1-25                           [128, 512]                524,800\n",
      "├─ReLU: 1-26                             [128, 512]                --\n",
      "├─BatchNorm1d: 1-27                      [128, 512]                1,024\n",
      "├─Dropout: 1-28                          [128, 512]                --\n",
      "├─Linear: 1-29                           [128, 128]                65,664\n",
      "├─ReLU: 1-30                             [128, 128]                --\n",
      "├─BatchNorm1d: 1-31                      [128, 128]                256\n",
      "├─Dropout: 1-32                          [128, 128]                --\n",
      "├─Linear: 1-33                           [128, 100]                12,900\n",
      "├─Softmax: 1-34                          [128, 100]                --\n",
      "==========================================================================================\n",
      "Total params: 1,751,844\n",
      "Trainable params: 1,751,844\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 19.66\n",
      "==========================================================================================\n",
      "Input size (MB): 1.57\n",
      "Forward/backward pass size (MB): 471.18\n",
      "Params size (MB): 7.01\n",
      "Estimated Total Size (MB): 479.76\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_dir = Path(output_dir, f'{model_name}-{study.best_trial.number}')\n",
    "pth_path = Path(model_dir, 'model.pth')\n",
    "best_model = simple_cnn.SimpleCNN(device, input_size=input_size, num_classes=num_classes, pth_path=pth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5377e5d0-105b-4b9b-94dd-3b0a5ef7fe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.77282,\n",
      " 'classification_report': {'0': {'f1-score': 0.9168241965973535,\n",
      "                                 'precision': 0.8691756272401434,\n",
      "                                 'recall': 0.97,\n",
      "                                 'support': 500},\n",
      "                           '1': {'f1-score': 0.887378640776699,\n",
      "                                 'precision': 0.8622641509433963,\n",
      "                                 'recall': 0.914,\n",
      "                                 'support': 500},\n",
      "                           '10': {'f1-score': 0.7916666666666667,\n",
      "                                  'precision': 0.8260869565217391,\n",
      "                                  'recall': 0.76,\n",
      "                                  'support': 500},\n",
      "                           '11': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 500},\n",
      "                           '12': {'f1-score': 0.8494845360824742,\n",
      "                                  'precision': 0.8765957446808511,\n",
      "                                  'recall': 0.824,\n",
      "                                  'support': 500},\n",
      "                           '13': {'f1-score': 0.898,\n",
      "                                  'precision': 0.898,\n",
      "                                  'recall': 0.898,\n",
      "                                  'support': 500},\n",
      "                           '14': {'f1-score': 0.8007850834151129,\n",
      "                                  'precision': 0.7861271676300579,\n",
      "                                  'recall': 0.816,\n",
      "                                  'support': 500},\n",
      "                           '15': {'f1-score': 0.8007662835249042,\n",
      "                                  'precision': 0.7683823529411765,\n",
      "                                  'recall': 0.836,\n",
      "                                  'support': 500},\n",
      "                           '16': {'f1-score': 0.83248730964467,\n",
      "                                  'precision': 0.845360824742268,\n",
      "                                  'recall': 0.82,\n",
      "                                  'support': 500},\n",
      "                           '17': {'f1-score': 0.925,\n",
      "                                  'precision': 0.8907407407407407,\n",
      "                                  'recall': 0.962,\n",
      "                                  'support': 500},\n",
      "                           '18': {'f1-score': 0.746588693957115,\n",
      "                                  'precision': 0.7281368821292775,\n",
      "                                  'recall': 0.766,\n",
      "                                  'support': 500},\n",
      "                           '19': {'f1-score': 0.7749077490774908,\n",
      "                                  'precision': 0.7191780821917808,\n",
      "                                  'recall': 0.84,\n",
      "                                  'support': 500},\n",
      "                           '2': {'f1-score': 0.6964467005076143,\n",
      "                                 'precision': 0.7072164948453609,\n",
      "                                 'recall': 0.686,\n",
      "                                 'support': 500},\n",
      "                           '20': {'f1-score': 0.9568627450980391,\n",
      "                                  'precision': 0.9384615384615385,\n",
      "                                  'recall': 0.976,\n",
      "                                  'support': 500},\n",
      "                           '21': {'f1-score': 0.8121827411167513,\n",
      "                                  'precision': 0.7038123167155426,\n",
      "                                  'recall': 0.96,\n",
      "                                  'support': 500},\n",
      "                           '22': {'f1-score': 0.7010804321728692,\n",
      "                                  'precision': 0.8768768768768769,\n",
      "                                  'recall': 0.584,\n",
      "                                  'support': 500},\n",
      "                           '23': {'f1-score': 0.8762278978388998,\n",
      "                                  'precision': 0.861003861003861,\n",
      "                                  'recall': 0.892,\n",
      "                                  'support': 500},\n",
      "                           '24': {'f1-score': 0.9013282732447817,\n",
      "                                  'precision': 0.8574007220216606,\n",
      "                                  'recall': 0.95,\n",
      "                                  'support': 500},\n",
      "                           '25': {'f1-score': 0.8359133126934984,\n",
      "                                  'precision': 0.8635394456289979,\n",
      "                                  'recall': 0.81,\n",
      "                                  'support': 500},\n",
      "                           '26': {'f1-score': 0.7474949899799599,\n",
      "                                  'precision': 0.748995983935743,\n",
      "                                  'recall': 0.746,\n",
      "                                  'support': 500},\n",
      "                           '27': {'f1-score': 0.7402933563416738,\n",
      "                                  'precision': 0.6509863429438544,\n",
      "                                  'recall': 0.858,\n",
      "                                  'support': 500},\n",
      "                           '28': {'f1-score': 0.8780487804878048,\n",
      "                                  'precision': 0.8571428571428571,\n",
      "                                  'recall': 0.9,\n",
      "                                  'support': 500},\n",
      "                           '29': {'f1-score': 0.7753846153846154,\n",
      "                                  'precision': 0.7957894736842105,\n",
      "                                  'recall': 0.756,\n",
      "                                  'support': 500},\n",
      "                           '3': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 500},\n",
      "                           '30': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 500},\n",
      "                           '31': {'f1-score': 0.8003549245785271,\n",
      "                                  'precision': 0.7192982456140351,\n",
      "                                  'recall': 0.902,\n",
      "                                  'support': 500},\n",
      "                           '32': {'f1-score': 0.6720554272517321,\n",
      "                                  'precision': 0.7950819672131147,\n",
      "                                  'recall': 0.582,\n",
      "                                  'support': 500},\n",
      "                           '33': {'f1-score': 0.7656826568265683,\n",
      "                                  'precision': 0.7106164383561644,\n",
      "                                  'recall': 0.83,\n",
      "                                  'support': 500},\n",
      "                           '34': {'f1-score': 0.7707762557077625,\n",
      "                                  'precision': 0.7092436974789916,\n",
      "                                  'recall': 0.844,\n",
      "                                  'support': 500},\n",
      "                           '35': {'f1-score': 0.7566182749786506,\n",
      "                                  'precision': 0.6602086438152012,\n",
      "                                  'recall': 0.886,\n",
      "                                  'support': 500},\n",
      "                           '36': {'f1-score': 0.870722433460076,\n",
      "                                  'precision': 0.8297101449275363,\n",
      "                                  'recall': 0.916,\n",
      "                                  'support': 500},\n",
      "                           '37': {'f1-score': 0.835538752362949,\n",
      "                                  'precision': 0.7921146953405018,\n",
      "                                  'recall': 0.884,\n",
      "                                  'support': 500},\n",
      "                           '38': {'f1-score': 0.6973365617433414,\n",
      "                                  'precision': 0.584573748308525,\n",
      "                                  'recall': 0.864,\n",
      "                                  'support': 500},\n",
      "                           '39': {'f1-score': 0.881578947368421,\n",
      "                                  'precision': 0.8315602836879432,\n",
      "                                  'recall': 0.938,\n",
      "                                  'support': 500},\n",
      "                           '4': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 500},\n",
      "                           '40': {'f1-score': 0.7976190476190476,\n",
      "                                  'precision': 0.7913385826771654,\n",
      "                                  'recall': 0.804,\n",
      "                                  'support': 500},\n",
      "                           '41': {'f1-score': 0.9082125603864735,\n",
      "                                  'precision': 0.8785046728971962,\n",
      "                                  'recall': 0.94,\n",
      "                                  'support': 500},\n",
      "                           '42': {'f1-score': 0.7849765258215962,\n",
      "                                  'precision': 0.7398230088495575,\n",
      "                                  'recall': 0.836,\n",
      "                                  'support': 500},\n",
      "                           '43': {'f1-score': 0.7986463620981387,\n",
      "                                  'precision': 0.6920821114369502,\n",
      "                                  'recall': 0.944,\n",
      "                                  'support': 500},\n",
      "                           '44': {'f1-score': 0.6487488415199258,\n",
      "                                  'precision': 0.6044905008635578,\n",
      "                                  'recall': 0.7,\n",
      "                                  'support': 500},\n",
      "                           '45': {'f1-score': 0.6920762286860581,\n",
      "                                  'precision': 0.6941649899396378,\n",
      "                                  'recall': 0.69,\n",
      "                                  'support': 500},\n",
      "                           '46': {'f1-score': 0.7238883143743536,\n",
      "                                  'precision': 0.7494646680942184,\n",
      "                                  'recall': 0.7,\n",
      "                                  'support': 500},\n",
      "                           '47': {'f1-score': 0.8560235063663075,\n",
      "                                  'precision': 0.8387715930902111,\n",
      "                                  'recall': 0.874,\n",
      "                                  'support': 500},\n",
      "                           '48': {'f1-score': 0.9193857965451057,\n",
      "                                  'precision': 0.8837638376383764,\n",
      "                                  'recall': 0.958,\n",
      "                                  'support': 500},\n",
      "                           '49': {'f1-score': 0.9235294117647058,\n",
      "                                  'precision': 0.9057692307692308,\n",
      "                                  'recall': 0.942,\n",
      "                                  'support': 500},\n",
      "                           '5': {'f1-score': 0.9063386944181645,\n",
      "                                 'precision': 0.8599640933572711,\n",
      "                                 'recall': 0.958,\n",
      "                                 'support': 500},\n",
      "                           '50': {'f1-score': 0.3258426966292135,\n",
      "                                  'precision': 0.5471698113207547,\n",
      "                                  'recall': 0.232,\n",
      "                                  'support': 500},\n",
      "                           '51': {'f1-score': 0.7766636280765724,\n",
      "                                  'precision': 0.7135678391959799,\n",
      "                                  'recall': 0.852,\n",
      "                                  'support': 500},\n",
      "                           '52': {'f1-score': 0.8667287977632805,\n",
      "                                  'precision': 0.8115183246073299,\n",
      "                                  'recall': 0.93,\n",
      "                                  'support': 500},\n",
      "                           '53': {'f1-score': 0.9065155807365439,\n",
      "                                  'precision': 0.8586762075134168,\n",
      "                                  'recall': 0.96,\n",
      "                                  'support': 500},\n",
      "                           '54': {'f1-score': 0.8318264014466545,\n",
      "                                  'precision': 0.759075907590759,\n",
      "                                  'recall': 0.92,\n",
      "                                  'support': 500},\n",
      "                           '55': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 500},\n",
      "                           '56': {'f1-score': 0.9323164918970447,\n",
      "                                  'precision': 0.8907103825136612,\n",
      "                                  'recall': 0.978,\n",
      "                                  'support': 500},\n",
      "                           '57': {'f1-score': 0.8655544651619234,\n",
      "                                  'precision': 0.8497109826589595,\n",
      "                                  'recall': 0.882,\n",
      "                                  'support': 500},\n",
      "                           '58': {'f1-score': 0.9260700389105058,\n",
      "                                  'precision': 0.9015151515151515,\n",
      "                                  'recall': 0.952,\n",
      "                                  'support': 500},\n",
      "                           '59': {'f1-score': 0.7630979498861048,\n",
      "                                  'precision': 0.8862433862433863,\n",
      "                                  'recall': 0.67,\n",
      "                                  'support': 500},\n",
      "                           '6': {'f1-score': 0.8205607476635514,\n",
      "                                 'precision': 0.7701754385964912,\n",
      "                                 'recall': 0.878,\n",
      "                                 'support': 500},\n",
      "                           '60': {'f1-score': 0.9293532338308458,\n",
      "                                  'precision': 0.9247524752475248,\n",
      "                                  'recall': 0.934,\n",
      "                                  'support': 500},\n",
      "                           '61': {'f1-score': 0.8553113553113553,\n",
      "                                  'precision': 0.7888513513513513,\n",
      "                                  'recall': 0.934,\n",
      "                                  'support': 500},\n",
      "                           '62': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 500},\n",
      "                           '63': {'f1-score': 0.7191558441558442,\n",
      "                                  'precision': 0.605191256830601,\n",
      "                                  'recall': 0.886,\n",
      "                                  'support': 500},\n",
      "                           '64': {'f1-score': 0.6133960047003526,\n",
      "                                  'precision': 0.7435897435897436,\n",
      "                                  'recall': 0.522,\n",
      "                                  'support': 500},\n",
      "                           '65': {'f1-score': 0.5423728813559322,\n",
      "                                  'precision': 0.6871165644171779,\n",
      "                                  'recall': 0.448,\n",
      "                                  'support': 500},\n",
      "                           '66': {'f1-score': 0.6708296164139161,\n",
      "                                  'precision': 0.605475040257649,\n",
      "                                  'recall': 0.752,\n",
      "                                  'support': 500},\n",
      "                           '67': {'f1-score': 0.7453531598513012,\n",
      "                                  'precision': 0.6961805555555556,\n",
      "                                  'recall': 0.802,\n",
      "                                  'support': 500},\n",
      "                           '68': {'f1-score': 0.9496124031007751,\n",
      "                                  'precision': 0.9210526315789473,\n",
      "                                  'recall': 0.98,\n",
      "                                  'support': 500},\n",
      "                           '69': {'f1-score': 0.9155908639523337,\n",
      "                                  'precision': 0.9092702169625246,\n",
      "                                  'recall': 0.922,\n",
      "                                  'support': 500},\n",
      "                           '7': {'f1-score': 0.7701260911736179,\n",
      "                                 'precision': 0.7476459510357816,\n",
      "                                 'recall': 0.794,\n",
      "                                 'support': 500},\n",
      "                           '70': {'f1-score': 0.6545454545454544,\n",
      "                                  'precision': 0.5032258064516129,\n",
      "                                  'recall': 0.936,\n",
      "                                  'support': 500},\n",
      "                           '71': {'f1-score': 0.885308056872038,\n",
      "                                  'precision': 0.8414414414414414,\n",
      "                                  'recall': 0.934,\n",
      "                                  'support': 500},\n",
      "                           '72': {'f1-score': 0.5992578849721707,\n",
      "                                  'precision': 0.5588235294117647,\n",
      "                                  'recall': 0.646,\n",
      "                                  'support': 500},\n",
      "                           '73': {'f1-score': 0.7236363636363636,\n",
      "                                  'precision': 0.6633333333333333,\n",
      "                                  'recall': 0.796,\n",
      "                                  'support': 500},\n",
      "                           '74': {'f1-score': 0.5482625482625482,\n",
      "                                  'precision': 0.44654088050314467,\n",
      "                                  'recall': 0.71,\n",
      "                                  'support': 500},\n",
      "                           '75': {'f1-score': 0.8983855650522318,\n",
      "                                  'precision': 0.8553345388788427,\n",
      "                                  'recall': 0.946,\n",
      "                                  'support': 500},\n",
      "                           '76': {'f1-score': 0.9339901477832512,\n",
      "                                  'precision': 0.920388349514563,\n",
      "                                  'recall': 0.948,\n",
      "                                  'support': 500},\n",
      "                           '77': {'f1-score': 0.7823585810162991,\n",
      "                                  'precision': 0.7513812154696132,\n",
      "                                  'recall': 0.816,\n",
      "                                  'support': 500},\n",
      "                           '78': {'f1-score': 0.5723905723905724,\n",
      "                                  'precision': 0.6521739130434783,\n",
      "                                  'recall': 0.51,\n",
      "                                  'support': 500},\n",
      "                           '79': {'f1-score': 0.7775735294117647,\n",
      "                                  'precision': 0.7193877551020408,\n",
      "                                  'recall': 0.846,\n",
      "                                  'support': 500},\n",
      "                           '8': {'f1-score': 0.9101229895931882,\n",
      "                                 'precision': 0.8635547576301615,\n",
      "                                 'recall': 0.962,\n",
      "                                 'support': 500},\n",
      "                           '80': {'f1-score': 0.007920792079207921,\n",
      "                                  'precision': 0.4,\n",
      "                                  'recall': 0.004,\n",
      "                                  'support': 500},\n",
      "                           '81': {'f1-score': 0.925343811394892,\n",
      "                                  'precision': 0.9092664092664092,\n",
      "                                  'recall': 0.942,\n",
      "                                  'support': 500},\n",
      "                           '82': {'f1-score': 0.8628158844765342,\n",
      "                                  'precision': 0.7861842105263158,\n",
      "                                  'recall': 0.956,\n",
      "                                  'support': 500},\n",
      "                           '83': {'f1-score': 0.8260073260073261,\n",
      "                                  'precision': 0.7618243243243243,\n",
      "                                  'recall': 0.902,\n",
      "                                  'support': 500},\n",
      "                           '84': {'f1-score': 0.6901874310915105,\n",
      "                                  'precision': 0.769041769041769,\n",
      "                                  'recall': 0.626,\n",
      "                                  'support': 500},\n",
      "                           '85': {'f1-score': 0.9114173228346457,\n",
      "                                  'precision': 0.8972868217054264,\n",
      "                                  'recall': 0.926,\n",
      "                                  'support': 500},\n",
      "                           '86': {'f1-score': 0.8754716981132076,\n",
      "                                  'precision': 0.8285714285714286,\n",
      "                                  'recall': 0.928,\n",
      "                                  'support': 500},\n",
      "                           '87': {'f1-score': 0.8935762224352829,\n",
      "                                  'precision': 0.858195211786372,\n",
      "                                  'recall': 0.932,\n",
      "                                  'support': 500},\n",
      "                           '88': {'f1-score': 0.8848484848484848,\n",
      "                                  'precision': 0.8938775510204081,\n",
      "                                  'recall': 0.876,\n",
      "                                  'support': 500},\n",
      "                           '89': {'f1-score': 0.8563586459286369,\n",
      "                                  'precision': 0.7892074198988196,\n",
      "                                  'recall': 0.936,\n",
      "                                  'support': 500},\n",
      "                           '9': {'f1-score': 0.8726554787759132,\n",
      "                                 'precision': 0.8615984405458089,\n",
      "                                 'recall': 0.884,\n",
      "                                 'support': 500},\n",
      "                           '90': {'f1-score': 0.8836291913214991,\n",
      "                                  'precision': 0.8715953307392996,\n",
      "                                  'recall': 0.896,\n",
      "                                  'support': 500},\n",
      "                           '91': {'f1-score': 0.8382642998027614,\n",
      "                                  'precision': 0.8268482490272373,\n",
      "                                  'recall': 0.85,\n",
      "                                  'support': 500},\n",
      "                           '92': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 500},\n",
      "                           '93': {'f1-score': 0.6174496644295302,\n",
      "                                  'precision': 0.700507614213198,\n",
      "                                  'recall': 0.552,\n",
      "                                  'support': 500},\n",
      "                           '94': {'f1-score': 0.920754716981132,\n",
      "                                  'precision': 0.8714285714285714,\n",
      "                                  'recall': 0.976,\n",
      "                                  'support': 500},\n",
      "                           '95': {'f1-score': 0.71284046692607,\n",
      "                                  'precision': 0.5834394904458599,\n",
      "                                  'recall': 0.916,\n",
      "                                  'support': 500},\n",
      "                           '96': {'f1-score': 0.8003663003663004,\n",
      "                                  'precision': 0.7381756756756757,\n",
      "                                  'recall': 0.874,\n",
      "                                  'support': 500},\n",
      "                           '97': {'f1-score': 0.7964912280701754,\n",
      "                                  'precision': 0.709375,\n",
      "                                  'recall': 0.908,\n",
      "                                  'support': 500},\n",
      "                           '98': {'f1-score': 0.7897727272727272,\n",
      "                                  'precision': 0.75,\n",
      "                                  'recall': 0.834,\n",
      "                                  'support': 500},\n",
      "                           '99': {'f1-score': 0.7954545454545454,\n",
      "                                  'precision': 0.8226495726495726,\n",
      "                                  'recall': 0.77,\n",
      "                                  'support': 500},\n",
      "                           'accuracy': 0.77282,\n",
      "                           'macro avg': {'f1-score': 0.7392899871265085,\n",
      "                                         'precision': 0.7222396742760654,\n",
      "                                         'recall': 0.77282,\n",
      "                                         'support': 50000},\n",
      "                           'weighted avg': {'f1-score': 0.7392899871265082,\n",
      "                                            'precision': 0.7222396742760656,\n",
      "                                            'recall': 0.77282,\n",
      "                                            'support': 50000}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "train_predictions, train_labels = best_model.predict(dataloader.dataset.trainloader)\n",
    "train_eval_result = best_model.evaluate(train_labels, train_predictions)\n",
    "pprint.pprint(train_eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1510ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5541,\n",
      " 'classification_report': {'0': {'f1-score': 0.7788461538461539,\n",
      "                                 'precision': 0.75,\n",
      "                                 'recall': 0.81,\n",
      "                                 'support': 100},\n",
      "                           '1': {'f1-score': 0.6918918918918918,\n",
      "                                 'precision': 0.7529411764705882,\n",
      "                                 'recall': 0.64,\n",
      "                                 'support': 100},\n",
      "                           '10': {'f1-score': 0.4692737430167598,\n",
      "                                  'precision': 0.5316455696202531,\n",
      "                                  'recall': 0.42,\n",
      "                                  'support': 100},\n",
      "                           '11': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 100},\n",
      "                           '12': {'f1-score': 0.6938775510204083,\n",
      "                                  'precision': 0.7083333333333334,\n",
      "                                  'recall': 0.68,\n",
      "                                  'support': 100},\n",
      "                           '13': {'f1-score': 0.5608465608465608,\n",
      "                                  'precision': 0.5955056179775281,\n",
      "                                  'recall': 0.53,\n",
      "                                  'support': 100},\n",
      "                           '14': {'f1-score': 0.47179487179487184,\n",
      "                                  'precision': 0.4842105263157895,\n",
      "                                  'recall': 0.46,\n",
      "                                  'support': 100},\n",
      "                           '15': {'f1-score': 0.5205479452054794,\n",
      "                                  'precision': 0.4789915966386555,\n",
      "                                  'recall': 0.57,\n",
      "                                  'support': 100},\n",
      "                           '16': {'f1-score': 0.6540284360189573,\n",
      "                                  'precision': 0.6216216216216216,\n",
      "                                  'recall': 0.69,\n",
      "                                  'support': 100},\n",
      "                           '17': {'f1-score': 0.7906976744186046,\n",
      "                                  'precision': 0.7391304347826086,\n",
      "                                  'recall': 0.85,\n",
      "                                  'support': 100},\n",
      "                           '18': {'f1-score': 0.4549763033175356,\n",
      "                                  'precision': 0.43243243243243246,\n",
      "                                  'recall': 0.48,\n",
      "                                  'support': 100},\n",
      "                           '19': {'f1-score': 0.5070422535211268,\n",
      "                                  'precision': 0.4778761061946903,\n",
      "                                  'recall': 0.54,\n",
      "                                  'support': 100},\n",
      "                           '2': {'f1-score': 0.4083769633507854,\n",
      "                                 'precision': 0.42857142857142855,\n",
      "                                 'recall': 0.39,\n",
      "                                 'support': 100},\n",
      "                           '20': {'f1-score': 0.7922705314009661,\n",
      "                                  'precision': 0.7663551401869159,\n",
      "                                  'recall': 0.82,\n",
      "                                  'support': 100},\n",
      "                           '21': {'f1-score': 0.7038626609442059,\n",
      "                                  'precision': 0.6165413533834586,\n",
      "                                  'recall': 0.82,\n",
      "                                  'support': 100},\n",
      "                           '22': {'f1-score': 0.5609756097560976,\n",
      "                                  'precision': 0.71875,\n",
      "                                  'recall': 0.46,\n",
      "                                  'support': 100},\n",
      "                           '23': {'f1-score': 0.7783251231527093,\n",
      "                                  'precision': 0.7669902912621359,\n",
      "                                  'recall': 0.79,\n",
      "                                  'support': 100},\n",
      "                           '24': {'f1-score': 0.7,\n",
      "                                  'precision': 0.7,\n",
      "                                  'recall': 0.7,\n",
      "                                  'support': 100},\n",
      "                           '25': {'f1-score': 0.49162011173184356,\n",
      "                                  'precision': 0.5569620253164557,\n",
      "                                  'recall': 0.44,\n",
      "                                  'support': 100},\n",
      "                           '26': {'f1-score': 0.55,\n",
      "                                  'precision': 0.55,\n",
      "                                  'recall': 0.55,\n",
      "                                  'support': 100},\n",
      "                           '27': {'f1-score': 0.3333333333333333,\n",
      "                                  'precision': 0.30327868852459017,\n",
      "                                  'recall': 0.37,\n",
      "                                  'support': 100},\n",
      "                           '28': {'f1-score': 0.7333333333333333,\n",
      "                                  'precision': 0.7,\n",
      "                                  'recall': 0.77,\n",
      "                                  'support': 100},\n",
      "                           '29': {'f1-score': 0.5380710659898477,\n",
      "                                  'precision': 0.5463917525773195,\n",
      "                                  'recall': 0.53,\n",
      "                                  'support': 100},\n",
      "                           '3': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 100},\n",
      "                           '30': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 100},\n",
      "                           '31': {'f1-score': 0.5277777777777778,\n",
      "                                  'precision': 0.49137931034482757,\n",
      "                                  'recall': 0.57,\n",
      "                                  'support': 100},\n",
      "                           '32': {'f1-score': 0.5207100591715976,\n",
      "                                  'precision': 0.6376811594202898,\n",
      "                                  'recall': 0.44,\n",
      "                                  'support': 100},\n",
      "                           '33': {'f1-score': 0.502127659574468,\n",
      "                                  'precision': 0.43703703703703706,\n",
      "                                  'recall': 0.59,\n",
      "                                  'support': 100},\n",
      "                           '34': {'f1-score': 0.5701357466063348,\n",
      "                                  'precision': 0.5206611570247934,\n",
      "                                  'recall': 0.63,\n",
      "                                  'support': 100},\n",
      "                           '35': {'f1-score': 0.3911111111111111,\n",
      "                                  'precision': 0.352,\n",
      "                                  'recall': 0.44,\n",
      "                                  'support': 100},\n",
      "                           '36': {'f1-score': 0.5959595959595959,\n",
      "                                  'precision': 0.6020408163265306,\n",
      "                                  'recall': 0.59,\n",
      "                                  'support': 100},\n",
      "                           '37': {'f1-score': 0.5263157894736842,\n",
      "                                  'precision': 0.46875,\n",
      "                                  'recall': 0.6,\n",
      "                                  'support': 100},\n",
      "                           '38': {'f1-score': 0.4089219330855019,\n",
      "                                  'precision': 0.3254437869822485,\n",
      "                                  'recall': 0.55,\n",
      "                                  'support': 100},\n",
      "                           '39': {'f1-score': 0.7123287671232876,\n",
      "                                  'precision': 0.6554621848739496,\n",
      "                                  'recall': 0.78,\n",
      "                                  'support': 100},\n",
      "                           '4': {'f1-score': 0.0,\n",
      "                                 'precision': 0.0,\n",
      "                                 'recall': 0.0,\n",
      "                                 'support': 100},\n",
      "                           '40': {'f1-score': 0.4999999999999999,\n",
      "                                  'precision': 0.5217391304347826,\n",
      "                                  'recall': 0.48,\n",
      "                                  'support': 100},\n",
      "                           '41': {'f1-score': 0.7342995169082127,\n",
      "                                  'precision': 0.7102803738317757,\n",
      "                                  'recall': 0.76,\n",
      "                                  'support': 100},\n",
      "                           '42': {'f1-score': 0.6079295154185023,\n",
      "                                  'precision': 0.5433070866141733,\n",
      "                                  'recall': 0.69,\n",
      "                                  'support': 100},\n",
      "                           '43': {'f1-score': 0.59375,\n",
      "                                  'precision': 0.48717948717948717,\n",
      "                                  'recall': 0.76,\n",
      "                                  'support': 100},\n",
      "                           '44': {'f1-score': 0.2928870292887029,\n",
      "                                  'precision': 0.2517985611510791,\n",
      "                                  'recall': 0.35,\n",
      "                                  'support': 100},\n",
      "                           '45': {'f1-score': 0.4444444444444445,\n",
      "                                  'precision': 0.42990654205607476,\n",
      "                                  'recall': 0.46,\n",
      "                                  'support': 100},\n",
      "                           '46': {'f1-score': 0.3204419889502762,\n",
      "                                  'precision': 0.35802469135802467,\n",
      "                                  'recall': 0.29,\n",
      "                                  'support': 100},\n",
      "                           '47': {'f1-score': 0.5877192982456141,\n",
      "                                  'precision': 0.5234375,\n",
      "                                  'recall': 0.67,\n",
      "                                  'support': 100},\n",
      "                           '48': {'f1-score': 0.8125,\n",
      "                                  'precision': 0.7338709677419355,\n",
      "                                  'recall': 0.91,\n",
      "                                  'support': 100},\n",
      "                           '49': {'f1-score': 0.7596153846153846,\n",
      "                                  'precision': 0.7314814814814815,\n",
      "                                  'recall': 0.79,\n",
      "                                  'support': 100},\n",
      "                           '5': {'f1-score': 0.5365853658536585,\n",
      "                                 'precision': 0.5238095238095238,\n",
      "                                 'recall': 0.55,\n",
      "                                 'support': 100},\n",
      "                           '50': {'f1-score': 0.15172413793103448,\n",
      "                                  'precision': 0.24444444444444444,\n",
      "                                  'recall': 0.11,\n",
      "                                  'support': 100},\n",
      "                           '51': {'f1-score': 0.5638766519823788,\n",
      "                                  'precision': 0.5039370078740157,\n",
      "                                  'recall': 0.64,\n",
      "                                  'support': 100},\n",
      "                           '52': {'f1-score': 0.6106194690265487,\n",
      "                                  'precision': 0.5476190476190477,\n",
      "                                  'recall': 0.69,\n",
      "                                  'support': 100},\n",
      "                           '53': {'f1-score': 0.8054298642533936,\n",
      "                                  'precision': 0.7355371900826446,\n",
      "                                  'recall': 0.89,\n",
      "                                  'support': 100},\n",
      "                           '54': {'f1-score': 0.6278026905829596,\n",
      "                                  'precision': 0.5691056910569106,\n",
      "                                  'recall': 0.7,\n",
      "                                  'support': 100},\n",
      "                           '55': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 100},\n",
      "                           '56': {'f1-score': 0.7939698492462312,\n",
      "                                  'precision': 0.797979797979798,\n",
      "                                  'recall': 0.79,\n",
      "                                  'support': 100},\n",
      "                           '57': {'f1-score': 0.6666666666666666,\n",
      "                                  'precision': 0.6538461538461539,\n",
      "                                  'recall': 0.68,\n",
      "                                  'support': 100},\n",
      "                           '58': {'f1-score': 0.75,\n",
      "                                  'precision': 0.75,\n",
      "                                  'recall': 0.75,\n",
      "                                  'support': 100},\n",
      "                           '59': {'f1-score': 0.46341463414634143,\n",
      "                                  'precision': 0.59375,\n",
      "                                  'recall': 0.38,\n",
      "                                  'support': 100},\n",
      "                           '6': {'f1-score': 0.5791855203619909,\n",
      "                                 'precision': 0.5289256198347108,\n",
      "                                 'recall': 0.64,\n",
      "                                 'support': 100},\n",
      "                           '60': {'f1-score': 0.8186046511627908,\n",
      "                                  'precision': 0.7652173913043478,\n",
      "                                  'recall': 0.88,\n",
      "                                  'support': 100},\n",
      "                           '61': {'f1-score': 0.6226415094339622,\n",
      "                                  'precision': 0.5892857142857143,\n",
      "                                  'recall': 0.66,\n",
      "                                  'support': 100},\n",
      "                           '62': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 100},\n",
      "                           '63': {'f1-score': 0.4505928853754941,\n",
      "                                  'precision': 0.37254901960784315,\n",
      "                                  'recall': 0.57,\n",
      "                                  'support': 100},\n",
      "                           '64': {'f1-score': 0.32098765432098764,\n",
      "                                  'precision': 0.41935483870967744,\n",
      "                                  'recall': 0.26,\n",
      "                                  'support': 100},\n",
      "                           '65': {'f1-score': 0.3253012048192771,\n",
      "                                  'precision': 0.4090909090909091,\n",
      "                                  'recall': 0.27,\n",
      "                                  'support': 100},\n",
      "                           '66': {'f1-score': 0.4776119402985074,\n",
      "                                  'precision': 0.38095238095238093,\n",
      "                                  'recall': 0.64,\n",
      "                                  'support': 100},\n",
      "                           '67': {'f1-score': 0.4757281553398058,\n",
      "                                  'precision': 0.46226415094339623,\n",
      "                                  'recall': 0.49,\n",
      "                                  'support': 100},\n",
      "                           '68': {'f1-score': 0.8262910798122066,\n",
      "                                  'precision': 0.7787610619469026,\n",
      "                                  'recall': 0.88,\n",
      "                                  'support': 100},\n",
      "                           '69': {'f1-score': 0.7562189054726369,\n",
      "                                  'precision': 0.7524752475247525,\n",
      "                                  'recall': 0.76,\n",
      "                                  'support': 100},\n",
      "                           '7': {'f1-score': 0.6176470588235293,\n",
      "                                 'precision': 0.6057692307692307,\n",
      "                                 'recall': 0.63,\n",
      "                                 'support': 100},\n",
      "                           '70': {'f1-score': 0.5,\n",
      "                                  'precision': 0.3858695652173913,\n",
      "                                  'recall': 0.71,\n",
      "                                  'support': 100},\n",
      "                           '71': {'f1-score': 0.7403846153846153,\n",
      "                                  'precision': 0.7129629629629629,\n",
      "                                  'recall': 0.77,\n",
      "                                  'support': 100},\n",
      "                           '72': {'f1-score': 0.22105263157894736,\n",
      "                                  'precision': 0.23333333333333334,\n",
      "                                  'recall': 0.21,\n",
      "                                  'support': 100},\n",
      "                           '73': {'f1-score': 0.3904761904761905,\n",
      "                                  'precision': 0.37272727272727274,\n",
      "                                  'recall': 0.41,\n",
      "                                  'support': 100},\n",
      "                           '74': {'f1-score': 0.31095406360424027,\n",
      "                                  'precision': 0.24043715846994534,\n",
      "                                  'recall': 0.44,\n",
      "                                  'support': 100},\n",
      "                           '75': {'f1-score': 0.7474747474747474,\n",
      "                                  'precision': 0.7551020408163265,\n",
      "                                  'recall': 0.74,\n",
      "                                  'support': 100},\n",
      "                           '76': {'f1-score': 0.7741935483870969,\n",
      "                                  'precision': 0.8372093023255814,\n",
      "                                  'recall': 0.72,\n",
      "                                  'support': 100},\n",
      "                           '77': {'f1-score': 0.4269662921348315,\n",
      "                                  'precision': 0.48717948717948717,\n",
      "                                  'recall': 0.38,\n",
      "                                  'support': 100},\n",
      "                           '78': {'f1-score': 0.3523316062176166,\n",
      "                                  'precision': 0.3655913978494624,\n",
      "                                  'recall': 0.34,\n",
      "                                  'support': 100},\n",
      "                           '79': {'f1-score': 0.5803571428571428,\n",
      "                                  'precision': 0.5241935483870968,\n",
      "                                  'recall': 0.65,\n",
      "                                  'support': 100},\n",
      "                           '8': {'f1-score': 0.7093596059113302,\n",
      "                                 'precision': 0.6990291262135923,\n",
      "                                 'recall': 0.72,\n",
      "                                 'support': 100},\n",
      "                           '80': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 100},\n",
      "                           '81': {'f1-score': 0.6439024390243901,\n",
      "                                  'precision': 0.6285714285714286,\n",
      "                                  'recall': 0.66,\n",
      "                                  'support': 100},\n",
      "                           '82': {'f1-score': 0.7857142857142856,\n",
      "                                  'precision': 0.7096774193548387,\n",
      "                                  'recall': 0.88,\n",
      "                                  'support': 100},\n",
      "                           '83': {'f1-score': 0.4923076923076923,\n",
      "                                  'precision': 0.5052631578947369,\n",
      "                                  'recall': 0.48,\n",
      "                                  'support': 100},\n",
      "                           '84': {'f1-score': 0.4830917874396135,\n",
      "                                  'precision': 0.4672897196261682,\n",
      "                                  'recall': 0.5,\n",
      "                                  'support': 100},\n",
      "                           '85': {'f1-score': 0.711340206185567,\n",
      "                                  'precision': 0.7340425531914894,\n",
      "                                  'recall': 0.69,\n",
      "                                  'support': 100},\n",
      "                           '86': {'f1-score': 0.6326530612244898,\n",
      "                                  'precision': 0.6458333333333334,\n",
      "                                  'recall': 0.62,\n",
      "                                  'support': 100},\n",
      "                           '87': {'f1-score': 0.6893203883495145,\n",
      "                                  'precision': 0.6698113207547169,\n",
      "                                  'recall': 0.71,\n",
      "                                  'support': 100},\n",
      "                           '88': {'f1-score': 0.6666666666666665,\n",
      "                                  'precision': 0.75,\n",
      "                                  'recall': 0.6,\n",
      "                                  'support': 100},\n",
      "                           '89': {'f1-score': 0.611111111111111,\n",
      "                                  'precision': 0.506578947368421,\n",
      "                                  'recall': 0.77,\n",
      "                                  'support': 100},\n",
      "                           '9': {'f1-score': 0.7378640776699028,\n",
      "                                 'precision': 0.7169811320754716,\n",
      "                                 'recall': 0.76,\n",
      "                                 'support': 100},\n",
      "                           '90': {'f1-score': 0.5978260869565217,\n",
      "                                  'precision': 0.6547619047619048,\n",
      "                                  'recall': 0.55,\n",
      "                                  'support': 100},\n",
      "                           '91': {'f1-score': 0.6421052631578946,\n",
      "                                  'precision': 0.6777777777777778,\n",
      "                                  'recall': 0.61,\n",
      "                                  'support': 100},\n",
      "                           '92': {'f1-score': 0.0,\n",
      "                                  'precision': 0.0,\n",
      "                                  'recall': 0.0,\n",
      "                                  'support': 100},\n",
      "                           '93': {'f1-score': 0.35955056179775285,\n",
      "                                  'precision': 0.41025641025641024,\n",
      "                                  'recall': 0.32,\n",
      "                                  'support': 100},\n",
      "                           '94': {'f1-score': 0.7965367965367965,\n",
      "                                  'precision': 0.7022900763358778,\n",
      "                                  'recall': 0.92,\n",
      "                                  'support': 100},\n",
      "                           '95': {'f1-score': 0.5399239543726235,\n",
      "                                  'precision': 0.43558282208588955,\n",
      "                                  'recall': 0.71,\n",
      "                                  'support': 100},\n",
      "                           '96': {'f1-score': 0.45581395348837206,\n",
      "                                  'precision': 0.4260869565217391,\n",
      "                                  'recall': 0.49,\n",
      "                                  'support': 100},\n",
      "                           '97': {'f1-score': 0.5579399141630902,\n",
      "                                  'precision': 0.48872180451127817,\n",
      "                                  'recall': 0.65,\n",
      "                                  'support': 100},\n",
      "                           '98': {'f1-score': 0.3626943005181347,\n",
      "                                  'precision': 0.3763440860215054,\n",
      "                                  'recall': 0.35,\n",
      "                                  'support': 100},\n",
      "                           '99': {'f1-score': 0.5955056179775281,\n",
      "                                  'precision': 0.6794871794871795,\n",
      "                                  'recall': 0.53,\n",
      "                                  'support': 100},\n",
      "                           'accuracy': 0.5541,\n",
      "                           'macro avg': {'f1-score': 0.5294928623924507,\n",
      "                                         'precision': 0.5179167801616331,\n",
      "                                         'recall': 0.5541,\n",
      "                                         'support': 10000},\n",
      "                           'weighted avg': {'f1-score': 0.5294928623924506,\n",
      "                                            'precision': 0.5179167801616332,\n",
      "                                            'recall': 0.5541,\n",
      "                                            'support': 10000}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_predictions, test_labels = best_model.predict(dataloader.dataset.testloader)\n",
    "test_eval_result = best_model.evaluate(test_labels, test_predictions)\n",
    "pprint.pprint(test_eval_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
